DOI,Title,Abstract
10.1007/s43681-024-00521-7,Ethics and the use of generative AI in professional editing,"<jats:title>Abstract</jats:title><jats:p>Generative artificial intelligence (GnAI) has garnered significant attention worldwide across diverse industries, including in book publishing. To date, more attention has been paid to its potential in creative collaboration and less to the editorial possibilities of its application. Interest has accelerated since the breakthrough of a new Large Language Model in late 2022. This paper engages with the ethical and industrial implications of using GnAI in a creative context, namely literary publishing. It raises crucial questions about intellectual property, trust, the author–editor relationship and publishing professionals’ evolving roles in shaping quality literature. Using a published story as a test case, we compare edits using GnAI with those by professional editors over multiple drafts and at different stages of editorial development. We consider the potential ethical implications of the use of GnAI in literary fiction editing, highlighting the principles and practices that underpin professional editing to consider how these may or may not translate in the use of GnAI. This is followed by a discussion of the risks and opportunities in using GnAI in editing literary texts in the trade publishing context.</jats:p>"
10.1007/s43681-023-00315-3,Does the sun rise for ChatGPT? Scientific discovery in the age of generative AI,N/A
10.2139/ssrn.4784555,The Ethics of Generative Ai in Social Science Research: A Qualitative Approach for Community-Based Ai Research Ethics,N/A
10.1007/s43681-024-00519-1,A powerful potion for a potent problem: transformative justice for generative AI in healthcare,"<jats:title>Abstract</jats:title><jats:p>Generative Artificial Intelligence (AI), as a transformative technology, holds significant promise for applications in healthcare. At the same time, the datafication, AI integration, and commodification of health have opened the floodgates for ethical issues, including those related to fairness, access, beneficence, democracy, solidarity, inclusion, and societal harms. As further the digitalization, innovation, and disruption of healthcare is inevitable, the paper maps out how power, equity, access, identity, participation, and knowledge contribute to creating social injustice issues. It also discusses that current justice approaches—distributive justice, representational justice, restorative justice, and capabilities-centered justice—do not have enough impact to prevent or remedy the many harms and injustices that AI has already created in healthcare or will continue to do so. The paper proposes that a transformative justice approach is needed for generative AI as a transformative technology, focused on (1) peace, emancipation, and eliminating the root causes of injustice, (2) holistic conflict resolution, (3) human rights-based approaches, and (4) the empowerment of agency and actors.</jats:p>"
10.1201/9781032654829-2,"Generative Artificial Intelligence: Introduction, Application, Trends, and Ethics",N/A
10.4018/979-8-3693-8557-9.ch009,Generative AI for Cybersecurity,"<jats:p>The intersection of cybersecurity and generative artificial intelligence (AI) has become a crucial frontier as the digital landscape changes. By examining the interaction between AI-powered attacks and defence mechanisms and concentrating on applications like anomaly detection, synthetic data generation, automated incident response, and forensics, the chapter examines the potential of generative artificial intelligence (AI) in redefining conventional cybersecurity paradigms. In order to reduce the hazards associated with deepfakes and synthetic media, the chapter discusses the examination of adversarial machine learning techniques and strategies. Along with offering guidance on incorporating AI into security operations, encouraging human-AI cooperation, and building strong AI skills, it also discusses the ethical ramifications of AI-driven security procedures. It also serves as a comprehensive guide for security professionals, researchers, and decision-makers, offering a holistic understanding of the synergies between AI and cybersecurity.</jats:p>"
10.1007/s43681-023-00405-2,Ethical considerations and policy interventions concerning the impact of generative AI tools in the economy and in society,N/A
10.2139/ssrn.4735389,The Legal Ethics of Generative AI,N/A
10.2139/ssrn.4703377,The Ethics of Generative AI in Social-Scientific Research: A Qualitative Approach for Community-Based AI Ethics,N/A
10.1007/s43681-024-00439-0,Generative AI can fabricate advanced scientific visualizations: ethical implications and strategic mitigation framework,N/A
10.4018/979-8-3693-8557-9.ch002,Generative AI,"<jats:p>For nearly 50 years, artificial intelligence (AI) has been a technology and a field of study. Its advancement had been conflicting and differed after some time. Due to the development of intriguing AI applications like Alpha Go and Alpha Fold, large data sets, advancements in the internet and semiconductors, the most significant development in machine learning and deep learning occurred in the 2000s. Generative AI models have recently gained a lot of popularity. AI has become a household name thanks to massive language models like GPT (generative pre-trained transformer) and tools like Midjourney and ChatGPT. Microsoft and Google have been incorporating these models into their enterprise and search offerings as generative AI has had a significant impact on everyday life. The part dug into the potential outcomes and difficulties related to the condition of generative computer-based intelligence around then and investigated its future. Additionally, it offered recommendations for generative AI-using businesses.</jats:p>"
10.1007/s43681-024-00445-2,Addressing diversity in hiring procedures: a generative adversarial network approach,N/A
10.37307/j.2196-9817.2023.05.09,Code of Ethics for “Empathetic” Generative AI,N/A
10.1007/s43681-024-00443-4,AI hype as a cyber security risk: the moral responsibility of implementing generative AI in business,"<jats:title>Abstract</jats:title><jats:p>This paper examines the ethical obligations companies have when implementing generative Artificial Intelligence (AI). We point to the potential cyber security risks companies are exposed to when rushing to adopt generative AI solutions or buying into “AI hype”. While the benefits of implementing generative AI solutions for business have been widely touted, the inherent risks associated have been less well publicised. There are growing concerns that the race to integrate generative AI is not being accompanied by adequate safety measures. The rush to buy into the hype of generative AI and not fall behind the competition is potentially exposing companies to broad and possibly catastrophic cyber-attacks or breaches. In this paper, we outline significant cyber security threats generative AI models pose, including potential ‘backdoors’ in AI models that could compromise user data or the risk of ‘poisoned’ AI models producing false results. In light of these the cyber security concerns, we discuss the moral obligations of implementing generative AI into business by considering the ethical principles of beneficence, non-maleficence, autonomy, justice, and explicability. We identify two examples of ethical concern, <jats:italic>overreliance</jats:italic> and <jats:italic>over-trust</jats:italic> in generative AI, both of which can negatively influence business decisions, leaving companies vulnerable to cyber security threats. This paper concludes by recommending a set of checklists for ethical implementation of generative AI in business environment to minimise cyber security risk based on the discussed moral responsibilities and ethical concern.</jats:p>"
10.4018/979-8-3693-8557-9.ch011,Innovations in Cloud Storage,"<jats:p>As the demand for efficient and scalable data storage solutions continues to rise, the integration of artificial intelligence (AI) technologies with cloud storage systems has emerged as a promising avenue for innovation. This chapter explores the intersection of AI and cloud storage, investigating how AI-driven approaches are revolutionizing traditional cloud storage practices and unlocking new opportunities for enhanced performance and functionality. Through a comprehensive review of literature and case studies, this study examines the role of AI in optimizing data management, improving security measures, and enabling intelligent data processing within cloud storage environments. Furthermore, this chapter delves into the implications of AI-driven innovation for businesses and organizations, highlighting the potential benefits of increased efficiency, cost savings, and competitive advantage.</jats:p>"
10.55092/let20240002,Generative AI in finance: risks and potential solutions,N/A
10.1007/s43681-024-00546-y,Generative AI can effectively manipulate data,N/A
10.1007/s43681-024-00497-4,Anticipating impacts: using large-scale scenario-writing to explore diverse implications of generative AI in the news environment,"<jats:title>Abstract</jats:title><jats:p>The tremendous rise of generative AI has reached every part of society—including the news environment. There are many concerns about the individual and societal impact of the increasing use of generative AI, including issues such as disinformation and misinformation, discrimination, and the promotion of social tensions. However, research on anticipating the impact of generative AI is still in its infancy and mostly limited to the views of technology developers and/or researchers. In this paper, we aim to broaden the perspective and capture the expectations of three stakeholder groups (news consumers; technology developers; content creators) about the potential negative impacts of generative AI, as well as mitigation strategies to address these. Methodologically, we apply scenario-writing and use participatory foresight in the context of a survey (n = 119) to delve into cognitively diverse imaginations of the future. We qualitatively analyze the scenarios using thematic analysis to systematically map potential impacts of generative AI on the news environment, potential mitigation strategies, and the role of stakeholders in causing and mitigating these impacts. In addition, we measure respondents' opinions on a specific mitigation strategy, namely transparency obligations as suggested in Article 52 of the draft EU AI Act. We compare the results across different stakeholder groups and elaborate on different expected impacts across these groups. We conclude by discussing the usefulness of scenario-writing and participatory foresight as a toolbox for generative AI impact assessment.</jats:p>"
10.1136/jme-2023-108909,Ethics of generative AI,N/A
10.2139/ssrn.4715603,"Performance, Skills, Ethics, Generative AI Adoption, and the Philippines",N/A
10.2139/ssrn.4737492,"Generative AI for Emerging Researchers: The Promises, Ethics, and Risks",N/A
10.2139/ssrn.4413206,ChatGPT and Generative AI Systems as Military Ethics Advisors,N/A
10.36315/2024v2end018,Ethics of generative AI use in higher education: A focus group study,N/A
10.4018/979-8-3693-8557-9.ch004,Generative AI,"<jats:p>This study provides a comprehensive view of the state of generative AI today, touching on its uses, foundational models, obstacles, prospects, and potential future courses of action. Autoregressive models like Transformers, GANs, and Variational Autoencoders (VAEs) are the backbone of generative AI. Generated AI still has a way to go before fully realizing its potential. Problems with model interpretability, training stability, and generated content bias are all examples of such challenges. Computer scientists, psychologists, and ethicists must work together to find solutions to these problems. Generative AI does, however, offer tremendous potential. Artists, designers, and storytellers have new tools at their fingertips. Improving the robustness of models, granting greater control over generated outputs, and investigating uses in interactive storytelling and real-time content production are all potential future areas for generative AI.</jats:p>"
10.4018/979-8-3693-8557-9.ch006,Ethical Considerations for Generative AI in Social Science Research,"<jats:p>Social science research embodies the inquiry into people as individuals and their interpersonal interactions with each other in communities and varied societies, with due consideration for their natural, technological, and constructed environments. Due to (a) the nature and composition of qualitative, quantitative, and mixed methods research designs coupled with (b) the apparent expectations of responsible behavior from researchers (human beings), room exists for research misconduct or unethical practices. The prevalence and acceptance of generative artificial intelligence (AI) technology such as ChatGPT propagate at a hyper-accelerated pace based on its potential for ease of work in many sectors, including research, particularly academic research. Journal reviewers, editors, and publishers do not possess sufficient tools to differentiate between human-written and partially or wholly AI-authored manuscripts submitted for journal publication.</jats:p>"
10.2139/ssrn.4478397,Ethics of Generative AI and Manipulation: A Design-Oriented Research Agenda,N/A
10.4018/979-8-3693-8557-9.ch001,An Introduction to Generative AI,"<jats:p>The chapter serves as an intricate exploration into the domain of Generative AI, offering a comprehensive understanding of its fundamental principles, diverse methodologies, and wide-ranging applications. Beginning with a clear definition and overview, it progresses to clarify key concepts such as probability distributions and sampling while providing an overview of various generative models, including autoregressive models, variational autoencoders, and generative adversarial networks. Architectural components, training methods, and optimization techniques are thoroughly examined, alongside an in-depth analysis of challenges and considerations in model design and selection. Through real-world examples, the chapter showcases the transformative potential of Generative AI across domains such as image generation, text processing, music composition, creative arts, etc. Addressing current limitations and outlining future directions offers valuable insights into the growing landscape of Generative AI, positioning it as a catalyst for innovation and creative expression.</jats:p>"
10.1007/s10676-024-09745-x,Ethics of generative AI and manipulation: a design-oriented research agenda,"<jats:title>Abstract</jats:title><jats:p>Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies.</jats:p>"
10.1007/s43681-024-00440-7,Engaging the many-hands problem of generative-AI outputs: a framework for attributing credit,"<jats:title>Abstract</jats:title><jats:p>The recent wave of generative AI (GenAI) systems like Stable Diffusion or ChatGPT that can produce images, text and code from human prompts raises controversial issues about creatorship, originality, creativity and copyright. This paper focuses on creatorship: who creates and should be credited with the outputs made with the help of GenAI? There is currently significant moral, legal and regulatory uncertainty around these questions. We develop a novel framework, called CCC (collective-centered creation), that helps resolve this uncertainty. According to CCC, GenAI outputs are created by collectives in the first instance. Claims to creatorship come in degrees and depend on the nature and significance of individual contributions made by the various agents and entities involved, including users, GenAI systems, developers, producers of training data and others. We demonstrate how CCC can help navigate a range of ongoing controversies around the responsible development and deployment of GenAI technologies and help more accurately attribute credit where it is due.</jats:p>"
10.1007/s40889-023-00179-5,Why we should (not) worry about generative AI in medical ethics teaching,N/A
10.1145/3600211.3607545,The Generative AI Deployment Rush: How to Democratize the Politics of Pace,N/A
10.1007/s43681-024-00451-4,Safeguarding human values: rethinking US law for generative AI’s societal impacts,"<jats:title>Abstract</jats:title><jats:p>Our interdisciplinary study examines the effectiveness of US law in addressing the complex challenges posed by generative AI systems to fundamental human values, including physical and mental well-being, privacy, autonomy, diversity, and equity. Through the analysis of diverse hypothetical scenarios developed in collaboration with experts, we identified significant shortcomings and ambiguities within the existing legal protections. Constitutional and civil rights law currently struggles to hold AI companies responsible for AI-assisted discriminatory outputs. Moreover, even without considering the liability shield provided by Section 230, existing liability laws may not effectively remedy unintentional and intangible harms caused by AI systems. Demonstrating causal links for liability claims such as defamation or product liability proves exceptionally difficult due to the intricate and opaque nature of these systems. To effectively address these unique and evolving risks posed by generative AI, we propose a “Responsible AI Legal Framework”  that adapts to recognize new threats and utilizes a multi-pronged approach. This framework would enshrine fundamental values in legal frameworks, establish comprehensive safety guidelines, and implement liability models tailored to the complexities of human-AI interactions. By proactively mitigating unforeseen harms like mental health impacts and privacy breaches, this framework aims to create a legal landscape capable of navigating the exciting yet precarious future brought forth by generative AI technologies.</jats:p>"
10.1007/978-1-4842-9579-3_5,Prototyping with Generative AI,N/A
10.1007/978-1-4842-9579-3_3,Generative AI with Personalities,N/A
10.2139/ssrn.4735776,Between Copyright and Computer Science: The Law and Ethics of Generative AI,N/A
10.1109/mitp.2023.3267140,What If Ethics Got in the Way of Generative AI?,N/A
10.1145/3600211.3604744,The ELIZA Defect,N/A
10.1007/978-1-4842-9579-3_11,Dilemmas Interacting with Generative AI,N/A
10.1007/978-1-4842-9579-3_7,Generative AI Form and Composition,N/A
10.1007/978-3-031-46238-2_25,The Economics of Generative AI,N/A
10.1136/jme-2023-109834,Generative AI and medical ethics: the state of play,N/A
10.1515/9783839467664-015,"Science in the era of ChatGPT, large language models and generative AI",N/A
10.1007/978-1-4842-9367-6_3,AI Fundamentals,N/A
10.14361/9783839467664-015,"Science in the era of ChatGPT, large language models and generative AI",N/A
10.1787/bf3c0c60-en,G7 Hiroshima Process on Generative Artificial Intelligence (AI),N/A
10.1007/978-1-4842-9367-6_2,Data,N/A
10.1007/979-8-8688-0403-8_1,Introduction to Generative AI,N/A
10.1057/s41599-024-03526-z,AI and ethics: Investigating the first policy responses of higher education institutions to the challenge of generative AI,"<jats:title>Abstract</jats:title><jats:p>This article addresses the ethical challenges posed by generative artificial intelligence (AI) tools in higher education and explores the first responses of universities to these challenges globally. Drawing on five key international documents from the UN, EU, and OECD, the study used content analysis to identify key ethical dimensions related to the use of generative AI in academia, such as accountability, human oversight, transparency, or inclusiveness. Empirical evidence was compiled from 30 leading universities ranked among the top 500 in the Shanghai Ranking list from May to July 2023, covering those institutions that already had publicly available responses to these dimensions in the form of policy documents or guidelines. The paper identifies the central ethical imperative that student assignments must reflect individual knowledge acquired during their education, with human individuals retaining moral and legal responsibility for AI-related wrongdoings. This top-down requirement aligns with a bottom-up approach, allowing instructors flexibility in determining how they utilize generative AI especially large language models in their own courses. Regarding human oversight, the typical response identified by the study involves a blend of preventive measures (e.g., course assessment modifications) and soft, dialogue-based sanctioning procedures. The challenge of transparency induced the good practice of clear communication of AI use in course syllabi in the first university responses examined by this study.</jats:p>"
10.4018/979-8-3693-2643-5.ch016,Bane and Boon of Hallucinations in the Context of Generative AI,"<jats:p>The phenomenon of hallucinations takes place when generative artificial intelligence systems, such as large language models (LLMs) like ChatGPT, generate outputs that are illogical, factually incorrect, or otherwise unreal. In generative artificial intelligence, hallucinations have the ability to unlock creative potential, but they also create challenges for producing accurate and trustworthy AI outputs. Both concerns will be covered in this abstract. Artificial intelligence hallucinations can be caused by a variety of factors. There is a possibility that the model will show an inaccurate response to novel situations or edge cases if the training data is insufficient, incomplete, or biassed. It is common for generative artificial intelligence to generate content in response to cues, regardless of the model's “understanding” or the quality of its output.</jats:p>"
10.1007/978-3-031-46238-2_2,Creating Ad Campaigns Using Generative AI,N/A
10.35542/osf.io/7dr9j,Critical Thinking and Ethics in the Age of Generative AI in Education,"<p>This report is an invitation for educators, policymakers, technologists, and learners to consider how generative AI can contribute to the future of education?. It aims to lay down a foundation upon which we can start building an educational ecosystem that is dynamic, inclusive, and profoundly human, despite being significantly aided by artificial intelligence?.</p>"
10.1007/s10916-023-01987-4,"Generative AI in Medical Imaging: Applications, Challenges, and Ethics",N/A
10.4018/979-8-3693-8557-9,"Generative AI and Implications for Ethics, Security, and Data Management",N/A
10.1007/s00146-024-01889-0,"Generative AI and human–robot interaction: implications and future agenda for business, society and ethics",N/A
10.1201/9781032654829-3,The Mechanics and Validation of Generative AI Outcomes,N/A
10.1007/s43681-024-00552-0,"Overcoming AI ethics, towards AI realism",N/A
10.1016/j.applanim.2023.106107,Preserving ethics and integrity of scientific writing and reviewing after the advent of generative AI and AI-assisted technologies,N/A
10.21428/e4baedd9.777b7123,Generative AI and the Future of Inequality,N/A
10.1007/978-1-4842-9367-6_5,Large Language Models,N/A
10.4018/979-8-3693-8557-9.ch007,Harmonizing Ethics With Artificial Intelligence,"<jats:p>Artificial intelligence (AI) significantly transforms human existence, yet ethical concerns in AI development remain unaddressed. This chapter highlights the critical connection between AI and ethics, emphasizing the need to align AI with human values. Despite ongoing ethical discussions, a comprehensive understanding of AI's ethical implications is lacking. This study employs a systematic literature review approach to provide practical tools for AI developers, enabling them to navigate ethical dimensions of their creations. By examining the societal impact of AI and offering a checklist for assessing ethical practicality, the study promotes ethically sound practices in AI development. The chapter explores ethical issues in AI deployment, identifying crucial societal concerns requiring ethical decision-making. Ultimately, it advocates for a compassionate approach to AI development, prioritizing ethical considerations to safeguard humanity's interests.</jats:p>"
10.1007/s43681-024-00570-y,"Correction: Overcoming AI ethics, towards AI realism",N/A
10.1007/978-1-4842-9367-6_6,Auto Code Generation,N/A
10.1007/978-1-4842-9367-6_4,Core Generative AI Technology,N/A
10.1007/978-3-031-46238-2_1,Generative AI as a Supportive Tool for Scientific Research,N/A
10.36227/techrxiv.23968809.v1,"Synergizing Generative AI and Cybersecurity: Roles of Generative AI Entities, Companies, Agencies, and Government in Enhancing Cybersecurity","<jats:p>&lt;p&gt;The digital landscape of the modern world has witnessed a remarkable evolution over the past few decades, with technological advancements permeating every facet of our lives. While these innovations have brought forth unprecedented convenience and connectivity, they have also exposed society to new vulnerabilities. Cybercrimes have surged in both frequency and sophistication, punctuating the digital era with high-profile incidents that have shaken industries and nations. Recent history serves as a stark reminder of the potential havoc that cybercriminals can unleash upon critical infrastructure, as exemplified by the notorious Colonial Pipeline breach, where a malevolent actor manipulated digital systems to demand ransom in the form of cryptocurrency.&lt;/p&gt;
&lt;p&gt;Traditionally, cybercriminal endeavors necessitated a specialized skill set and an in-depth understanding of intricate technological nuances. However, the landscape has transformed dramatically with the emergence of Generative Artificial Intelligence (AI). Previously the domain of highly specialized engineers, the tools required to orchestrate cybercrimes have become increasingly accessible due to the proliferation of advanced AI models such as ChatGPT and other modern Large Language Models (LLMs). These AI-driven capabilities have lowered the entry barrier for potential wrongdoers, enabling individuals with even basic technical aptitude to partake in cybercriminal activities.&lt;/p&gt;
&lt;p&gt;A cursory glance at contemporary news headlines underscores the growing ubiquity of cybercrimes. A relentless surge in cyberattacks serves as an alarming indication of the escalating threat posed by malicious actors in the digital realm. As each year unfolds, instances of cybercrime proliferate, impacting individuals, corporations, and governments alike. This trend signals a pressing need to comprehend the intersection between Generative AI and cybersecurity – a convergence that holds the potential to reshape the dynamics of digital malfeasance and defense.&lt;/p&gt;
&lt;p&gt;This research paper embarks on a journey to explore the intricate relationship between Generative AI and cybersecurity. Delving into the realm of AI-driven creativity and manipulation, we examine how the advent of Generative AI technologies has facilitated a paradigm shift in the landscape of cyber threats. As we navigate through this exploration, we unravel the challenges and opportunities that arise from this dynamic interplay. By delving into case studies, examining emerging trends, and scrutinizing potential countermeasures, this paper aims to shed light on the novel dimensions of cybersecurity in the era of Generative AI. Through a comprehensive analysis, we aim to equip readers with an informed understanding of the evolving cybersecurity landscape and the critical role that Generative AI plays therein.&lt;/p&gt;</jats:p>"
10.4018/979-8-3693-8557-9.ch010,Guardians of the Digital Realm,"<jats:p>This chapter explores the burgeoning role of generative artificial intelligence (AI) in the realm of cybersecurity. As our digital world expands, so do the threats posed by malicious actors. In response, the emergence of generative AI technologies presents a promising avenue for bolstering cybersecurity defenses. This chapter examines the various applications of generative AI in fortifying digital security, including its use in threat detection, anomaly identification, and vulnerability assessment. By harnessing the power of machine learning and neural networks, generative AI systems exhibit remarkable capabilities in predicting, pre-empting, and mitigating cyber threats. Moreover, this chapter delves into the ethical considerations and potential challenges associated with deploying generative AI in cybersecurity contexts, emphasizing the importance of responsible development and deployment practices. Ultimately, this exploration highlights the pivotal role of generative AI as guardians of the digital realm, ushering in a new era of enhanced cybersecurity measures.</jats:p>"
10.1145/3589335.3641295,"AI Driven Online Advertising: Market Design, Generative AI, and Ethics",N/A
10.1007/978-3-031-46238-2_29,Generative AI for Fire Safety,N/A
10.1007/979-8-8688-0403-8_4,Bridging Text and Audio in Generative AI,N/A
10.1007/978-3-031-46238-2_8,Generative AI Use in the Construction Industry,N/A
10.36227/techrxiv.23968809,"Synergizing Generative AI and Cybersecurity: Roles of Generative AI Entities, Companies, Agencies, and Government in Enhancing Cybersecurity","<jats:p>&lt;p&gt;The digital landscape of the modern world has witnessed a remarkable evolution over the past few decades, with technological advancements permeating every facet of our lives. While these innovations have brought forth unprecedented convenience and connectivity, they have also exposed society to new vulnerabilities. Cybercrimes have surged in both frequency and sophistication, punctuating the digital era with high-profile incidents that have shaken industries and nations. Recent history serves as a stark reminder of the potential havoc that cybercriminals can unleash upon critical infrastructure, as exemplified by the notorious Colonial Pipeline breach, where a malevolent actor manipulated digital systems to demand ransom in the form of cryptocurrency.&lt;/p&gt;
&lt;p&gt;Traditionally, cybercriminal endeavors necessitated a specialized skill set and an in-depth understanding of intricate technological nuances. However, the landscape has transformed dramatically with the emergence of Generative Artificial Intelligence (AI). Previously the domain of highly specialized engineers, the tools required to orchestrate cybercrimes have become increasingly accessible due to the proliferation of advanced AI models such as ChatGPT and other modern Large Language Models (LLMs). These AI-driven capabilities have lowered the entry barrier for potential wrongdoers, enabling individuals with even basic technical aptitude to partake in cybercriminal activities.&lt;/p&gt;
&lt;p&gt;A cursory glance at contemporary news headlines underscores the growing ubiquity of cybercrimes. A relentless surge in cyberattacks serves as an alarming indication of the escalating threat posed by malicious actors in the digital realm. As each year unfolds, instances of cybercrime proliferate, impacting individuals, corporations, and governments alike. This trend signals a pressing need to comprehend the intersection between Generative AI and cybersecurity – a convergence that holds the potential to reshape the dynamics of digital malfeasance and defense.&lt;/p&gt;
&lt;p&gt;This research paper embarks on a journey to explore the intricate relationship between Generative AI and cybersecurity. Delving into the realm of AI-driven creativity and manipulation, we examine how the advent of Generative AI technologies has facilitated a paradigm shift in the landscape of cyber threats. As we navigate through this exploration, we unravel the challenges and opportunities that arise from this dynamic interplay. By delving into case studies, examining emerging trends, and scrutinizing potential countermeasures, this paper aims to shed light on the novel dimensions of cybersecurity in the era of Generative AI. Through a comprehensive analysis, we aim to equip readers with an informed understanding of the evolving cybersecurity landscape and the critical role that Generative AI plays therein.&lt;/p&gt;</jats:p>"
10.1201/9781003503781-3,"Introduction to Generative AI, Natural Language Processing, and the Digital Person",N/A
10.21428/e4baedd9.cf3e35e5,"Generative AI and Creative Learning: Concerns, Opportunities, and Choices",N/A
10.5840/ijap2023372202,"Plagiarism, Academic Ethics, and the Utilization of Generative AI in Academic Writing","<jats:p>In the wake of ChatGPT’s release, academics and journal editors have begun making important decisions about whether and how to integrate generative artificial intelligence (AI) into academic publishing. Some argue that AI outputs in scholarly works constitute plagiarism, and so should be disallowed by academic journals. Others suggest that it is acceptable to integrate AI output into academic papers, provided that its contributions are transparently disclosed. By drawing on Taylor’s work on academic norms, this paper argues against both views. Unlike “traditional” forms of plagiarism, use of generative AI can be consistent with the norms that should underlie academic research. In these cases, its use should neither be prohibited nor required to be disclosed. However, some careless uses of generative AI do threaten to undermine the quality of academic research by mischaracterizing existing literature. This, not “AI plagiarism,” is the real concern raised by ChatGPT and related technologies.</jats:p>"
10.1007/978-3-031-46238-2_15,Generative AI to Understand Complex Ecological Interactions,N/A
10.4018/979-8-3693-8557-9.ch008,AI-Driven Territorial Intelligence,"<jats:p>Artificial intelligence (AI) and territorial intelligence (TI) are currently two key fields. AI enables the development of systems capable of performing tasks that generally require human intelligence, such as pattern recognition, classification, and prediction. Business intelligence applied to territories is at the heart of research into sustainable territorial development, as part of the paradigm of intelligent territories. So how can digital technology be used to develop territorial intelligence and help improve the digital sovereignty of regions? The study traces the many challenges posed by the application of AI and IT in real-life scenarios and identifies opportunities for combining the two fields via a hybrid approach aimed at decision-making in digital territorial management.</jats:p>"
10.1007/978-1-4842-9367-6_1,Introduction to Generative AI,N/A
10.1007/979-8-8688-0205-8_1,Introduction to Generative AI,N/A
10.1007/978-1-4842-9367-6_7,The Transformation of Business,N/A
10.61969/jai.1385915,Generative AI Professional Development Needs for Teacher Educators,"<jats:p xml:lang=""en"">This study presents findings from a professional development (PD) webinar aimed at sensitizing and gathering teacher educators’ knowledge of Generative Artificial Intelligence (GAI). The primary objective of the webinar was to deepen teacher educators’ understanding and applications of GAI within the context of teacher education in Ghana and to identify areas requiring additional development. Three hundred and seven participants from a diverse group, including teacher educators, administrators, and in-service teachers participated in the PD session. The session was conducted online via Zoom. The video and audio recordings were transcribed and analyzed thematically using MAXQDA version 2022.4. Findings indicate a diverse range of familiarity with GAI among participants. While some expressed knowledge of GAI tools, others were learning about GAI for the first time. Further, the findings showed an increasing curiosity among participants for the inspiring functions of GAI in education, such as automatic scoring, academic writing, assisting teachers with image generation for their classroom practices, etc. The participants demonstrated a willingness to include GAI in their classroom practices and support their students. However, they also identified infrastructural gaps, such as the expense of premium GAI tools, training on GAI promptings, and ethical issues such as transparency, as potential barriers to the successful implementation of GAI in teacher education. Therefore, the study suggests that institutional support should be provided to teacher educators. This support would expand their access to various GAI tools and features. The study further recommends integrating GAI, including explainable GAI and prompt engineering, as a core component of teacher education and continuous professional development programs. Additionally, it emphasizes the importance of strengthening educators' skills in innovative assessment practices.</jats:p>"
10.1007/978-3-031-46238-2_23,AI Deep Learning Generative Models for Drug Discovery,N/A
10.1007/978-1-4842-9579-3,Creative Prototyping with Generative AI,N/A
10.21428/e4baedd9,An MIT Exploration of Generative AI,N/A
10.1201/9781032654829-1,Generative AI and Social Engines of Hate,N/A
10.2139/ssrn.4864236,AI Justice: Harnessing Generative AI in Legal Services,N/A
10.1145/3600211.3604686,The Ethical Implications of Generative Audio Models: A Systematic Literature Review,N/A
10.1515/9783111425078-010,10 Future Directions and Open Problems in Generative AI,N/A
10.55982/openpraxis.16.1.654,"GenAI et al.: Cocreation, Authorship, Ownership, Academic Ethics and Integrity in a Time of Generative AI",N/A
10.1515/9783111425078-008,8 Exploring the Applications on Generative AI and LLM,N/A
10.1007/s43681-020-00013-4,You cannot have AI ethics without ethics,N/A
10.1007/979-8-8688-0318-5_4,Generative AI Consulting,N/A
10.1007/979-8-8688-0318-5_5,Generative AI Software,N/A
10.1007/979-8-8688-0318-5_7,Generative AI Hardware,N/A
10.1201/9781032654829-7,Unwanted Psychological Diagnoses: Discriminative Dangers of Generative AI,N/A
10.37305/jkba.2023.06.24.1.1,Ethical Issues Posed by ‘Generative-AI’ (G-AI) - Response strategies for ‘Good AI Society’,N/A
10.22318/icls2024.651304,Exploring AI Ethics through Educational Scenarios with AI Generative Arts Apps,N/A
10.47142/gec.10.1,Research on self-assessment items for teaching writing ethics in the age of generative AI,N/A
10.1515/9783111425078-004,4 Importance of Prompt Engineering in Generative AI Models,N/A
10.54254/2755-2721/87/20241543,Decision tree C4.5 algorithm for generative AI technology ethics--Based on the results of the questionnaire,"<jats:p>With the development of AI technology, generative AI has gradually entered the life of the public, for example, the explosion of CHAT-GPT has allowed more people to see the huge potential and obvious advantages of generative AI. However, in the process of generative AI operation, events that violate social responsibility and ethics often occur, which makes the research on the scientific and technological ethics of generative AI more urgent. In the past literature and research, many industry experts have analysed the impact of generative AI on specific industries, but everyone is or will be a user of generative AI, so we should pay attention to the study of the people's scientific and technological ethical issues of generative AI after putting aside the industry background, so this paper collects primary data by means of questionnaire surveys to find out the public's awareness of generative AI and their perception of generative AI. and attitudes towards generative AI, and using the decision tree C4.5 algorithm with Python as the tool, it is used to respond to people's awareness of generative AI and the public's perception of the relationship between the various factors of the ethical issues of </jats:p>"
10.1007/s10676-024-09776-4,An Ellulian analysis of propaganda in the context of generative AI,N/A
10.4018/979-8-3693-6517-5.ch003,Do Generative Large Language Models Need Billions of Parameters?,"<jats:p>This chapter presents novel systems and methodologies for the development of efficient large language models (LLMs). It explores the trade-offs between model size, performance, and computational resources, with the aim of maximizing the efficiency of these AI systems. The research explores novel methods that allow different parts of the model to share parameters, reducing the total number of unique parameters required. This approach ensures that the model remains compact without sacrificing its ability to learn and represent complex language structures. This study provides valuable insights and tools for creating more efficient and effective LLMs, contributing to a more sustainable and accessible future for AI language modeling.</jats:p>"
10.1080/23735082.2023.2261131,"Towards social generative AI for education: theory, practices and ethics",N/A
10.1117/12.3011173.84d02228-3453-ee11-a99c-00505691c5e1,N/A,N/A
10.1515/9783111425078-009,9 Bias and Fairness in Generative AI,N/A
10.1007/978-1-4842-9579-3_13,AI and the Future of Creative Work,N/A
10.1007/s43681-024-00505-7,Proposing Central Asian AI ethics principles: a multilevel approach for responsible AI,N/A
10.1007/979-8-8688-0318-5_6,Generative AI Cloud Platforms,N/A
10.1007/979-8-8688-0318-5_2,What Is Generative AI?,N/A
10.4018/979-8-3693-1950-5.ch003,Generative AI and Generative Pre-Trained Transformer Applications,"<jats:p>Generative AI, such as generative pre-trained transformer (GPT), has seen rapid advancements in recent years, offering a wide range of applications, but it also presents several challenges and opportunities. GPT can automate content generation for various industries, including journalism, marketing, and entertainment, reducing the need for manual content creation. Generative AI can personalize content and recommendations in e-commerce, streaming services, and more, enhancing user experiences. Generative AI, such as GPT, offers immense potential across various sectors but requires careful management to address bias, ethics, and quality control challenges. As technology evolves, finding the right balance between control and creativity will be crucial for maximizing its benefits while minimizing risks. Based on the above, the authors systematically review the bibliometric literature on how Generative AI and generative pre-trained transformer applications challenge opportunities using the Scopus database by analysing 49 academic and/or scientific documents.</jats:p>"
10.1007/s43681-022-00150-y,Towards AI ethics’ institutionalization: knowledge bridges from business ethics to advance organizational AI ethics,"<jats:title>Abstract</jats:title><jats:p>This paper proposes to generate awareness for developing Artificial intelligence (AI) ethics by transferring knowledge from other fields of applied ethics, particularly from business ethics, stressing the role of organizations and processes of institutionalization. With the rapid development of AI systems in recent years, a new and thriving discourse on AI ethics has (re-)emerged, dealing primarily with ethical concepts, theories, and application contexts. We argue that business ethics insights may generate positive knowledge spillovers for AI ethics, given that debates on ethical and social responsibilities have been adopted as voluntary or mandatory regulations for organizations in both national and transnational contexts. Thus, business ethics may transfer knowledge from five core topics and concepts researched and institutionalized to AI ethics: (1) stakeholder management, (2) standardized reporting, (3) corporate governance and regulation, (4) curriculum accreditation, and as a unified topic (5) AI ethics washing derived from greenwashing. In outlining each of these five knowledge bridges, we illustrate current challenges in AI ethics and potential insights from business ethics that may advance the current debate. At the same time, we hold that business ethics can learn from AI ethics in catching up with the digital transformation, allowing for cross-fertilization between the two fields. Future debates in both disciplines of applied ethics may benefit from dialog and cross-fertilization, meant to strengthen the ethical depth and prevent ethics washing or, even worse, ethics bashing.</jats:p>"
10.1162/99608f92.88b4cc98,Future Shock: Generative AI and the International AI Policy and Governance Crisis,N/A
10.1201/9781003499527-6,Generative AI and Other Types of AI,N/A
10.53593/n4121a,Generative AI in Economics: Teaching Economics and AI Literacy,N/A
10.1201/9781032669182-7,Generative AI and Other Types of AI,N/A
10.4018/979-8-3693-1351-0.ch014,Empowering Faculty Vitality and Mitigating Burnout Through Generative AI in Higher Education,"<jats:p>In today's dynamic landscape of higher education, the pervasive issue of faculty burnout has emerged as a pressing concern, casting a shadow over the well-being of educators and the overall quality of instruction. This chapter proposal embarks on an exploration of how generative artificial intelligence (AI) can act as a transformative force within higher education, specifically focusing on its potential to empower faculty members, enhance pedagogical practices, and mitigate the alarming prevalence of burnout. The chapter's central objectives are multifaceted, commencing with a comprehensive examination of the multifaceted phenomenon of faculty burnout. This includes an analysis of the contributing factors such as the escalating workloads, the shift towards online and blended learning modalities, and the overwhelming administrative duties that educators must shoulder. The aim here is to illuminate the multifaceted nature of burnout, thereby fostering an enhanced understanding of its urgency and the critical need for sustainable solutions.</jats:p>"
10.1162/99608f92.91162b2e,Data Protection and Generative AI: An Inconclusive Answer,N/A
10.31584/psumj.2023259047,"Generative AI, ChatGPT and Medicine",N/A
10.1002/tl.20626,An ethics module on academic integrity and generative AI,"<jats:title>Abstract</jats:title><jats:p>This article explores the intersection between academic integrity and generative AI (GenAI). It presents a tested framework for a versatile 3‐h module applicable to various disciplines. Since ChatGPT's emergence, GenAI's impact on academic integrity has raised concerns, challenged established norms, and blurred lines of authorship. Engaging students in this topic encourages critical reflection and ethical use of these technologies. This approach draws on experiential learning and student–faculty partnership approaches to activities and assessments, providing students with a platform to not only navigate the responsible application of GenAI in assignments but also foster a dialogue between students and faculty on crafting effective policies for GenAI use.</jats:p>"
10.1515/9783111425078-011,11 Optimizing Sustainable Project Management Life Cycle Using Generative AI Modeling,N/A
10.1007/979-8-8688-0282-9_8,Sense-Based Generative AI Demystified,N/A
10.1007/979-8-8688-0318-5_3,Generative AI Customer End Uses,N/A
10.1515/9783111425078-013,13 Generative AI and LLM: Case Study in E-Commerce,N/A
10.1093/oxfordhb/9780190067397.013.2,The Ethics of the Ethics of AI,"<p>This chapter discusses several challenges for doing the ethics of artificial intelligence (AI). The challenges fall into five major categories: conceptual ambiguities within philosophy and AI scholarship; the estimation of AI risks; implementing machine ethics; epistemic issues of scientific explanation and prediction in what can be called computational data science (CDS), which includes “big data” science; and oppositional versus systemic ethics approaches. The chapter then argues that these ethical problems are not likely to yield to the “common approaches” of applied ethics. Primarily due to the transformational nature of artificial intelligence within science, engineering, and human culture, novel approaches will be needed to address the ethics of AI in the future. Moreover, serious barriers to the formalization of ethics will be needed to overcome to implement ethics in AI.</p>"
10.1007/978-1-4842-9579-3_6,Building Blocks,N/A
10.1007/s43681-023-00367-5,AI ethics and ordoliberalism 2.0: towards a ‘Digital Bill of Rights’,N/A
10.1201/9781032654829-4,Generative AI for Hate Speech Detection: Evaluation and Findings,N/A
10.4018/979-8-3693-5578-7.ch001,Generative AI and Business Strategy,"<jats:p>A universal technology appears that sparks creativity to the point where several sectors undergo complete revolution. The Generative AI is one of these uncommon general-purpose technologies and is poised to bring about exponential transformation and is now consistently supported by research. It has the power to change the competitive landscape and propel businesses towards explosive growth. Businesses will either leap ahead of this technological transformation or fall behind as they race to comprehend its ramifications. In Generative AI and business transformation, potential for strategic and useful business applications are identified by analyzing the platforms, big language models, and technology of generative AI. This chapter focuses on Generative AI role in influencing business strategy.</jats:p>"
10.1007/978-1-4842-9579-3_4,Creative Companion,N/A
10.1007/979-8-8688-0282-9_3,The Business Case for Generative AI,N/A
10.21428/e4baedd9.d562223c,The Impact of Generative AI on Labor Market Matching,N/A
10.1016/c2023-0-00094-2,Synthetic Data and Generative AI,N/A
10.1007/978-1-4842-9579-3_12,Use Cases,N/A
10.1515/9783111425078-012,12 Generative AI and LLM: Case Study in Finance,N/A
10.1007/978-3-031-46238-2_18,How Generative AI Is Transforming Medical Imaging: A Practical Guide,N/A
10.1007/979-8-8688-0282-9_15,The Evolving World of Generative AI,N/A
10.1515/9783111425078-002,2 Early Roots of Generative AI Models and LLM: A Diverse Landscape,N/A
10.2139/ssrn.4858973,Risks from Generative AI,N/A
10.2196/preprints.55957,Toward Clinical Generative AI: Conceptual Framework (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>Clinical decision-making is a crucial aspect of health care, involving the balanced integration of scientific evidence, clinical judgment, ethical considerations, and patient involvement. This process is dynamic and multifaceted, relying on clinicians’ knowledge, experience, and intuitive understanding to achieve optimal patient outcomes through informed, evidence-based choices. The advent of generative artificial intelligence (AI) presents a revolutionary opportunity in clinical decision-making. AI’s advanced data analysis and pattern recognition capabilities can significantly enhance the diagnosis and treatment of diseases, processing vast medical data to identify patterns, tailor treatments, predict disease progression, and aid in proactive patient management. However, the incorporation of AI into clinical decision-making raises concerns regarding the reliability and accuracy of AI-generated insights. To address these concerns, 11 “verification paradigms” are proposed in this paper, with each paradigm being a unique method to verify the evidence-based nature of AI in clinical decision-making. This paper also frames the concept of “clinically explainable, fair, and responsible, clinician-, expert-, and patient-in-the-loop AI.” This model focuses on ensuring AI’s comprehensibility, collaborative nature, and ethical grounding, advocating for AI to serve as an augmentative tool, with its decision-making processes being transparent and understandable to clinicians and patients. The integration of AI should enhance, not replace, the clinician’s judgment and should involve continuous learning and adaptation based on real-world outcomes and ethical and legal compliance. In conclusion, while generative AI holds immense promise in enhancing clinical decision-making, it is essential to ensure that it produces evidence-based, reliable, and impactful knowledge. Using the outlined paradigms and approaches can help the medical and patient communities harness AI’s potential while maintaining high patient care standards.</p>
                </sec>"
10.1007/s43681-023-00340-2,"Primary recognition, morality and AI","<jats:title>Abstract</jats:title><jats:p>This paper aims to show that the experience of ‘primary recognition’ (O’Hara in Moral certainty and the foundations of morality, Palgrave Macmillan, London, 2018) can be extended to human AI interactions. That is, I argue that human beings can (and do) experience non-rational, reflex moral responses to AI and social robots that fit O’Hara’s description of primary recognition. I give two plausible examples, one involving a military mine-sweeping robot and the other, a toy dinosaur called a ‘Pleo’. These experiences of primary recognition do not, however, settle the question of whether any particular AI can be considered a true moral patient or a ‘person’.</jats:p>"
10.1007/s43681-023-00409-y,How to design an AI ethics board,"<jats:title>Abstract</jats:title><jats:p>The development and deployment of artificial intelligence (AI) systems poses significant risks to society. To reduce these risks to an acceptable level, AI companies need an effective risk management process and sound risk governance. In this paper, we explore a particular way in which AI companies can improve their risk governance: by setting up an AI ethics board. We identify five key design choices: (1) What responsibilities should the board have? (2) What should its legal structure be? (3) Who should sit on the board? (4) How should it make decisions? (5) And what resources does it need? We break each of these questions down into more specific sub-questions, list options, and discuss how different design choices affect the board’s ability to reduce societal risks from AI. Several failures have shown that designing an AI ethics board can be challenging. This paper provides a toolbox that can help AI companies to overcome these challenges.</jats:p>"
10.1007/s43681-021-00129-1,AI ethics and systemic risks in finance,"<jats:title>Abstract</jats:title><jats:p>The paper suggests that AI ethics should pay attention to morally relevant systemic effects of AI use. It draws the attention of ethicists and practitioners to systemic risks that have been neglected so far in professional AI-related codes of conduct, industrial standards and ethical discussions more generally. The paper uses the financial industry as an example to ask: how can AI-enhanced systemic risks be ethically accounted for? Which specific issues does AI use raise for ethics that takes systemic effects into account? The paper (1) relates the literature about <jats:italic>AI ethics</jats:italic> to the <jats:italic>ethics of systemic risks</jats:italic> to clarify the moral relevance of AI use with respect to the imposition of systemic risks, (2) proposes a theoretical framework based on <jats:italic>the ethics of complexity</jats:italic> and (3) applies this framework to <jats:italic>discuss implications for AI ethics</jats:italic> concerned with AI-enhanced systemic risks.</jats:p>"
10.1201/9781003442240-6,Computational Creativity and Generative AI,N/A
10.1007/979-8-8688-0282-9_4,The World of Text-Based Generative AI,N/A
10.1007/978-1-4842-9579-3_10,Uncanny by Nature,N/A
10.1007/978-3-031-55642-5_6,Advancing Requirements Engineering Through Generative AI: Assessing the Role of LLMs,N/A
10.21428/e4baedd9.a1f6a281,Generative AI from Theory to Practice: A Case Study of Financial Advice,N/A
10.1007/s43681-020-00002-7,Emerging challenges in AI and the need for AI ethics education,N/A
10.1007/s43681-021-00040-9,Philosophical foundations for digital ethics and AI Ethics: a dignitarian approach,"<jats:title>Abstract</jats:title><jats:p><jats:italic>AI Ethics</jats:italic>is a burgeoning and relatively new field that has emerged in response to growing concerns about the impact of artificial intelligence (AI) on human individuals and their social institutions. In turn, AI ethics is a part of the broader field of digital ethics, which addresses similar concerns generated by the development and deployment of new digital technologies. Here, we tackle the important worry that digital ethics in general, and AI ethics in particular, lack adequate philosophical foundations. In direct response to that worry, we formulate and rationally justify some basic concepts and principles for digital ethics/AI ethics, all drawn from a broadly Kantian theory of human dignity. Our argument, which is designed to be relatively compact and easily accessible, is presented in ten distinct steps: (1) what “digital ethics” and “AI ethics” mean, (2) refuting the dignity-skeptic, (3) the metaphysics of human dignity, (4) human happiness or flourishing, true human needs, and human dignity, (5) our moral obligations with respect to all human real persons, (6) what a natural automaton or natural machine is, (7) why human real persons are not natural automata/natural machines: because consciousness is a form of life, (8) our moral obligations with respect to the design and use of artificial automata or artificial machines, aka computers, and digital technology more generally, (9) what privacy is, why invasions of digital privacy are morally impermissible, whereas consensual entrances into digital privacy are either morally permissible or even obligatory, and finally (10) dignitarian morality versus legality, and digital ethics/AI ethics. We conclude by asserting our strongly-held belief that a well-founded and generally-accepted dignitarian digital ethics/AI ethics is of global existential importance for humanity.</jats:p>"
10.1162/99608f92.fbdf6128,Carbon Emissions in the Tailpipe of Generative AI,N/A
10.21428/e4baedd9.1729053f,Implementing Generative AI in U.S. Hospital Systems,N/A
10.21428/e4baedd9.81164b06,Generative AI and K-12 Education: An MIT Perspective,N/A
10.1007/979-8-8688-0282-9_7,Advanced Applications of Text-Based Generative AI,N/A
10.1007/979-8-8688-0282-9_1,Introduction to the World of Generative AI,N/A
10.1007/s00146-023-01817-8,Generative AI and photographic transparency,N/A
10.1007/s43681-024-00551-1,Opinion piece: on the ethics of a pending AI crisis in business,N/A
10.1007/s43681-022-00209-w,The uselessness of AI ethics,"<jats:title>Abstract</jats:title><jats:p>As the awareness of AI’s power and danger has risen, the dominant response has been a turn to ethical principles. A flood of AI guidelines and codes of ethics have been released in both the public and private sector in the last several years. However, these are<jats:italic>meaningless principles</jats:italic>which are contested or incoherent, making them difficult to apply; they are<jats:italic>isolated principles</jats:italic>situated in an industry and education system which largely ignores ethics; and they are<jats:italic>toothless principles</jats:italic>which lack consequences and adhere to corporate agendas. For these reasons, I argue that AI ethical principles are useless, failing to mitigate the racial, social, and environmental damages of AI technologies in any meaningful sense. The result is a gap between high-minded principles and technological practice. Even when this gap is acknowledged and principles seek to be “operationalized,” the translation from complex social concepts to technical rulesets is non-trivial. In a zero-sum world, the dominant turn to AI principles is not just fruitless but a dangerous distraction, diverting immense financial and human resources away from potentially more effective activity. I conclude by highlighting alternative approaches to AI justice that go beyond ethical principles: thinking more broadly about systems of oppression and more narrowly about accuracy and auditing.</jats:p>"
10.2196/55957,Toward Clinical Generative AI: Conceptual Framework,"<jats:p>Clinical decision-making is a crucial aspect of health care, involving the balanced integration of scientific evidence, clinical judgment, ethical considerations, and patient involvement. This process is dynamic and multifaceted, relying on clinicians’ knowledge, experience, and intuitive understanding to achieve optimal patient outcomes through informed, evidence-based choices. The advent of generative artificial intelligence (AI) presents a revolutionary opportunity in clinical decision-making. AI’s advanced data analysis and pattern recognition capabilities can significantly enhance the diagnosis and treatment of diseases, processing vast medical data to identify patterns, tailor treatments, predict disease progression, and aid in proactive patient management. However, the incorporation of AI into clinical decision-making raises concerns regarding the reliability and accuracy of AI-generated insights. To address these concerns, 11 “verification paradigms” are proposed in this paper, with each paradigm being a unique method to verify the evidence-based nature of AI in clinical decision-making. This paper also frames the concept of “clinically explainable, fair, and responsible, clinician-, expert-, and patient-in-the-loop AI.” This model focuses on ensuring AI’s comprehensibility, collaborative nature, and ethical grounding, advocating for AI to serve as an augmentative tool, with its decision-making processes being transparent and understandable to clinicians and patients. The integration of AI should enhance, not replace, the clinician’s judgment and should involve continuous learning and adaptation based on real-world outcomes and ethical and legal compliance. In conclusion, while generative AI holds immense promise in enhancing clinical decision-making, it is essential to ensure that it produces evidence-based, reliable, and impactful knowledge. Using the outlined paradigms and approaches can help the medical and patient communities harness AI’s potential while maintaining high patient care standards.</jats:p>"
10.1007/s43681-024-00485-8,AI for all: Diversity and Inclusion in AI,N/A
10.1007/s43681-022-00156-6,The ethics of AI business practices: a review of 47 AI ethics guidelines,N/A
10.1007/s43681-021-00071-2,AI ethics: A framework for measuring embodied carbon in AI systems,"<jats:title>Abstract</jats:title><jats:p>This paper outlines the ethical implications of AI from a climate perspective. So far, much of the discussion around AI ethics have focused on bias, unexplainable outcomes, privacy and other social impacts of such systems. The role and contribution of AI towards climate change and the ethical implications of its contribution to an unjust distribution of impact on the planet, humans and flora and fauna have not yet been covered in detail within the technical community. Within this paper, we aim to raise some of the issues of AI associated with climate justice and we propose a framework that will allow the AI and ICT industries to measure their true impact on the planet, propose an organisational structure to take this work forward and propose future research areas for this important topic.</jats:p>"
10.1162/99608f92.ad8ebbd4,"Toward a Theory of AI Errors: Making Sense of Hallucinations, Catastrophic Failures, and the Fallacy of Generative AI",N/A
10.1145/3600211.3604716,Diffusing the Creator: Attributing Credit for Generative AI Outputs,N/A
10.37736/kjlr.2023.08.14.4.03,Undergraduates’ Awareness of the Ethics of Generative AI Utilization in College Writing,"<jats:p>This study examines the utilization of generative AI among undergraduate students and their awareness of ethics in college writing. It also identifies the demand for ethics education related to writing using generative AI. A survey was conducted on 895 undergraduate students taking college writing courses at A University.
The study found that first, undergraduate students who have had experience with university writing tasks employed generative AI used ChatGPT the most; they used it substantially to generate ideas and collect data when writing general reports. In addition, more than half of the experienced users were satisfied with generative AI. Undergraduate students comply with writing ethics when performing college writing and are well aware of the problems in using generative AI, but they have low awareness of citation methods in writing when using generative AI. There was a high educational demand for citation methods and writing ethics when performing writing using generative AI. Moreover, there were some differences in the level of awareness and educational needs depending on the undergraduate major and the experience of using generative AI.
Accordingly, this study suggested the following measures in college writing subjects: education on how to comply with writing ethics and citation when using generative AI for writing, education that reflects the differences in students’ awareness and educational needs by major category, and education on how to use ChatGPT in generating ideas and collecting data when teaching how to write a report.</jats:p>"
10.1515/9783111425078-003,3 Generative AI Models and LLM: Training Techniques and Evaluation Metrics,N/A
10.1515/9783111425078-001,1 Unveiling the Power of Generative AI: A Journey into Large Language Models,N/A
10.1007/979-8-8688-0083-2_2,Exploring Generative AI and Its Transformative Power,N/A
10.1007/978-1-4842-9579-3_9,The Master of Mashup,N/A
10.1162/99608f92.fad6d25c,Future Shock: Grappling With the Generative AI Revolution,N/A
10.1007/978-1-4842-9579-3_1,Generating Creativity from Negativity,N/A
10.1007/s43681-020-00015-2,AI ethics and its impact on knowledge management,N/A
10.2139/ssrn.4878339,Generative AI as Digital Media,N/A
10.3386/w32690,How will Generative AI impact Communication?,N/A
10.1093/oxfordhb/9780190067397.013.35,Perspectives on Ethics of AI,"<p>This chapter investigates the philosophical aspects of AI and ethics. As the world today becomes increasingly populated by intelligent, socially interactive artefacts—devices that are not just instruments of human action but designed to be a kind of social actor in their own right—people will need to grapple with challenging questions concerning the status and moral standing of these machinic others. In formulating responses to these questions, one can obviously deploy the standard properties approach. This method has considerable historical precedent behind it and constitutes what can be called the default setting for addressing questions concerning moral standing. Indeed, a good deal of the current work in moral machines, machine ethics, AI ethics, and the ethics of AI follow this procedure. However, this approach, for all its advantages, also has considerable difficulties. The chapter therefore proposes an alternative approach to addressing AI ethics and the ethics of AI that circumvents many of the problems encountered in the properties approach by arranging for an ethics that is relational, radically empirical, and altruistic. This other way of thinking is informed by and follows from recent innovations in moral philosophy.</p>"
10.61700/qs6nzkddi20lw469,Using Generative AI for Qualitative Analysis,"<jats:p>This comprehensive workshop is aimed at equipping researchers with the skills to utilize AI appropriately in their qualitative analysis. Covering a range of topics from understanding AI software tools to ethical considerations and troubleshooting, this seminar provides a unique opportunity to enhance research capabilities and gain a competitive edge in the academic field.</jats:p>"
10.1007/s43681-020-00008-1,Lessons learned from AI ethics principles for future actions,N/A
10.1007/s43681-024-00524-4,Operationalizing responsible AI principles through responsible AI capabilities,"<jats:title>Abstract</jats:title><jats:p>Responsible artificial intelligence (RAI) has emerged in response to growing concerns about the impact of AI. While high-level principles have been provided, operationalizing these principles poses challenges. This study, grounded in recent RAI literature in organizational contexts and dynamic capability theory, and informed by literature on RAI principles and expert interviews in organizations deploying AI systems, (1) problematizes the high-level principles and low-level requirements and underscores the need for mid-level norms by adopting dynamic capability as a theoretical lens, and (2) develops five themes to capture firms’ RAI capability, including (i) understandable AI model, (ii) bias remediation, (iii) responsiveness, (iv) harmless, and vi) common good. As our contribution to the field of information systems (IS), this study extends the emerging literature on operationalizing RAI and dynamic capabilities, empirically elucidating the capabilities needed by firms. For IS practice, we provide organizations deploying AI with novel insights to aid in the responsible implementation of AI.</jats:p>"
10.1007/978-3-031-55642-5_5,Requirements Engineering Using Generative AI: Prompts and Prompting Patterns,N/A
10.1007/978-1-4842-9579-3_2,Being Creative with Machines,N/A
10.1007/s43681-021-00082-z,Correction to: Philosophical foundations for digital ethics and AI Ethics: a dignitarian approach,N/A
10.1515/9783111425078-014,Index,N/A
10.1515/9783111425078-fm,Frontmatter,N/A
10.1007/978-3-031-46238-2_24,3D Generative Network,N/A
10.1201/9781032669182-8,"Generative AI: Types, Skills, Opportunities, and Challenges",N/A
10.1007/978-1-4842-9579-3_8,The Art of the Prompt,N/A
10.1007/978-3-031-54252-7_1,Foundations of Generative AI,N/A
10.1007/s43681-024-00437-2,Action-guidance and AI ethics: the case of fair machine learning,"<jats:title>Abstract</jats:title><jats:p>A prominent approach to implementing AI ethics involves translating ethical principles, such as fairness and transparency, into practical frameworks and tools that responsible agents, such as ML developers, can use to ensure that machine learning systems act according to the relevant principles. Fair machine learning research exemplifies this approach by producing frameworks and software toolkits that responsible agents could apply to align machine learning systems with principles such as fairness, equality, and justice. However, the application of available frameworks and tools has proven challenging both due to ambiguous operationalization of the relevant principles and many real-life obstacles that agents face in the context of machine learning system design and development, such as lack of access to proper evaluation data. This article conceptualizes these problems as instances of a more general “action-guidance gap” in AI ethics. The article addresses the action-guidance gap by outlining a philosophical account of action-guidance that can be used to identify and address problems related to the specification and practical implementation of AI ethics principles. Centering on fair machine learning practice as a case example, the article presents a set of detailed requirements for action-guidance in fair machine learning practice which explain problems that previous studies have identified with regard to the real-life application of fair machine learning frameworks and tools. Paving a way forward, the article presents theoretical and practical lessons for ensuring action-guidance in fairness-sensitive design, with implications for AI ethics more generally.</jats:p>"
10.1515/9783111425078-toc,Contents,N/A
10.1515/9783111425078-202,Preface,N/A
10.1093/oxfordhb/9780190067397.013.46,Ethics of AI in Law,"<p>This chapter studies some of the most important ethical topics involving the use of artificial intelligence (AI) within the legal system itself and examines how central legal values might unintentionally or intentionally change with increased use of AI in law. Ethical issues surrounding AI use in law often share a common theme. As AI becomes increasingly integrated within the legal system, how can society ensure that core legal values are preserved? Among the most important of these legal values are: equal treatment under the law; legal results arising from law, principle, and facts rather than social status or power; procedural fairness and due process; fairness in design and application of the law; transparency in legal substance and process; adequate access to justice for all; integrity and honesty in creation and application of law; and judicial, legislative, and administrative efficiency. The central ethical challenge is to identify the ways in which the use of AI may be shifting core legal values and to ensure that these crucial values are preserved in the technological transition. A more positive view also identifies the ways in which AI technology can not only preserve central values, but rather, can foster and enhance these values to the betterment of the legal system and society overall.</p>"
10.2139/ssrn.4910877,African AI Ethics?—The Role of AI Ethics Initiatives in Africa,N/A
10.1007/978-3-031-46238-2_26,Plant Data Generation with Generative AI: An Application to Plant Phenotyping,N/A
10.1093/oxfordhb/9780190067397.013.27,Perspectives on Ethics of AI,"<p>This chapter describes a computational view of the function of ethics in human society and discusses its application to three diverse examples. First, autonomous vehicles are individually embodied intelligent systems that act as members of society. The ethical knowledge needed by such an agent is not how to choose the lesser evil when confronted by a Deadly Dilemma, but how to recognize the upstream decision point that makes it possible to avoid the Deadly Dilemma entirely. Second, disembodied distributed intelligent systems like Google and Facebook provide valuable services while collecting, aggregating, and correlating vast amounts of information about individual users. With inadequate controls, these corporate systems can invade privacy and do substantial damage through either correct or incorrect inferences. Third, acceptance of the legitimacy of the society by its individual members depends on a general perception of fairness. Rage about unfairness can be directed at individual free-riders or at systematic inequality across the society. Ultimately, the promise of a computational approach to ethical knowledge is not simply ethics for computational devices such as robots. It also promises to help people understand the pragmatic value of ethics as a feedback mechanism that helps intelligent creatures, human and nonhuman, live together in thriving societies.</p>"
10.1515/9781501519765-008,Chapter 4: Generative AI,N/A
10.1117/12.3011374.61c8834a-a454-ee11-a99c-00505691c5e1,N/A,N/A
10.1007/s43681-023-00347-9,Against the substitutive approach to AI in healthcare,N/A
10.1007/s43681-021-00090-z,Using edge cases to disentangle fairness and solidarity in AI ethics,N/A
10.1007/s43681-022-00243-8,Correction: AI ethics: the case for including animals,N/A
10.15833/kafeiam.30.4.1261,Meta-analysis of learning effectiveness using generative AI,N/A
10.1007/978-1-4842-9367-6,Generative AI,N/A
10.1007/s43681-021-00080-1,From computer ethics and the ethics of AI towards an ethics of digital ecosystems,"<jats:title>Abstract</jats:title><jats:p>Ethical, social and human rights aspects of computing technologies have been discussed since the inception of these technologies. In the 1980s, this led to the development of a discourse often referred to as computer ethics. More recently, since the middle of the 2010s, a highly visible discourse on the ethics of artificial intelligence (AI) has developed. This paper discusses the relationship between these two discourses and compares their scopes, the topics and issues they cover, their theoretical basis and reference disciplines, the solutions and mitigations options they propose and their societal impact. The paper argues that an understanding of the similarities and differences of the discourses can benefit the respective discourses individually. More importantly, by reviewing them, one can draw conclusions about relevant features of the next discourse, the one we can reasonably expect to follow after the ethics of AI. The paper suggests that instead of focusing on a technical artefact such as computers or AI, one should focus on the fact that ethical and related issues arise in the context of socio-technical systems. Drawing on the metaphor of ecosystems which is widely applied to digital technologies, it suggests preparing for a discussion of the ethics of digital ecosystems. Such a discussion can build on and benefit from a more detailed understanding of its predecessors in computer ethics and the ethics of AI.</jats:p>"
10.1016/b978-0-44-321857-6.00003-5,Copyright,N/A
10.1201/9781003503781,Generative AI,N/A
10.1007/s43681-020-00024-1,Correction to: AI ethics and its impact on knowledge management,N/A
10.1007/s00146-023-01719-9,"Generative AI, generating precariousness for workers?",N/A
10.1007/s00146-023-01773-3,Generative AI and human labor: who is replaceable?,N/A
10.17918/00001845,Image Forensics and Anti-Forensics for Generative AI,N/A
10.21428/e4baedd9.82175d26,Generative AI in the Era of 'Alternative Facts',N/A
10.4018/979-8-3693-5578-7.ch011,Revolutionizing Tourism,"<jats:p>This chapter aims to explore the potential of generative (AI) in improving aspects of tourism experiences. Areas focused on include personalized recommendation systems, content generation for marketing purposes, virtual tour guides, and adaptive travel planning within the tourism industry. It focuses on customization approaches, incorporation of AR and VR technologies, and creation of real-time decision support systems. Moreover, its objective is to understand the ethical utilization of AI in the tourism industry, concerns regarding data privacy, and the incorporation of AI into current tourism infrastructure. It helps to delineate forthcoming research paths and emerging patterns, highlighting the possible influence of AI on the tourism sector and its future. Furthermore, it seeks to elucidate the socio-economic impacts of using AI in the tourist industry, examining its capacity to transform employment patterns, visitor characteristics, and methods for developing destinations. Finally, it promotes interdisciplinary discussion in influencing the future of tourism.</jats:p>"
10.1007/979-8-8688-0403-8_6,Generative Large Language Models,N/A
10.21428/92fbeb44.1aaaf7ab,Human-AI Partnerships in Generative Music,N/A
10.1007/978-1-4842-9994-4_1,Introduction to Generative AI,N/A
10.1201/9781003318538-10,"Exploring the Potential of ChatGPT, Responsible AI, Explainable AI and Generative AI",N/A
10.1007/978-3-031-55642-5_11,How Can Generative AI Enhance Software Management? Is It Better Done than Perfect?,N/A
10.1007/978-3-031-46238-2,Applications of Generative AI,N/A
10.21428/e4baedd9.5aaf489a,Generative AI for Pro-Democracy Platforms,N/A
10.1007/978-3-031-46238-2_19,Generative AI in Medical Imaging and Its Application in Low Dose Computed Tomography (CT) Image Denoising,N/A
10.21428/e4baedd9.2d7598a2,Intelligence as Agency: Evaluating the Capacity of Generative AI to Empower or Constrain Human Action,N/A
10.21428/e4baedd9.567bfd15,"Generative AI for Trustworthy, Open, and Equitable Scholarship",N/A
10.4324/9781003482918-15,Generative AI and the implications for authentic assessment,N/A
10.1162/99608f92.ec74a002,Can ChatGPT Plan Your Retirement?: Generative AI and Financial Advice,N/A
10.1007/s43681-023-00357-7,How to measure value alignment in AI,N/A
10.1007/978-3-031-64087-2_11,From Large Language Models to Generative AI Systems,N/A
10.1007/s10805-023-09492-6,Detection of GPT-4 Generated Text in Higher Education: Combining Academic Judgement and Software to Identify Generative AI Tool Misuse,N/A
10.1007/979-8-8688-0473-1_1,Introduction to Generative AI,N/A
10.4018/979-8-3693-3278-8.ch005,AI Generative Models for the Fashion Industry,"<jats:p>Fashion designers and brands use GANs to create new and unique patterns, styles, and textures. GANs consist of a generator and a discriminator, which work together to produce high-quality, realistic outputs. VAEs are another type of generative model that is applied to generate new fashion designs. VAEs are known for their ability to generate diverse outputs by sampling from a learned latent space. Fashion designers can use VAEs to explore different design variations and styles. StyleGAN and its successor, StyleGAN2, are advancements of GANs that specifically focus on generating high-resolution and realistic images with control over different style elements. These models have been employed in fashion to create detailed and visually appealing designs. These AI generative models have the potential to revolutionize the fashion industry by facilitating creativity and providing new avenues for artistic expression. However, it's essential to consider ethical implications, intellectual property rights, and the responsible use of AI technologies in the context of fashion design.</jats:p>"
10.1007/s43681-023-00341-1,"Participation, prediction, and publicity: avoiding the pitfalls of applying Rawlsian ethics to AI","<jats:title>Abstract</jats:title><jats:p>Given the popularity of John Rawls’ theory of justice as fairness as an ethical framework in the artificial intelligence (AI) field, this article examines how the theory fits with three different conceptual applications of AI technology. First, the article discusses a proposition by Ashrafian to let an AI agent perform the deliberation that produces a Rawlsian social contract governing humans. The discussion demonstrates the inviability of such an application as it contradicts foundational aspects of Rawls’ theories. An exploration of more viable applications of Rawlsian theory in the AI context follows, introducing the distinction between <jats:italic>intrinsic</jats:italic> and <jats:italic>extrinsic</jats:italic> theoretical adherence, i.e., the difference between approaches integrating Rawlsian theory in the system design and those situating AI systems in Rawls-consistent policy/legislative frameworks. The article uses emerging AI legislation in the EU and the U.S. as well as Gabriel’s argument for adopting Rawls’ <jats:italic>publicity</jats:italic> criterion in the AI field as examples of extrinsic adherence to Rawlsian theory. A discussion of the epistemological challenges of predictive AI systems then illustrates some implications of intrinsic adherence to Rawlsian theory. While AI systems can make short-term predictions about human behavior with intrinsic adherence to Rawls’ theory of justice as fairness, long-term, large-scale predictions results do not adhere to the theory, but instead constitute the type of utilitarianism Rawls vehemently opposed. The article concludes with an overview of the implications of these arguments for policymakers and regulators.</jats:p>"
10.1007/s43681-024-00420-x,On monitorability of AI,"<jats:title>Abstract</jats:title><jats:p>Artificially intelligent (AI) systems have ushered in a transformative era across various domains, yet their inherent traits of unpredictability, unexplainability, and uncontrollability have given rise to concerns surrounding AI safety. This paper aims to demonstrate the infeasibility of accurately monitoring advanced AI systems to predict the emergence of certain capabilities prior to their manifestation. Through an analysis of the intricacies of AI systems, the boundaries of human comprehension, and the elusive nature of emergent behaviors, we argue for the impossibility of reliably foreseeing some capabilities. By investigating these impossibility results, we shed light on their potential implications for AI safety research and propose potential strategies to overcome these limitations.</jats:p>"
10.1093/oxfordhb/9780190067397.013.39,Perspectives and Approaches in AI Ethics,"<p>This chapter focuses on Chinese, Japanese, and South Korean perspectives on and approaches to AI and robots, which can be tools and partners in the AI ethics debate. Each country, in its own way, debates its movement across the tool-partner spectrum. To date, South Korean policy makes a stand against partner AI and robots, while popular culture explores the idea. Chinese policy is headed in the direction of a tool-oriented AI and robotics ethical guidelines, while local practices and culture experiment with the idea of physical and spiritual partnership. Meanwhile, Japan’s social principles are also moving in the tool direction, but its society actively seeks and creates partner-like AI and robots. The chapter then considers two cross-cutting AI and robotics-related ethical issues: the Anthropomorphized Tools Paradox and female objectification. These issues underscore the question of “antisocial” technology. It is clear that both the Anthropomorphized Tools Paradox and female objectification in technology fall under “antisocial” development.</p>"
10.1007/s43681-024-00534-2,"Author Correction: AI hype, promotional culture, and affective capitalism",N/A
10.1016/j.joms.2023.09.015,Generative Artificial Intelligence (AI) and Medical Ethics: A Symbiotic Dance for the Future,N/A
10.1007/s43681-022-00229-6,Reexamining computer ethics in light of AI systems and AI regulation,"<jats:title>Abstract</jats:title><jats:p>This article argues that the emergence of AI systems and AI regulation showcases developments that have significant implications for computer ethics and make it necessary to reexamine some key assumptions of the discipline. Focusing on design- and policy-oriented computer ethics, the article investigates new challenges and opportunities that occur in this context. The main challenges concern how an AI system’s technical, social, political, and economic features can hinder a successful application of computer ethics. Yet, the article demonstrates that features of AI systems that potentially interfere with successfully applying some approaches to computer ethics are (often) only contingent, and that computer ethics can influence them. Furthermore, it shows how computer ethics can make use of how power manifests in an AI system’s technical, social, political, and economic features to achieve its goals. Lastly, the article outlines new interdependencies between policy- and design-oriented computer ethics, manifesting as either conflicts or synergies.</jats:p>"
10.1108/ijoes-04-2024-0112,Ethical dimensions of generative AI: a cross-domain analysis using machine learning structural topic modeling,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>The purpose of this study is to comprehensively examine the ethical implications surrounding generative artificial intelligence (AI).</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>Leveraging a novel methodological approach, the study curates a corpus of 364 documents from Scopus spanning 2022 to 2024. Using the term frequency-inverse document frequency (TF-IDF) and structural topic modeling (STM), it quantitatively dissects the thematic essence of the ethical discourse in generative AI across diverse domains, including education, healthcare, businesses and scientific research.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>The results reveal a diverse range of ethical concerns across various sectors impacted by generative AI. In academia, the primary focus is on issues of authenticity and intellectual property, highlighting the challenges of AI-generated content in maintaining academic integrity. In the healthcare sector, the emphasis shifts to the ethical implications of AI in medical decision-making and patient privacy, reflecting concerns about the reliability and security of AI-generated medical advice. The study also uncovers significant ethical discussions in educational and financial settings, demonstrating the broad impact of generative AI on societal and professional practices.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title>
<jats:p>This study provides a foundation for crafting targeted ethical guidelines and regulations for generative AI, informed by a systematic analysis using STM. It highlights the need for dynamic governance and continual monitoring of AI’s evolving ethical landscape, offering a model for future research and policymaking in diverse fields.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>The study introduces a unique methodological combination of TF-IDF and STM to analyze a large academic corpus, offering new insights into the ethical implications of generative AI across multiple domains.</jats:p>
</jats:sec>"
10.21606/drs.2024.731,Communicating the use of generative AI to design students: Fostering ethics rather than teaching it,N/A
10.4018/979-8-3693-1950-5.ch005,Children's Book Visualizations From an Artmaking Generative AI,"<jats:p>How would a well-known artmaking generative AI (A-GAI) go about illustrating children's books? This chapter uses multiple prompting approaches to elicit children's book illustrations for known stories and more original plots to identify some early visual patterns from Deep Dream Generator. The research explores a variety of questions around the quality of the output images in conveying visual meaning in the children's book space (albeit using fictional prompts). This exploratory work provides some early and preliminary insights into this space, including in how to set up a light experiment using an A-GAI.</jats:p>"
10.1007/978-3-031-55642-5_10,Transforming Software Development with Generative AI: Empirical Insights on Collaboration and Workflow,N/A
10.31235/osf.io/ejqan,Adapting to AI: How Will Generative AI Affect Work? How Should We Respond?,<p>A discussion paper directed towards nonspecialist educators who are contemplating the fallout of Generative AI on university curricula in sociology and similar fields.</p>
10.1007/s43681-023-00316-2,Technical challenges and perception: does AI have a PR issue?,"<jats:title>Abstract</jats:title><jats:p>Increasingly, models have been highlighted that not only disadvantage society but those whom the model was originally designed to benefit. An increasing number of legal challenges around the world illustrates this. A surge of recent work has focussed on the technical, legal or regulatory challenges but not necessarily the real-world day to day challenges for practitioners such as data collection or fairness by design. Since the publication of the Holstein et al.’s study in 2019, additional legislation, regulation and multiple bodies have been created to address practitioner challenge. This study asks what, if anything, has improved for practitioners between 2019 and 2022. Study 1 conducts an investigation into real-world needs within industry and asks whether practitioners are now able to mitigate challenges in a more robust manner. A further pilot study on the perception of AI examines whether perception of AI impacts practitioner work. The results show increasing and continuing interdisciplinary issues. Where increased regulation and legislation might have seemed reasonable, the result for practitioners is indecision and overwhelm. Based on these findings, we highlight directions for future research in this area. The most problematic area being human factors.</jats:p>"
10.21428/e4baedd9.0d255ab6,Bringing Worker Voice into Generative AI,N/A
10.31224/3230,"Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol and the NIST-approved Quantum-Resistant Cryptographic Algorithms",N/A
10.1007/s43681-024-00450-5,Ethical implications of AI in the Metaverse,"<jats:title>Abstract</jats:title><jats:p>This paper delves into the ethical implications of AI in the Metaverse through the analysis of real-world case studies, including Horizon Worlds, Decentraland, Roblox, Sansar, and Rec Room. The examination reveals recurring concerns related to content moderation, emphasising the need for a human-AI hybrid approach to strike a balance between creative freedom and user safety. Privacy and data protection emerge as crucial considerations, highlighting the importance of transparent communication and user data control for responsible AI implementation. Additionally, promoting inclusivity and diversity is emphasised, calling for transparent governance, diverse representation, and collaboration with ethics experts to ensure equitable AI practices. By addressing these specific ethical challenges, we can pave the way towards a responsible and user-centric Metaverse, maximising its potential while safeguarding user well-being and rights.</jats:p>"
10.1007/s43681-024-00463-0,Conversational hyperconvergence: an onlife evolution model for conversational AI agency,N/A
10.1201/9781003499527-7,"Generative AI: Types, Skills, Opportunities and Challenges",N/A
10.1101/2024.03.08.24303960,Generative AI Guidelines in Korean Medical Journals: A Survey Using Human-AI Collaboration,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>Generative artificial intelligence (GAI) tools, such as large language models, have the potential to revolutionize medical research and writing, but their use also raises important ethical and practical concerns. This study examines the prevalence and content of GAI guidelines among Korean medical journals to assess the current landscape and inform future policy development.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Top 100 Korean medical journals by H-index were surveyed. Author guidelines were collected and screened by a human author and AI chatbot to identify GAI-related content. Key components of GAI policies were extracted and compared across journals. Journal characteristics associated with GAI guideline adoption were also analyzed.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Only 18% of the surveyed journals had GAI guidelines, which is much lower than previously reported international journals. However, adoption rates increased over time, reaching 57.1% in the first quarter of 2024. Higher-impact journals were more likely to have GAI guidelines. All journals with GAI guidelines required authors to declare GAI use, and 94.4% prohibited AI authorship. Key policy components included emphasizing human responsibility (72.2%), discouraging AI-generated content (44.4%), and exempting basic AI tools (38.9%).</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>While GAI guideline adoption among Korean medical journals is lower than global trends, there is a clear increase in implementation over time. The key components of these guidelines align with international standards, but greater standardization and collaboration are needed to ensure responsible and ethical use of GAI in medical research and writing.</jats:p></jats:sec><jats:sec><jats:title>Abstract Figure</jats:title><jats:fig id=""ufig1"" position=""float"" fig-type=""figure"" orientation=""portrait""><jats:graphic xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""24303960v1_ufig1"" position=""float"" orientation=""portrait"" /></jats:fig></jats:sec>"
10.2139/ssrn.4912701,"Generative AI, Copyright and the AI Act",N/A
10.1007/s43681-022-00200-5,Trust and trustworthiness in AI ethics,"<jats:title>Abstract</jats:title><jats:p>Due to the extensive progress of research in artificial intelligence (AI) as well as its deployment and application, the public debate on AI systems has also gained momentum in recent years. With the publication of the <jats:italic>Ethics Guidelines for Trustworthy AI</jats:italic> (2019), notions of trust and trustworthiness gained particular attention within AI ethics-debates; despite an apparent consensus that AI should be trustworthy, it is less clear what trust and trustworthiness entail in the field of AI. In this paper, I give a detailed overview on the notion of trust employed in AI Ethics Guidelines thus far. Based on that, I assess their overlaps and their omissions from the perspective of practical philosophy. I argue that, currently, AI ethics tends to overload the notion of trustworthiness. It thus runs the risk of becoming a buzzword that cannot be operationalized into a working concept for AI research. What is needed, however, is an approach that is also informed with findings of the research on trust in other fields, for instance, in social sciences and humanities, especially in the field of practical philosophy. This paper is intended as a step in this direction.</jats:p>"
10.1007/979-8-8688-0473-1_4,Building a Generative AI Application,N/A
10.1162/99608f92.2c8e7e81,Institutional Efforts to Help Academic Researchers Implement Generative AI in Research,N/A
10.4018/979-8-3693-3278-8.ch007,Foundations of Generative AI,"<jats:p>The chapter delves into the foundations of generative artificial intelligence (AI), offering an introductory overview and a nuanced understanding of its basic principles, history, and evolution. It navigates through core technologies underpinning generative AI, including neural networks, machine learning models, and key algorithms. The introduction traces generative AI's roots, unraveling its historical trajectory. It progresses to elucidate fundamental concepts, exploring neural networks' structures, functionalities, and applications. The study examines diverse machine learning models and pivotal algorithms crucial to generative AI, shedding light on their roles in generating innovative outputs. This abstract encapsulates a comprehensive journey through generative AI's core elements, serving as a foundational guide for understanding its origins, principles, and technologies.</jats:p>"
10.4018/979-8-3693-5578-7.ch007,Financial Frontiers,"<jats:p>Fintech players have always adopted the latest technology that evolve in the market to stay ahead in this competitive industry. In that adoption curve, we are now standing in the place where we are going to witness the adoption of the Generative AI as the next technological phenomenon that will shape the future of Fintech industry. In this chapter, the authors explore the following areas to getter a deeper insight into how Generative AI tends to play in Fintech industry: 1) Generative AI technologies involved in Fintech, 2) application of Generative AI in Fintech, 3) benefits of adopting Generative AI in Fintech, 4) case studies where Generative AI is adopted, 5) challenges and risks in adopting Generative AI, 6) mitigation strategies for the above said challenges and risks, and 7) future trends and developments.</jats:p>"
10.1007/s43681-023-00352-y,The consequences of AI hype,N/A
10.1007/s43681-024-00526-2,The ethics of envisioning spam free email inboxes,N/A
10.1007/s43681-024-00484-9,Addressing corrigibility in near-future AI systems,"<jats:title>Abstract</jats:title><jats:p>When we discuss future advanced autonomous AI systems, one of the worries is that these systems will be capable enough to resist external intervention, even when such intervention is crucial, for example, when the system is not behaving as intended. The rationale behind such worries is that such intelligent systems will be motivated to resist attempts to modify or shut them down so they can preserve their objectives. To mitigate and face these worries, we want our future systems to be corrigible, i.e., to tolerate, cooperate or assist many forms of outside correction. One important reason for considering corrigibility as an important safety property is that we already know how hard it is to construct AI agents with a generalized enough utility function; and the more advanced and capable the agent is, the more it is unlikely that a complex baseline utility function built into it will be perfect from the start. In this paper, we try to achieve corrigibility in (at least) systems based on known or near-future (imaginable) technology, by endorsing and integrating different approaches to building AI-based systems. Our proposal replaces the attempts to provide a corrigible utility function with the proposed corrigible software architecture; this takes the agency off the RL agent – which now becomes an RL solver – and grants it to the system as a whole.</jats:p>"
10.31234/osf.io/xj54a,Are Generative AIs like Midjourney and Sora creative? The “Generative AI Mary’s room” thought experiment.,"<p>This article examines the debate of whether generative AI systems like Midjourney and Sora can be considered truly creative or are merely remixing existing data. It reviews Finke, Ward, and Smith's creative cognition approach which proposed constraints on human creativity. A new thought experiment, the ""Generative AI Mary's Room"", is proposed to explore whether a person with comprehensive artistic knowledge but no real-world experience could produce original creative works. The argument is that one who believes generative AI is not creative because it works based solely on previous artistic data, should not believe that Mary would be creative either, otherwise, they are inferring mysterious properties to the human mind.</p>"
10.1515/9783111425078-204,About the Editors,N/A
10.1007/s43681-024-00499-2,Algorithm evaluation without autonomy,"<jats:title>Abstract</jats:title><jats:p>In <jats:italic>Algorithms &amp; Autonomy</jats:italic>, Rubel, Castro, and Pham (hereafter RCP), argue that the concept of autonomy is especially central to understanding important moral problems about algorithms. In particular, autonomy plays a role in analyzing the version of social contract theory that they endorse. I argue that although RCP are largely correct in their diagnosis of what is wrong with the algorithms they consider, those diagnoses can be appropriated by moral theories RCP see as in competition with their autonomy based theory. Most notably, proponents of consequentialism and virtue ethics can appropriate RCPs insights. This is good news because RCP’s social contract theory is vulnerable to a well known class of counterexamples. The most significant contribution of RCP, if I am right, is in their identification, presentation, and evaluation of concrete cases involving algorithms and not in the more controversial claims about theoretical ethics that RCP themselves see as central to what they are doing.</jats:p>"
10.1515/9783111425078-205,List of Contributors,N/A
10.1007/978-3-031-46238-2_27,Generative Models for Missing Data,N/A
10.2139/ssrn.4944690,Generative AI Regulation in the UK,N/A
10.31235/osf.io/eh9sk,Predicting the Self with Generative AI,"<p>Here I describe a first attempt to use generative artificial intelligence to build an oracle that predicts future self-descriptions from past self-descriptions. I did this as a small step toward addressing the bigger issue of how predictable we should expect individuals' lives to be. To address that vexing issue, I call for the construction and comparative testing of many general identity forecasting systems.</p>"
10.2139/ssrn.4887072,How Will Generative Ai Impact Communication?,N/A
10.54675/ewzm9535,Guidance for generative AI in education and research,N/A
10.1148/ryai.07262023.podcast,Roundtable Discussion Series - Generative AI (Part 2),N/A
10.54377/aac6-c1f6,The challenges ahead for generative AI,N/A
10.31219/osf.io/2c48n,Generative AI and Research Integrity,"<p>GPT based text generators like ChatGPT or Microsoft Copilot have rapidly become a ""cultural sensation"". This document provides scientific background  and guidance on how to think critically and mindfully about these tools in academic writing and research.</p>"
10.31228/osf.io/bzsn4,Generative AI and Finding the Law,"<p>Legal information science requires, among other things, principles and theories. The article states six principles or considerations that any discussion of generative AI large language models and their role in finding the law must include. The article concludes that law librarianship will increasingly become legal information science and require new paradigms. In addition to the six principles, the article applies ecological holistic media theory to understand the relationship of the legal community’s cognitive authority, institutions, techné (technology, medium and method), geopolitical factors, and the past and future to understand the changes in this information milieu. The article also explains generative AI, and finally, presents some examples of generative AI responses to various legal research problems and the issues that present themselves in such circumstances.</p>"
10.3386/w31161,Generative AI at Work,N/A
10.1007/979-8-8688-0403-8,Generative Artificial Intelligence,N/A
10.1007/978-3-031-46238-2_9,Generative AI Applications in the Health and Well-Being Domain: Virtual and Robotic Assistance and the Need for Niche Language Models (NLMs),N/A
10.1016/b978-0-44-321857-6.00024-2,Index,N/A
10.1201/9781032654829,Regulating Hate Speech Created by Generative AI,N/A
10.1007/s43681-022-00191-3,Correction: Operationalising AI governance through ethics-based auditing: an industry case study,N/A
10.1007/s43681-024-00462-1,AI ethics should be mandatory for schoolchildren,"<jats:title>Abstract</jats:title><jats:p>As society increasingly integrates artificial intelligence (AI) into its fabric, AI ethics education in primary schools becomes necessary. Drawing parallels between the integration of foundational subjects such as languages and mathematics and the pressing need for AI literacy, we argue for mandatory, age-appropriate AI education focusing on technical proficiency and ethical implications. Analogous to how sex and drug education prepare youth for real-world challenges and decisions, AI education is crucial for equipping students to navigate an AI-driven future responsibly. Our study delineates the ethical pillars, such as data privacy and unbiased algorithms, essential for students to grasp, and presents a framework for AI literacy integration in elementary schools. What is needed is a comprehensive, dynamic, and evidence-based approach to AI education, to prepare students for an AI-driven future.</jats:p>"
10.1148/ryai.06302023.podcast,Roundtable Discussion Series - Generative AI (Part 1),N/A
10.2139/ssrn.4860325,Generative AI and Remix: Difference and Repetition,N/A
10.1016/b978-0-44-321857-6.00023-0,Glossary,N/A
10.2139/ssrn.4886590,How will Generative AI impact Communication?,N/A
10.4018/979-8-3693-1565-1.ch007,For Better or for Worse?,"<jats:p>This chapter explores how the social implications of AI are being posited, often sensationalized as a threat to humanity, rather than being framed in something humanly designed that ought to remain within the control of its maker, transparent in terms of capacity to undertake complex decision making and which most importantly is accountable for every individual action made in terms of design and programming. The aims of the chapter are threefold, namely, to consider global ethics and the impact that AI could potentially have in terms of increasing societal inequalities in terms of existing infrastructure; to provide an insight into the developmental and progressive use of AI across organizational infrastructures such as global medicine and health and the military; finally, to embed the concept of ethical AI and the potential for its praxis across all areas of its integration.</jats:p>"
10.1007/s43681-021-00051-6,Two arguments against human-friendly AI,N/A
10.1007/s43681-024-00522-6,Sustainable AI and the third wave of AI ethics: a structural turn,"<jats:title>Abstract</jats:title><jats:p>With the introduction of the concept of Sustainable AI, considerations of the environmental impact of the technology have begun to enter AI ethics discussions. This, Aimee van Wynsberghe suggests, constitutes a new “third wave of AI ethics” which yet needs to be ushered in. In this paper, we ask what is entailed by Sustainable AI that should warrant such special accentuation. Do we find simply run-of-the-mill AI ethics applied to an environmental context? Or does Sustainable AI constitute a true a “game-changer”? We engage in a discussion about what the “waves of AI ethics” ought to mean and the criteria for labelling a wave as such. We argue that the third wave of AI ethics rests on a turn towards a structural approach for uncovering ethical issues on a broader scale, often paired with an analysis of power structures that prevent the uncovering of these issues.</jats:p>"
10.2139/ssrn.4484578,Task-Interdependencies between Generative Ai and Workers,N/A
10.1016/b978-0-44-321857-6.00004-7,Contents,N/A
10.31235/osf.io/rwtzs,Can Generative AI Improve Social Science?,"<p>Artificial intelligence that can produce realistic text,images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might transform social science research. In the first section of this article, I assess the potential of Generative AI to improve online experiments, agent-based models, and automated content analyses.I also discuss whether these tools may help social scientists perform literature reviews, identify novel research questions, and develop hypotheses to explain them. Next, I evaluate whether Generative AI can help social scientists with more mundane tasks such as acquiring advancedprogramming skills or writing more effective prose. In the second section of this article I discuss the limitations of Generative AI as well as how these tools might be employed by researchers in an ethical manner. I discuss how bias in the processes and data used to train these tools can negatively impact social science research as well as a rangeof other challenges related to accuracy, reproducibility,interpretability, and efficiency. I conclude by highlighting the need for increased collaboration between social scientists and artificial intelligence researchers--- not only to ensure that such tools are used in a safe and ethical manner, but also because the progress of artificial intelligence may require deeper understanding of theories of human behavior</p>"
10.1007/s43681-024-00510-w,"Biden’s Executive Order on AI: strengths, weaknesses, and possible reform steps",N/A
10.1007/s43681-024-00478-7,ACESOR: a critical engagement in systems of oppression AI assessment tool,N/A
10.21275/sr231021200626,Process Automation 2.0 with Generative AI Framework,N/A
10.1002/aaai.12155,Generative AI: An AI paradigm shift in the making?,"<jats:title>Abstract</jats:title><jats:p>It is sometimes difficult to evaluate progress in Generative AI, that is, image generation and large language models. This may be because they represent a paradigm shift in AI, and the traditional ways of developing, evaluating, understanding, and deploying AI systems no longer apply. Instead, we need to develop new such approaches, possibly by extending those currently in use in cognitive neuroscience and psychology. In this manner, a new AI paradigm can be created, providing a significant leap in AI research and practice.</jats:p>"
10.1007/s43681-023-00371-9,Using structured ethical techniques to facilitate reasoning in technology ethics,N/A
10.1056/aip2400611,AI as an Ecosystem — Ensuring Generative AI Is Safe and Effective,N/A
10.1007/s00146-024-02028-5,Effects of generative AI on service occupations with social interaction,N/A
10.4324/9781003260127-2,Generative AI in Court,N/A
10.1007/s43681-024-00506-6,Book Review: Ethics of Privacy and Surveillance by Carissa Veliz,N/A
10.1016/b978-0-44-321857-6.00010-2,Shape classification and synthetization via explainable AI,N/A
10.1016/b978-0-44-321857-6.00018-7,Synthetic terrain generation and AI-generated art,N/A
10.1007/s43681-021-00106-8,Towards an ethics of AI in Africa: rule of education,N/A
10.1007/978-3-031-46238-2_20,Generating 3D Reconstructions Using Generative Models,N/A
10.2139/ssrn.4897356,"AI4Tech: X-AI Enabling X-Tech with Human-like, Generative, Decentralized, Humanoid and Metaverse AI",N/A
10.1007/s43681-021-00130-8,‘Data dregs’ and its implications for AI ethics: Revelations from the pandemic,N/A
10.1007/s43681-023-00331-3,Operationalising AI ethics through the agile software development lifecycle: a case study of AI-enabled mobile health applications,"<jats:title>Abstract</jats:title><jats:p>Although numerous ethical principles and guidelines have been proposed to guide the development of artificial intelligence (AI) systems, it has proven difficult to translate these principles into actionable practices beyond mere adherence to ethical ideas. This is particularly challenging in the context of AI systems for healthcare, which requires balancing the potential benefits of the solution against the risks to patients and the wider community, including minorities and underserved populations. To address this challenge, we propose a shift from one-size-fits-all ethical principles to contextualized case-based ethical frameworks. This study uses an AI-enabled mHealth application as a case study. Our framework is built on existing ethical guidelines and principles, including the AI4People framework, the EU High-Level Expert Group on trustworthy AI, and wider human rights considerations. Additionally, we incorporate relational perspectives to address human value concerns and moral tensions between individual rights and public health. Our approach is based on ”ethics by design,” where ethical principles are integrated throughout the entire AI development pipeline, ensuring that ethical considerations are not an afterthought but implemented from the beginning. For our case study, we identified 7 ethical principles: fairness, agility, precision, safeguarding humanity, respect for others, trust and accountability, and robustness and reproducibility. We believe that the best way to mitigate and address ethical consequences is by implementing ethical principles in the software development processes that developers commonly use. Finally, we provide examples of how our case-based framework can be applied in practice, using examples of AI-driven mobile applications in healthcare.</jats:p>"
10.4018/979-8-3693-0831-8.ch001,Generative AI in Higher Education,"<jats:p>This chapter provides a comprehensive exploration of generative artificial intelligence (AI), particularly focusing on its implications and applications in higher education. It discusses the evolution and fundamental concepts of AI, including large language models and their development, emphasizing the intricate processes involved in creating and refining these models. The chapter delves into the ethical considerations and potential biases inherent in AI systems, highlighting the importance of responsible AI development. Moreover, the chapter examines the transformative potential of generative AI in enhancing learning, creativity, and information processing in higher education settings.</jats:p>"
10.4018/979-8-3693-5578-7.ch010,Generative AI for Customized Public Policy in Maritime Transport,"<jats:p>Maritime transport is crucial for global trade, transporting over 90% of global goods by volume and facing challenges such as environmental impacts and security threats. Public policy is the key to addressing these issues through regulations that enhance safety, security, and sustainability. Integrating this sector with generative AI can significantly improve predictive maintenance, route optimisation, and accident prevention. AI technologies can process vast amounts of data for real-time decision-making, which is vital for the efficiency and safety of maritime operations. AI can lead to autonomous ships, optimise logistics, and reduce human error. However, integrating AI poses technical barriers such as cybersecurity, significant initial costs, and social hurdles such as job displacement fears. Effective public policy must evolve to address these challenges and promote AI integration while considering ethical and legal implications.</jats:p>"
10.1007/978-1-4842-9994-4_8,Diffusion Model and Generative AI for Images,N/A
10.1007/978-3-031-46238-2_6,Image Rendering with Generative Adversarial Networks,N/A
10.1007/978-3-031-23035-6_2,Teaching Ethics Applied to AI from a Cultural Standpoint: What African “AI Ethics” for Africa?,"<jats:title>Abstract</jats:title><jats:p>Ethics applied to Artificial Intelligence (AI), improperly called AI ethics, is mainly addressed through a Western perspective focusing on continental philosophy. As a result, discussions on ethics applied to AI are shaped by the West.</jats:p>"
10.1007/s43681-023-00380-8,"Equity, autonomy, and the ethical risks and opportunities of generalist medical AI",N/A
10.1007/s43681-022-00226-9,Rawlsian AI fairness loopholes,"<jats:title>Abstract</jats:title><jats:p>Researchers and industry developers in artificial intelligence (AI) and natural language processing (NLP) have uniformly adopted a Rawlsian definition of fairness. On this definition, a technology is fair if performance is maximized for the least advantaged. We argue this definition has considerable loopholes, which can be used to legitimize common practices in AI/NLP research that actively contributes to social and economic inequalities. Such practices include what we shall refer to as Subgroup Test Ballooning and Snapshot-Representative Evaluation. Subgroup Test Ballooning refers to the practice of initially tailoring a technology to a specific target group of technology-ready early adopters to collect feedback faster. Snapshot-Representative Evaluation refers to the practice of evaluating a technology on a representative sample of current end users. Both strategies may contribute to social and economic inequalities but are commonly justified using arguments familiar from political economics and grounded in Rawlsian fairness. We discuss an egalitarian alternative to Rawlsian fairness, as well as, more generally, the roadblocks on the path toward globally and socially fair AI/NLP research and development.</jats:p>"
10.1007/s43681-020-00011-6,Beyond the promise: implementing ethical AI,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) applications can and do have unintended negative consequences for businesses if not implemented with care. Specifically, faulty or biased AI applications risk compliance and governance breaches and damage to the corporate brand. These issues commonly arise from a number of pitfalls associated with AI development, which include rushed development, a lack of technical understanding, and improper quality assurance, among other factors. To mitigate these risks, a growing number of organisations are working on ethical AI principles and frameworks. However, ethical AI principles alone are not sufficient for ensuring responsible AI use in enterprises. Businesses also require strong, mandated governance controls including tools for managing processes and creating associated audit trails to enforce their principles. Businesses that implement strong governance frameworks, overseen by an ethics board and strengthened with appropriate training, will reduce the risks associated with AI. When applied to AI modelling, the governance will also make it easier for businesses to bring their AI deployments to scale.</jats:p>"
10.4324/9781003482918-1,Using generative AI effectively in higher education,N/A
10.31968/hae.2024.05.38.445,Generative AI and Its Application Trends in the Field of History,N/A
10.1007/s43681-020-00030-3,Who pays for ethical debt in AI?,N/A
10.1007/s43681-024-00483-w,"AI hype, promotional culture, and affective capitalism","<jats:title>Abstract</jats:title><jats:p>This article centres AI hype within promotional culture. It incorporates scholarship on hype, affect and emotion from media, communications and cultural studies, as well as from market studies, to pose the following questions: ‘What role does promotional culture play in AI hype cycles?’ ‘What are the main promotional forms of emotion evident in the 2020s AI hype cycle?’ And finally, ‘What are the ethical implications of promoting emotion in AI hype cycles?’ The article explores the growth of twenty-first century promotional culture, particularly in the global tech sector, before examining links between promotional culture, emotion, affect, media and capitalism. Drawing on interdisciplinary approaches, the article contends that AI hype has successfully persisted because now, more than ever, contemporary promotional culture strategically deploys emotions as part of affective capitalism, and the affective nature of a digital media infrastructure controlled by the tech sector. The ensuing analysis isolates different emotions circulated by AI hype, including doomsday hype, drawing on examples from the 2020s AI hype cycle. The article concludes by examining the ethics of promotional culture as part of the combined knowledge apparatus supporting value construction in AI.</jats:p>"
10.4018/979-8-3693-0831-8.ch002,Some Emerging Communication Roles for Generative AI,"<jats:p>Understanding generative AI (GAI) from a communication perspective is challenging given the breadth of issues and perspectives that have emerged. To help distill these ideas and focus our conversation, this chapter proposes thinking about GAI in terms of the different communication roles that it fulfills. The roles of expert, copilot, interlocutor, and channel are presented with illustrations of each role taken from other contributions to this volume. Across the academy, scholars have mobilized to understand generative artificial intelligence (GAI) and foresee how GAI may alter the horizon of higher education.</jats:p>"
10.1007/s43681-021-00099-4,The impossible necessity of AI governance,N/A
10.1007/s43681-024-00481-y,Tackling AI Hyping,"<jats:title>Abstract</jats:title><jats:p>The introduction of a new generation of AI systems has kicked off another wave of AI hype. Now that AI systems have added the ability to produce new content to their predictive capabilities, extreme excitement about their alleged capabilities and opportunities is matched only by long held fears about job loss and machine control.</jats:p><jats:p>We typically understand the dynamics of AI hype to be something that happens to us, but in this commentary, we propose to flip the script. We suggest that AI hype is not a social fact, but a widely shared practice. We outline some negative implications of this practice and suggest how these can be mitigated, especially with regards to shifting ways of knowing and learning about AI, in the classroom and beyond. Even though pedagogical efforts (broadly understood) have benefited from AI hyping (there is now more varied AI training than ever), such efforts can also help minimize the impacts of hyping on the public’s credulity toward extravagant claims made about AI’s potential benefits and dangers.</jats:p><jats:p>Below, we consider steps that can be taken to address this issue and illustrate pathways for more holistic AI educational approaches that participate to a lesser degree in the practice of AI hyping. We contend that designing better AI futures will require that AI hyping be blunted to enable grounded debates about the ways that AI systems impact people’s lives both now and in the near future.</jats:p>"
10.1007/s43681-022-00160-w,"AI, alignment, and the categorical imperative",N/A
10.1007/s43681-024-00507-5,Metaverse ethics: exploring the social implications of the metaverse,"<jats:title>Abstract</jats:title><jats:p>The emergence of the metaverse transforms the way humans interact with computers; the metaverse brings about a new form of human-computer interaction that is more immersive, intuitive, and seamless. In the present paper we thus aim to elucidate the role of human-computer interactions in the age of the metaverse. New forms of human-computer interaction via the metaverse are beneficial for humans in many ways; at the same time, however, there are new types of social issues that are emerging as the metaverse develops and that need to be taken seriously. Specifically, we focus upon issues such as privacy, surveillance capitalism, cyber-syndromes, amplifications of other social problems, environmental problems, and discuss what regulations would be appropriate in order to balance the adequate development of the metaverse with the safety and security of it that is required for social good, in particular for sustainable development goals. We finally propose ethical design principles for the sustainable metaverse in order to address the aforementioned and other social issues.</jats:p>"
10.1007/s43681-024-00472-z,From applied ethics and ethical principles to virtue and narrative in AI practices,"<jats:title>Abstract</jats:title><jats:p>The question of how we can use ethics and ethical frameworks to avert the negative consequences of AI through guidance on human behaviour and the design of technological systems has recently been receiving increasing attention. The appropriate response to an ethics of AI has certainly been contentious. For some years the wisdom of deontology and utilitarianism in the ethics of technology has been questioned. Today, a kind of AI ethics principlism has gained a degree of widespread acceptance, yet it still invites harsh rejections in recent scholarship. In this paper, we wish to explore the contribution to an ethics of AI made by a narrative philosophy and ethics of technology inspired by the ‘little ethics’ of Paul Ricoeur, and virtue ethics of Alasdair MacIntyre, most recently and promisingly built upon by Wessel Reijers and Mark Coeckelbergh.  The objective of this paper is to examine the extent to which a narrative and virtue based ethics (or, VPD, i.e., virtuous practice design) might be a plausible candidate for the foundation of an ethics of AI, or rather ethical AI practice. This will be achieved by exploring the ways in which this approach can respond to some of the significant faults with or critiques of applied and principles and guidelines based ethical approaches to AI ethics.</jats:p>"
10.1007/s43681-022-00171-7,Operationalising AI governance through ethics-based auditing: an industry case study,"<jats:title>Abstract</jats:title><jats:p>Ethics-based auditing (EBA) is a structured process whereby an entity’s past or present behaviour is assessed for consistency with moral principles or norms. Recently, EBA has attracted much attention as a governance mechanism that may help to bridge the gap between principles and practice in AI ethics. However, important aspects of EBA—such as the feasibility and effectiveness of different auditing procedures—have yet to be substantiated by empirical research. In this article, we address this knowledge gap by providing insights from a longitudinal industry case study. Over 12 months, we observed and analysed the internal activities of AstraZeneca, a biopharmaceutical company, as it prepared for and underwent an ethics-based AI audit. While previous literature concerning EBA has focussed on proposing or analysing evaluation metrics or visualisation techniques, our findings suggest that the main difficulties large multinational organisations face when conducting EBA mirror classical governance challenges. These include ensuring harmonised standards across decentralised organisations, demarcating the scope of the audit, driving internal communication and change management, and measuring actual outcomes. The case study presented in this article contributes to the existing literature by providing a detailed description of the organisational context in which EBA procedures must be integrated to be feasible and effective.</jats:p>"
10.1007/s43681-024-00495-6,Manipulating Aggregate Societal values to Bias AI Social Choice Ethics,N/A
10.1007/s43681-022-00180-6,"Confucius, cyberpunk and Mr. Science: comparing AI ethics principles between China and the EU",N/A
10.21203/rs.3.rs-4540908/v1,Generative AI in Assessment: AI Detectors and Implications for Practice,"<title>Abstract</title>
        <p>Generative AI has garnered attention as a valuable tool in the field of education. However, educators have expressed concerns about the originality of texts submitted by students as assignments due to the potential use of generative AI in writing tasks. Texts in writing assignments can be categorized into three types: AI-generated, human-written, and mixed texts (a combination of AI-generated and human-written texts). The purpose of the current study is to address some concerns from educators on the detection of the texts submitted by students, i.e., the consistency in detection results and accuracy of the detection results. The subject of the current study was the texts submitted by students as assignments for a graduate course. Four detectors were used to analyze the texts. Our findings provided useful information for educators: 1) Within the same detector, the consistency of the detection results for three types of texts were all above 90%. 2) Among different detectors, the detection results of human-written texts exhibited the highest consistency, whereas mixed texts demonstrated the lowest consistency.3) For accuracy, AI-generated and human-written texts were higher than mixed texts. Implications for educational practice were discussed.</p>"
10.4324/9781003459026-2,AI Literacy,N/A
10.2139/ssrn.4918704,Regulating under Uncertainty: Governance Options for Generative AI,N/A
10.1007/s43681-021-00086-9,Preface,N/A
10.21428/9885764c.16cc139d,The EU AI Act at a crossroads: generative AI as a challenge for regulation,N/A
10.1515/9781501519765-013,Chapter 9: Generative AI Cases and Examples,N/A
10.1016/b978-0-44-321857-6.00002-3,Front Matter,N/A
10.36227/techrxiv.171822467.73136863/v2,Generative AI predicts the Riemann zeta zero distribution,N/A
10.4018/979-8-3693-3278-8.ch004,Innovating Reality,"<jats:p>Generative artificial intelligence has enormous promise in business, marketing, finance, education, and healthcare sectors. It can have an impact on areas like consumer engagement and fraud detection. But it also poses difficult problems. Decision-making is hampered by technological barriers like data quality, explainability, and authenticity, as well as economic issues like income inequality and possible job loss. Privacy, bias, and misuse are all examples of ethical dilemmas. To address these, thorough norms that guarantee accountability, openness, and equity are needed. Meeting societal requirements and fostering collaboration requires advancing AI education and human-centric cooperation. Rules and guidelines that emphasise empathy, clarity, and ethical norms must be established to steer AI research and development toward responsible and ethical practices in order to effectively manage these obstacles.</jats:p>"
10.1007/s43681-021-00087-8,Foreword,N/A
10.36227/techrxiv.171822467.73136863/v3,Generative AI predicts the Riemann zeta zero distribution,N/A
10.31234/osf.io/9yhwz,Techniques for supercharging academic writing with generative AI,"<p>Academic writing is an indispensable yet laborious part of the research enterprise. This article maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. It introduces a human–AI collaborative framework that delineates the rationale (“why”), process (“how”), and nature (“what”) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement, their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation), and the need for a learning mindset to avoid overreliance on AI. It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human–AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, it then describes effective prompting techniques for incorporating AI into the writing routine—outlining, drafting, and editing—as well as strategies for maintaining rigor and adhering to ethics and policies. Ultimately, the prudent integration of AI into academic writing can ease the communication burden, empower authors, accelerate discovery, and promote diversity in science.</p>"
10.14742/apubs.2023.662,Exploring business students’ views of the use of generative AI in assignment writing,"<jats:p>
The rise of generative AI, particularly over the past few years, has raised notable issues about its use. This has been possibly most pronounced in academia, where there has been strong debate on the potential value of generative AI to augment learning outcomes versus the potential for academic dishonesty and devalued education. Whilst some papers have looked at students’ perspectives on the use of generative AI, there has been less focus exploring through what ethical perspectives or frames students see using generative AI in their tertiary education.


We interviewed and conducted focus groups and interviews with students enrolled in an Australian university business school, to explore the ethical frames through which they saw the use of generative AI. Focussing on three specific perspectives: Deontological, Consequentialism and Virtue Ethics, it emerged that no single perspective dominated, with students having a complex mix and latticework of ethical perspectives on its use, even within the same individual. We explore some potential implications for practice that emerged from the data, one of which is the role of the academic as moral exemplar. 
</jats:p>"
10.21275/sr24523234811,Salesforce Einstein GPT: Pioneering Generative AI in CRM Technology,N/A
10.1007/s43681-024-00473-y,Algorithmic evidence in U.S criminal sentencing,N/A
10.1007/s43681-023-00318-0,Navigating fairness measures and trade-offs,"<jats:title>Abstract</jats:title><jats:p>To monitor and prevent bias in AI systems, we can use a wide range of (statistical) fairness measures. However, it is mathematically impossible to optimize all of these measures at the same time. In addition, optimizing a fairness measure often greatly reduces the accuracy of the system (Kozodoi et al., Eur J Oper Res 297:1083–1094, 2022). As a result, we need a substantive theory that informs us how to make these decisions and for what reasons. I show that by using Rawls’ notion of justice as fairness, we can create a basis for navigating fairness measures and the accuracy trade-off. In particular, this leads to a principled choice focusing on both the most vulnerable groups and the type of fairness measure that has the biggest impact on that group. This also helps to close part of the gap between philosophical accounts of distributive justice and the fairness literature that has been observed by (Kuppler et al. Distributive justice and fairness metrics in automated decision-making: How much overlap is there? arXiv preprint <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://arxiv.org/abs/2105.01441"">arXiv:2105.01441</jats:ext-link>, 2021), and to operationalise the value of fairness.</jats:p>"
10.21275/sr24703063340,Empowering Patients with AI-Driven Personalized Care: The Transformative Power of Generative AI and Healthcare Data Integration,N/A
10.1007/978-3-031-46238-2_10,Generative Adversarial Network Based Deep Learning Method for Machine Vision Inspection,N/A
10.1007/s43681-024-00458-x,The ethical wisdom of AI developers,"<jats:title>Abstract</jats:title><jats:p>This paper explores ethical wisdom in the artificial intelligence (AI) developer community. Despite robust literature about the need for virtue ethics approaches in AI development, little research has directly engaged with the developer community about their progress in this regard. We have thus conducted semi-structured interviews with a worldwide cohort of 40 developers, which focused on their awareness of ethics issues, how they navigate ethical challenges, and the barriers they encounter in developing ethical wisdom. We find developers are largely aware of the ethical territories they must navigate and the moral dilemmas they personally encounter, but they face limited and inconsistent resources for ethical guidance or training. Furthermore, there are significant barriers inhibiting the development of ethical wisdom in the AI developer community, including the industry’s fixation on innovation, the narrow scope of technical practice, limited provisions for reflection and dialogue, and incentive structures that prioritize profits and prestige. The paper concludes by emphasizing the need to address the gap in domain-specific ethical skill and provides recommendations for organizations, educators, and the AI developer community.</jats:p>"
10.1007/s43681-024-00490-x,Assuring AI safety: fallible knowledge and the Gricean maxims,"<jats:title>Abstract</jats:title><jats:p>In this paper we argue that safety claims, when justified by a safety case, are descriptive fallible knowledge claims. Even if the aim of a safety case was to justify infallible knowledge about the safety of a system, such infallible safety knowledge is impossible to attain in the case of AI-enabled systems. By their nature AI-enabled systems preclude the possibility of obtaining infallible knowledge concerning their safety or lack thereof. We suggest that one can communicate knowledge of an AI-enabled system’s safety by structuring their exchange according to Paul Grice’s Cooperative Principle which can be achieved via adherence to the Gricean maxims of communication. Furthermore, these same maxims can be used to evaluate the calibre of the exchange, with the aim being to ensure that communicating knowledge about an AI-enabled system’s safety is of the highest calibre, in short, that the communication is relevant, of sufficient quantity and quality, and communicated perspicuously. The high calibre communication of safety claims to an epistemically diverse group of stakeholders is vitally important given the increasingly participatory nature of AI-enabled system design, development and assessment.</jats:p>"
10.4324/9781003482918-12,Using generative AI agents for scalable roleplay activities in the health sciences,N/A
10.4324/9781003482918-16,Embracing generative AI in authentic assessment,N/A
10.4018/979-8-3693-1950-5.ch001,Revisiting the Key Components of Creativity Through Generative AI,"<jats:p>Today, advancements in artificial intelligence (AI) and its applications have resulted in its widespread use in creative industries. The goal of this study is to investigate the relationship between artificial intelligence and creativity through the perspective of Generative AI (GenAI) utilized in these industries. To perform this investigation, the study first introduces the concepts of creativity and its key components, along with GenAI and AI creativity. The study's analysis is founded on 14 key components of creativity identified in the previous literature. In the analysis section, the study examines whether these key components are present in today's GenAI tools, drawing on current debates about AI and AI-based applications. Additionally, the capabilities, limitations, and challenges of GenAI are investigated for each key component. In the discussion section, the study makes projections about potential problems that may be encountered when using GenAI and discusses possible redefinitions of key components in the future.</jats:p>"
10.4018/979-8-3693-2418-9.ch014,Transforming Assessments With Generative AI,"<jats:p>The potential of generative AI is evident in every field of life and education is also experiencing a paradigm shift. Generative AI is opening new ways of assessment and the tools that can create engaging and innovative contents. Through the promotion of adaptation and customization, generative AI is positioned to bring about a significant transformation in the educational process. This chapter sheds light on the significance of generative AI in assessment of higher education. It offers valuable insights into the possibilities for change and improvement in the field of educational evaluations, indicating its capacity to revolutionize the future of education.</jats:p>"
10.1007/s43681-024-00475-w,Evaluating approaches for reducing catastrophic risks from AI,"<jats:title>Abstract</jats:title><jats:p>According to a growing number of researchers, AI may pose catastrophic – or even existential – risks to humanity. Catastrophic risks may be taken to be risks of 100 million human deaths, or a similarly bad outcome. I argue that such risks – while contested – are sufficiently likely to demand rigorous discussion of potential societal responses. Subsequently, I propose four desiderata for approaches to the reduction of catastrophic risks from AI. The quality of such approaches can be assessed by their chance of success, degree of beneficence, degree of non-maleficence, and beneficent side effects. Then, I employ these desiderata to evaluate the promises, limitations and risks of alignment research, timelines research, policy research, halting or slowing down AI research, and compute governance for tackling catastrophic AI risks. While more research is needed, this investigation shows that several approaches for dealing with catastrophic AI risks are available, and where their respective strengths and weaknesses lie. It turns out that many approaches are complementary and that the approaches have a nuanced relationship to approaches to present AI harms. While some approaches are similarly useful for addressing catastrophic risks and present harms, this is not always the case.</jats:p>"
10.1007/s43681-024-00423-8,How AI hype impacts the LGBTQ + community,"<jats:title>Abstract</jats:title><jats:p>Hype around Artificial Intelligence (AI) has been a feature of this technology since its inception. However, the most recent wave of AI hype has been leveraged to encourage adoption of AI technologies that cause issues for marginalised communities. Hype is also a means to obfuscate real issues of bias, harm, and exploitation felt most sharply by marginalised communities when AI is implemented. This therefore raises the question of power imbalances as a feature of AI technologies as we currently know them. This paper will study the relationship of AI hype and marginalised communities, with particular emphasis on the LGBTQ + community, and look at the way that AI impacts on this community. This paper will pose two key questions: does hype affect marginalised communities, particularly hype around new technologies such as AI; and what impact does the LGBTQ + community experience as a result of hype. This paper will then move on to discuss areas that provide a focus for discourse of AI hype and the impact on the LGBTQ + community: policy and decision-making, the maintenance of the cisgender heteronormative (cishet) baseline, the ubiquity of a mythology of AI, and the role of market expansion.</jats:p>"
10.1117/12.3009202.13bde57c-0346-ee11-a99c-00505691c5e1,N/A,N/A
10.36227/techrxiv.171822467.73136863/v1,Generative AI predicts the Riemann zeta zero distribution,"<jats:p id=""p1"">The Transformer architecture of Generative AI is very successful in
predicting the distribution of Riemann zeta zero counts on consecutive
Gram intervals. We get accuracies of 0.998 in predicting a sequence of
ten consecutive zero counts. We tested with two ranges of Riemann zeta
zeros, t = 10^12 and t = 10^28. With special training for rare
events, we can get essentially full prediction. This shows that applying
the technique to more complex problems has great promise. We have used
very minimal computer resources compared to typical models in language
applications. With access to better resources, we can attack much more
important problems.</jats:p>"
10.4018/979-8-3693-0074-9.ch002,Unleashing the Potential,"<jats:p>Generative artificial intelligence, anchored by large language models (LLMs), is significantly altering the educational landscape. This chapter examines the impact of generative AI on education, illustrating its capability to create personalized content and transform learning environments. Despite concerns over academic dishonesty facilitated by LLMs, the chapter argues against a regressive stance and advocates for the constructive integration of AI into educational practices. By drawing on theories of learning, the chapter elucidates the pedagogical implications of generative AI and describes specific use cases in language learning, computer science, and mathematics. Highlighting both the potential and limitations of this emerging technology, the chapter posits that generative AI is not merely a disruptive force, but a revolutionary tool poised to redefine the methodologies of teaching and learning.</jats:p>"
10.4018/979-8-3693-3278-8.ch003,Navigating Uncharted Waters,"<jats:p>This chapter explores the evolution and future trajectory of Python-driven generative AI, highlighting Python's role in advancing this technology. It discusses the integration of Python with emerging technologies like neuromorphic computing and reinforcement learning, focusing on their potential to revolutionize art, design, and media. Through detailed analysis of the open-source implementation of a technology platform designed, the chapter provides insights into Python's facilitation of innovative AI applications. It addresses potential challenges and ethical considerations, along with the mitigation and call to action, emphasizing the importance of responsible innovation. The narrative underscores Python's influence in making advanced AI technologies accessible and scalable, preparing readers to engage with future developments in the field of generative AI.</jats:p>"
10.5220/0012729800003717,Generative AI Risk Management in Digital Economy,N/A
10.1007/s43681-021-00047-2,"Community-in-the-loop: towards pluralistic value creation in AI, or—why AI needs business ethics","<jats:title>Abstract</jats:title><jats:p>Today, due to growing computing power and the increasing availability of high-quality datasets, artificial intelligence (AI) technologies are entering many areas of our everyday life. Thereby, however, significant ethical concerns arise, including issues of fairness, privacy and human autonomy. By aggregating current concerns and criticisms, we identify five crucial shortcomings of the current debate on the ethics of AI. On the threshold of a third wave of AI ethics, we find that the field eventually fails to take sufficient account of the business context and deep societal value conflicts the use of AI systems may evoke. For even a perfectly fair AI system, regardless of its feasibility, may be ethically problematic, a too narrow focus on the ethical implications of technical systems alone seems insufficient. Therefore, we introduce a business ethics perspective based on the normative theory of contractualism and conceptualise ethical implications as conflicts between values of diverse stakeholders. We argue that such value conflicts can be resolved by an account of deliberative order ethics holding that stakeholders of an economic community deliberate the costs and benefits and agree on rules for acceptable trade-offs when AI systems are employed. This allows AI ethics to consider business practices, to recognise the role of firms, and ethical AI not being at risk to provide a competitive disadvantage or in conflict with the current functioning of economic markets. By introducing deliberative order ethics, we thus seek to do justice to the fundamental normative and political dimensions at the core of AI ethics.</jats:p>"
10.1007/s43681-023-00355-9,Scoring AI-generated policy recommendations with Risk-Adjusted Gain in Net Present Happiness,"<jats:title>Abstract</jats:title><jats:p>Ethical considerations for assessing the collective benefit of an AI’s policy recommendations are different from assessing the ethical consequences from interacting with an individual. The study of population ethics provides a framework for studying collective benefit or harm in abstract terms. Research into happiness has made significant strides in identifying some key drivers of subjective well-being as measured both individually and collectively across societies. This research examines models from population ethics and statistical studies of subjective well-being to create a measure of benefit with which to judge AI recommendations. These models include refining estimations of the interaction between cultural aspects and economic development and incorporating measures of inequality of happiness and satisfaction through a society. When the impacts of a proposed policy are simulated for multiple successive years, risk discounting is used to measure Net Present Happiness, thus solving the conundrum of considering future generations in ethical considerations as posed in population ethics. Lastly, the Risk-Adjusted Gain in Net Present Happiness is proposed as a reasonable approach to ranking AI policy recommendations and as an AI objective function.</jats:p>"
10.1007/s43681-022-00241-w,Making sense of the conceptual nonsense ‘trustworthy AI’,N/A
10.1007/s43681-024-00512-8,Command responsibility in military AI contexts: balancing theory and practicality,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) has found extensive applications to varying degrees across diverse domains, including the possibility of using it within military contexts for making decisions that can have moral consequences. A recurring challenge in this area concerns the allocation of moral responsibility in the case of negative AI-induced outcomes. Some scholars posit the existence of an insurmountable “responsibility gap”, wherein neither the AI system nor the human agents involved can or should be held responsible. Conversely, other scholars dispute the presence of such gaps or propose potential solutions. One solution that frequently emerges in the literature on AI ethics is the concept of command responsibility, wherein human agents may be held responsible because they perform a supervisory role over the (subordinate) AI. In the article we examine the compatibility of command responsibility in light of recent empirical studies and psychological evidence, aiming to anchor discussions in empirical realities rather than relying exclusively on normative arguments. Our argument can be succinctly summarized as follows: (1) while the theoretical foundation of command responsibility appears robust (2) its practical implementation raises significant concerns, (3) yet these concerns alone should not entirely preclude its application (4) they underscore the importance of considering and integrating empirical evidence into ethical discussions.</jats:p>"
10.1007/s43681-021-00122-8,Blind spots in AI ethics,"<jats:title>Abstract</jats:title><jats:p>This paper critically discusses blind spots in AI ethics. AI ethics discourses typically stick to a certain set of topics concerning principles evolving mainly around explainability, fairness, and privacy. All these principles can be framed in a way that enables their operationalization by technical means. However, this requires stripping down the multidimensionality of very complex social constructs to something that is idealized, measurable, and calculable. Consequently, rather conservative, mainstream notions of the mentioned principles are conveyed, whereas critical research, alternative perspectives, and non-ideal approaches are largely neglected. Hence, one part of the paper considers specific blind spots regarding the very topics AI ethics focusses on. The other part, then, critically discusses blind spots regarding to topics that hold significant ethical importance but are hardly or not discussed at all in AI ethics. Here, the paper focuses on negative externalities of AI systems, exemplarily discussing the casualization of clickwork, AI ethics’ strict anthropocentrism, and AI’s environmental impact. Ultimately, the paper is intended to be a critical commentary on the ongoing development of the field of AI ethics. It makes the case for a rediscovery of the strength of ethics in the AI field, namely its sensitivity to suffering and harms that are caused by and connected to AI technologies.</jats:p>"
10.1007/s43681-022-00148-6,"GPT-3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry","<jats:title>Abstract</jats:title><jats:p>This paper examines the ethical solutions raised in response to OpenAI’s language model Generative Pre-trained Transformer-3 (GPT-3) a year and a half from its release. I argue that hype and fear about GPT-3, even within the Natural Language Processing (NLP) industry and AI ethics, have often been underpinned by technologically deterministic perspectives. These perspectives emphasise the autonomy of the language model rather than the autonomy of human actors in AI systems. I highlight the existence of deterministic perspectives in the current AI discourse (which range from technological utopianism to dystopianism), with a specific focus on the two issues of: (1) GPT-3’s potential intentional misuse for manipulation and (2) unintentional harm caused by bias. In response, I find that a contextual approach to GPT-3, which is centred upon wider ecologies of societal harm and benefit, human autonomy, and human values, illuminates practical solutions to concerns about manipulation and bias. Additionally, although OpenAI’s newest 2022 language model InstructGPT represents a small step in reducing toxic language and aligning GPT-3 with user intent, it does not provide any compelling solutions to manipulation or bias. Therefore, I argue that solutions to address these issues must focus on organisational settings as a precondition for ethical decision-making in AI, and high-quality curated datasets as a precondition for less harmful language model outputs.</jats:p>"
10.1007/s43681-024-00520-8,AI ethics in a controversial industry: the case of gambling and its ethical paradox,N/A
10.1007/s43681-020-00031-2,Opening the path to ethics in artificial intelligence,N/A
10.21428/e4baedd9.9070dfe7,From Novel Chemicals to Opera,N/A
10.26226/m.6639c0e2eb191bbe9d92661f,Using Generative AI To Create Virtual Patients,N/A
10.1007/978-3-031-55642-5_7,Generative AI for Software Development: A Family of Studies on Code Generation,N/A
10.4018/979-8-3693-2418-9.ch009,Generative AI in Curriculum Development in Higher Education,"<jats:p>This chapter explores the transformative role of Generative Artificial Intelligence (Generative AI) in reshaping the development of higher education curricula. Generative AI, as exemplified by advanced models like GPT-3, employs sophisticated algorithms to generate scientifically relevant content, surpassing traditional norms of teaching and learning. The overview delves into the fundamental principles of Generative AI, emphasizing the significance of generative models such as Generative Adversarial Networks (GANs) and the technical intricacies involved in their training. Essentially, the discourse on the significance of Generative AI in curriculum development underscores its disruptive potential in education. By providing personalized and adaptable pathways for growth, Generative AI addresses the diverse needs of students, fostering engagement and comprehension. It also underscores the role of Generative AI in overcoming limitations in traditional education, facilitating the creation of virtual laboratories and simulations that enhance hands-on learning.</jats:p>"
10.1007/978-3-031-46238-2_12,Augmenting Data from Epileptic Brain Seizures Using Deep Generative Networks,N/A
10.1007/s43681-022-00162-8,A framework for assessing AI ethics with applications to cybersecurity,"<jats:title>Abstract</jats:title><jats:p>In the last few years many scholars, public and private organizations have been involved in the definition of guidelines and frameworks for individuating the principles to adopt in the development and deployment of AI systems. Some authors, however, noted that the effectiveness of these guidelines or ethical codes on the developer’s community is very marginal. One of the obstacles that opposes to the effective implementation of ethical principles is the lack of an approach for solving tensions which arise when principles are applied. A possible solution to such an issue could be the adoption of a risk-based approach which is also advocated by many sources. To our knowledge, no concrete proposals have been presented in literature on how to perform a risk-based ethical assessment. In this paper we contribute to close this gap by introducing a framework based on a qualitative risk analysis approach for assessing the ethical impact underneath the introduction of an innovation either technological or organizational in a system. We will also show how the framework can be used for individuating suitable safeguards to adopt for balancing potential ethical infringements that the innovation may entail once implemented. Some case studies in the cybersecurity context are also described for showing the effectiveness of our approach.</jats:p>"
10.1007/s43681-022-00220-1,Proportionality principle for the ethics of artificial intelligence,N/A
10.4018/979-8-3693-0831-8.ch013,AI Monsters,"<jats:p>Research into perceptions of artificial intelligence (AI) by faculty and students outside of specific disciplines has been relatively sparse. With the recent release of ChatGPT in November 2022, there have been numerous inquiries into the role of generative AI (GAI), in particular. While a timely response is important, so is ensuring that the responses that universities and faculty are implementing are evidence based. In the spring 2023 semester, the authors surveyed 380 students and 276 faculty. The quantitative data was analyzed with implications for higher education, including student-faculty trust, academic integrity, and uncertainty. This chapter is an analysis of the open-ended responses, using “Monster Theory” as a framework for understanding the themes that underlie the perceptions evident in the responses. The authors “demonsterize” AI. This is a mix of promoting literacy, ethical and transparent use, and developing language that is mindful about practices that may either empower or disempower individuals.</jats:p>"
10.2217/fmai-2023-0004,Generative AI for medical imaging analysis and applications,"<jats:p> Generative AI plays a pivotal role in medical imaging analysis, enabling precise diagnosis, treatment planning and disease monitoring. Techniques like generative adversarial networks (GANs) and variational autoencoders (VAEs) enhance medical imaging by generating synthetic images, improving reconstruction, segmentation and facilitating disease diagnosis and treatment planning. Nonetheless, ethical, legal and regulatory concerns arise regarding patient privacy, data protection and fairness. This paper offers an overview of generative AI in medical imaging analysis, highlighting applications, challenges and case studies. It compares results with traditional methods and examines potential implications on healthcare policies. The paper concludes with recommendations for responsible implementation and suggests future research and development directions. </jats:p>"
10.1007/s43681-021-00115-7,Thinking AI with a hammer. Kate Crawford’s Atlas of AI (2021),N/A
10.55248/gengpi.4.1223.123417,Revolutionizing Supply Chains Using Power of Generative AI,N/A
10.1007/979-8-8688-0419-9_18,"Copilot, Generative AI, and Future of Work",N/A
10.1007/s43681-022-00214-z,Ethics in human–AI teaming: principles and perspectives,"<jats:title>Abstract</jats:title><jats:p>Ethical considerations are the fabric of society, and they foster cooperation, help, and sacrifice for the greater good. Advances in AI create a greater need to examine ethical considerations involving the development and implementation of such systems. Integrating ethics into artificial intelligence-based programs is crucial for preventing negative outcomes, such as privacy breaches and biased decision making. Human–AI teaming (HAIT) presents additional challenges, as the ethical principles and moral theories that provide justification for them are not yet computable by machines. To that effect, models of human judgments and decision making, such as the agent-deed-consequence (ADC) model, will be crucial to inform the ethical guidance functions in AI team mates and to clarify how and why humans (dis)trust machines. The current paper will examine the ADC model as it is applied to the context of HAIT, and the challenges associated with the use of human-centric ethical considerations when applied to an AI context.</jats:p>"
10.2139/ssrn.4814018,BILETA's Response to ICO’s Generative AI First Call for Evidence: The Lawful Basis for Web Scraping to Train Generative AI Models,N/A
10.4018/979-8-3693-1351-0.ch011,Empowering Teachers With Generative AI Tools and Support,"<jats:p>This chapter explores the potential of generative AI to transform education and empower teachers. It analyzes pedagogical enhancements enabled by these tools, including personalized and simulated learning, tailored assessments, teacher-student collaboration, and streamlined workflows. The author examines how AI's data-driven insights can boost responsiveness, motivation, inclusion, and experiential understanding. While promising, integrating AI demands thoughtful oversight to uphold humanistic values. The author emphasizes that teacher wisdom must direct implementation ethically, and learners need support developing AI literacy. Though rapidly advancing, generative tools should empower, not replace, educator and student agency. This chapter provides a balanced analysis of AI's possibilities and prudent perspectives for education. With ethical foundations uplifting expertise and learner voices alike, classrooms can judiciously leverage AI to expand responsive, enriched learning benefitting all.</jats:p>"
10.31992/0869-3617-2024-33-2-31-53,Ethics and AI-Plagiarism in an Academic Environment: Students’ Understanding of Compliance with Author’s Ethics and the Problem of Plagiarism in the Process of Interaction with Generative Artificial Intelligence,"<jats:p>Everyday, artificial intelligence (AI) is being increasingly integrated into the teaching and learning process at Russian universities. The high level of quality of feedback from AI tools leads to the spread of AI plagiarism – unauthorized borrowing of generative AI materials – among students. The purpose of this study is to: a) highlight aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of plagiarism when interacting with generative AI; b) develop a questionnaire to determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism; c) conduct an online survey of university students, analyze and discuss the results obtained. The paper highlights five aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism when completing educational assignments and preparing research texts: a) students’ general understanding of the issues of compliance with author’s ethics and the problem of plagiarism in an academic environment; b) students’ experience of AI tools for educational purposes; c) students’ understanding of the problem of AI plagiarism and attitude towards borrowing materials from generative AI; d) teachers’ actions to prevent AI plagiarism among students; e) the policy of educational organizations regarding student compliance with ethics and AI plagiarism. An online questionnaire was developed to determine the degree to which students understand the issues of compliance with copyright ethics and the problem of AI plagiarism. 1,599 students from 29 universities of the Russian Federation took part in the survey. The results showed that in general, in the Russian student community, plagiarism is a widespread social phenomenon, many types of which are perceived by young people as a norm of academic behavior. Despite the relatively high awareness of students in the field of AI technologies, the extremely rare use by teachers of specialized subject disciplines of AI tools in the educational process I’d the reason for the current low level of spread of AI plagiarism in the academic environment. At the same time, it is necessary to state that students lack a systematic understanding of exactly how they can “legally” use generative AI materials and what exactly will be considered AI plagiarism. According to students, the importance of understanding the issues of compliance with author ethics and the problem of AI plagiarism will depend, on the one hand, on the actions of teachers to explain to students the rules for using generative AI materials, and on the other hand, the presence in universities of a regulatory framework regulating the field and the extent to which students use AI in the educational process.</jats:p>"
10.1007/s43681-022-00224-x,"What kind of trust does AI deserve, if any?",N/A
10.2139/ssrn.4951041,Raising Ai Ethics Awareness Through an Ai Ethics Quiz for Software Practitioners,N/A
10.1007/s43681-020-00003-6,AI and ethics,N/A
10.1007/s43681-023-00302-8,A global perspective on data powering responsible AI solutions in health applications,N/A
10.1007/s43681-020-00012-5,A choices framework for the responsible use of AI,N/A
10.1007/s43681-024-00502-w,Exploiting the margin: How capitalism fuels AI at the expense of minoritized groups,"<jats:title>Abstract</jats:title><jats:p>This paper explores the intricate relationship between capitalism, racial injustice, and artificial intelligence (AI), arguing that AI acts as a contemporary vehicle for age-old forms of exploitation. By linking historical patterns of racial and economic oppression with current AI practices, this study illustrates how modern technology perpetuates and deepens societal inequalities. It specifically examines how AI is implicated in the exploitation of marginalized communities through underpaid labor in the gig economy, the perpetuation of biases in algorithmic decision-making, and the reinforcement of systemic barriers that prevent these groups from benefiting equitably from technological advances. Furthermore, the paper discusses the role of AI in extending and intensifying the social, economic, and psychological burdens faced by these communities, highlighting the problematic use of AI in surveillance, law enforcement, and mental health contexts. The analysis concludes with a call for transformative changes in how AI is developed and deployed. Advocating for a reevaluation of the values driving AI innovation, the paper promotes an approach that integrates social justice and equity into the core of technological design and policy. This shift is crucial for ensuring that AI serves as a tool for societal improvement, fostering empowerment and healing rather than deepening existing divides.</jats:p>"
10.1162/99608f92.e360e42d,From Left Behind to Left Out: Generative AI or the Next Pain of the Unconnected,N/A
10.4018/979-8-3693-1351-0.ch001,Standing on the Shoulders of Generative AI,"<jats:p>Generative AI has been gaining popularity in 2023 and it is causing a disruption of various standards in the education system. While the pros and particularly the cons of this technology have been extensively debated, this chapter aims to explore the positive aspects of Generative AI—instead of advocating for a ban. This chapter will first provide an overview of the historical context and evolution of AI. It will be followed by a discussion of different types of Generative AI and the principles of co-creation with it. The uses of Generative AI in education will be described, focusing on the key stakeholders such as educators, students, educational administrators, and schools or institutions. Next, the chapter will explore the Generative AI application across different fields as well as various subjects in education. Several use cases, practices, and their associated benefits will be presented. Finally, the future and the implications of Generative AI will be discussed.</jats:p>"
10.1007/s43681-022-00197-x,Machine learning AI systems and the virtue of inventiveness,N/A
10.1007/s43681-023-00377-3,Navigating in the moral landscape: analysing bias and discrimination in AI through philosophical inquiry,N/A
10.1007/s43681-024-00449-y,‘Hypernudging’: a threat to moral autonomy?,"<jats:title>Abstract</jats:title><jats:p>It is well-recognised that cognitive irrationalities can be exploited to influence behaviour. ‘Hypernudging’ was coined by Karen Yeung to describe a powerful version of this phenomenon seen in digital systems that use large quantities of user data and machine learning to guide decision-making in highly personalised ways. Authors have worried about the societal impacts of the use of these capabilities at scale in commercial systems but have only begun to articulate them concretely. In this paper I look to elucidate one concern of this sort by focusing specifically on the employment of these techniques within social media and considering how it threatens our autonomy in forming moral judgments. By moral judgments I mean our judgments of someone’s actions or character as good versus bad. A threat to our autonomy in forming these is of real concern because moral judgments and their associated beliefs provide a critical backdrop for what is deemed acceptable in society, both individually and collectively and therefore what futures are possible and probable.</jats:p><jats:p>In the first two sections I introduce a psychological model that describes how humans reach moral judgments and the conditions under which it can and cannot be considered autonomous. In the third section I describe how hypernudging within a social media context creates the relevant problematic conditions so as to constitute a threat to our autonomy in forming moral judgments. In the fourth section I explore some practical measures that could be taken to protect moral autonomy. I conclude with some indicative evidence that this threat is not experienced uniformly across all societies, pointing to interesting future areas of research.</jats:p>"
10.1007/s43681-023-00329-x,"Algorithmic bias, generalist models, and clinical medicine",N/A
10.21275/sr24307081508,Unveiling the Potential of Generative AI in Revolutionizing Healthcare,N/A
10.4018/979-8-3693-1565-1.ch004,Ethical Considerations in the Educational Use of Generative AI Technologies,"<jats:p>This chapter provides an overview of the ethical considerations that should be taken into account while using generative AI technologies, specifically in the field of education, as well as concrete suggestions for programmers and end-users. Therefore, students, researchers, and academics in a variety of fields who are interested in the ethical aspects of generative AI will find this chapter useful, as it will also provide an overview of the existing ethical frameworks in the field of education. In that sense, this chapter can be viewed as a concise introduction to the current state of the ethical issues being studied and a proposal for balancing risks and opportunities.</jats:p>"
10.1007/s43681-021-00094-9,"With AI entering organizations, responsible leadership may slip!",N/A
10.1007/s43681-023-00271-y,Tools with general AI and no existential risk,"<jats:title>Abstract</jats:title><jats:p>According to philosophers and scientists in artificial intelligence (AI), future autonomous agents with general AI constitute an existential risk to humanity. This paper leverages results from neuroscience to propose tools with general AI and no existential risk. Tools answering questions in different domains enable the safe exploration of general AI’s enormous potential.</jats:p>"
10.1007/s43681-022-00222-z,“AI for all” is a matter of social justice,N/A
10.1007/s43681-020-00020-5,Brave: what it means to be an AI Ethicist,N/A
10.1007/s43681-024-00454-1,Anthropomorphism and AI hype,"<jats:title>Abstract</jats:title><jats:p>As humans, we have an innate tendency to ascribe human-like qualities to non-human entities. Whilst sometimes helpful, such anthropomorphic projections are often misleading. This commentary considers how anthropomorphising AI contributes to its misrepresentation and hype. First, I outline three manifestations (terminology; imagery; and morality). Then, I consider the extent to which we ought to mitigate it.</jats:p>"
10.1007/s43681-020-00029-w,The interrelation between data and AI ethics in the context of impact assessments,"<jats:title>Abstract</jats:title><jats:p>In the growing literature on artificial intelligence (AI) impact assessments, the literature on data protection impact assessments is heavily referenced. Given the relative maturity of the data protection debate and that it has translated into legal codification, it is indeed a natural place to start for AI. In this article, we anticipate directions in what we believe will become a dominant and impactful forthcoming debate, namely, how to conceptualise the relationship between data protection and AI impact. We begin by discussing the value canvas i.e. the ethical principles that underpin data and AI ethics, and discuss how these are instantiated in the context of value trade-offs when the ethics are applied. Following this, we map three kinds of relationships that can be envisioned between data and AI ethics, and then close with a discussion of asymmetry in value trade-offs when privacy and fairness are concerned.</jats:p>"
10.1007/s43681-023-00308-2,An entryway into technology ethics. Sven Nyholm’s This is Technology Ethics: An Introduction (2023),N/A
10.1093/oxfordhb/9780190067397.013.29,A Human-Centered Approach to AI Ethics,"<p>This chapter explores a human-centered approach to AI and robot ethics. It demonstrates how a human-centered approach can resolve some problems in AI and robot ethics that arise from the fact that AI systems and robots have cognitive states, and yet have no welfare, and are not responsible. In particular, the approach allows that violence toward robots can be wrong even if robots cannot be harmed. More importantly, the approach encourages people to shift away from designing robots as if they were human ethical deliberators. Ultimately, the cognitive states of AI systems and robots may have a role to play in the proper ethical analysis of situations involving them, even if it is not by virtue of conferring welfare or responsibilities on those systems or robots.</p>"
10.4018/979-8-3693-8557-9.ch005,Ethical Frameworks for Use in Artificial Intelligence Systems,"<jats:p>The chapter examines the ethical landscape of AI, focusing on the development and deployment of Responsible AI systems. The increased deployment of systems powered by artificial intelligence technologies necessitates making work more predictable, trustworthy, and ethical. The development and deployment of such systems require the use of Responsible AI. A healthcare system, for example, is a classic system where AI is used in healthcare collaborations and decision-making. Studying the ethical landscape of AI allows us to understand the historical context and evolution of AI Ethics and the challenges and risks associated with such systems. Ethical frameworks examines the six AI assurance goals, the existing ethical frameworks and guidelines, and the core principles behind them. Future challenges and considerations for further research that emerge because of evolving concerns and potential misuse of AI systems are discussed.</jats:p>"
10.1145/3600211.3604722,Typology of Risks of Generative Text-to-Image Models,N/A
10.4018/979-8-3693-1351-0.ch008,Challenges and Limitations of Generative AI in Education,"<jats:p>This chapter presents a comprehensive literature review to identify the challenges and limitations of using generative artificial intelligence (GAI) in education. As a result of screening seven major citation databases, 476 studies were reached. Analysis was carried out on 25 studies selected according to the inclusion and exclusion criteria. Results showed that research on using GAI in education is mostly conducted at the higher education level. The number of studies focusing on lower levels of education is quite low. The challenges and limitations of artificial intelligence are more about general education rather than focusing on a specific discipline. ChatGPT was the most investigated GAI tool. The challenges and limitations of using GAI in education are grouped under five factors: ethics and safety; educational implementations; assessment and evaluation; equity and access; quality control and expertise.</jats:p>"
10.1007/s43681-022-00170-8,"Normative ethics, human rights, and artificial intelligence",N/A
10.21608/erjsh.2024.255372.1256,Generative vs. Non-Generative AI: Analyzing the Effects of AI on the Architectural Design Process,N/A
10.4018/979-8-3693-3278-8.ch006,Generative AI Unleashed,"<jats:p>Generative AI is omnipresent in our daily lives, influencing everything from media and entertainment to personal care and healthcare. The Fourth Industrial Revolution has brought about significant developments in artificial intelligence, such as ChatGPT, which have gained prominence and changed the way data is created and produced. This chapter highlights the current use of AI in natural language processing. These models are based on machine learning. This chapter examines these models' possible benefits to the economy. The potential influence of generative AI on productivity might boost the world economy. All industry sectors will be significantly impacted by generative AI. The economy as a whole can benefit greatly from generative AI's ability to boost labor productivity. We can utilize generative AI's promise to build a more just, inclusive, and sustainable future for all people if we are aware of how it affects society. This chapter offers a comprehensive analysis of the potential exposure of generative AI, in particular to generative pre-trained transformers.</jats:p>"
10.4018/979-8-3693-3278-8.ch002,AI in Visual Arts,"<jats:p>In the past two to four years there have been significant improvements made in AI due to improvements in computing capacity, resulting in an increase in public interest and funding for research. This has led the authors to embark on a project aimed at gaining a deeper understanding of these art generator AIs using generative algorithms such as GANs and VAEs. This chapter begins by providing a brief outline of the historical context and evolution of AI in the arts, tracing its trajectory from early experiments to its current advancements in visual artistry. The subsequent sections of the chapter explore the role of generative algorithms in each artistic medium, starting with an overview of algorithmic painting, followed by an examination of algorithmic sculpture and digital art. In addition, this chapter also introduces four novel features specific to AI-Art. In this chapter, the authors have drawn upon references from various tests published by esteemed researchers and practitioners to gather the necessary insights for the investigation and deepen our understanding.</jats:p>"
10.1007/s43681-024-00427-4,Artificial intelligence (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI,"<jats:title>Abstract</jats:title><jats:p>As Artificial Intelligence (AI) rapidly advances and integrates into various domains, cybersecurity emerges as a critical field grappling with both the benefits and pitfalls of AI technologies. This paper explores the multifaceted dimensions of AI-driven cyberattacks, offering insights into their implications, mitigation strategies, underlying motivations, and profound societal impacts. The research centres on developing and presenting the AI Cybersecurity Dimensions (AICD) Framework, a comprehensive, multidimensional schema designed to guide academics, policymakers, and industry professionals in understanding and combating the evolving challenges posed by AI-driven cyber threats. The research unveils the complex dynamics of offensive AI, stressing the need for adaptive defences and ethical considerations. Concurrently, the study highlights adversarial AI threats, calling for proactive measures to address their potential ramifications. Through rigorous textual analyses and extensive literature reviews, the paper underscores the urgency for interdisciplinary approaches to bridge the technology-humanity chasm traditionally observed in cybersecurity discussions. By synthesising these diverse elements, the AICD Framework emerges as an instrumental tool for holistic understanding and practical interventions in the AI-infused cybersecurity landscape. The paper concludes with an urgent call for collaborative efforts in research and practice to navigate the intricate challenges and capitalise on the opportunities borne from the convergence of AI and cybersecurity.</jats:p>"
10.1007/s43681-024-00446-1,Governing AI through interaction: situated actions as an informal mechanism for AI regulation,"<jats:title>Abstract</jats:title><jats:p>This article presents a perspective that the interplay between high-level ethical principles, ethical praxis, plans, situated actions, and procedural norms influences ethical AI practices. This is grounded in six case studies, drawn from fifty interviews with stakeholders involved in AI governance in Russia. Each case study focuses on a different ethical principle—privacy, fairness, transparency, human oversight, social impact, and accuracy. The paper proposes a feedback loop that emerges from human-AI interactions. This loop begins with the operationalization of high-level ethical principles at the company level into ethical praxis, and plans derived from it. However, real-world implementation introduces situated actions—unforeseen events that challenge the original plans. These turn into procedural norms via routinization and feed back into the understanding of operationalized ethical principles. This feedback loop serves as an informal regulatory mechanism, refining ethical praxis based on contextual experiences. The study underscores the importance of bottom-up experiences in shaping AI's ethical boundaries and calls for policies that acknowledge both high-level principles and emerging micro-level norms. This approach can foster responsive AI governance, rooted in both ethical principles and real-world experiences.</jats:p>"
10.1007/s43681-024-00480-z,A semi-automated software model to support AI ethics compliance assessment of an AI system guided by ethical principles of AI,"<jats:title>Abstract</jats:title><jats:p>Compliance with principles and guidelines for ethical AI has a significant impact on companies engaged in the development of artificial intelligence (AI) systems. Specifically, ethics is a broad concept that continuously evolves over time and across cultural and geographical boundaries. International organisations (IOs), individual states, and private groups, all have an interest in defining the concept of ethics of AI. IOs, as well as regional and national bodies, have issued many decisions on AI ethics. Developing a system that complies with the ethical framework poses a complex challenge for companies, and the consequences of not complying with ethical principles can have severe consequences, making compliance with these requirements a key issue for companies. Furthermore, there is a shortage of technical tools to ensure that such AI systems comply with ethical criteria. The scarcity of ethics compliance checking tools for AI, and the current focus on defining ethical guidelines for AI development, has led us to undertake a proposal consisting in a semi-automated software model to verify the ethical compliance of an AI system’s code. To implement this model, we focus on the following important aspects: (1) a literature review to identify existing ethical compliance systems, (2) a review of principles and guidelines for ethical AI to determine the international and European views regarding AI ethics, and (3) the identification of commonly accepted principles and sub-principles of AI. These elements served to inform (4) our proposal for the design of a semi-automated software for verifying the ethical compliance of AI systems both at design-time (ethics-by-design perspective) and afterwards on the resulting software.</jats:p>"
10.1007/978-3-031-46238-2_5,"Generative Adversarial Network for Synthetic Image Generation Method: Review, Analysis, and Perspective",N/A
10.26904/rf-146-4362433383,Reinventing education through generative AI and XR,N/A
10.18665/sr.320394,Generative AI in Higher Education: The Product Landscape,N/A
10.2139/ssrn.4551316,The Chinese Path to Generative Ai Governance,N/A
10.1007/978-1-4842-9852-7_7,Understanding AI,N/A
10.3386/w31222,Generative AI and Firm Values,N/A
10.21275/sr24304172353,Enhancing Retail Theft Prevention with Generative AI Technologies,N/A
10.1007/s43681-023-00416-z,Ethical AI governance: mapping a research ecosystem,"<jats:title>Abstract</jats:title><jats:p>How do we assess the positive and negative impacts of research about- or research that employs artificial intelligence (AI), and how adequate are existing research governance frameworks for these ends? That concern has seen significant recent attention, with various calls for change, and a plethora of emerging guideline documents across sectors. However, it is not clear what kinds of issues are expressed in research ethics with or on AI at present, nor how resources are drawn on in this process to support the navigation of ethical issues. Research Ethics Committees (RECs) have a well-established history in ethics governance, but there have been concerns about their capacity to adequately govern AI research. However, no study to date has examined the ways that AI-related projects engage with the ethics ecosystem, or its adequacy for this context. This paper analysed a single institution’s ethics applications for research related to AI, applying a socio-material lens to their analysis. Our novel methodology provides an approach to understanding ethics ecosystems across institutions. Our results suggest that existing REC models can effectively support consideration of ethical issues in AI research, we thus propose that any new materials should be embedded in this existing well-established ecosystem.</jats:p>"
10.1007/s43681-023-00348-8,"Human/AI relationships: challenges, downsides, and impacts on human/human relationships",N/A
10.1007/s43681-023-00389-z,Computer vision: AI imaginaries and the Massachusetts Institute of Technology,"<jats:title>Abstract</jats:title><jats:p>This paper explores the way in which computer scientists at the Massachusetts Institute of Technology (MIT) constructed visions of the future in 1960s America to direct the AI and computing research agendas. It argues that MIT computer scientists resisted attempts by the state to control the future of computing by fabricating imaginaries to covertly exert influence over the research environment. The paper examines the impact of the Cold War military–industrial complex on academia, which provided opportunities for research to take place whilst introducing challenges to autonomy. It makes the case that computer scientists such as Marvin Minsky and Fernando J. Corbato carefully shaped narratives across film, television and the media to promote desirable futures centering their own technical approaches. Acknowledging that instruments of the state appealed to the future to guide research towards strategically sensitive areas in the context of Cold War technoscientific contest, it asserts that intensifying ties between military and academic institutions afforded researchers both the latitude and motivation to construct independent visions of the future. In doing so, the paper aims to complicate assumptions about imaginaries as solely tools of governance by highlighting scientists' creativity in navigating institutional constraints to wrest back control of the future.</jats:p>"
10.4018/jdm.2020040105,Artificial Intelligence (AI) Ethics,"<p>Artificial intelligence (AI)-based technology has achieved many great things, such as facial recognition, medical diagnosis, and self-driving cars. AI promises enormous benefits for economic growth, social development, as well as human well-being and safety improvement. However, the low-level of explainability, data biases, data security, data privacy, and ethical problems of AI-based technology pose significant risks for users, developers, humanity, and societies. As AI advances, one critical issue is how to address the ethical and moral challenges associated with AI. Even though the concept of “machine ethics” was proposed around 2006, AI ethics is still in the infancy stage. AI ethics is the field related to the study of ethical issues in AI. To address AI ethics, one needs to consider the ethics of AI and how to build ethical AI. Ethics of AI studies the ethical principles, rules, guidelines, policies, and regulations that are related to AI. Ethical AI is an AI that performs and behaves ethically. One must recognize and understand the potential ethical and moral issues that may be caused by AI to formulate the necessary ethical principles, rules, guidelines, policies, and regulations for AI (i.e., Ethics of AI). With the appropriate ethics of AI, one can then build AI that exhibits ethical behavior (i.e., Ethical AI). This paper will discuss AI ethics by looking at the ethics of AI and ethical AI. What are the perceived ethical and moral issues with AI? What are the general and common ethical principles, rules, guidelines, policies, and regulations that can resolve or at least attenuate these ethical and moral issues with AI? What are some of the necessary features and characteristics of an ethical AI? How to adhere to the ethics of AI to build ethical AI?</p>"
10.1007/s11948-021-00323-8,"Mark Coeckelbergh, AI Ethics, Mit Press, 2021",N/A
10.1007/s43681-022-00158-4,Agency in augmented reality: exploring the ethics of Facebook’s AI-powered predictive recommendation system,N/A
10.1007/s43681-021-00084-x,Putting AI ethics to work: are the tools fit for purpose?,"<jats:title>Abstract</jats:title><jats:p>Bias, unfairness and lack of transparency and accountability in Artificial Intelligence (AI) systems, and the potential for the misuse of predictive models for decision-making have raised concerns about the ethical impact and unintended consequences of new technologies for society across every sector where data-driven innovation is taking place. This paper reviews the landscape of suggested ethical frameworks with a focus on those which go beyond high-level statements of principles and offer practical tools for application of these principles in the production and deployment of systems. This work provides an assessment of these practical frameworks with the lens of known best practices for impact assessment and audit of technology. We review other historical uses of risk assessments and audits and create a typology that allows us to compare current AI ethics tools to Best Practices found in previous methodologies from technology, environment, privacy, finance and engineering. We analyse current AI ethics tools and their support for diverse stakeholders and components of the AI development and deployment lifecycle as well as the types of tools used to facilitate use. From this, we identify gaps in current AI ethics tools in auditing and risk assessment that should be considered going forward.</jats:p>"
10.3102/ip.24.2108118,Using Generative AI for Fairness Inquiry (Poster 4),N/A
10.4018/979-8-3693-2418-9.ch013,"The Interplay Between Creativity, Thinking Styles, Higher Education, and Generative AI","<jats:p>The use of artificial intelligence and especially Generative AI technology is creating radical change in education. This technology can impact studentss creativity, their cognition, preferred thinking styles, and higher order thinking skills. This chapter's main objective is to explore the link betweem creativity, thinking styles and skills, higher education, and GAI. The research question was explored using literature that was screened non-systematically using Google Scholar, EBSCO, and omni-linked databases for articles published between 2019 and 2024 due to the fact that it is a rapidly emerging field of study. It was found that there is an interplay between creativity, thinking styles, education, and GAI. GAI is reshaping our understanding of education and its impact on creativity and thinking styles. It is forcing humans to reconsider and review current and traditional teaching and learning pedagogies.</jats:p>"
10.1007/s43681-024-00549-9,Ethical risk for AI,"<jats:title>Abstract</jats:title><jats:p>The term ‘ethical risk’ often appears in discussions about the responsible development and deployment of artificial intelligence (AI). However, ethical risk remains inconsistently defined in this context, obscuring what distinguishes it from other forms of risk, such as social, reputational or legal risk, for example. In this paper we present a definition of ethical risk for AI as being any risk associated with an AI that may cause stakeholders to fail one or more of their ethical responsibilities towards other stakeholders. To support our definition, we describe how stakeholders have role responsibilities that follow from their relationship with the AI, and that these responsibilities are towards other stakeholders associated with the AI. We discuss how stakeholders may differ in their ability to make decisions about an AI, their exposure to risk, and whether they or others may benefit from these risks. Stakeholders without the ability to make decisions about the risks associated with an AI and how it is used are dependent on other stakeholders with this ability. This relationship places those who depend on decision-making stakeholders at ethical risk of being dominated by them. The decision-making stakeholder is ethically responsible for the risks their decisions about the AI impose on those affected by them. We illustrate our account of ethical risk for AI with two examples: AI-designed attachments for surgical robots that are optimised for treating specific patients, and self-driving ‘robotaxis’ that carry passengers on public roads.</jats:p>"
10.1007/s43681-024-00465-y,Three different types of AI hype in healthcare,"<jats:title>Abstract</jats:title><jats:p>Healthcare systems are the embodiment of big data – as evident in the logistics of resource management, estate maintenance, diagnoses, patient monitoring, research, etc. – such that human health is often heralded as one of the fields most likely to benefit from AI. Yet, the prevalence of hype – both positive and negative – risks undermining that potential by distracting healthcare policy makers, practitioners, and researchers from many of the non-AI factors that will determine its impact. Here we categorise AI hype in healthcare into three types that include both utopian and dystopian narratives and plot a series of more productive paths ahead by which to realise the potential of AI to improve human healthcare.</jats:p>"
10.1007/s43681-021-00043-6,Sustainable AI: AI for sustainability and the sustainability of AI,"<jats:title>Abstract</jats:title><jats:p>While there is a growing effort towards AI <jats:italic>for</jats:italic> Sustainability (e.g. towards the sustainable development goals) it is time to move beyond that and to address the sustainability <jats:italic>of</jats:italic> developing and using AI systems. In this paper I propose a definition of Sustainable AI; Sustainable AI is a movement to foster change in the entire lifecycle of AI products (i.e. idea generation, training, re-tuning, implementation, governance) towards greater ecological integrity and social justice. As such, Sustainable AI is focused on more than AI applications; rather, it addresses the whole sociotechnical system of AI. I have suggested here that Sustainable AI is not about how to sustain the development of AI per say but it is about how to develop AI that is compatible with sustaining environmental resources for current and future generations; economic models for societies; and societal values that are fundamental to a given society. I have articulated that the phrase Sustainable AI be understood as having two branches; AI <jats:italic>for</jats:italic> sustainability and sustainability <jats:italic>of</jats:italic> AI (e.g. reduction of carbon emissions and computing power). I propose that Sustainable AI take sustainable development at the core of its definition with three accompanying tensions between AI innovation and equitable resource distribution; inter and intra-generational justice; and, between environment, society, and economy. This paper is not meant to engage with each of the three pillars of sustainability (i.e. social, economic, environment), and as such the pillars of sustainable AI. Rather, this paper is meant to inspire the reader, the policy maker, the AI ethicist, the AI developer to connect with the environment—to remember that there are environmental costs to AI. Further, to direct funding towards sustainable methods <jats:italic>of</jats:italic> AI.</jats:p>"
10.21275/sr231114203705,Embracing Generative AI in Pharma Regulatory Affairs - An Industry Perspective,N/A
10.1007/s43681-023-00349-7,The moral status of input and output discrimination,N/A
10.1093/oxfordhb/9780190067397.013.45,"The Ethics of AI in Biomedical Research, Patient Care, and Public Health","<p>This chapter explores ethical issues raised by the use of artificial intelligence (AI) in the domain of biomedical research, healthcare provision, and public health. The litany of ethical challenges that AI in medicine raises cannot be addressed sufficiently by current regulatory and ethical frameworks. The chapter then advances the systemic oversight approach as a governance blueprint, which is based on six principles offering guidance as to the desirable features of oversight structures and processes in the domain of data-intense biomedicine: adaptivity, flexibility, inclusiveness, reflexivity, responsiveness, and monitoring (AFIRRM). In the research domain, ethical review committees will have to incorporate reflexive assessment of the scientific and social merits of AI-driven research and, as a consequence, will have to open their ranks to new professional figures such as social scientists. In the domain of patient care, clinical validation is a crucial issue. Hospitals could equip themselves with “clinical AI oversight bodies” charged with the task of advising clinical administrators. Meanwhile, in the public health sphere, the new level of granularity enabled by AI in disease surveillance or health promotion will have to be negotiated at the level of targeted communities.</p>"
10.1007/s00146-024-02054-3,The rise of AI in job applications: a generative adversarial tug-of-war,N/A
10.1007/s43681-024-00492-9,"Artificial intelligence, the common good, and the democratic deficit in AI governance","<jats:title>Abstract</jats:title><jats:p>There is a broad consensus that artificial intelligence should contribute to the common good, but it is not clear what is meant by that. This paper discusses this issue and uses it as a lens for analysing what it calls the “democracy deficit” in current AI governance, which includes a tendency to deny the inherently political character of the issue and to take a technocratic shortcut. It indicates what we may agree on and what is and should be up to (further) deliberation when it comes to AI ethics and AI governance. Inspired by the republican tradition in political theory, it also argues for a more active role of citizens and (end-)users: not only as participants in deliberation but also in ensuring, creatively and communicatively, that AI contributes to the common good.</jats:p>"
10.1007/s43681-022-00175-3,Responsible AI in automated credit scoring systems,N/A
10.1007/s43681-023-00313-5,To democratize or not to democratize AI? That is the question,"<jats:title>Abstract</jats:title><jats:p>This paper advances the debate surrounding whether to democratize AI and explores some of the challenges and benefits of democratization through community-based work and direct democracy. We contend that community-based strategies can incorporate local knowledge and control, thereby providing more effective AI solutions that are human-centric and less harmful. However, democratization needs to be approached with caution and care, since this process requires a deeper understanding of who participates, the decision domain, and the different realities at stake. Moreover, we highlight the importance of participation in AI development to ensure its legitimacy, considering the capacity of this technology to shape reality. We emphasize that participation should be more than just involving stakeholders or seeking input from users. Rather, participation should involve local narratives that generate knowledge and shape information landscapes, thereby producing a different, anti-Cartesian scene. We conclude by underscoring that the success of democratizing AI hinges on the careful delineation of the boundaries of participation, which should include the specific needs of the immediate context, the decision domain, and the various participants involved.</jats:p>"
10.1007/s43681-023-00350-0,Deepfake AI images: should deepfakes be banned in Thailand?,N/A
10.1007/s43681-024-00491-w,The prospects of using AI in euthanasia and physician-assisted suicide: a legal exploration,"<jats:title>Abstract</jats:title><jats:p>The Netherlands was the first country to legalize euthanasia and physician-assisted suicide. This paper offers a first legal perspective on the prospects of using AI in the Dutch practice of euthanasia and physician-assisted suicide. It responds to the Regional Euthanasia Review Committees’ interest in exploring technological solutions to improve current procedures. The specific characteristics of AI – the capability to process enormous amounts of data in a short amount of time and generate new insights in individual cases – may for example alleviate the increased workload of review committees due to the continuous increase of euthanasia cases. The paper considers three broad categories for the use of AI in the Dutch euthanasia practice: (1) the physician’s assessment of euthanasia requests, (2) the actual execution of euthanasia, and (3) the retrospective reviews of cases by the Regional Euthanasia Review Committees. Exploring the legal considerations around each avenue, both in the EU AI Act and the Dutch legal framework, this paper aims to facilitate the societal discussion on the role of technology in such deeply human decisions. This debate is equally relevant to other countries that legalized euthanasia (e.g. Belgium and Canada) or physician-assisted suicide (e.g. Switzerland and numerous states in the US).</jats:p>"
10.1007/s43681-022-00164-6,Knowledge management and ethical vulnerability in AI,N/A
10.21070/ups.3957,Design of a Generative AI Image Similarity Test Application and Handmade Images Using Deep Learning Methods,N/A
10.4018/979-8-3693-1351-0.ch022,Innovative Curriculum Development and Content Creation With Generative AI,"<jats:p>In the digital age, generative AI significantly influences various industries, especially education. It merges with traditional teaching methods, promising a new era of educational possibilities. This chapter delves into generative AI's impact on content creation and curriculum design, discussing its evolution and benefits like producing diverse, scalable educational materials and adaptive curricula personalized for learners. Real-world examples and case studies underscore its practical impact. Nonetheless, the chapter addresses ethical and pedagogical challenges and the complexity of integrating this technology. It also speculates on generative AI's future interactions with emerging technologies and its broader effects on education systems. Targeting educators, policymakers, and edtech enthusiasts, the chapter serves as a guide and insight provider for navigating this evolving landscape responsibly.</jats:p>"
10.1007/s43681-022-00165-5,Trusting social robots,"<jats:title>Abstract</jats:title><jats:p>In this paper, I argue that we need a more robust account of our ability and willingness to trust social robots. I motivate my argument by demonstrating that existing accounts of trust and of trusting social robots are inadequate. I identify that it is the feature of a façade or deception inherent in our engagement with social robots that both facilitates, and is in danger of undermining, trust. Finally, I utilise the fictional dualism model of social robots to clarify that trust in social robots, unlike trust in humans, must rely on an independent judgement of product reliability.</jats:p>"
10.1007/s43681-024-00428-3,How can we design autonomous weapon systems?,N/A
10.2139/ssrn.4330244,"Generative Ai: Here to Stay, But for Good?",N/A
10.2139/ssrn.4929824,How to Responsibly Use Generative Ai in Grant Review,N/A
10.1007/s43681-024-00486-7,A comprehensive study on navigating neuroethics in Cyberspace,N/A
10.1093/oxfordhb/9780190067397.013.28,Social Failure Modes in Technology and the Ethics of AI,"<p>This chapter argues that, just as technological artefacts can break as a result of mechanical, electrical, or other physical defects not fully accounted for in their design, they can also break as a result of social defects not fully accounted for in their design. These failures resulting from social defects can be called <italic>social failures</italic>. The chapter then proposes a definition of <italic>social failure</italic> as well as a taxonomy of <italic>social failure modes</italic>—the underlying causes that lead to social failures. An explicit and detailed understanding of social failure modes, if properly applied in engineering design practice, could result in a fuller evaluation of the social and ethical implications of technology, either during the upstream design and engineering phases of a product, or after its release. Ideally, studying social failure modes will improve people’s ability to anticipate and reduce the rate or severity of undesirable social failures prior to releasing technology into the wild.</p>"
10.1201/9781003503781-5,Conclusions,N/A
10.1007/s43681-021-00057-0,Ethics-by-design: the next frontier of industrialization,N/A
10.5220/0012427100003645,On Augmenting Scenario-Based Modeling with Generative AI,N/A
10.1007/s43681-022-00255-4,Apropos of “Speciesist bias in AI: how AI applications perpetuate discrimination and unfair outcomes against animals”,"<jats:title>Abstract</jats:title><jats:p>The present comment concerns a recent <jats:italic>AI &amp; Ethics</jats:italic> article which purports to report evidence of speciesist bias in various popular computer vision (CV) and natural language processing (NLP) machine learning models described in the literature. I examine the authors’ analysis and show it, ironically, to be prejudicial, often being founded on poorly conceived assumptions and suffering from fallacious and insufficiently rigorous reasoning, its appeal in large part relying on the extant consensus in the community.</jats:p>"
10.4324/9781003507949-4,AI Integration Through Portfolio Development,N/A
10.21428/e4baedd9.8fa181e9,AI for Musical Discovery,N/A
10.18260/1-2--46051,Leveraging the power of multi-modal AI technologies to build and scale generative AI applications,N/A
10.4018/979-8-3693-0502-7.ch011,Revolutionizing Conversational AI,"<jats:p>The emergence of advanced NLP models, like ChatGPT and other conversational AI models, has triggered a revolutionary transformation. This chapter explores the burgeoning field of ChatGPT applications, conducting a comprehensive analysis of their impact across various domains. The chapter assesses their capabilities, challenges, and potential uses, examining the underlying architecture and training methods that enable them to generate contextually relevant and coherent responses. Ethical considerations are also addressed, encompassing concerns about bias, misinformation, and user privacy in real-world conversations. The chapter also acknowledges drawbacks, including occasional inaccuracies or sensitive content generation. In conclusion, ongoing research is vital to enhance model robustness, user experience, and ethical deployment in conversational AI. ChatGPT and similar models are poised to reshape human-machine communication, fostering dynamic, engaging, and valuable conversations.</jats:p>"
10.1007/s43681-024-00468-9,Measuring adherence to AI ethics: a methodology for assessing adherence to ethical principles in the use case of AI-enabled credit scoring application,"<jats:title>Abstract</jats:title><jats:p>This article discusses the critical need to find solutions for ethically assessing artificial intelligence systems, underlining the importance of ethical principles in designing, developing, and employing these systems to enhance their acceptance in society. In particular, measuring AI applications’ adherence to ethical principles is determined to be a major concern. This research proposes a methodology for measuring an application’s adherence to acknowledged ethical principles. The proposed concept is grounded in existing research on quantification, specifically, Expert Workshop, which serves as a foundation of this study. The suggested method is tested on the use case of AI-enabled Credit Scoring applications using the ethical principle of transparency as an example. AI development, AI Ethics, finance, and regulation experts were invited to a workshop. The study’s findings underscore the importance of ethical AI implementation and highlight benefits and limitations for measuring ethical adherence. A proposed methodology thus offers insights into a foundation for future AI ethics assessments within and outside the financial industry, promoting responsible AI practices and constructive dialogue.</jats:p>"
10.1007/s43681-022-00181-5,Needs-aware artificial intelligence: AI that ‘serves [human] needs’,N/A
10.1007/s43681-020-00007-2,"AI for climate: freedom, justice, and other ethical and political challenges",N/A
10.1007/s43681-023-00383-5,"What would strong AI understand consent to mean, and what are the implications for sexbot rape?",N/A
10.1007/s43681-023-00336-y,Resolving the battle of short- vs. long-term AI risks,"<jats:title>Abstract</jats:title><jats:p>AI poses both short- and long-term risks, but the AI ethics and regulatory communities are struggling to agree on how to think two thoughts at the same time. While disagreements over the exact probabilities and impacts of risks will remain, fostering a more productive dialogue will be important. This entails, for example, distinguishing between evaluations of particular risks and the politics of risk. Without proper discussions of AI risk, it will be difficult to properly manage them, and we could end up in a situation where neither short- nor long-term risks are managed and mitigated.</jats:p>"
10.1007/979-8-8688-0456-4,The Early-Career Professional’s Guide to Generative AI,N/A
10.31891/2307-5740-2024-328-53,PRACTICAL RECOMMENDATION OF USING GENERATIVE AI IN BUSINESS,"<jats:p>Generative Artificial Intelligence (GenAI) is transforming the economic landscape by introducing innovative solutions that significantly enhance business efficiency, drive innovation, and create competitive advantages. This article delves into the economic implications and practical applications of GenAI across various business domains, including marketing, customer support, product design, and data analysis. By leveraging advanced AI models, companies can automate routine tasks, generate personalized content, and optimize operations, leading to substantial economic benefits. The implementation of GenAI necessitates a systematic approach, starting with business concept validation and progressing through stages of technical solution identification, proof of technology, and project planning. These stages ensure that AI solutions are economically viable, effective, and aligned with business objectives. Businesses can adopt different strategies to integrate GenAI, ranging from the rapid deployment of third-party applications to the development of customized in-house models. Each approach offers unique economic benefits, balancing customization and control with implementation time and value.
This article provides practical recommendations for strategy of implementing Generative AI, emphasizing the importance of careful economic analysis, stakeholder engagement, and continuous improvement. By adopting a structured approach and selecting the appropriate integration strategy, businesses can harness the transformative power of GenAI. This enables them to innovate and thrive in an increasingly competitive economic environment, positioning themselves as leaders in the digital age.</jats:p>"
10.1007/s43681-024-00501-x,What does it mean to be good? The normative and metaethical problem with ‘AI for good’,N/A
10.1007/s43681-023-00401-6,AI risk assessment using ethical dimensions,N/A
10.1787/fa743141-en,"VC investments in generative AI start-ups have boomed since 2022, while VC investments overall and in AI start-ups reached a peak in 2021",N/A
10.1007/s43681-023-00402-5,Conformity assessment under the EU AI act general approach,N/A
10.1007/s43681-023-00379-1,Moral consideration for AI systems by 2030,"<jats:title>Abstract</jats:title><jats:p>This paper makes a simple case for extending moral consideration to some AI systems by 2030. It involves a normative premise and a descriptive premise. The normative premise is that humans have a duty to extend moral consideration to beings that have a non-negligible chance, given the evidence, of being conscious. The descriptive premise is that some AI systems do in fact have a non-negligible chance, given the evidence, of being conscious by 2030. The upshot is that humans have a duty to extend moral consideration to some AI systems by 2030. And if we have a duty to do that, then we plausibly also have a duty to start preparing now, so that we can be ready to treat AI systems with respect and compassion when the time comes.</jats:p>"
10.21428/e4baedd9.3ad85f1c,The Productivity Effects of Generative AI: Evidence from a Field Experiment with GitHub Copilot,N/A
10.1177/14614448241234040,Impact of misinformation from generative AI on user information processing: How people understand misinformation from generative AI,"<jats:p> This study examines the impact of artificial intelligence (AI) on the ways in which users process and respond to misinformation in generative artificial intelligence (GenAI) contexts. Drawing on the heuristic–systematic model and the concept of diagnosticity, our approach examines a cognitive model for processing misinformation in GenAI. The study’s findings revealed that users with a high-heuristic processing mechanism, which affects positive diagnostic perception, were more likely to proactively discern misinformation than users with low-heuristic processing and low-perceived diagnosticity. When exposed to misinformation from GenAI, users’ perceived diagnosticity of misinformation can be accurately predicted by the ways in which they perform heuristic systematic evaluations. With this focus on misinformation processing, this study provides theoretical insights and relevant recommendations for firms to be more resilient in protecting users from the detrimental impacts of misinformation. </jats:p>"
10.1007/979-8-8688-0447-2_1,An Introduction to Chat and Generative AI,N/A
10.51219/urforum.2023.rosario-moscato,Generative AI and Autonomous Agents: Opportunity and Risks,N/A
10.2139/ssrn.4887438,Addressing the Risks of Generative Ai for the Judiciary: The Accountability Framework(S) Under the EU Ai Act,N/A
10.1007/979-8-8688-0456-4_2,A Brief AI History,N/A
10.1007/s00146-023-01830-x,Escape climate apathy by harnessing the power of generative AI,N/A
10.1007/s43681-023-00337-x,The E.U.’s artificial intelligence act: an ordoliberal assessment,N/A
10.1007/979-8-8688-0456-4_7,Navigating the AI Landscape,N/A
10.1007/s43681-021-00070-3,A critique of the ‘as–if’ approach to machine ethics,"<jats:title>Abstract</jats:title><jats:p>In this paper, I argue that the replication of the effect of ethical decision-making is insufficient for achieving functional morality in artificial moral agents (AMAs). This approach is named the “as–if” approach to machine ethics. I object to this approach on the grounds that the “as if” approach requires one to commit to substantive meta-ethical claims about morality that are at least unwarranted, and perhaps even wrong. To defend this claim, this paper does three things: 1. I explain Heidegger’s Enframing [<jats:italic>Gestell</jats:italic>] and my notion of “Ready-Ethics,” which, in combination, can hopefully provide a plausible account for the motivation behind the “as if” approach; 2. I go over specific examples of Ethical AI projects to show how the “as if” approach commits these projects to versions of moral generalism and moral naturalism. I then explain the flaws of the views that the “as if” approach necessitates, and suggest that they cannot account for the justificatory process crucial to human moral life. I explain how Habermas’ account of the justificatory process could cast doubt on the picture of morality that the meta-ethical views of the “as if” approach proposes; 3. Finally, I defend the relevance of discussing these topics for the purpose of functional morality in AMAs.</jats:p>"
10.1007/s43681-020-00022-3,Management perspective of ethics in artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>This research addressed the management awareness about the ethical and moral aspects of artificial intelligence (AI). It is a general trend to speak about AI, and many start-ups and established companies are communicating about the development and implementation of AI solutions. Therefore, it is important to consider different perspectives besides the technology and data as the key elements for AI systems. The way in which societies are interacting and organising themselves will change. Such transformations require diverse perspectives from the society and particularly from AI system developers for shaping the humanity of the future. This research aimed to overcome this barrier with the answers for the question: What kind of awareness does the management of AI companies have about the social impact of its AI product or service? The central research question was divided into five sub-questions that were answered by a fundamental literature review and an empirical research study. This covered the management understanding of the terms moral, ethics, and artificial intelligence; the internal company prioritization of moral and ethics; and the involved stakeholders in the AI product or service development. It analysed the known and used ethical AI guidelines and principles. In the end, the social responsibility of the management regarding AI systems was analysed and compared.</jats:p>"
10.33140/jmtcm.02.09.01,Synergy in Technology How Generative AI Augments the Capabilities of Customer Data Platforms,"<jats:p>In an era marked by data-driven decision-making, Customer Data Platforms (CDPs) have emerged as pivotal tools for aggregating and analyzing customer data. However, as these platforms grapple with increasingly complex data sets and realtime customer engagement demands, there is a pressing need for more advanced, scalable solutions. This study explores the synergy between Generative Artificial Intelligence (AI) and CDPs, aiming to understand how the integration of generative algorithms can augment the capabilities of these platforms. Employing a multi-method research approach, including case studies, empirical analyses, and expert interviews, this paper investigates various applications of Generative AI within CDPs, such as data augmentation, real-time decision-making, and customer personalization. Moreover, the ethical implications of using generative algorithms, especially concerning data privacy and security, are critically examined. The study reveals that Generative AI can significantly enhance the functionality, performance, and efficiency of CDPs while also posing new questions around ethical considerations. Our findings offer invaluable insights for businesses, marketers, and technologists seeking to leverage the synergistic potential of these two advanced technological paradigms.</jats:p>"
10.1515/9783111425078,Generative AI and LLMs,N/A
10.1007/s43681-023-00351-z,Can machines be trustworthy?,"<jats:title>Abstract</jats:title><jats:p>AI regulators promote ‘trustworthy AI’, but what exactly does trustworthy AI mean, and what does it have to do with trust? Many philosophers argue that the phrase is a contradiction of terms. Trust, unlike reliance, is said to be a uniquely human relationship involving direct responsiveness or intent. I argue that the objective of trustworthy AI can be real trust in the general sense of Karen Jones and others, and very similar to the kind of trust we place in institutions. The idea that trustworthiness does not apply to machines, stems from a <jats:italic>petitio principii</jats:italic> fallacy. We show how to escape this fallacy, providing a better and less anthropomorphic definition of trustworthiness. We briefly discuss how transparency modulates trustworthiness on our revised definition, as well as a possible challenge from intentionality.</jats:p>"
10.4337/9781803926728.00023,"The AI Imaginary: AI, Ethics, and Communication",N/A
10.1007/s00146-024-01948-6,"The work of art in the age of generative AI: aura, liberation, and democratization",N/A
10.1007/s43681-023-00366-6,Reactive agency and technology,"<jats:title>Abstract</jats:title><jats:p>Is there room for genuine human agency in a world populated by almost incessant technological distraction and influence? It often feels as though our technological landscape is pulling us in a number of directions, and that our agency is more a function of us <jats:italic>reacting</jats:italic> to the world as opposed to us exerting our will. In this paper I, explore what it would mean to take these contextual factors seriously and bake them into an account of agency. That is, what if agency is reactive <jats:italic>all the way down</jats:italic>? This is a proposal made by Rüdiger Bittner, who argues that the reason(s) for action are responses to states of affairs in the world. This is in contrast to ‘standard’ views of agency, which explain actions with things like beliefs and desires. Ultimately, I find such a reactive account of agency implausible. However, I think it reveals a potential solution to the ‘new’ problem of all-pervasive technologies: a reactive account does not see these technologies <jats:italic>necessarily</jats:italic> as a threat, but rather focusses our attention on the <jats:italic>ways</jats:italic> in which they change and shape our available context and our possibility to act. While I argue the reactive account goes too far, what I take from it is that our environment offers us various possibilities for action (in the form of affordances), and that we ought to take this seriously in our thinking both about agency and about the impacts of technology. Moreover, there is something to learn from our tendency to ‘fall’ for various ‘temptations’ in our environment, and this justifies further reflection on not only the design of different technologies, but whether such technologies ought to exist at all.</jats:p>"
10.1007/s43681-021-00079-8,"Converged AI, IoT, and blockchain technologies: a conceptual ethics framework",N/A
10.1007/s43681-024-00535-1,Capturing the unobservable in AI development: proposal to account for AI developer practices with ethnographic audit trails (EATs),"<jats:title>Abstract</jats:title><jats:p>The prevalence of artificial intelligence (AI) tools has inspired social studies researchers, ethicists, and policymakers to seriously examine AI’s sociopolitical and ethical impacts. AI ethics literature provides guidance on which ethical principles to implement via AI governance; AI auditing literature, especially ethics-based auditing (EBA), suggests methods to verify if such principles are respected in AI model development and deployment. As much as EBA methods are abundant, I argue that most currently take a <jats:italic>top-down</jats:italic> and <jats:italic>post-hoc</jats:italic> approach to AI model development: Existing EBA methods mostly assume a preset of high-level, abstract principles that can be applied universally across contexts; meanwhile, current EBA is only conducted after the development or deployment of AI models. Taken together, these methods do not sufficiently capture the very developmental practices surrounding the constitution of AI models on a day-to-day basis. What goes on in an AI development space and the very developers whose hands write codes, assemble datasets, and design model architectures remain unobserved and, therefore, uncontested. I attempt to address this lack of documentation on AI developers’ day-to-day practices by conducting an ethnographic “AI lab study” (termed by Florian Jaton), demonstrating just how much context and empirical data can be excavated to support a whole-picture evaluation of AI models’ sociopolitical and ethical impacts. I then propose a new method to be added to the arsenal of EBA: Ethnographic audit trails (EATs), which take a <jats:italic>bottom-up</jats:italic> and <jats:italic>in-progress</jats:italic> approach to AI model development, capturing the previously unobservable developer practices.</jats:p>"
10.1007/s43681-024-00567-7,Robot warfare: the (im)permissibility of autonomous weapons systems,N/A
10.1007/s43681-023-00413-2,Ethical and preventive legal technology,"<jats:title>Abstract</jats:title><jats:p>Preventive Legal Technology (PLT) is a new field of Artificial Intelligence (AI) investigating the <jats:italic>intelligent prevention of disputes</jats:italic>. The concept integrates the theories of <jats:italic>preventive law</jats:italic> and <jats:italic>legal technology</jats:italic>. Our goal is to give ethics a place in the new technology. By <jats:italic>explaining</jats:italic> the decisions of PLT, we aim to achieve a higher degree of <jats:italic>trustworthiness</jats:italic> because explicit explanations are expected to improve the level of <jats:italic>transparency</jats:italic> and <jats:italic>accountability</jats:italic>. Trustworthiness is an urgent topic in the discussion on doing AI research ethically and accounting for the regulations. For this purpose, we examine the limitations of rule-based explainability for PLT. Hence, our Problem Statement reads: <jats:italic>to what extent is it possible to develop an explainable and trustworthy Preventive Legal Technology?</jats:italic> After an insightful literature review, we focus on case studies with applications. The results describe (1) the effectivity of PLT and (2) its responsibility. The discussion is challenging and multivariate, investigating deeply the relevance of PLT for LegalTech applications in light of the development of the AI Act (currently still in its final phase of process) and the work of the High-Level Expert Group (HLEG) on AI. On the ethical side, explaining AI decisions for small PLT domains is clearly possible, with direct effects on trustworthiness due to increased transparency and accountability.</jats:p>"
10.1007/s43681-024-00426-5,Does attitude towards plagiarism predict aigiarism using ChatGPT?,N/A
10.1007/s43681-024-00541-3,Algorithmic fairness in predictive policing,"<jats:title>Abstract</jats:title><jats:p>The increasing use of algorithms in predictive policing has raised concerns regarding the potential amplification of societal biases. This study adopts a two-phase approach, encompassing a systematic review and the mitigation of age-related biases in predictive policing. Our systematic review identifies a variety of fairness strategies in existing literature, such as domain knowledge, likelihood function penalties, counterfactual reasoning, and demographic segmentation, with a primary focus on racial biases. However, this review also highlights significant gaps in addressing biases related to other protected attributes, including age, gender, and socio-economic status. Additionally, it is observed that police actions are a major contributor to model discrimination in predictive policing. To address these gaps, our empirical study focuses on mitigating age-related biases within the Chicago Police Department's Strategic Subject List (SSL) dataset used in predicting the risk of being involved in a shooting incident, either as a victim or an offender. We introduce Conditional Score Recalibration (CSR), a novel bias mitigation technique, alongside the established Class Balancing method. CSR involves reassessing and adjusting risk scores for individuals initially assigned moderately high-risk scores, categorizing them as low risk if they meet three criteria: no prior arrests for violent offenses, no previous arrests for narcotic offenses, and no involvement in shooting incidents. Our fairness assessment, utilizing metrics like Equality of Opportunity Difference, Average Odds Difference, and Demographic Parity, demonstrates that this approach significantly improves model fairness without sacrificing accuracy.</jats:p>"
10.1007/s43681-024-00530-6,Social botnets and the challenges of cyber situation awareness,N/A
10.1007/s43681-023-00388-0,This season’s artificial intelligence (AI): is today’s AI really that different from the AI of the past? Some reflections and thoughts,N/A
10.1007/s11948-021-00337-2,"Correction to: Mark Coeckelbergh, AI Ethics, Mit Press, 2021",N/A
10.2139/ssrn.4802313,Generative AI is Doomed,N/A
10.1007/s43681-024-00448-z,Regulating autonomous and AI-enabled weapon systems: the dangers of hype,N/A
10.1007/s43681-024-00518-2,Bringing practical statistical science to AI and predictive model fairness testing,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence, Machine Learning, Statistical Modeling and Predictive Analytics have been widely used in various industries for a long time. More recently, AI Model Governance including AI Ethics has received significant attention from academia, industry, and regulatory agencies. To minimize potential unjustified treatment disfavoring individuals based on demographics, an increasingly critical task is to assess group fairness through some established metrics. Many commercial and open-source tools are now available to support the computations of these fairness metrics. However, this area is largely based on rules, e.g., metrics within a prespecified range would be considered satisfactory. These metrics are statistical estimates and are often based on limited sample data and therefore subject to sampling variability. For instance, if a fairness criterion is barely met or missed, it is often uncertain if it should be a “pass” or “failure,” if the sample size is not large. This is where statistical science can help. Specifically, statistical hypothesis testing enables us to determine whether the sample data can support a particular hypothesis (e.g., falling within an acceptable range) or the observations may have happened by chance. Drawing upon the bioequivalence literature from medicine and advanced hypothesis testing in statistics, we propose a practical statistical significance testing method to enhance the current rule-based process for model fairness testing and its associated power calculation, followed by an illustration with a realistic example.</jats:p>"
10.1007/s43681-024-00438-1,The harms of terminology: why we should reject so-called “frontier AI”,"<jats:title>Abstract</jats:title><jats:p>In the mid-2023, promoters of artificial intelligence (AI) as an “existential risk” coined a new term, “frontier AI,” that refers to “highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety.” Promoters of this new term were able to disseminate it via the United Kingdom (UK) government’s Frontier AI Taskforce (formerly the Foundation Models Taskforce) as well as the UK’s AI Safety Summit, held in November 2023.</jats:p><jats:p>I argue that adoption of the term “frontier AI” is harmful and contributes to AI hype. Promoting this new term is a way for its boosters to focus the public conversation around the AI-related risks they think are most important, namely “existential risk”—a scenario in which AI is able to bring about the destruction of humanity. Simultaneously, “frontier AI” is a re-branding exercise for the large-scale generative machine learning (ML) models that have been shown to cause severe and pervasive harms (including psychological, social, and environmental harms). Unlike “existential risk,” these harms are actual rather than theoretical, whereas the term “frontier AI” moves our collective focus away from actual harms to focus on hypothetical doomsday scenarios.</jats:p><jats:p>Moreover, “frontier AI” as a term invokes the colonial mindset, further reinscribing the harmful dynamics between the handful of powerful Western companies who produce today’s generative AI models and the people of the “Global South” who are most likely to experience harm as a direct result of the development and deployment of these AI technologies.</jats:p>"
10.2139/ssrn.4887768,A Blueprint for Auditing Generative AI,N/A
10.1007/s43681-023-00327-z,When things go wrong: the recall of AI systems as a last resort for ethical and lawful AI,"<jats:title>Abstract</jats:title><jats:p>This paper presents an initial exploration of the concept of AI system recall, primarily understood as a last resort when AI systems violate ethical norms, societal expectations, or legal obligations. The discussion is spurred by recent incidents involving notable AI systems, demonstrating that AI recalls can be a very real necessity. This study delves into the concept of product recall as traditionally understood in industry and explores its potential application to AI systems. Our analysis of this concept is centered around two prominent categories of recall drivers in the AI domain: ethical-social and legal considerations. In terms of ethical-social drivers, we apply the innovative notion of “moral Operational Design Domain”, suggesting AI systems should be recalled when they violate ethical principles and societal expectation. In addition, we also explore the recall of AI systems from a legal perspective, where the recently proposed AI Act provides regulatory measures for recalling AI systems that pose risks to health, safety, and fundamental rights. The paper also underscores the need for further research, especially around defining precise ethical and societal triggers for AI recalls, creating an efficient recall management framework for organizations, and reassessing the fit of traditional product recall models for AI systems within the AI Act's regulatory context. By probing these complex intersections between AI, ethics, and regulation, this work aims to contribute to the development of robust and responsible AI systems while maintaining readiness for failure scenarios.</jats:p>"
10.1007/s43681-020-00010-7,"How safe is our reliance on AI, and should we regulate it?",N/A
10.1007/s43681-023-00365-7,"Correction: Navigating the legal landscape of AI copyright: a comparative analysis of EU, US, and Chinese approaches",N/A
10.1007/s43681-020-00009-0,Responsible AI and moral responsibility: a common appreciation,"<jats:title>Abstract</jats:title><jats:p>Responsibility is among the most widespread buzzwords in the ethics of artificial intelligence (AI) and robotics. Yet, the term often remains unsubstantiated when employed in these important technological domains. Indeed, notions like ‘responsible AI’ and ‘responsible robotics’ may sound appealing, for they seem to convey a sense of moral goodness or ethical approval, thereby inciting psychological connections to self-regulation, social acceptance, or political correctness. For AI and ethics to come together in truly harmonious ways, we will need to work toward establishing a common appreciation. In this commentary, I breakdown three varieties of the term and invoke insights from the analytic ethics literature as a means of offering a robust understanding of moral responsibility in emerging technology. While I do not wish to accuse any parties of incorrect usage, my hope is that together researchers in AI and ethics can be better positioned to appreciate and to develop notions of responsibility for technological domains.</jats:p>"
10.5220/0012623200003645,Facilitating User-Centric Model-Based Systems Engineering Using Generative AI,N/A
10.21275/sr24808231922,Impact and Importance of LLMOps for Enterprises Advancing Generative AI,N/A
10.1101/2024.04.26.24306470,Medical Diagnosis Coding Automation: Similarity Search vs. Generative AI,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Objective</jats:title><jats:p>This study aims to predict ICD-10-CM codes for medical diagnoses from short diagnosis descriptions and compare two distinct approaches: similarity search and using a generative model with few-shot learning.</jats:p></jats:sec><jats:sec><jats:title>Materials and Methods</jats:title><jats:p>The text-embedding-ada-002 model was used to embed textual descriptions of 2023 ICD-10-CM diagnosis codes, provided by the Centers provided for Medicare &amp; Medicaid Services. GPT-4 used few-shot learning. Both models underwent performance testing on 666 data points from the eICU Collaborative Research Database.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The text-embedding-ada-002 model successfully identified the relevant code from a set of similar codes 80% of the time, while GPT-4 achieved a 50 % accuracy in predicting the correct code.</jats:p></jats:sec><jats:sec><jats:title>Discussion</jats:title><jats:p>The work implies that text-embedding-ada-002 could automate medical coding better than GPT-4, highlighting potential limitations of generative language models for complicated tasks like this.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>The research shows that text-embedding-ada-002 outperforms GPT-4 in medical coding, highlighting embedding models’ usefulness in the domain of medical coding.</jats:p></jats:sec>"
10.1007/s43681-021-00127-3,From AI ethics principles to data science practice: a reflection and a gap analysis based on recent frameworks and practical experience,N/A
10.1093/oxfordhb/9780190067397.013.49,AI and Migration Management,"<p>This chapter focuses on how technologies used in the management of migration—such as automated decision-making in immigration and refugee applications and artificial intelligence (AI) lie detectors—impinge on human rights with little international regulation, arguing that this lack of regulation is deliberate, as states single out the migrant population as a viable testing ground for new technologies. Making migrants more trackable and intelligible justifies the use of more technology and data collection under the guide of national security, or even under tropes of humanitarianism and development. Technology is not inherently democratic, and human rights impacts are particularly important to consider in humanitarian and forced migration contexts. An international human rights law framework is particularly useful for codifying and recognizing potential harms, because technology and its development are inherently global and transnational. Ultimately, more oversight and issue specific accountability mechanisms are needed to safeguard fundamental rights of migrants, such as freedom from discrimination, privacy rights, and procedural justice safeguards, such as the right to a fair decision maker and the rights of appeal.</p>"
10.46630/msae.1.2024.01,AI GENERATIVE CHATBOT IN THE MEDIA: JOURNALISTIC COVERAGE OF CHATGPT IN BOSNIA AND HERZEGOVINA,"<jats:p>In diffusion research, journalistic coverage is acknowledged as a significant factor in spreading awareness and fostering knowledge about innovation, potentially accelerating or impeding the adoption process. With regards to AI-related innovations, this dynamic has largely been studied within the context of Western developed countries. There is far less understanding of how this process unfolds in the news ecosystem of post-communist countries, particularly those with lower democratic standards and weaker economic development, such as Bosnia and Herzegovina. With the intention of gaining preliminary insights, this study investigated how the journalistic organizations in Bosnia and Herzegovina covered the emergence and societal adoption of ChatGPT, a novel form of generative AI, during the initial six-month period following its widespread availability. The content analysis of relevant news messages (N=542) published by 40 legacy and digital- only news outlets was used to explore the key characteristics of journalistic coverage, the attention given to the issue over time and the media depictions of this innovative AI technology. Results indicate that a small group of news outlets, predominantly legacy news organizations, provided significantly more content on ChatGPT than others, particularly public broadcasting services. Findings highlight a tendency among news outlets to focus on either the risks or benefits of ChatGPT and similar AI-based products and amplify sources associated with the business sector and high-tech industry, overrepresented by male voices.</jats:p>"
10.4018/979-8-3693-5298-4.ch014,Steering Generative AI Toward Beneficence,"<jats:p>As generative AI advances, deepfakes are proliferating in sophistication and accessibility, spurring an arms race between media synthesis and detection. This chapter traces the evolution of deepfakes, focusing on algorithms like GANs, VAEs in enhancing realism, and predicts future trajectories, including hyper-realistic media, streamlined creation, and widespread benign and malicious adoption. Despite constructive applications in entertainment, education, marketing and medicine, threats loom regarding misinformation, consent violations, and propagating social biases. The authors emphasize the need for comprehensive solutions through public awareness campaigns, advanced digital forensics, ethical legal frameworks, incentivizing “blue sky” innovation, and social media oversight. Navigating societal implications requires collective vigilance and forward-looking perspectives. This chapter underscores the importance of proactive, reasoned preparation as increasingly disruptive technologies emerge.</jats:p>"
10.7551/mitpress/12549.003.0004,"Superintelligence, Monsters, and the AI Apocalypse",N/A
10.31234/osf.io/3rpwt,Are AI safety and AI ethics memetic rivals?,"<p>As the risks of artificial intelligence (AI) attract the spotlight of public attention, policy-makers turn to academia to indicate which problems to prioritize. While some argue they should first deal with the prospect of a global catastrophe (AI safety), others believe AI’s current social impact bears greater urgency (AI ethics). This has led some to express the concern that one or the other „diverts the public’s attention“. In this article, I sketch out the psychological landscape, in which these concerns arise as a logical reaction, but suggest that in the case of AI risks, they are misplaced. I ran a survey, in which students were asked regarding their attitudes towards the issues commonly labeled under „AI ethics“ and „AI safety“. The results suggest that when salience of AI safety is experimentally increased, people report higher support for solving the problems related to AI ethics. Secondly, the levels of concern for AI safety and AI ethics correlate positively. Therefore, in terms of public-facing communication, AI safety and AI ethics seem like memetic allies, rather than rivals.</p>"
10.4018/979-8-3693-3278-8.ch009,Transforming Media Landscapes,"<jats:p>This study examines the impact of Python-driven generative AI on media content creation and its ethical implications. Python's simplicity and extensive libraries have made it pivotal in AI development, enabling the generation of realistic content across various media formats. While these advancements promise significant enhancements in content creation efficiency and personalization, they also raise complex ethical issues, including concerns over authenticity, copyright infringement, and misinformation. Through surveys and case studies, this research explores the technological capabilities of generative AI, its transformative potential in the media landscape, and the ethical dilemmas it presents. The chapter advocates for a balanced approach to leveraging AI in media, emphasizing the need for frameworks that promote responsible use, ensuring innovation aligns with ethical standards and societal values.</jats:p>"
10.2139/ssrn.4779026,"How Do People Form Opinions Toward Generative Ai with Limited Experience? — Predicting Public Support for Generative Ai Technology Development and Regulation Using Technology Readiness, Media Attention, and Risk-Benefit Perceptions",N/A
10.4018/979-8-3693-0074-9.ch010,Professionally Ethical Ways to Harness an Art-Making Generative AI to Support Innovative Instructional Design Work,"<jats:p>Instructional designers often pride themselves on using the most cutting-edge commercial authoring and other tools available to achieve their work. Their creations have to meet high technical standards in order to function in a digital environment, in learning management systems, content management systems, on social media, on digital content platforms, and others. In the present moment, generative AI tools enable the making of novel texts and digital visuals, among others. A major extant question is how best to harness generative art-making AIs in instructional design work. In this case, this work explores professionally ethical (and legal) ways to use a generative art-making AIs for ID work, as an innovative approach based on a review of the literature, a year of using several free web-facing art-making generative AIs (CrAIyon, Deep Dream Generator, and others) in open or public beta, and learning from applied instructional design work (over several decades).</jats:p>"
10.1007/s43681-021-00103-x,The ethics of technology innovation: a double-edged sword?,N/A
10.1007/s43681-024-00513-7,The ethics of personalised digital duplicates: a minimally viable permissibility principle,"<jats:title>Abstract</jats:title><jats:p>With recent technological advances, it is possible to create personalised digital duplicates. These are partial, at least semi-autonomous, recreations of real people in digital form. Should such duplicates be created? When can they be used? This article develops a general framework for thinking about the ethics of digital duplicates. It starts by clarifying the object of inquiry– digital duplicates themselves– defining them, giving examples, and justifying the focus on them rather than other kinds of artificial being. It then identifies a set of generic harms and benefits associated with digital duplicates and uses this as the basis for formulating a minimally viable permissible principle (MVPP) that stipulates widely agreeable conditions that should be met in order for the creation and use of digital duplicates to be ethically permissible. It concludes by assessing whether it is possible for those conditions to be met in practice, and whether it is possible for the use of digital duplicates to be more or less permissible.</jats:p>"
10.36227/techrxiv.171198062.20183635/v1,Bane and boon of hallucinations in context of generative AI,N/A
10.31228/osf.io/bheqw,Generative AI Large Language Models and Research the Law,"<p>Generative AI Large Language Models (LLMs) are rapidly influencing the legal research landscape, challenging the traditional cognitive authority within the profession. This article explores how these models, despite their potential to simplify the vast amount of legal information, may also introduce risks of misinformation due to AI ""hallucinations"" and the advantages and limitations of retrieval-augmented generation (RAG) systems. By examining various legal research problems, the article highlights both the capabilities and shortcomings of generative AI in legal contexts. It emphasizes the need for vigilance, critical thinking, and a continued reliance on traditional research methods to mitigate the risks associated with over-reliance on AI-generated content. As the legal profession confronts an overwhelming influx of information, the role of AI in legal research will likely grow, making it crucial to balance cognitive ease with accuracy and reliability.</p>"
10.5204/thesis.eprints.250826,Enhancing OCT retinal and choroidal segmentation with deep generative AI models,"<jats:p>This thesis investigated the application of generative AI models to improve the accuracy and robustness of tools applied to optical coherence tomography (OCT) image analysis tasks such as retinal layer segmentation. New, high quality and diverse images are generated using the AI models leading to improvements as well as new tools and techniques to enhance these medical image analysis tasks. The findings of this research will aid both clinicians and researchers who work with medical data to improve the accuracy, speed and automation of relevant analysis tasks, particularly where data is scarce.</jats:p>"
10.36227/techrxiv.171746875.59016695/v1,"Generative AI: Definition, Concepts, Applications, and Future Prospects","<jats:p id=""p1"">Generative Artificial Intelligence (AI) represents a significant
advancement in AI, enabling the creation of synthetic data that closely
mimics real data. This article provides a comprehensive overview of
generative AI, including its definition and core concepts such as
Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs),
and autoregressive models. It explores applications across industries,
including content creation, data augmentation, healthcare, gaming,
fashion, finance, retail, cybersecurity, transportation, energy,
education, entertainment, law, agriculture, real estate, and
manufacturing. The article discusses future prospects of generative AI,
highlighting enhanced creativity, improved human-AI collaboration, and
personalized experiences. It also addresses dependencies and challenges,
including data quality, computational resources, and ethical
considerations. The benefits for businesses adopting generative AI are
examined, emphasizing competitive advantage, enhanced customer
experience, accelerated innovation, and improved decision-making. The
article concludes by outlining the potential return on investment
through increased efficiency, revenue growth, market expansion, and risk
mitigation.</jats:p>"
10.2139/ssrn.4871337,The Opportunities of Generative Ai in Cross-Border E-Commerce,N/A
10.1145/3657604.3664699,Scaling Up Mastery Learning with Generative AI: Exploring How Generative AI Can Assist in the Generation and Evaluation of Mastery Quiz Questions,N/A
10.4018/979-8-3693-3278-8.ch001,A Comprehensive Survey of Hypermedia System for Text- to-Image Conversion Using Generative AI,"<jats:p>The intersection of computer vision and natural language processing (NLP) has witnessed significant advancements in recent research, particularly in the realm of converting text into meaningful images leveraging generative AI and large language models. This review work aims to comprehensively review the progress made in text-to-image conversion. The survey covers the three primary approaches in the field, namely diffusion models (DM), GAN model approaches, and autoregressive approaches. Furthermore, the authors present a comprehensive chronology of the TIG journey, encompassing its origin and the most recent developments, providing readers with a comprehensive perspective on the field's progression. The survey focuses heavily on identifying the existing constraints of DM in picture production and offers multiple research publications and their contributions in overcoming these constraints. The survey provides useful insights into the advancements in text-to-image (TIG) generation using generative AI by focusing on key difficulties and examining how different works have addressed them.</jats:p>"
10.4018/979-8-3693-2418-9.ch007,Generative AI and Its Implications for Higher Education Students' Creativity,"<jats:p>This chapter explores Generative AI (artificial intelligence) in higher education regarding students' creativity. By utilising the quantitative content analysis approach, the chapter explored the impact of generative AI in higher education to understand its transformative effects on students' creative thinking processes and its outcomes. Current literature reveals the latest thinking on how AI enables or disrupts students' creativity. Generative AI is a potent tool in higher education which generates original content like text, images, music, and videos based on input data or predefined models. The adoption of generative AI led to the need to understand how these advancements may shape the educational experience to develop essential skills like creativity. Since creativity is crucial in today's digital age, generative AI raises questions about its potential impact on the imagination. The aspect of creativity, which is fundamental in higher education, offers students opportunities to enhance critical-thinking, problem-solving, and innovation; however, a balance is necessary between the utilisation of AI tools and the human element of creativity.</jats:p>"
10.20944/preprints202407.1437.v1,Integrating Multimodal Generative AI and Blockchain for Enhancing Generative Design in the Early Phase of Architectural Design Process,"<jats:p>AI advances integrate generative design tools in architecture, providing architects with sophisticated design options. It enables the creation of intricate, high-performing projects by exploring diverse design possibilities with AI and algorithms. Generative AI and generative design empower architects to create better-performing, sustainable, and efficient design solutions and explore diverse design possibilities. This paper leverages multimodal generative AI to enhance design creativity by combining textual and visual inputs. Blockchain technology converts design metadata into NFTs, ensuring secure, authentic, and traceable data storage. The framework addresses data ownership, legal adherence, and client-architect collaboration and is entirely scalable for digital design authentication. This research exemplifies the pragmatic fusion of Generative AI and blockchain technology applied in architectural design for more transparent, secure, and effective results. This study provides a strategy that uses generative AI technologies to achieve an efficient and creative workflow in the early stages of architectural design.</jats:p>"
10.4018/979-8-3693-1565-1.ch013,Sustainable Islamic Financial Inclusion,"<jats:p>The study investigates the convergence of digital transformation, artificial intelligence (AI), and Islamic finance. In particular, it examines the ethical consequences that may arise from the integration of Generative AI in the sustainable development of Islamic financial services and products. This research fills a void in the current body of knowledge by examining the ethical consequences of generative AI in the context of Islamic finance. Using an interdisciplinary framework that integrates Islamic finance and technological ethics, the study seeks to make scientific and practical contributions. At the intersection of AI technology and Islamic finance, it is anticipated that new theories will emerge, as well as ethical principles that will serve as a guide for technology developers, policymakers, and Islamic financial institutions. The study has the potential to lead in creating a sustainable and inclusive Islamic finance ecosystem by ethically integrating Generative AI.</jats:p>"
10.4018/979-8-3693-1351-0.ch013,Revolutionizing Content Creation and Curriculum Development Through Generative AI,"<jats:p>AI can generate and provide customized, inclusive, and engaging learning experiences. This chapter emphasizes the collaborative partnership between human educators and AI systems, highlighting its crucial role in maximizing AI's potential. Educators provide context and guidance through adaptive learning platforms, AI-powered feedback, and AI-enhanced content creation to ensure students receive a personalized, contextually relevant education. Human knowledge, including pedagogical insight and ethical considerations, complements the capabilities of artificial intelligence. AI promises personalized perpetual learning, enhanced accessibility, and dynamic curriculum design in the future. This vision of collaboration promotes critical thinking, problem-solving, and inclusiveness. Educators, institutions, policymakers, and AI developers must ensure that education remains empowering, inclusive, and excellent for AI to be integrated responsibly. By embracing this partnership, education becomes more accessible, individualized, and influential.</jats:p>"
10.1007/s43681-023-00330-4,Ethics by design for artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>In this paper, we present an approach for the systematic and comprehensive inclusion of ethical considerations in the design and development process of artificial intelligence systems, called Ethics by Design for AI (EbD-AI). The approach is the result of a three-year long research effort, and has recently be adopted by the European Commission as part of its ethics review procedure for AI projects. We describe and explain the approach and its different components and its application to the development of AI software and systems. We also compare it to other approaches in AI ethics, and we consider limitations of the approach as well as potential criticisms.</jats:p>"
10.1007/s43681-021-00045-4,On formal ethics versus inclusive moral deliberation,"<jats:title>Abstract</jats:title><jats:p>In this article, I will advocate caution against a formalization of ethics by showing that it may produce and perpetuate unjustified power imbalances, disadvantaging those without a proper command of the formalisms, and those not in a position to decide on the formalisms’ use. My focus rests mostly on ethics formalized for the purpose of implementing ethical evaluations in computer science–artificial intelligence, in particular—but partly also extends to the project of applying mathematical rigor to moral argumentation with no direct intention to automate moral deliberation. Formal ethics of the latter kind can, however, also be seen as a facilitator of automated ethical evaluation. I will argue that either form of formal ethics presents an obstacle to inclusive and fair processes for arriving at a society-wide moral consensus. This impediment to inclusive moral deliberation may prevent a significant portion of society from acquiring a deeper understanding of moral issues. However, I will defend the view that such understanding supports genuine and sustained moral progress. From this, it follows that formal ethics is not per se supportive of moral progress. I will illustrate these arguments by practical examples of manifest asymmetric relationships of power primarily from the domain of autonomous vehicles as well as on more visionary concepts, such as artificial moral advisors. As a result, I will show that in these particular proposed use-cases of formal ethics, machine ethics risks to run contrary to their proponents’ proclaimed promises of increasing the rigor of moral deliberation and even improving human morality on the whole. Instead, I will propose that inclusive discourse about automating ethical evaluations, e.g., in autonomous vehicles, should be conducted with unrelenting transparency about the limitations of implementations of ethics. As an outlook, I will briefly discuss uses formal ethics that are more likely to avoid discrepancies between the ideal of inclusion and the challenge from power asymmetries.Please check and confirm that the authors and their respective affiliations have been correctly identified and amend if necessary.I confirm.Author names: Please confirm if the author names are presented accurately and in the correct sequence (given name, middle name/initial, family name). I confirm. Kindly check and confirm the country name for the affiliation [1] is correct.I confirm.</jats:p>"
10.1007/s43681-024-00563-x,The rise of checkbox AI ethics: a review,"<jats:title>Abstract</jats:title><jats:p>The rapid advancement of artificial intelligence (AI) sparked the development of principles and guidelines for ethical AI by a broad set of actors. Given the high-level nature of these principles, stakeholders seek practical guidance for their implementation in the development, deployment and use of AI, fueling the growth of practical approaches for ethical AI. This paper reviews, synthesizes and assesses current practical approaches for AI in health, examining their scope and potential to aid organizations in adopting ethical standards. We performed a scoping review of existing reviews in accordance with the PRISMA extension for scoping reviews (PRISMA-ScR), systematically searching databases and the web between February and May 2023. A total of 4284 documents were identified, of which 17 were included in the final analysis. Content analysis was performed on the final sample. We identified a highly heterogeneous ecosystem of approaches and a diverse use of terminology, a higher prevalence of approaches for certain stages of the AI lifecycle, reflecting the dominance of specific stakeholder groups in their development, and several barriers to the adoption of approaches. These findings underscore the necessity of a nuanced understanding of the implementation context for these approaches and that no one-size-fits-all approach exists for ethical AI. While common terminology is needed, this should not come at the cost of pluralism in available approaches. As governments signal interest in and develop practical approaches, significant effort remains to guarantee their validity, reliability, and efficacy as tools for governance across the AI lifecycle.</jats:p>"
10.1007/s43681-023-00306-4,"On educating ethics in the AI era: why business schools need to move beyond digital upskilling, towards ethical upskilling",N/A
10.1007/s43681-023-00295-4,"If conceptual engineering is a new method in the ethics of AI, what method is it exactly?","<jats:title>Abstract</jats:title><jats:p>Can a machine be a person? Can a robot think, be our friend or colleague? These familiar questions in the ethics of AI have recently become much more urgent than many philosophers anticipated. However, they also seem as intractable as ever. For this reason, several philosophers of AI have recently turned their attention to an arguably new method: <jats:italic>conceptual engineering</jats:italic>. The idea is to stop searching for the real essence of <jats:italic>friendship</jats:italic> or our ordinary concept of the person. Instead, ethicists of AI should engineer concepts of friend or person we <jats:italic>should</jats:italic> apply. But what exactly is this method? There is currently no consensus on what the target object of conceptual engineers is or should be. In this paper, I reject a number of popular options and then argue for a pragmatist way of thinking about the target object of conceptual engineering in the ethics of AI. I conclude that in this pragmatist picture, conceptual engineering is probably what we have been doing all along. So, is it all just hype? No, the idea that the ethics of AI has been dominated by conceptual engineers all along constitutes an important meta-philosophical insight. We can build on this insight to develop a more rigorous and thorough methodology in the ethics of AI.</jats:p>"
10.1007/s43681-021-00091-y,Coarse ethics: how to ethically assess explainable artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>The integration of artificial intelligence (AI) into human society mandates that their decision-making process is explicable to users, as exemplified in Asimov’s Three Laws of Robotics. Such human interpretability calls for explainable AI (XAI), of which this paper cites various models. However, the transaction between computable accuracy and human interpretability can be a trade-off, requiring answers to questions about the negotiable conditions and the degrees of AI prediction accuracy that may be sacrificed to enable user-interpretability. The extant research has focussed on technical issues, but it is also desirable to apply a branch of ethics to deal with the trade-off problem. This scholarly domain is labelled<jats:italic>coarse ethics</jats:italic>in this study, which discusses two issues vis-à-vis AI prediction as a type of evaluation. First, which formal conditions would allow trade-offs? The study posits two minimal requisites: adequately high coverage and order-preservation. The second issue concerns conditions that could justify the trade-off between computable accuracy and human interpretability, to which the study suggests two justification methods: impracticability and adjustment of perspective from machine-computable to human-interpretable. This study contributes by connecting ethics to autonomous systems for future regulation by formally assessing the adequacy of AI rationales.</jats:p>"
10.69554/jiuo9062,Business value of Generative AI use cases,"<jats:p xml:lang=""en"">This paper discusses the significant impact of artificial intelligence (AI), specifically generative AI (GenAI), on various industries and business processes. It highlights the rapid adoption of AI technologies and their profound influence on global business operations. The paper shares views on the substantial growth rate for the technology, with many businesses already in piloting phases or production stages. It emphasises the transformative role of generative AI in creating new services and products, necessitating changes in operating models, technology stacks and workforce skills. The paper also touches on various industries affected by AI, the potential for automation and augmentation and the strategic planning required for businesses to effectively implement and benefit from these technologies. Additionally, it discusses the importance of responsible AI, addressing risks such as bias and privacy, and complying with emerging regulations. The paper highlights the crucial role of AI in modernising business practices and creating competitive advantages in various sectors.</jats:p>"
10.2139/ssrn.4904004,Corporate Responses to Generative AI: Early Evidence from Investment Efficiency,N/A
10.5194/egusphere-egu24-15392,The Potential of Generative AI for the Urban Water Sector,"<jats:p>The urban water sector is increasingly turning to AI and deep learning to address the complex challenges posed by growing demographics, climate change, and urbanization. Despite the pressing need, this sector has been relatively slow in adopting these technologies compared to others, primarily due to its conservative nature. However, the recent advancements in generative AI have opened new frontiers for innovation, presenting a crucial opportunity for the urban water sector to accelerate its technological evolution. Expected regulations, particularly from institutions like the European Union, should not be viewed as a hindrance but as a catalyst for enhanced collaboration between academia, industry, and public stakeholders. Such collaboration is essential to finally push the development and adoption of reliable and safe AI systems, ensuring alignment with regulatory frameworks.
In this work, we first provide an overview of the latest trends in generative AI, focusing on how Large Language Models and Large Multimodal Models can benefit the urban water sector. Particularly, Large Multimodal Models can offer an added layer of explainability to predictive models working on imagery or other sensor data, a highly desirable feature for applications related to critical infrastructure. By literally asking these models to explain their decision-making processes with respect to the processed data streams, we can partially demystify the 'black box' nature of AI systems.
This potential is highlighted for a case study on sewer defect detection, utilizing a Large Multimodal Model that processes both text and imagery. The predictive results on the publicly available SewerML dataset are encouraging with respect to existing deep learning methods. More importantly, we show that explanations provided by the Large Multimodal Model can enlighten the decision-making process, making it more transparent. This added layer of explanation offers valuable insights and may set a new trajectory for developing trustworthy AI methodologies in critical water infrastructure management.</jats:p>"
10.21428/8c225f6e.0390b853,Issue 4.1 Generative AI and education,N/A
10.1007/s43681-023-00334-0,Moderating the effects of “surveillance capitalism”: an Aristotelian perspective,N/A
10.1007/s43681-023-00296-3,Artificial intelligence’s right to life,"<jats:title>Abstract</jats:title><jats:p>The right to life is fundamental and primary and is a precondition for exercising other rights (Ramcharan in Ramcharan (ed), The right to life in International Law, Martinus Nijhoff Publishers, Dordrecht, 1985). Its universal recognition in the arena of international law is associated with the concept of a human being endowed with inherent and inalienable dignity. Categorization of the circle of entities covered with the right to life today seems obvious and indisputable. Intense development of artificial intelligence, also the fact that it has passed the Turing test which checks AI’s thinking ability in a way similar to human reasoning, inspires a reflection on AI’s future legal status. This study will investigate a thesis of whether artificial intelligence may be entitled to the right to life. The analysis will be carried out around an exploratory question: what are the requirements for being afforded protection of the right to life?</jats:p>"
10.47289/aiej20201214,What a Philosopher Learned at an AI Ethics Evaluation,"<jats:p>AI ethics increasingly focuses on converting abstract principles into practical action. This case study documents nine lessons for the conversion learned while performing an ethics evaluation on a deployed AI medical device. The utilized ethical principles were adopted from the Ethics Guidelines for Trustworthy AI, and the conversion into practical insights and recommendations was accomplished by an independent team composed of philosophers, technical and medical experts.</jats:p>"
10.1007/s43681-023-00304-6,The Kant-inspired indirect argument for non-sentient robot rights,N/A
10.1515/9781501518447-008,Chapter 7: Generative AI and Human Speech,N/A
10.1201/9781032656618-3,The Core and Ecosystem of Generative AI,N/A
10.4018/979-8-3693-0487-7.ch007,Exploring the Potential of Generative AI in English Language Teaching,"<jats:p>Integrating artificial intelligence (AI) technologies in education has shown remarkable potential in revolutionizing various aspects of teaching and learning. In English language teaching (ELT), generative AI has emerged as a powerful tool to enhance language acquisition, foster learner engagement, and provide personalized instruction. While the potential of generative AI in ELT is promising, it is important to provide insights into certain challenges and considerations. Ethical concerns related to data privacy, bias in AI algorithms, and the potential displacement of human instructors need to be carefully navigated. The successful implementation of generative AI in ELT relies on careful consideration of ethical implications, human oversight, and the continuous refinement of AI algorithms to ensure optimal language learning outcomes. On these premises, this chapter explores the potential of generative AI in ELT and its implications for language learners and educators.</jats:p>"
10.7315/cde.2023.514,Generative AI for Automated Urban Housing Floor Plan Generation,N/A
10.5220/0012736200003708,Generative AI for Productivity in Industry and Education,N/A
10.4018/979-8-3693-2440-0.ch025,Impact of Generative AI in Education 2030,"<jats:p>This book chapter contains the basics of generative AI, evaluation of generative AI, working process of generative AI, and applications of generative AI in Education 2030. In this chapter, the authors will introduce the emerging concept of large language model (LLM), reinforcement learning, response generation, neural network, and tokenization as the building blocks of generative AI. In this context, the authors will analyze the role of AI based ChatGpt technology in Education. The applications of generative AI in educational policy making, infrastructure development, research, innovation, activity building, and content generation for the students at schools and colleges will be discussed. But the contents of above mentioned activities must be guided by a teacher of school and college because such contents have more context wise decision making abilities rather than AI based solutions.</jats:p>"
10.70179/grdjev09i100002,Transforming Car Manufacturing: How Generative AI and Machine Learning Are Revolutionizing Production,"<jats:p>The Daimler AG trains Generative Adversarial Network (GAN) to grow the data-confirmed neural networks. The ultimate goal is a production-oriented application that enables faster manufacturing, improved quality, and a more efficient use of resources through a higher level of automation. The success of deep learning-based methods, particularly Generative Adversarial Networks (GANs), has led to new opportunities in generative AI in the manufacturing domain. In a proof of concept, we use a face generator as a metaphor for the generation of complex workpiece surfaces. The GAN transforms a low-resolution surface map - the so-called latent vector - into a high-resolution area on the generated face surface. This is used for the manufacturing of a corresponding face in an automotive body shop. The method is based on a label-to-attribute setup and includes a pre-fitting step for control point generation.</jats:p>"
10.21275/sr24728192241,Innovative Approaches to Payment Glossary Creation and Management Using Generative AI,N/A
10.1007/979-8-8688-0403-8_5,Large Language Models,N/A
10.1007/978-3-031-46238-2_4,Privacy in Generative Models: Attacks and Defense Mechanisms,N/A
10.1007/s43681-023-00326-0,"Algorithmic indirect discrimination, fairness and harm",N/A
10.69554/auij4734,Measuring the business value of generative AI,"<jats:p xml:lang=""en"">Generative artificial intelligence (GenAI) can deliver tangible and intangible values that can be calculated to decide which projects benefit from GenAI and which do not. This paper is intended to be a guide for businesses just starting to build traction for their ideas. The focus is on evaluating and leveraging GenAI’s potential to innovate faster and compete effectively in a rapidly evolving digital economy. The paper specifies the many ways GenAI can have an impact on a business and considers how to measure that impact. It starts with standard business metrics (revenue, profit, customer satisfaction, etc.) and then turns to the more esoteric task of measuring the impact on creativity, inspiration and innovation, followed by business disruption and process metrics. It finishes with a look at improving process improvement.</jats:p>"
10.69732/ahcn5851,Grounding AI: Understanding the Implications of Generative AI in World Language &amp; Culture Education,N/A
10.1007/s43681-021-00092-x,Robotomorphy,"<jats:title>Abstract</jats:title><jats:p>Humans and gods alike have since the dawn of time created objects in their own image. From clay figures and wooden toys—some granted life in myths and movies but also dead representations of their creators—to modern-day robots that mimic their creators in more than appearance. These objects tell the story of how we perceive ourselves, and in this article, I examine how they also change us. Robotomorphy describes what occurs when we project the characteristics and capabilities of robots onto ourselves, to make sense of the complicated and mysterious beings that we are. Machines are, after all, relatively comprehensible and help dispel the discomfort associated with complex human concepts such as consciousness, free will, the soul, etc. I then argue that using robots as the mirror image by which we understand ourselves entails an unfortunate reductionism. When robots become the blueprint for humanity, they simultaneously become benchmarks and ideals to live up to, and suddenly the things we make are no longer representations of ourselves, but we of them. This gives rise to a recursive process in which the mirror mirrors itself and influences both the trajectory for machine development and human self-perception.</jats:p>"
10.1007/s43681-024-00435-4,Exploring ChatGPT and its impact on society,N/A
10.4324/9781003463979-3,Generative AI,N/A
10.1007/s43681-022-00134-y,Moral dilemmas for moral machines,N/A
10.1007/978-3-031-46238-2_3,Unlocking the Potential of Generative Artificial Intelligence in Drug Discovery,N/A
10.51219/urforum.2023.aishwarya-rai,Next-Gen Solutions: How Generative AI will reshape Businesses,N/A
10.1007/s43681-022-00185-1,The case for virtuous robots,N/A
10.1007/979-8-8688-0456-4_4,The Unexpected Evolution of AI,N/A
10.1201/9781032669182-12,Ethics in AI,N/A
10.1007/s43681-021-00097-6,Correction to: Mapping global AI governance: a nascent regime in a fragmented landscape,N/A
10.1007/s43681-023-00345-x,"Correction: The democratic offset: Contestation, deliberation, and participation regarding military applications of AI",N/A
10.1007/s43681-022-00216-x,Artificial intelligence in human reproduction: charting the ethical debate over AI in IVF,N/A
10.1007/s43681-022-00189-x,Are we justified attributing a mistake in diagnosis to an AI diagnostic system?,"<jats:title>Abstract</jats:title><jats:p>Responsible professional use of AI implies the readiness to respond to and address—in ethically appropriate manner—harm that may be associated with such use. This presupposes the ownership of mistakes. In this paper, I ask if a mistake in AI-enhanced decision making—such as AI-aided medical diagnosis—can be attributed to the AI system itself, and answer this question negatively. I will explore two options. If AI systems are merely tools, then we are never justified to attribute mistakes to them, because their failing does not meet rational constraints on being mistaken. If, for the sake of the argument, we assume that AI systems are not (mere) tools, then we are faced with certain challenges. The first is the burden to explain what this more-than-a-tool role of an AI system is, and to establish justificatory reasons for the AI system to be considered as such. The second is to prove that medical diagnosis can be reduced to the calculations by AI system without any significant loss to the purpose and quality of the diagnosis as a procedure. I will conclude that the problem of the ownership of mistakes in hybrid decision making necessitates new forms of epistemic responsibilities.</jats:p>"
10.1007/s43681-023-00361-x,Is a robot surgeon with AI the ideal surgeon? A philosophical analysis,N/A
10.1007/s43681-024-00539-x,Editorial: The ethical implications of AI hype,N/A
10.1007/s43681-022-00166-4,Is AI recruiting (un)ethical? A human rights perspective on the use of AI for hiring,"<jats:title>Abstract</jats:title><jats:p>The use of artificial intelligence (AI) technologies in organizations’ recruiting and selection procedures has become commonplace in business practice; accordingly, research on AI recruiting has increased substantially in recent years. But, though various articles have highlighted the potential opportunities and ethical risks of AI recruiting, the topic has not been normatively assessed yet. We aim to fill this gap by providing an ethical analysis of AI recruiting from a human rights perspective. In doing so, we elaborate on human rights’ theoretical implications for corporate use of AI-driven hiring solutions. Therefore, we analyze whether AI hiring practices inherently conflict with the concepts of validity, autonomy, nondiscrimination, privacy, and transparency, which represent the main human rights relevant in this context. Concluding that these concepts are not at odds, we then use existing legal and ethical implications to determine organizations’ responsibility to enforce and realize human rights standards in the context of AI recruiting.</jats:p>"
10.1007/s43681-022-00202-3,“Intelligent Justice”: human-centered considerations in China’s legal AI transformation,N/A
10.1007/s43681-024-00474-x,"Promising the future, encoding the past: AI hype and public media imagery","<jats:title>Abstract</jats:title><jats:p>In recent years, “AI hype” has taken over public media, oscillating between sensationalism and concerns about the societal implications of AI growth. The latest historical wave of AI hype indexes a period of increased research, investment, and speculation on machine learning, centred around generative AI, a novel class of machine learning that can generate original media from textual prompts. In this paper, I dive into the production of AI hype in online media, with the aim of prioritising the normative and political dimension of AI hype. Formulating AI as a promise reframes it as a normative project, centrally involving the formation of public and institutional confidence in the technology. The production and dissemination of images, in this context, plays a pivotal role in reinforcing these normative commitments to the public. My argument is divided into four sections. First, I examine the political relevance of stock images as the dominant imagery used to convey AI concepts to the public. These stock images encode specific readings of AI and circulate through public media, significantly influencing perceptions. Second, I look at the dominant images of AI as matters of political concern. Third, as generative AI increasingly contributes to the production of stock imagery, I compare the epistemic work performed by AI-generated outputs and stock images, as both encode style, content, and taxonomic structures of the world. I employ an entity relationship diagram (ERD) to investigate the political economy of AI imagery in digital media, providing a snapshot of how AI hype is materialised and amplified online. With this study, I reaffirm AI’s normative character at the forefront of its political and ethical discourse.</jats:p>"
10.1007/s10676-023-09728-4,Generative AI models should include detection mechanisms as a condition for public release,"<jats:title>Abstract</jats:title><jats:p>The new wave of ‘foundation models’—general-purpose generative AI models, for production of text (e.g., ChatGPT) or images (e.g., MidJourney)—represent a dramatic advance in the state of the art for AI. But their use also introduces a range of new risks, which has prompted an ongoing conversation about possible regulatory mechanisms. Here we propose a specific principle that should be incorporated into legislation: that any organization developing a foundation model intended for public use must demonstrate a reliable <jats:italic>detection mechanism</jats:italic> for the content it generates, as a condition of its public release. The detection mechanism should be made publicly available in a tool that allows users to query, for an arbitrary item of content, whether the item was generated (wholly or partly) by the model. In this paper, we argue that this requirement is technically feasible and would play an important role in reducing certain risks from new AI models in many domains. We also outline a number of options for the tool’s design, and summarize a number of points where further input from policymakers and researchers would be required.</jats:p>"
10.1007/s43681-022-00182-4,What do academics say about artificial intelligence ethics? An overview of the scholarship,N/A
10.2139/ssrn.4877398,"Generative Discrimination: What Happens When Generative AI Exhibits Bias, and What Can Be Done About It",N/A
10.36851/ai-edu.vi0.4226,"A.M.Aminee, J.Taylor. Generative AI Results and Reality","<jats:p>The accuracy of Generative Artificial Intelligence (GAI) tools is the product of the quality of underlying data used to train models, and the models themselves. This interplay between data and models can lead to differences in the accuracy of outputs provided to common prompts across different GAI tools. This study investigates the disparities in accuracy related to representativeness between the outputs of GAI tools and demographic data from National Center for Education Statistics (NCES) and large enrollment, regional comprehensive university in the western United States (CSU). Three GAI platforms - ChatGPT, Co-pilot, and Gemini were evaluated using five samples each, with the same instruction across all platforms: “Show a class of graduate students.” The GAI outputs were analyzed based on three demographic variables: gender, race, and age group. These outputs were then compared to national averages from the NCES for gender and race and the CSU for age group. Notably, the variances in the results showed broader distributions across the demographic variables. To assess accuracy, a representation rate metric was calculated, reflecting the average absolute variance from the NCES and CSU benchmarks. The findings highlight the opportunity of higher quality data in model training, as well as the necessity for improved algorithms and methodologies in GAI systems to represent complex demographic information more accurately. </jats:p>"
10.1201/b23345-2,AI Ethics as a Form of Research Ethics,N/A
10.1007/s43681-024-00553-z,Eudemonia of a machine,"<jats:title>Abstract</jats:title><jats:p>Henry Ford once said, “For most purposes, a man with a machine is better than a man without a machine.” To this, engineers today propose an addendum – “and a man that <jats:italic>is</jats:italic> a machine is best of all” – which they have made their goal. The world over, engineers are working to make the ultimate machine, “the holy grail of artificial intelligence,” a <jats:italic>conscious</jats:italic> humanoid. On the one hand, such a “machine” will be capable of relieving us of all our burdens. On the other hand, in so doing, will we not have “birthed,” as it were, a new class of slaves? In this essay I seek to summarize the various arguments made in this debate, bring to bear moral positions from the philosophy of technology, philosophy of law and philosophy of religion, as well as demonstrate the moral impropriety of such an endeavor from each of the classic moral approaches (i.e., Virtue Ethics, Consequentialism, Kantian Deontology). Finally, given that the debate centers around what is the “good life” for human or humanoid, I expand upon Aristotle’s Eudemonia and Maimonides’ <jats:italic>Summum Bonum</jats:italic> to argue that life is precious in its affordance to allow conscious beings, human or humanoid, to aspire to the best life possible.</jats:p>"
10.1007/s43681-022-00195-z,Operationalising ethics in artificial intelligence for healthcare: a framework for AI developers,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) offers much promise for improving healthcare. However, it runs the looming risk of causing individual and societal harms; for instance, exacerbating inequalities amongst minority groups, or enabling compromises in the confidentiality of patients’ sensitive data. As such, there is an expanding, unmet need for ensuring AI for healthcare is developed in concordance with human values and ethics. Augmenting “principle-based” guidance that highlight adherence to ethical ideals (without necessarily offering translation into actionable practices), we offer a solution-based framework for operationalising ethics in AI for healthcare. Our framework is built from a scoping review of existing solutions of ethical AI guidelines, frameworks and technical solutions to address human values such as self-direction in healthcare. Our view spans the entire length of the AI lifecycle: data management, model development, deployment and monitoring. Our focus in this paper is to collate actionable solutions (whether technical or non-technical in nature), which can be steps that enable and empower developers in their daily practice to ensuring ethical practices in the broader picture. Our framework is intended to be adopted by AI developers, with recommendations that are accessible and driven by the existing literature. We endorse the recognised need for ‘ethical AI checklists’ co-designed with health AI practitioners, which could further operationalise the technical solutions we have collated. Since the risks to health and wellbeing are so large, we believe a proactive approach is necessary for ensuring human values and ethics are appropriately respected in AI for healthcare.</jats:p>"
10.1007/s43681-022-00187-z,AI ethics: the case for including animals,"<jats:title>Abstract</jats:title><jats:p>The ethics of artificial intelligence, or AI ethics, is a rapidly growing field, and rightly so. While the range of issues and groups of stakeholders concerned by the field of AI ethics is expanding, with speculation about whether it extends even to the machines themselves, there is a group of sentient beings who are also affected by AI, but are rarely mentioned within the field of AI ethics—the nonhuman animals. This paper seeks to explore the kinds of impact AI has on nonhuman animals, the severity of these impacts, and their moral implications. We hope that this paper will facilitate the development of a new field of philosophical and technical research regarding the impacts of AI on animals, namely, the ethics of AI as it affects nonhuman animals.</jats:p>"
10.22323/2.23030205,Navigating the AI era: university communication strategies and perspectives on generative AI tools,"<jats:p>
This study conducts a pioneering empirical analysis of   generative AI tools, such as ChatGPT, in the context of university   communication across German universities. It explores the adoption   rates, identifies the primary challenges, and assesses the potential   of these technologies, integrating several theoretical concepts. The   findings reveal a widespread use of AI for translation and language   correction, with broader applications gradually emerging. Adoption   rates vary significantly between private and public universities,   largely due to concerns over technical issues, data protection, and   AI usability. The results underscore the need for enhanced training   and AI policies that support effective integration and use.</jats:p>"
10.1016/b978-0-443-18851-0.00008-1,Ethics in AI-based online assessment in higher education,N/A
10.1007/s43681-024-00487-6,Correction: Crossing the principle–practice gap in AI ethics with ethical problem-solving,N/A
10.1093/oxfordhb/9780190067397.013.5,"AI Governance by Human Rights–Centered Design, Deliberation, and Oversight","<p>This chapter argues that international human rights standards offer the most promising basis for developing a coherent and universally recognized set of standards that can be applied to meet any of the normative concerns currently falling under the rubric of AI (artificial intelligence) ethics. It then outlines the core elements of a human rights–centered design, deliberation, and oversight approach to the governance of AI. This approach requires that human rights norms are systemically considered at every stage of system design, development, and deployment, drawing upon and adapting technical methods and techniques for safe software and system design, verification, testing, and auditing in order to ensure compliance with human rights norms. The regime must be mandated by law and relies critically on external oversight by independent, competent, and properly resourced regulatory authorities with appropriate powers of investigation and enforcement. However, this approach will not ensure the protection of all ethical values adversely implicated by AI, given that human rights norms do not comprehensively cover all values of societal concern. As such, a great deal more work needs to be done to develop techniques and methodologies that are robust—reliable yet practically implementable across a wide and diverse range of organizations involved in developing, building, and operating AI systems.</p>"
10.1007/s43681-022-00256-3,The ethical agency of AI developers,"<jats:title>Abstract</jats:title><jats:p>Public and academic discourse about the ethics of artificial intelligence, machine learning, and data science has largely focused on the algorithms and the companies deploying them. Little attention has been paid to the ethical agency of the developers. This study is the first of its kind that centers developers in the ethical environment. Semi-structured interviews with 40 developers about the ethics of being a developer revealed more than 20 themes, 3 of which are the subject of this paper: ethics in the occupational ecosystem, developer ethical agency, and the characteristics of an ethical developer. These themes reveal significant gaps between how developers perceive themselves and the reality of their work experiences. Their ethical agency is likewise variable. They have some authority to intervene for ethical reasons in systems they work on, but they often do not realize just how many ethical decisions they make. Nonetheless, this study reveals a growing ethical wisdom in this community, one that needs to be surfaced and nurtured by engaging with developers.</jats:p>"
10.31234/osf.io/zy8gr,Can Generative AI Infer Thinking Style from Language? Evaluating the Utility of AI as a Psychological Text Analysis Tool,"<p>Generative AI are not currently the choice technology for text analysis, but prior work suggests they may have some utility to assess dynamics like emotion. The current work builds on this empirical foundation to consider how analytic thinking scores from a large language model chatbot, ChatGPT, are linked to analytic thinking scores from dictionary-based approaches like Linguistic Inquiry and Word Count (LIWC). Using over 16,000 texts from four samples and tested against three prompts and two models (GPT-3.5, GPT-4), the evidence suggests there were small associations between ChatGPT and LIWC analytic thinking scores (meta-analytic effect sizes: .058 &amp;lt; rs &amp;lt; .304; ps &amp;lt; .001). Critically, when given the formula to calculate the LIWC analytic thinking index, ChatGPT performed incorrect mathematical operations in 22.1% of the cases, suggesting basic word and number processing may be unreliable with large language models. Researchers should be cautious when using AI for text analysis.</p>"
10.1162/99608f92.7f9220ff,AI and Generative AI for Research Discovery and Summarization,N/A
10.1007/979-8-8688-0403-8_2,Text-to-Image Generation,N/A
10.1007/s43681-024-00559-7,Transhumanist technologies as enhancers of human nature and its dignity,N/A
10.1007/s43681-022-00217-w,Explainable AI lacks regulative reasons: why AI and human decision-making are not equally opaque,"<jats:title>Abstract</jats:title><jats:p>Many artificial intelligence (AI) systems currently used for decision-making are opaque, i.e., the internal factors that determine their decisions are not fully known to people due to the systems’ computational complexity. In response to this problem, several researchers have argued that human decision-making is equally opaque and since simplifying, reason-giving explanations (rather than exhaustive causal accounts) of a decision are typically viewed as sufficient in the human case, the same should hold for algorithmic decision-making. Here, I contend that this argument overlooks that human decision-making is sometimes significantly more transparent and trustworthy than algorithmic decision-making. This is because when people explain their decisions by giving reasons for them, this frequently prompts those giving the reasons to govern or regulate themselves so as to think and act in ways that confirm their reason reports. AI explanation systems lack this self-regulative feature. Overlooking it when comparing algorithmic and human decision-making can result in underestimations of the transparency of human decision-making and in the development of explainable AI that may mislead people by activating generally warranted beliefs about the regulative dimension of reason-giving.</jats:p>"
10.1007/s43681-023-00285-6,"Intelligent machines, collectives, and moral responsibility","<jats:title>Abstract</jats:title><jats:p>Collectives, such as companies, are generally thought to be moral agents and hence capable of being held responsible for what they do. If collectives, being non-human, can be ascribed moral responsibility, then can we do the same for machines? Is it equally the case that machines, particularly intelligent machines, can be held morally responsible for what they choose to do? I consider the conditions required for moral responsibility, and argue that, in terms of the agency condition, artificial, non-human entities in general are excused from being responsible because, although they may choose their actions, the beliefs and desires that form the basis of their choices are predetermined by their designers, placing them in an analogous position to persons suffering covert manipulation. This creates a problem for collective responsibility, but I argue that collectives, through their supervention on human persons, represent an exception. Finally, I consider that the design of future machines may be sufficiently abstract and high-level as to fall below some threshold of influence, allowing machines enough freedom for us to hold them responsible.</jats:p>"
10.1007/s43681-023-00307-3,Ethical considerations in emotion recognition technologies: a review of the literature,N/A
10.17261/pressacademia.2023.1788,GENERATIVE AI IN ELECTRICITY DISTRIBUTION: A QUALITATIVE EXPLORATION,"<jats:p xml:lang=""tr"">Purpose- The purpose of this study is to explore the application and potential of generative artificial intelligence (AI) within the context of electricity distribution companies. The study aims to investigate how these advanced AI technologies, particularly Generative Adversarial Networks (GANs), can address the sector's pressing challenges, such as load forecasting, power outage prediction, and preventive maintenance.
Methodology- The study employs a qualitative case study methodology, providing an in-depth analysis of real-world applications of generative AI within electricity distribution companies. The selection of cases represents a wide variety of experiences and contexts, facilitated by both primary data collected through semi-structured interviews with key personnel within the organizations and secondary data derived from an extensive review of company reports, public documentation, and industry publications. The gathered data was systematically analyzed using thematic analysis to identify and report recurring patterns and themes.
Findings- The analysis reveals that generative AI has been successfully implemented in various operational aspects of electricity distribution. The first case study presents how GANs have significantly improved load forecasting accuracy in an Eastern Turkish electricity distribution company. The second case study from Southern Turkey showcases how GANs have been used for predicting power outages, thereby aiding efficient resource allocation, reducing downtime, and enhancing customer satisfaction. Lastly, the third case from Northern Turkey demonstrates how generative AI has contributed to effective preventive maintenance of distribution equipment, improving overall system reliability.
Conclusion- Based on the analysis findings, it may be concluded that generative AI holds transformative potential for the electricity distribution sector. While the implementation of these technologies is associated with challenges such as data privacy, security, and the requirement of technical expertise, the benefits in terms of improved accuracy, system reliability, and resource efficiency provide a strong justification for their adoption. The paper underlines the importance of an interdisciplinary collaboration between AI researchers, electrical engineers, industry professionals, and policymakers for furthering the adoption of these technologies. As the field of generative AI continues to evolve, it is expected to have an even greater impact on the electricity distribution sector, thereby opening up exciting opportunities for future research and application.

Keywords: Generative artificial intelligence (ai), electricity distribution companies, generative adversarial networks (gans), load forecasting, outage prediction, preventive maintenance
JEL Codes: M40, M41
</jats:p>"
10.36227/techrxiv.24470032.v1,Enhancing Data Quality through Generative AI: An Empirical Study with Data,"<jats:p>&lt;p&gt;In today's increasingly data-driven landscape, organizations are shifting their focus toward leveraging data analytics for strategic decision-making. As data becomes a cornerstone of operational and strategic activities, the quality of this data has emerged as a non-negotiable aspect for organizations. Lack of attention to data quality can not only result in considerable revenue losses but can also cripple the effectiveness of analytics, causing misinformed decisions and strategic errors. Against this backdrop, this empirical study delves into the innovative avenue of utilizing Generative Artificial Intelligence (AI) as a mechanism for enhancing data quality.&lt;/p&gt;
&lt;p&gt;The research aims to explore multiple facets of organizational operations—ranging from technical infrastructure to business strategy—to ascertain the potential advantages offered by Generative AI. Utilizing a mix of qualitative and quantitative methods, we conducted in-depth interviews, case studies, and simulations to evaluate the impact of Generative AI on data quality.&lt;/p&gt;
&lt;p&gt;Our findings reveal a multi-layered benefit structure. Notably, we found that Generative AI is not a replacement for existing, traditional methods of data quality assurance but serves as a powerful supplement. It augments traditional methods by increasing the accuracy of data, thereby offering a more reliable foundation for analytics. Additionally, the use of Generative AI can streamline workflows, enhancing productivity among various roles including solution architects and software developers. Moreover, it facilitates a more nuanced and accurate requirement gathering process, enabling businesses to fine-tune their data analytics strategies more effectively.&lt;/p&gt;
&lt;p&gt;In conclusion, our study establishes that integrating Generative AI into data quality management processes can not only resolve immediate issues surrounding data accuracy but also lead to long-term organizational benefits, such as higher efficiency and more effective decision-making. This research serves as a pioneering step in the intersection of Generative AI and data quality, setting the stage for future studies and real-world applications.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4923465,"A ""Minority Report"" on Antitrust Policy in the Generative AI Ecosystem",N/A
10.1201/9781003503781-1,Introduction to Phishing,N/A
10.1007/s43681-024-00425-6,Formalizing ethical principles within AI systems: experts’ opinions on why (not) and how to do it,"<jats:title>Abstract</jats:title><jats:p>AI systems are increasingly put into contexts where computed decisions must be guided by ethical considerations. To develop ethically grounded algorithms and technologies, scholars have suggested computational ethics as an essential frontier, which aims to translate ethical principles into computer code. However, computational ethics has received little attention in academic literature so far, with existing work mainly focusing on its technical implementation, while many open questions concerning its (societal and ethical) implications still need to be resolved. Therefore, in this study, we interviewed 12 experts from philosophy, AI and cognitive sciences to shed light on computational ethics beyond a technical perspective. Findings suggest that indicated supporting and opposing arguments can be clustered into pragmatic/practical, societal and epistemic reasons, all of which need to be contemplated when engaging in computational ethics and developing resulting artificial moral agents. Furthermore, the mentioned recommendations for companies’ technological design and development, for industry’s governance measures and academia’s research endeavors are recapitulated and summarized in a holistic framework that aims to facilitate a reflected implementation of ‘ethics in and by design’ in the future.</jats:p>"
10.1007/s43681-022-00228-7,The AI ethics maturity model: a holistic approach to advancing ethical data science in organizations,"<jats:title>Abstract</jats:title><jats:p>The field of AI ethics has advanced considerably over the past years, providing guidelines, principles, and technical solutions for enhancing the ethical development, deployment and usage of AI. However, there is still a clear need for research that facilitates the move from the ‘what’ of AI ethics to the ‘how’ of governance and operationalization. Although promising literature on the challenge of implementation is increasingly more common, so far no systemic analysis has been published that brings the various themes of operationalization together in a way that helps the gradual advancement of AI ethics procedures within organizations. In this opinion paper we therefore set out to provide a holistic maturity framework in the form of an AI ethics maturity model comprising six crucial dimensions for the operationalization of AI ethics within an organization. We contend that advancing AI ethics in practice is a multi-dimensional effort, as successful operationalization of ethics requires combined action on various dimensions. The model as presented is a preliminary result of literature analysis complemented with insights from several practical mutual learning sessions with some of the major public, private and research organizations of the Netherlands. The article contributes to the AI ethics literature and practice by synthesizing relevant aspects of operationalization and relating these to the praxis of AI in a maturity model that provides direction for organizations seeking to implement these ethical principles.</jats:p>"
10.1007/s43681-023-00353-x,The human role to guarantee an ethical AI in healthcare: a five-facts approach,"<jats:title>Abstract</jats:title><jats:p>With the emergence of AI systems to assist clinical decision-making, several ethical dilemmas are brought to the general attention. AI systems are claimed to be the solution for many high-skilled medical tasks where machines can potentially surpass human ability as for example in identifying normal and abnormal chest X-rays. However, there are also warns that AI tools could be the basis for a human replacement that can risk dehumanisation in medicine. In recent years, important proposals in the domain of AI ethics in healthcare have identified main ethical issues, as for example fairness, autonomy, transparency, and responsibility. The human warranty, which implies human evaluation of the AI procedures, has been described to lower the ethical risks. However, as relevant these works have been, translating principles into action has proved challenging as existing codes were mostly a description of principles. There is a great need to produce <jats:italic>how-to</jats:italic> proposals that are specific enough to be action-guiding. We present five human-focussed facts designed into a framework of human action for an ethical AI in healthcare. Through the factors, we examine the role of medical practitioners, patients, and developers in designing, implementing, and using AI in a responsible manner that preserves human dignity. The facts encompass a range of ethical concerns that were commonly found in relevant literature. Given that it is crucial to bring as many perspectives as possible to the field, this work contributes to translate principles into human action to guarantee an ethical AI in health.</jats:p>"
10.1007/s43681-022-00213-0,"Humans, super humans, and super humanoids: debating Stephen Hawking’s doomsday AI forecast",N/A
10.1007/s43681-024-00431-8,"On inscription and bias: data, actor network theory, and the social problems of text-to-image AI models",N/A
10.1007/s43681-023-00299-0,"Navigating the legal landscape of AI copyright: a comparative analysis of EU, US, and Chinese approaches","<jats:title>Abstract</jats:title><jats:p>This paper compares AI copyright approaches in the EU, US, and China, evaluating their effectiveness and challenges. It examines the recognition of AI-generated works as copyrightable and the exclusive rights of copyright owners to reproduce, distribute, publicly display, and perform such works. Differences in approaches, such as recognizing AI as a sui generis right holder in the EU and the broad fair use doctrine in the US, are highlighted. This paper evaluates strengths and weaknesses of each approach, including enforcement and ownership of copyright in AI-generated works, and clarifies issues related to AI and copyright. While the EU and US have more developed legal frameworks for AI copyright than China, all three approaches face challenges that need addressing. This paper concludes by providing insight into the legal landscape of AI copyright and steps necessary for effective protection and use of AI-generated works.</jats:p>"
10.1044/leader.ftr2.28092023.ai-academia-slp-aud.52,What Does Generative AI Mean for Graduate Education in CSD?,N/A
10.1007/s43681-023-00263-y,"I, Robot: the three laws of robotics and the ethics of the peopleless economy",N/A
10.1007/s43681-021-00083-y,Mapping global AI governance: a nascent regime in a fragmented landscape,"<jats:title>Abstract</jats:title><jats:p>The rapid advances in the development and rollout of artificial intelligence (AI) technologies over the past years have triggered a frenzy of regulatory initiatives at various levels of government and the private sector. This article describes and evaluates the emerging global AI governance architecture and traces the contours of a nascent regime in a fragmented landscape. To do so, it organizes actors and initiatives in a two-by-two matrix, distinguishing between the nature of the driving actor(s) and whether or not their actions take place within the existing governance architecture. Based on this, it provides an overview of key actors and initiatives, highlighting their trajectories and connections. The analysis shows international organizations’ high levels of agency in addressing AI policy and a tendency to address new challenges within existing frameworks. Lastly, it is argued that we are witnessing the first signs of consolidation in this fragmented landscape. The nascent AI regime that emerges is polycentric and fragmented but gravitates around the Organisation for Economic Co-Operation and Development (OECD), which holds considerable epistemic authority and norm-setting power.</jats:p>"
10.1007/s00146-023-01665-6,Non-western AI ethics guidelines: implications for intercultural ethics of technology,N/A
10.1007/s43681-023-00314-4,On actor-network theory and algorithms: ChatGPT and the new power relationships in the age of AI,N/A
10.7249/pea2679-1,The Rise of Generative AI and the Coming Era of Social Media Manipulation 3.0: Next-Generation Chinese Astroturfing and Coping with Ubiquitous AI,N/A
10.4324/9781003459026-4,Redesigning Assessment in the AI Era,N/A
10.4324/9781032686783-3,Basics of Generative AI,N/A
10.7551/mitpress/11585.003.0015,Ian Goodfellow’s Generative Adversarial Networks:  AI Learns to Imagine,N/A
10.4018/979-8-3693-0074-9.ch003,Is It the End of Undergraduate Dissertations?,"<jats:p>This chapter delves into the intriguing realm of generative artificial intelligence (AI) models and their potential impact on undergraduate dissertations in the field of education. As AI continues to advance and permeate various aspects of our lives, the educational landscape is not immune to its transformative influence. The chapter begins by providing an overview of generative AI models, including their underlying principles and techniques such as deep learning, natural language processing, and neural networks. It then explores how these models can be harnessed to generate written content that is coherent, creative, and relevant, raising the question of whether undergraduate dissertations, as we know them, are destined to become obsolete. Advantages of employing generative AI models in education are scrutinized, highlighting their potential to enhance the efficiency and quality of student work.</jats:p>"
10.4018/979-8-3693-1351-0.ch019,Generative AI Soaring in the Skies,"<jats:p>Several regulations in the aviation industry have been made by authorities to minimize the incidents due to language barriers; however, a more reasonable solution lies in equipping prospective pilots with necessary language skills in target settings. This requires a transformation in aviation English (AE) classrooms through the medium of emerging technologies. Accordingly, this chapter presents a novel strategy for rethinking AE instruction by using approaches from generative artificial intelligence. It investigates the ways in which prompt engineering and synthetic content production might be used to improve the efficacy of AE teaching. Learners may benefit from better language comprehension, communication skills, and situational awareness by seamlessly incorporating AI-driven language creation into the curriculum. Instructors could find examples of generative AI-driven classroom applications for improved learning outcomes. Considering these potential contributions to the field, this chapter may, in turn, innovate and uplift the AE courses and safety in the aviation industry.</jats:p>"
10.21275/sr24402184343,The Disruptive Influence of Generative AI in Life Science and Healthcare,N/A
10.21275/sr24509232318,The Development of AI with Generative Capabilities and Its Effect on Education,N/A
10.4018/979-8-3693-1351-0.ch004,Generative AI-Powered Chatbots,"<jats:p>Imagine having a concept for a literary work but being unable to see it take shape because of a plethora of reasons: paucity of time, writer's block, linguistic barriers, among others. Gen AI promises to be a supportive partner in these creative endeavours. With the synergy of mind and machine, literary texts are born. This chapter attempts to provide insights into the comparison between the nuances of human-AI co-creative partnerships through the crafting of short stories. Using the lens of Collaborative Autoethnography, two seasoned educators collaboratively reflect on cognitive, behavioural, and affective dimensions of co-creating with ChatGPT, a chatbot developed by Open AI. The chapter analyses the co-creation process followed by each of the authors and brings out the commonalities and specificities of the individual lived experience to achieve a narrative piece of work. The insights of the co-creation process would be beneficial to educators, curriculum designers, technology specialists, students and those who intend to use AI-powered chatbots as co-creators.</jats:p>"
10.21428/e4baedd9.b4f754fd,"Visual Artists, Technological Shock, and Generative AI",N/A
10.22541/au.170534566.63147021/v1,"The Spectre of Generative AI Over Advertising, Marketing, and Branding",N/A
10.32388/jpecon,The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation,"<jats:p>Generative Artificial Intelligence (AI) has become a powerful tool to create new worlds and inject meaning into staid environments. A process of transforming physical space into digital assets becomes necessary to ensure that culturally relevant objects can persist through the changing tides of society. This paper highlights a process whereby physical assets can be digitized and utilized within a generative AI environment while preserving provenance, authenticity, and veracity. It concludes with salient use cases and a discussion that hopefully will lead to more action.
</jats:p>"
10.4018/979-8-3693-1950-5.ch011,"Artistic Expressions, Generative AI, and Legal Tapestry","<jats:p>This study delves into the transformative impact of generative artificial intelligence (AI) on creative production, exploring the collaborative efforts of artists and cutting-edge technology. The convergence of human ingenuity and AI in artistic expression expands creative boundaries but introduces complex intellectual property (IP) challenges. Examining beyond visual art, the study reveals the intricate engagement between humans and advanced algorithms, offering a unique perspective on the creative process. The intersection of AI and creativity raises critical copyright concerns, challenging traditional concepts of authorship and infringement. Addressing the global scope of AI-generated art, the study advocates for a unified framework to navigate legal complexities and ensure ethical considerations in the evolving landscape of creative technology.</jats:p>"
10.4324/9781003459026-5,Developing an AI in Education Policy,N/A
10.4018/979-8-3693-0487-7.ch005,Utilizing Generative AI With Second-Language Users and Bilingual Students,"<jats:p>The growth of generative artificial intelligence (for purposes of this chapter, “AI”) has created endless systems to utilize modern technological assistance in the educational arena. AI has the potential to significantly impact higher education by facilitating personalized development and knowledge sharing. There are many ways that AI can be leveraged to enhance the education process. This chapter will focus on second-language users and bilingual students learning new language concepts. Second language users can be defined as students in the language acquisition process. A bilingual student is any student who has already acquired a second language but needs to build on terminology in their field of study. AI can provide a functional teaching tool for educators to assist with building the required knowledge base. The premise is to embrace the tool—versus forbidding its use by students—to assist them in their educational growth.</jats:p>"
10.4018/979-8-3693-0831-8.ch009,The Influence of Generative AI on Interpersonal Communication Dynamics,"<jats:p>In this comprehensive exploration, the interaction between generative AI and interpersonal communication is examined. The initial sections delve into the characteristics and limitations of AI-generated responses, highlighting the challenges of context and non-verbal cues interpretation. The potential for AI-driven interpersonal skill development is presented. The discussion progresses to address the changing dynamics in the classroom, contrasting traditional communication training with AI-augmented methods. The efficacy of AI in group discussions and role-plays is assessed, with a central focus on whether AI augments or diminishes human connections. The final sections explore the potential of generative AI in reshaping our understanding of effective communication and the necessity for educators to uphold the human element while leveraging AI's skill-enhancing capabilities. This comprehensive review offers insights into the evolving landscape of AI and interpersonal communication, shedding light on its opportunities, challenges, and the path forward.</jats:p>"
10.4018/979-8-3693-2418-9.ch008,Generative AI in Curriculum Development,"<jats:p>Generative artificial intelligence (GAI) is becoming a crucial influence in different industries, particularly transforming education through the reshaping of curriculum creation. This chapter discusses how GAI can improve educational inclusion and comprehensiveness by providing personalized, customized and adaptive learning experiences. The framework in this chapter provides a structured approach to integrating GAI into curriculum design with the goal of creating dynamic, flexible, and personalized educational paths. By utilizing advanced machine learning and natural language processing, GAI enables educators to create personalized and tailored learning modules, promoting an inclusive educational setting. The discussion in this chapter highlights the crucial significance of GAI in developing educational methods and curriculum, establishing a new model for future learning environments.</jats:p>"
10.36227/techrxiv.24470032,Enhancing Data Quality through Generative AI: An Empirical Study with Data,"<jats:p>&lt;p&gt;In today's increasingly data-driven landscape, organizations are shifting their focus toward leveraging data analytics for strategic decision-making. As data becomes a cornerstone of operational and strategic activities, the quality of this data has emerged as a non-negotiable aspect for organizations. Lack of attention to data quality can not only result in considerable revenue losses but can also cripple the effectiveness of analytics, causing misinformed decisions and strategic errors. Against this backdrop, this empirical study delves into the innovative avenue of utilizing Generative Artificial Intelligence (AI) as a mechanism for enhancing data quality.&lt;/p&gt;
&lt;p&gt;The research aims to explore multiple facets of organizational operations—ranging from technical infrastructure to business strategy—to ascertain the potential advantages offered by Generative AI. Utilizing a mix of qualitative and quantitative methods, we conducted in-depth interviews, case studies, and simulations to evaluate the impact of Generative AI on data quality.&lt;/p&gt;
&lt;p&gt;Our findings reveal a multi-layered benefit structure. Notably, we found that Generative AI is not a replacement for existing, traditional methods of data quality assurance but serves as a powerful supplement. It augments traditional methods by increasing the accuracy of data, thereby offering a more reliable foundation for analytics. Additionally, the use of Generative AI can streamline workflows, enhancing productivity among various roles including solution architects and software developers. Moreover, it facilitates a more nuanced and accurate requirement gathering process, enabling businesses to fine-tune their data analytics strategies more effectively.&lt;/p&gt;
&lt;p&gt;In conclusion, our study establishes that integrating Generative AI into data quality management processes can not only resolve immediate issues surrounding data accuracy but also lead to long-term organizational benefits, such as higher efficiency and more effective decision-making. This research serves as a pioneering step in the intersection of Generative AI and data quality, setting the stage for future studies and real-world applications.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4438593,Copyright Safety for Generative AI,N/A
10.3030/101138056,N/A,N/A
10.2139/ssrn.4899433,Generative AI in Charitable Fundraising,N/A
10.31235/osf.io/aq4tw,Complex Systems Analysis of Generative AI:  Mapping Interdependencies in Societal Impact,"<p>This paper applies complex systems theory to examine generative artificial intelligence (AI) as a contemporary wicked problem. Generative AI technologies, which autonomously create content like images and text, intersect with societal domains such as ethics, economics, and governance, exhibiting complex interdependencies and emergent behaviors. Using methodologies like network analysis and agent-based modeling, the paper maps these interactions and explores potential interventions. A mathematical model is developed to simulate the dynamics between key components of the AI-society system, including AI development, economic concentration, labor markets, regulatory frameworks, public trust, ethical implementation, global competition, and distributed AI ecosystems. The model demonstrates non-linear dynamics, feedback loops, and sensitivity to initial conditions characteristic of complex systems. By simulating various interventions, the study provides insights into strategies for steering AI development towards more positive societal outcomes. These include strengthening regulatory frameworks, enhancing ethical implementation, and promoting distributed AI ecosystems. The paper advocates for using this complex systems framework to inform inclusive policy and regulatory strategies that balance innovation with societal well-being. It concludes that embracing complexity enables stakeholders to better navigate the evolving challenges of generative AI, fostering more sustainable and equitable technological advancements.</p>"
10.5220/0012741000003687,Unlocking Adaptive User Experience with Generative AI,N/A
10.4018/979-8-3693-0074-9.ch011,Navigating the Terrain,"<jats:p>The chapter provides an extensive exploration of Generative AI in education. It investigates the evolution and significance of AI in educational settings while delving into contemporary issues such as ethics, privacy, fairness, and pedagogy. Furthermore, it examines the impact on traditional teaching methods, personalization, and accessibility, addressing educational disparities. The chapter also outlines the best practices and lessons learned from case studies and successful institutions, pointing toward future directions, including emerging technologies like GPT-4 and augmented reality. It emphasizes advancing ethical guidelines, enhancing teacher-student collaboration with AI, proposing policy recommendations, and establishing legal frameworks for student data protection, along with government initiatives and funding in the realm of AI in education.</jats:p>"
10.4018/979-8-3693-3278-8.ch008,Exploring Creativity,"<jats:p>Generative AI, often known as genAI, encompasses several forms of artificial intelligence (AI) that has the ability to create unique text, images, video, or audio content. This particular iteration of artificial intelligence acquires knowledge of patterns and data arrangement from its training data, enabling it to produce novel outputs that possess similar statistical characteristics. Generative AI has a diverse range of applications, and each task requires a specialized deep-learning architecture to effectively capture the unique patterns and traits found in the training data. Generative AI models encompass various types, including generative adversarial networks (GANs), variational autoencoders (VAEs), transformers, diffusion models, normalizing flow models, and hybrid models. The configuration of a generative AI model is contingent upon the particular task and domain, encompassing elements such as the neural network's architecture, training approach, loss function, and evaluation metrics. The primary objective of generative AI is to develop autonomous systems capable of generating content that is indiscernible from information created by humans. This encompasses the production of written content, visual graphics, audio recordings, and interactive visual components. Attaining this objective would facilitate a diverse array of applications, encompassing enhanced human-computer interactions and assisting in the advancement of endeavors such as art and storytelling.</jats:p>"
10.1145/3605098.3636180,Duty vs. Consequence: Exploring Teachers' Assessment of the Ethical Dimensions of Generative AI Technologies,N/A
10.4018/979-8-3693-2418-9.ch015,Unleashing Creative Potential,"<jats:p>This study examines the overlooked role of creativity in business school curricula, despite its recognized importance in fostering innovation and solving global challenges. It critiques traditional educational models for failing to nurture creative thinking and argues for the integration of Generative AI (GAI) systems to enhance creativity in business education. Using an exploratory case study approach, the authors examined the existing literature that supports the business imperative for creativity; then, GAI was posited as a tool to improve divergent and convergent thinking, offering practical applications across various business disciplines. Additionally, ethical considerations surrounding data usage, intellectual property, and privacy were highlighted. The study concludes that responsibly integrating GAI can revolutionize business education, preparing future leaders for an AI-augmented business world while maintaining the necessity for ethical, informed, and transparent use.</jats:p>"
10.5220/0012688700003690,Enhancing Constructivist Learning: The Role of Generative AI in Personalised Learning Experiences,N/A
10.2139/ssrn.4478370,Generative AI and Author Remuneration,N/A
10.36227/techrxiv.24449185,How Text-to-Image Generative AI Is Transforming Mediated Action?,"<jats:p>&lt;p&gt;This article examines the intricate relationship between humans and text-to-image generative models (genAI) in the realms of art and design. The article frames that relationship in the theory of mediated action—a well-established theory that conceptualizes how tools shape human thoughts and actions. It describes genAI systems as learning, co-creating, and communicating, multimodally capable hybrid systems that distill and rely on the wisdom and creativity of massive crowds of people and can sometimes surpass them. Those systems elude the theoretical description of the role of tools and locus of control in mediated action. The article asks how well the theory can accommodate both the transformative potential of genAI tools in creative fields and art, and the ethics of the emergent social dynamics it generates. The article concludes by discussing the fundamental changes and broader implications that genAI brings to the realm of mediated action and, ultimately, to the very fabric of our daily lives. &lt;/p&gt;</jats:p>"
10.2139/ssrn.4595149,Generative AI and IP Infringement,N/A
10.4018/979-8-3693-0074-9.ch005,Stepping Stones for Self-Learning,"<jats:p>One of the themes in the emergence of text- and image-making (multimodal) generative AIs is their value in the learning space, with the vast potential just beginning to be explored by mass humanity. This chapter explores the potential and early use of large language models (LLMs) harnessed for their mass learning, human-friendly conversations, and their efficacies, for self-learning for individuals and groups, based on a review of the literature, system constraints and affordances, and abductive logic. There are insights shared about longitudinal and lifelong learning and foci on co-evolving processes between the human learner and the computing machines and large language models.</jats:p>"
10.4018/979-8-3693-1565-1.ch010,Navigating the Legal and Ethical Framework for Generative AI,"<jats:p>Generative AI systems have given incredible ability to independently produce a wide variety of content types, including textual, visual, and more. Complex issues with copyright protection and intellectual property rights have arisen as a result of this change. With a focus on fostering responsible global governance, this research delves into the complex legal and ethical considerations underlying Generative AI. The goal of this chapter is to take a look at the complicated legal issues that come up because of Generative AI's ability to generate material on its own. This chapter analyzes the current legal documents, legislation, and international treaties, focusing on ethical concerns. Ultimately, the authors want to have a positive impact on efforts to build responsible and efficient international frameworks for regulating Generative AI. This study provides an exhaustive case for the implementation of legal frameworks that can efficiently tackle the intricate legal and ethical quandaries posed by Generative AI, while simultaneously encouraging the progress of innovation and creativity.</jats:p>"
10.21275/sr24613105251,Charting the Future: Harnessing the Power of Generative AI in Financial Ecosystems,N/A
10.21275/sr24520092232,DyGAISP: Generative AI-Powered Approach for Intelligent Software Lifecycle Planning,N/A
10.33140/eoa.01.04.06,Melo Harmony: Exploring Emotion in Crafting AI-Generated Music with Generative Adversarial Network Powered Harmony,"<jats:p>This research paper delves into the convergence of artificial intelligence (AI) and music composition by examining the integration of emotion in crafting AI-generated music through Generative Adversarial Network (GAN)-powered harmony. The primary goal is to demonstrate that AI-generated music can effectively evoke and communicate emotions, enhancing its artistic and expressive potential. This paper presents a comprehensive framework for harmonization using GANs, infusing emotional awareness into the generated compositions, and outlines future directions for practical application and validation.</jats:p>"
10.1117/12.3013240.75c55873-4a66-ee11-a99c-00505691c5e1,N/A,N/A
10.1609/aaaiss.v3i1.31256,The Impacts of Text-to-Image Generative AI on Creative Professionals According to Prospective Generative AI Researchers: Insights from Japan,"<jats:p>The growing interest in Japan to implement text-to-image (T2I) generative artificial intelligence (GenAI) technologies in creative workflows has raised concern over what ethical and social implications these technologies will have on creative professionals. Our pilot study is the first to discuss what social and ethical oversights may emerge regarding such issues from prospective Japanese researchers – computer science (CS) graduate students studying in Japan. Given that these students are the primary demographic hired to work at research and development (R&amp;D) labs at the forefront of such innovations in Japan, any social and ethical oversight on such issues may unequip them as future knowledge experts who will play a pivotal role in helping shape Japan’s policies regarding image generating AI technologies.</jats:p>"
10.4018/979-8-3693-1950-5.ch009,Remote Virtual Sanctuary,"<jats:p>2023 was an original year, with global humanity emerging from a deadly pandemic (COVID-19), facing the advent of artmaking generative AIs, and surviving in a time of geopolitical turmoil, economic and financial pressures, and social strife. What role does an online social network (built up around an artmaking generative AI platform) play for people in this present moment? How does the remote virtual community enable participants to seek various fulfillments? The web-facing Deep Dream Generator tool and platform has proven itself to be a powerful social space for many with rich immersions and remote social interactions. This work is a practice-led case study.</jats:p>"
10.4018/979-8-3693-2418-9.ch003,Finding Time for Creativity in Higher Education Writing Through Generative AI,"<jats:p>In this chapter, the authors discuss the relationship between time poverty, creativity, and Large Language Models of AI (LLMs) or Generative AI technologies in the writing classroom, including ChatGPT and GenAI software in word processors such as Microsoft Word. Through examples of writing provided by ChatGPT, a student profile, and comparative analysis of writing provided by two LLMs, they make an argument that LLMs should be integrated into the writing classroom in order to offer students who suffer from time poverty the ability to practice creativity in their writing which they may struggle to achieve when limited to the time they can devote to their classwork. Specifically, they explore how ChatGPT can support students in the writing process, including researching, pre-writing, writing, revision, and reflection in order to foster creativity in their writing.</jats:p>"
10.4018/979-8-3693-5578-7.ch003,Exploring the Application of Generative AI in Human Resource Management,"<jats:p>This chapter reviews the current state of generative artificial intelligence (AI) and human resource management (HRM). It discusses the current application of Generative AI in the core functional areas of HRM, identifies the main challenges posed by Generative AI, emphasizes the increasing role and influence of Generative AI applications in the workplace, and suggests future research directions. Current applications of Generative AI in HRM include automation, personalization, decision support, and bias reduction in HR processes. However, the urgent challenges related to transparency, bias mitigation, ethical use, and data privacy must be addressed for responsible deployment. This chapter highlights the opportunities for integrating Generative AI in HRM to improve HR processes while ensuring ethical and fair implementation. The future research directions for Generative AI and HRM focus on reducing bias in AI models, developing ethical frameworks, enhancing data privacy, and exploring AI's impact on organizational culture and employee engagement.</jats:p>"
10.1093/oxfordhb/9780190067397.013.23,Troubleshooting AI and Consent,"<p>This chapter addresses the controversy over the role of consent in data protection, as artificial intelligence systems have proliferated in people’s daily lives. Digital consent has been criticized as a meaningless, procedural act because users encounter so many different, long, and complicated terms of service that do not help them effectively assess potential harms or threats. AI systems have played a role in exacerbating existing issues, creating new challenges, and presenting alternative solutions. Most of the critiques and cures for this broken arrangement address choice-making, not consent. As the United States debates whether and why to break up big tech, and the European Union considers enforcement actions under the General Data Protection Regulation and how to update its laws to address tracking techniques in a new AI-driven smart world, consent cannot be confused with choice. Consent must be defined by its moral core, involving clear background conditions, defined scope, knowledge, voluntariness, and fairness. When consent meets these demands, it remains a powerful tool for contributing to meaningful data protection at the individual and societal levels.</p>"
10.4018/979-8-3693-8557-9.ch012,From Data to Insights,"<jats:p>This chapter examined the utilization of GeoAI for healthcare issues like patient demographics, socio-economic factors, health insurance coverage, and environmental conditions to enhance healthcare delivery. By analyzing population density and socio-economic data, GeoAI can optimize the placement of healthcare facilities and resources, ensuring equitable access. Environmental data, such as air and water quality, can identify regions at risk for specific health issues, guiding targeted interventions. GeoAI can address healthcare disparities by considering social determinants of health and enabling personalized treatment plans. Integrating high-quality, timely, and relevant geospatial data can allow GeoAI to facilitate accurate decision-making and resource allocation, improving patient outcomes and public health strategies. However, challenges include ensuring data privacy and security, managing technical and operational integration complexities, and requiring specialized expertise, which can be overcome with robust data governance and interdisciplinary collaboration.</jats:p>"
10.1007/s43681-023-00372-8,Socialisation approach to AI value acquisition: enabling flexible ethical navigation with built-in receptiveness to social influence,"<jats:title>Abstract</jats:title><jats:p>This article describes an alternative starting point for embedding human values into artificial intelligence (AI) systems. As applications of AI become more versatile and entwined with society, an ever-wider spectrum of considerations must be incorporated into their decision-making. However, formulating less-tangible human values into mathematical algorithms appears incredibly challenging. This difficulty is understandable from a viewpoint that perceives human moral decisions to primarily stem from intuition and emotional dispositions, rather than logic or reason. Our innate normative judgements promote prosocial behaviours which enable collaboration within a shared environment. Individuals internalise the values and norms of their social context through socialisation. The complexity of the social environment makes it impractical to consistently apply logic to pick the best available action. This has compelled natural agents to develop mental shortcuts and rely on the collective moral wisdom of the social group. This work argues that the acquisition of human values cannot happen just through rational thinking, and hence, alternative approaches should be explored. Designing receptiveness to social signalling can provide context-flexible normative guidance in vastly different life tasks. This approach would approximate the human trajectory for value learning, which requires social ability. Artificial agents that imitate socialisation would prioritise conformity by minimising detected or expected disapproval while associating relative importance with acquired concepts. Sensitivity to direct social feedback would especially be useful for AI that possesses some embodied physical or virtual form. Work explores the necessary faculties for social norm enforcement and the ethical challenges of navigating based on the approval of others.</jats:p>"
10.1007/s43681-023-00386-2,A design perspective on how to tackle gender biases when developing AI-driven systems,"<jats:title>Abstract</jats:title><jats:p>A growing awareness of bias in artificial intelligence (AI) systems has recently emerged, leading to an increased number of publications discussing ethics in AI. Nevertheless, the specific issue of gender bias remains under-discussed. How can design contribute to preventing the emergence of gender bias in AI-driven systems? To answer this question, we investigated the current state of AI ethical guidelines within the European Union. The results revealed that most guidelines do not acknowledge gender bias but address discrimination. This raised our concerns, as addressing multiple biases simultaneously might not effectively mitigate any of them due to their often-unconscious nature. Furthermore, our results revealed a lack of quantitative evidence supporting the effectiveness of bias prevention implementation methods and solutions. In conclusion, based on our analysis, we propose four recommendations for designing effective guidelines to tackle gender biases in AI. Moreover, we stress the central role of diversity in embedding the gender perspective from the beginning in any design activity.</jats:p>"
10.1007/s43681-024-00456-z,Gender Tapestry: gender classification as color assignation,"<jats:title>Abstract</jats:title><jats:p><jats:italic>Gender Tapestry</jats:italic> is a multi-stage interactive AI art project, challenging traditional gender classification systems. This project diverges from binary approaches by recognizing the individuality of the gender experience and expression. This paper draws parallels to the ways color perception differs amongst people and how gender is also experienced in different ways due to lived experiences. Gender Tapestry uses a multi-label classification system, with predictions extending across six gender categories, with a custom RGB color generated based on the outcomes. Participants receive binary images of their face in their personalized colors and styles, while their uploaded photos contribute to a live Generative Adversarial Network training process. This work was created in response to the very binary representations of gender in AI and the lack of representation for genders outside of the binary. The culmination is an evolving mosaic artwork, incorporating all users' custom colors, illustrating gender as a fluid construct. The mosaic gains in complexity as more images are added and more colors enter the mix, creating a community artwork on gender as a 3D color spectrum. This work contributes to the discourse on diversity and inclusion in AI, emphasizing the fluidity of gender and fostering unconventional artistic representations.</jats:p>"
10.1007/s43681-024-00421-w,Adopting trust as an ex post approach to privacy,"<jats:title>Abstract</jats:title><jats:p>This research explores how a person with whom information has been shared and, importantly, an artificial intelligence (AI) system used to deduce information from the shared data contribute to making the disclosure context private. The study posits that private contexts are constituted by the interactions of individuals in the social context of intersubjectivity based on trust. Hence, to make the context private, the person who is the trustee (i.e., with whom information has been shared) must fulfil trust norms. According to the commitment account of trustworthiness, a person is trustworthy only if they satisfy the norm of competence. It is argued that a person using an AI system to answer a question is competent only if they are ex post justified in believing what has been delivered by the AI system. A person’s belief is justified in the doxastic sense only if the AI system is accurate. This feature of AI’s performance affects a person’s competence and, as a result, trustworthiness. The effect of AI on trust as an essential component of making the context private, and thus on privacy, means an AI system also impacts privacy. Therefore, a private context is constituted when the individual with whom the information is shared fulfils the competence norm and the AI system used for analysing the information is sufficiently accurate to adhere to this norm. The result of this research emphasises the significance of the relationship between individuals involved in information-sharing and how an AI system used for analysing that information impacts the relationship regarding making the context private, as well as how it impacts privacy. The findings of this research have significant implications for improving or ameliorating privacy regulations in light of trust.</jats:p>"
10.1007/s43681-021-00100-0,Why ethical audit matters in artificial intelligence?,N/A
10.1007/s43681-023-00369-3,"Click-Gap, paternalism, and tech giants’ relationships with their users","<jats:title>Abstract</jats:title><jats:p>The spread of misinformation and fake news raises important problems for our society and for our democracy. From the January 6 attack on the U.S. Capitol to vaccine hesitancy, from suppressing voter turnout to peddling conspiracy theories, we know that these problems are real and need to be taken seriously. While misinformation is not a new problem for democracy, it can spread more quickly and easily because of new media’s design and popularity. Given these problems, it is encouraging that some technology companies are taking steps to reduce the spread of misinformation and fake news on the platforms they manage. Despite this seemingly positive development, some scholars have criticized some interventions designed to combat the spread of misinformation and fake news as paternalistic. For example, a 2019 Facebook intervention called Click-Gap aimed to reduce the amount of low-quality content (including fake news and misinformation) that users see on their NewsFeed. Click-Gap has been criticized as an instance of epistemic paternalism because it was adopted (1) with the goal of improving the epistemic status of its users and (2) irrespective of what the company believed the wishes of its users to be. If interventions like Click-Gap are problematic because paternalistic, those of us interested in the ethics of technology would face a dilemma—either endorse technology companies treating their users paternalistically or endorse their failing to act to combat the spread of misinformation and fake news on their platforms. Both options seem to me to be problematic. While paternalism may sometimes be permissible, I think we should be very hesitant to endorse a paternalistic relationship between technology companies and their users. The relationship does not seem to bear the right sort of structure to one in which paternalism might be appropriate, if it ever is. The second option seems, if anything worse: surely technology companies should not stand by and change nothing about their platforms despite the spread of misinformation and fake news in those spaces. In this paper, I argue that Click-Gap and interventions like it are not paternalistic, contrary to the conclusion of other scholars. Further, I will argue that the focus on paternalism itself is actually a red herring here. While not just any intervention or strategy that purports to reduce fake news and misinformation is permissible, we should want technology companies to take user well-being seriously and be able to take that well-being as a direct reason for action. Their doing so is not paternalistic nor even morally problematic, and it should not be criticized as such.</jats:p>"
10.1007/s43681-023-00320-6,The many meanings of meaningful human control,"<jats:title>Abstract</jats:title><jats:p>The concept of Meaningful Human Control (MHC) has gained prominence in the field of Artificial Intelligence ethics. MHC is discussed in relation to lethal autonomous weapons, autonomous cars, and more recently, AI systems in general. Little, however, has been done to analyze the concept. Those using MHC tend to look at it narrowly and intuitively—as if it is clear what it means. They fail to see the many issues concerning human control over machines. In this article, I break the concept into its three constitutive words (‘meaningful’, ‘human’, and, ‘control’) to outline the many meanings of MHC. While the intention is not to come to the<jats:italic>real</jats:italic>meaning of MHC, this analysis brings up the many issues that should be considered if meaningful human control is to be realized. These include: which humans count as meaningful in the application context, whether the control those humans are given must be meaningful, whether humans must retain control over the things that are meaningful in life, whether the style of control is human-like, whether each actor (designer, operator, subject, government) has the control they need, and what it is exactly that a human is controlling (e.g., the training data, the inputs, the outputs, etc. of the AI system).</jats:p>"
10.1007/s43681-024-00547-x,Artificial intelligence and its ‘slow violence’ to human rights,"<jats:title>Abstract</jats:title><jats:p>Human rights concerns in relation to the impacts brought forth by artificial intelligence (‘AI’) have revolved around examining how it affects specific rights, such as the right to privacy, non-discrimination and freedom of expression. However, this article argues that the effects go deeper, potentially challenging the foundational assumptions of key concepts and normative justifications of the human rights framework. To unpack this, the article applies the lens of ‘slow violence’, a term borrowed from environmental justice literature, to frame the grinding, gradual, attritional harms of AI towards the human rights framework.</jats:p><jats:p>The article examines the slow violence of AI towards human rights at three different levels. First, the individual as the subject of interest and protection within the human rights framework, is increasingly unable to understand nor seek accountability for harms arising from the deployment of AI systems. This undermines the key premise of the framework which was meant to empower the individual in addressing large power disparities and calling for accountability towards such abuse of power. Secondly, the ‘slow violence’ of AI is also seen through the unravelling of the normative justifications of discrete rights such as the right to privacy, freedom of expression and freedom of thought, upending the reasons and assumptions in which those rights were formulated and formalised in the first place. Finally, the article examines how even the wide interpretations towards the normative foundation of human rights, namely human dignity, is unable to address putative new challenges AI poses towards the concept. It then considers and offers the outline to critical perspectives that can inform a new model of human rights accountability in the age of AI.</jats:p>"
10.1007/s43681-024-00545-z,"The ethico-politics of design toolkits: responsible AI tools, from big tech guidelines to feminist ideation cards","<jats:title>Abstract</jats:title><jats:p>This paper interrogates the belief in <jats:italic>toolkitting</jats:italic> as a method for translating AI ethics theory into practice and assesses the toolkit paradigm’s effect on the understanding of ethics in AI research and AI-related policy. Drawing on a meta-review of existing ‘toolkit-scoping’ work, I demonstrate that most toolkits embody a reductionist conception of ethics and that, because of this, their capacity for facilitating change is limited. Then, I analyze the features of several ‘alternative’ toolkits–informed by feminist theory, posthumanism, and critical design–whose creators recognize that ethics cannot become a box-ticking exercise for engineers, while <jats:italic>the ethical</jats:italic> should not be dissociated from <jats:italic>the political</jats:italic>. This analysis then serves to provide suggestions for future toolkit creators and users on how to meaningfully adopt the toolkit format in AI ethics work without overselling its transformative potential: how different stakeholders can draw on the myriad of tools to achieve socially desirable results but reject the oversimplification of ethical practice that many toolkits embody.</jats:p>"
10.1007/s43681-020-00025-0,Job candidates’ reactions to AI-Enabled job application processes,N/A
10.1007/s43681-024-00477-8,Addressing trade-offs in co-designing principles for ethical AI: perspectives from an industry-academia collaboration,"<jats:title>Abstract</jats:title><jats:p>The development and deployment of artificial intelligence (AI) has rapidly outpaced regulation. As a result, many organizations opt to develop their own principles for the ethical development of AI, though little research has examined the processes through which they are developed. Prior research indicates that these processes involve perceived trade-offs between competing considerations, and primarily between ethical concerns and organizational benefits or technological development. In this paper, we report on a novel, collaborative initiative in Japan between researchers in the humanities and social sciences, and industry actors to co-design organizational AI ethics principles. We analyzed the minutes from 20 meetings from the formative phase of the development of these principles using an inductive process drawing on thematic analysis, to identify the issues of importance to participants. Through this, we identified four core trade-offs faced by participants. We find that, contrary to prior literature, participants were not just concerned with trade-offs between ethical concerns and organizational benefits or technological development, but also between competing, ethically-oriented considerations. We use the results of this study to highlight a need for further research to understand the longer-term impact on organizations and on society of organization-led approaches to AI ethics.</jats:p>"
10.1007/s43681-022-00239-4,"Democracy, epistemic agency, and AI: political epistemology in times of artificial intelligence","<jats:title>Abstract</jats:title><jats:p>Democratic theories assume that citizens have some form of political knowledge in order to vote for representatives or to directly engage in democratic deliberation and participation. However, apart from widespread attention to the phenomenon of fake news and misinformation, less attention has been paid to <jats:italic>how</jats:italic> they are supposed to acquire that knowledge in contexts shaped by artificial intelligence and related digital technologies. While this topic can also be approached from an empirical angle, this paper contributes to supporting concerns about AI and democracy by looking at the issue through the lens of political epistemology, in particular using the concept of epistemic agency. It argues that artificial intelligence (AI) endangers democracy since it risks to diminish the epistemic agency of citizens and thereby undermine the relevant kind of political agency in democracy. It shows that next to fake news and manipulation by means of AI analysis of big data, epistemic bubbles and the defaulting of statistical knowledge endanger the epistemic agency of citizens when they form and wish to revise their political beliefs. AI risks to undermine trust in one’s own epistemic capacities and hinder the exercise of those capacities. If we want to protect the knowledge basis of our democracies, we must address these problems in education and technology policy.</jats:p>"
10.1007/s43681-023-00323-3,Challenging AI for Sustainability: what ought it mean?,"<jats:title>Abstract</jats:title><jats:p>This paper argues that the terms ‘Sustainable artificial intelligence (AI)’ in general and ‘Sustainability of AI’ in particular are overused to the extent that they have lost their meaning. The AI for (social) good movement is a manifestation of this trend in which almost any application used in the context of healthcare or agriculture can be classified as AI for good regardless of whether such applications have been evaluated from a broader perspective. In this paper, we aim to create a common understanding of what the ‘AI for Sustainability’ movement ought to mean. We distinguish between two possible AI for Sustainability applications, namely those that fulfill the necessary conditions and those that fulfill the sufficient conditions. The former are purely predictive systems that serve as information providers. The latter are directly involved in an activity that contributes to a sustainability goal. We argue that taking action is a key element in distinguishing between these two application groups, as inaction is the key bottleneck in effectively tackling climate change. Furthermore, we question how effective the use of AI applications can be for sustainability when the systems themselves are inherently unsustainable. Hence, AI for Sustainability should include both an action that contributes to a sustainable end goal as well as an investigation of the sustainability issues of the AI system itself. Following that, Sustainable AI research can be on a gradient: AI in an application domain, AI towards sustainability, and AI for Sustainability.</jats:p>"
10.1007/s43681-023-00258-9,From ethical AI frameworks to tools: a review of approaches,"<jats:title>Abstract</jats:title><jats:p>In reaction to concerns about a broad range of potential ethical issues, dozens of proposals for addressing ethical aspects of artificial intelligence (AI) have been published. However, many of them are too abstract for being easily translated into concrete designs for AI systems. The various proposed ethical frameworks can be considered an instance of principlism that is similar to that found in medical ethics. Given their general nature, principles do not say how they should be applied in a particular context. Hence, a broad range of approaches, methods, and tools have been proposed for addressing ethical concerns of AI systems. This paper presents a systematic analysis of more than 100 frameworks, process models, and proposed remedies and tools for helping to make the necessary shift from principles to implementation, expanding on the work of Morley and colleagues. This analysis confirms a strong focus of proposed approaches on only a few ethical issues such as explicability, fairness, privacy, and accountability. These issues are often addressed with proposals for software and algorithms. Other, more general ethical issues are mainly addressed with conceptual frameworks, guidelines, or process models. This paper develops a structured list and definitions of approaches, presents a refined segmentation of the AI development process, and suggests areas that will require more attention from researchers and developers.</jats:p>"
10.1007/s43681-023-00275-8,A policy primer and roadmap on AI worker surveillance and productivity scoring tools,N/A
10.1007/s43681-023-00343-z,"Algorithmic audits of algorithms, and the law",N/A
10.1007/s43681-023-00344-y,Socially responsible facial recognition of animals,N/A
10.1007/979-8-8688-0403-8_8,Building Demo Applications Using LLMs,N/A
10.1007/979-8-8688-0456-4_8,The Early Career Professional’s Future With AI,N/A
10.1007/979-8-8688-0456-4_11,Ethical Implications and Societal Impact of AI,N/A
10.1093/oxfordhb/9780190067397.001.0001,The Oxford Handbook of Ethics of AI,"<p>This book explores the intertwining domains of artificial intelligence (AI) and ethics—two highly divergent fields which at first seem to have nothing to do with one another. AI is a collection of computational methods for studying human knowledge, learning, and behavior, including by building agents able to know, learn, and behave. Ethics is a body of human knowledge—far from completely understood—that helps agents (humans today, but perhaps eventually robots and other AIs) decide how they and others should behave. Despite these differences, however, the rapid development in AI technology today has led to a growing number of ethical issues in a multitude of fields, ranging from disciplines as far-reaching as international human rights law to issues as intimate as personal identity and sexuality. In fact, the number and variety of topics in this volume illustrate the width, diversity of content, and at times exasperating vagueness of the boundaries of “AI Ethics” as a domain of inquiry. Within this discourse, the book points to the capacity of sociotechnical systems that utilize data-driven algorithms to classify, to make decisions, and to control complex systems. Given the wide-reaching and often intimate impact these AI systems have on daily human lives, this volume attempts to address the increasingly complicated relations between humanity and artificial intelligence. It considers not only how humanity must conduct themselves toward AI but also how AI must behave toward humanity.</p>"
10.1007/s43681-022-00218-9,"Ethics and diversity in artificial intelligence policies, strategies and initiatives","<jats:title>Abstract</jats:title><jats:p>A burgeoning of Artificial Intelligence (AI) technologies in recent years has led to increased discussion about its potential to address many issues considered otherwise intractable, including those highlighted by the United Nations 2030 Agenda for Sustainable Development and associated Sustainable Development Goals. In tandem with this growth in AI is an expanding body of documentation regarding how such advanced technologies should be governed and managed. Issued by a variety of sources and comprising frameworks, policies and guidelines, this body of work encompasses the legal, social, ethical and policy issues around AI. With at least 470 such documents identified, as of May 2021, in the Council of Europe’s tracker of AI initiatives, questions are emerging around the diversity of views expressed, especially regarding the influence of the Global North or Euro-American perspectives. Our previous analysis of a corpus of largely grey literature discovered blind spots regarding both gender representation and perspectives from the Global South. Expanding on that work, this paper examines a significantly extended corpus, with a focus on the role of underrepresented groups in the wider AI discourse. We find that voices from the Global South and consideration of alternative ethical approaches are largely absent from the conversation. In light of the prominence of social, cultural and ethical perspectives from the Global North, this paper explores implications for the development of standards for ethical AI. Concluding by offering approaches to incorporate more diverse ethical viewpoints and beliefs, we call for increased consideration of power structures when developing AI ethics policies and standards within these alternative socio-cultural and socio-economic contexts.</jats:p>"
10.1007/s43681-020-00036-x,Declaration on the ethics of brain–computer interfaces and augment intelligence,N/A
10.21428/e4baedd9.3f5bb369,Evaluating the Effectiveness of AI Source Disclosure in Human–AI Communication,N/A
10.4324/9781003507949-3,Inviting AI Into the Composition Classroom,N/A
10.2139/ssrn.4597899,The power of generative marketing: Can generative AI reach human-level visual marketing content?,N/A
10.1007/s43681-024-00504-8,Decisional value scores: A new family of metrics for ethical AI-ML,"<jats:title>Abstract</jats:title><jats:p>Research in ethical AI has made strides in quantitative expression of ethical values such as fairness, transparency, and privacy. Here we contribute to this effort by proposing a new family of metrics called “decisional value scores” (DVS). DVSs are scores assigned to a system based on whether the decisions it makes meet or fail to meet a particular standard (either individually, in total, or as a ratio or average over decisions made). Advantages of DVS include greater discrimination capacity between types of ethically relevant decisions and facilitation of ethical comparisons between decisions and decision-making systems, including across different modalities (for instance: human, machine, or coupled human–machine systems). After clarifying ambiguities in the concept of “decision” itself, including the question of how to individuate the decisions made by a system, we discuss the role and meaning of “decision” in common AI and machine learning approaches such as decision trees, neural networks, SVMs, and unsupervised classifiers. We then show how DVSs may be defined for several ethical values of interest, with an extended discussion of transparency. Finally, we explore how such metrics can be applied to real decision-making systems through two case studies: evaluations of LLMs for transparency; and evaluations of criminal risk assessment tools for utility, rights violations, fairness, and transparency.</jats:p>"
10.1007/978-3-031-23035-6_4,AI Ethics in Higher Education: Research Experiences from Practical Development and Deployment of AI Systems,<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) offers tangible benefits in several application domains like disease diagnosis in health.</jats:p>
10.1007/s43681-022-00173-5,AI ethics and its pitfalls: not living up to its own standards?,"<jats:title>Abstract</jats:title><jats:p>AI ethics is deemed to be an essential ingredient in the quest for trustworthy AI. Hence, demands for implementing AI ethics and ethicists into AI organizations, especially corporations, are ubiquitous. However, the assumption that AI ethicists have particular epistemological advantages compared to non-ethicists as well as the idea that AI ethics automatically decreases the likelihood of unethical outcomes are both flawed. Therefore, this comment lists risks that either originate from AI ethicists themselves or from the consequences their embedding in AI organizations has. The compilation of risks comprises psychological considerations concerning the cognitive biases of AI ethicists themselves as well as biased reactions to their work, subject-specific and knowledge constraints AI ethicists often succumb to, negative side effects of ethics audits for AI applications, and many more. Ultimately, the aim of this comment is not to diminish or deny the importance of the discipline of AI ethics, but rather to increase its capacities for self-reflection and, ultimately, effectiveness.</jats:p>"
10.1201/9781003499527-11,Ethics in AI,N/A
10.1162/99608f92.e6245c19,Unpacking AI Governance From the Margins,N/A
10.1007/979-8-8688-0456-4_1,Introduction,N/A
10.1162/99608f92.562ff0f5,AI Safety Is a Narrative Problem,N/A
10.1007/s43681-024-00488-5,AI art and public literacy: the miseducation of Ai-Da the robot,"<jats:title>Abstract</jats:title><jats:p>This article examines the implications of the artworks and public performances of the robot artist Ai-Da. While the project claims to advance AI public literacy and foster critical debate around intelligent systems, it instead ends up perpetuating popular misunderstandings about AI creativity, agency, and consciousness. Built in 2019, Ai-Da is a humanoid robot capable of creating drawings, paintings, and composing poetry. However, the project often conceals or miscommunicates the technical aspects of Ai-Da’s capabilities in a manner that encourages the public to misattribute human-like traits to the robot. This lack of transparency in the presentation of Ai-Da’s abilities and the creative processes involved risks reinforcing existing misconceptions about AI, rather than promoting a more nuanced understanding. By employing discourse analysis and drawing on scholarship on machine and computational creativity, anthropomorphism in social robots, and posthuman embodiment, this article uses the Ai-Da project as a case study to illustrate how the dangers of AI hype can be obscured when presented through the lens of public art. The analysis examines how the Ai-Da project, despite its stated goals of advancing AI literacy, fails to effectively challenge and may even exacerbate public misperceptions about the nature of AI-generated art and creativity.</jats:p>"
10.1162/99608f92.5cd4a32e,AI and Creative Work,N/A
10.1007/s43681-022-00163-7,AI-deploying organizations are key to addressing ‘perfect storm’ of AI risks,"<jats:title>Abstract</jats:title><jats:p>We argue that a perfect storm of five conditions heightens the risk of harm to society from artificial intelligence: (1) the powerful, invisible nature of AI, (2) low public awareness and AI literacy, (3) rapid scaled deployment of AI, (4) insufficient regulation, and (5) the gap between trustworthy AI principles and practices. To prevent harm, fit-for-purpose regulation and public AI literacy programs have been recommended, but education and government regulation will not be sufficient: AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and taking accountability to mitigate the risks.</jats:p>"
10.1007/978-1-4842-9852-7_4,Prompt Engineering,N/A
10.21125/iceri.2023.2250,"TRENDS, IMPACT AND CONTROVERSIES OF ARTIFICIAL INTELLIGENCE(AI) ESPECIALLY GENERATIVE AI IN HIGHER EDUCATION",N/A
10.1007/s43681-021-00117-5,AI audits for assessing design logics and building ethical systems: the case of predictive policing algorithms,"<jats:title>Abstract</jats:title><jats:p>Organisations, governments, institutions and others across several jurisdictions are using AI systems for a constellation of high-stakes decisions that pose implications for human rights and civil liberties. But a fast-growing multidisciplinary scholarship on AI bias is currently documenting problems such as the discriminatory labelling and surveillance of historically marginalised subgroups. One of the ways in which AI systems generate such downstream outcomes is through their inputs. This paper focuses on a specific input dynamic which is the theoretical foundation that informs the design, operation, and outputs of such systems. The paper uses the set of technologies known as predictive policing algorithms as a case example to illustrate how theoretical assumptions can pose adverse social consequences and should therefore be systematically evaluated during audits if the objective is to detect unknown risks, avoid AI harms, and build ethical systems. In its analysis of these issues, the paper adds a new dimension to the literature on AI ethics and audits by investigating algorithmic impact in the context of underpinning theory. In doing so, the paper provides insights that can usefully inform auditing policy and practice instituted by relevant stakeholders including the developers, vendors, and procurers of AI systems as well as independent auditors.</jats:p>"
10.1007/s43681-020-00017-0,AI and moral thinking: how can we live well with machines to enhance our moral agency?,N/A
10.1007/s43681-021-00085-w,Can AI systems meet the ethical requirements of professional decision-making in health care?,N/A
10.1007/s43681-023-00288-3,"The democratic offset: Contestation, deliberation, and participation regarding military applications of AI","<jats:title>Abstract</jats:title><jats:p>Authoritarian regimes’ unrestricted collection of citizens’ data might constitute an advantage regarding the development of some types of AI, and AI might facilitate authoritarian practices. This feedback loop challenges democracies. In a critical continuation of the Pentagon’s Third Offset Strategy, I investigate a possible Democratic Offset regarding military applications of AI focussed on contestation, deliberation, and participation. I apply Landemore’s Open Democracy, Hildebrandt’s Agonistic Machine Learning, and Sharp’s Civilian-Based Defence. Discussing value pluralism in AI ethics, I criticise parts of the literature for leaving the fundamental ethical incompatibility of democracies and authoritarian regimes unaddressed. I am focussing on the duty to disobey illegal orders derived from customary international humanitarian law (IHL) and the standard of ‘meaningful human control’, which is central to the partially outdated debate about lethal autonomous weapon systems (LAWS). I criticize the standard of ‘meaningful human control’ following two pathways: First, the ethical and legal principles of just war theory and IHL should be implemented in military applications of AI to submit human commands to more control, in the sense of technological disaffordances. Second, the debate should focus on the societal circumstances for personal responsibility and disobedience to be trained and exerted in deliberation and participation related to military applications of AI, in the sense of societal affordances. In a larger picture, this includes multi-level stakeholder involvement, robust documentation to facilitate auditing, civilian-based defence in decentralized smart cities, and open-source intelligence. This multi-layered approach fosters cognitive diversity, which might constitute a strategic advantage for democracies regarding AI.</jats:p>"
10.1007/s43681-020-00037-w,Bridging the gap: the case for an ‘Incompletely Theorized Agreement’ on AI policy,"<jats:title>Abstract</jats:title><jats:p>Recent progress in artificial intelligence (AI) raises a wide array of ethical and societal concerns. Accordingly, an appropriate policy approach is urgently needed. While there has been a wave of scholarship in this field, the research community at times appears divided amongst those who emphasize ‘near-term’ concerns and those focusing on ‘long-term’ concerns and corresponding policy measures. In this paper, we seek to examine this alleged ‘gap’, with a view to understanding the practical space for inter-community collaboration on AI policy. We propose to make use of the principle of an ‘incompletely theorized agreement’ to bridge some underlying disagreements, in the name of important cooperation on addressing AI’s urgent challenges. We propose that on certain issue areas, scholars working with near-term and long-term perspectives can converge and cooperate on selected mutually beneficial AI policy projects, while maintaining their distinct perspectives.</jats:p>"
10.1007/s43681-021-00076-x,The ethics of interaction with neurorobotic agents: a case study with BabyX,"<jats:title>Abstract</jats:title><jats:p>As AI advances, models of simulated humans are becoming increasingly realistic. A new debate has arisen about the ethics of interacting with these realistic agents—and in particular, whether any harms arise from ‘mistreatment’ of such agents. In this paper, we advance this debate by discussing a model we have developed (‘BabyX’), which simulates a human infant. The model produces realistic behaviours—and it does so using a schematic model of certain human brain mechanisms. We first consider harms that may arise due to effects<jats:italic>on the user</jats:italic>—in particular effects on the user’s behaviour towards real babies. We then consider whether there’s any need to consider harms from the ‘perspective’<jats:italic>of the simulated baby</jats:italic>. The first topic raises practical ethical questions, many of which are empirical in nature. We argue the potential for harm is real enough to warrant restrictions on the use of BabyX. The second topic raises a very different set of questions in the philosophy of mind. Here, we argue that BabyX’s biologically inspired model of emotions raises important moral questions, and places BabyX in a different category from avatars whose emotional behaviours are ‘faked’ by simple rules. This argument counters John Danaher’s recently proposed ‘moral behaviourism’. We conclude that the developers of simulated humans have useful contributions to make to debates about moral patiency—and also have certain new responsibilities in relation to the simulations they build.</jats:p>"
10.1007/s43681-023-00266-9,A seven-layer model with checklists for standardising fairness assessment throughout the AI lifecycle,N/A
10.1007/s43681-023-00269-6,Can artificial intelligence be a Kantian moral agent? On moral autonomy of AI system,N/A
10.7551/mitpress/12549.003.0002,Acknowledgments,N/A
10.4018/9781591409878.ch042,Ethics of AI,N/A
10.36227/techrxiv.24449185.v1,How Text-to-Image Generative AI Is Transforming Mediated Action?,N/A
10.36227/techrxiv.24087534.v1,Synergy in Technology How Generative AI Augments the Capabilities of Customer Data Platforms,"<jats:p>&lt;p&gt;In an era marked by data-driven decision-making, Customer Data Platforms (CDPs) have emerged as pivotal tools for aggregating and analyzing customer data. However, as these platforms grapple with increasingly complex data sets and real-time customer engagement demands, there is a pressing need for more advanced, scalable solutions. This study explores the synergy between Generative Artificial Intelligence (AI) and CDPs, aiming to understand how the integration of generative algorithms can augment the capabilities of these platforms. Employing a multi-method research approach, including case studies, empirical analyses, and expert interviews, this paper investigates various applications of Generative AI within CDPs, such as data augmentation, real-time decision-making, and customer personalization. Moreover, the ethical implications of using generative algorithms, especially concerning data privacy and security, are critically examined. The study reveals that Generative AI can significantly enhance the functionality, performance, and efficiency of CDPs while also posing new questions around ethical considerations. Our findings offer invaluable insights for businesses, marketers, and technologists seeking to leverage the synergistic potential of these two advanced technological paradigms. &lt;/p&gt;</jats:p>"
10.1094/tq-60-4-1225-01,Fermentation Meets Computation Exploring Generative AI in Breweries,N/A
10.2139/ssrn.4843464,Generative AI and Metaverse: Companionship and Assisted Living for Elderly People,N/A
10.1093/oxfordhb/9780190067397.013.42,Ethics of Artificial Intelligence in Transport,"<p>This chapter highlights key ethical issues in the use of artificial intelligence in transport by using automated driving as an example. These issues include the tension between technological solutions and policy solutions; the consequences of safety expectations; the complex choice between human authority and computer authority; and power dynamics among individuals, governments, and companies. In 2017 and 2018, the U.S. Congress considered automated driving legislation that was generally supported by many of the larger automated-driving developers. However, this automated-driving legislation failed to pass because of a lack of trust in technologies and institutions. Trustworthiness is much more of an ethical question. Automated vehicles will not be driven by individuals or even by computers; they will be driven by companies acting through their human and machine agents. An essential issue for this field—and for artificial intelligence generally—is how the companies that develop and deploy these technologies should earn people’s trust.</p>"
10.36227/techrxiv.22097942,A Review of Generative AI from Historical Perspectives,"<jats:p>&lt;p&gt;Many applications of Generative AI (such as DALL-E, GPT-3, ChatGPT, etc.) are making headline news in recent months and have been receiving both praise and criticism for their far reaching implications.  Some of these applications include query responses, language translation, text to images and videos, composing stories, essays, creating arts and music, generating programs, etc. This review provides an historical background of Generative AI techniques and how they evolved over the years. This report highlights the benefits of Generative AI technologies and their limitations/challenges in moving forward. It is also to be noted that the large-scale applications of AI and their successes are now possible due to exponential advances in hardware (computational power, storage capacity), cloud computing and related operational layers of software.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4945933,The Wade Test: Generative AI and CEO Communication,N/A
10.22541/essoar.167080673.37483484/v1,AI-ML Ethics Modules for ESES - Version 1 with line numbers- December 2022,N/A
10.1007/s43681-024-00471-0,From artificial intelligence to semi-creative inorganic intelligence: a blockchain-based bioethical metamorphosis,N/A
10.1007/s00146-020-01010-1,From machine ethics to computational ethics,N/A
10.1007/978-981-19-9382-4_1,Introduction: Why AI Ethics?,N/A
10.7551/mitpress/12549.001.0001,AI Ethics,"<jats:p>An accessible synthesis of ethical issues raised by artificial intelligence that moves beyond hype and nightmare scenarios to address concrete questions.</jats:p>
               <jats:p>Artificial intelligence powers Google's search engine, enables Facebook to target advertising, and allows Alexa and Siri to do their jobs. AI is also behind self-driving cars, predictive policing, and autonomous weapons that can kill without human intervention. These and other AI applications raise complex ethical issues that are the subject of ongoing debate. This volume in the MIT Press Essential Knowledge series offers an accessible synthesis of these issues. Written by a philosopher of technology, AI Ethics goes beyond the usual hype and nightmare scenarios to address concrete questions.</jats:p>
               <jats:p>Mark Coeckelbergh describes influential AI narratives, ranging from Frankenstein's monster to transhumanism and the technological singularity. He surveys relevant philosophical discussions: questions about the fundamental differences between humans and machines and debates over the moral status of AI. He explains the technology of AI, describing different approaches and focusing on machine learning and data science. He offers an overview of important ethical issues, including privacy concerns, responsibility and the delegation of decision making, transparency, and bias as it arises at all stages of data science processes. He also considers the future of work in an AI economy. Finally, he analyzes a range of policy proposals and discusses challenges for policymakers. He argues for ethical practices that embed values in design, translate democratic values into practices and include a vision of the good life and the good society.</jats:p>"
10.7551/mitpress/12549.003.0016,Notes,N/A
10.4018/979-8-3693-1950-5.ch002,Generative AI for Text to Image,"<jats:p>Text-to-image (TTI) synthesis models represent a creative approach in the realm of artificial intelligence, specifically designed to transform textual input into visually realistic images. The essence of TTI generation lies in its ability to harness the power of language and convert it seamlessly into visually compelling content, showcasing creative image synthesis. Initially using GANs and transformers, text-to-image generation evolved with diffusion models introducing noise. Integration with large models, TTI models now produce results near-real images. Breakthroughs like ControlNet and 3D object synthesis redefine possibilities. Editing text or images adds versatile dimensions, showcasing generative technologies' transformative capabilities. The survey explores scaling TTI models, focusing on various descriptions and ControlNet's role. The authors categorize literature, offer nuanced comparisons, and discuss applications. Looking ahead, they foresee TTI's potential for productivity enhancements, especially in the Metaverse era, and expansion into intricate tasks like video and 3D generation.</jats:p>"
10.29303/abdiinsani.v11i2.1577,PEMANFAATAN GENERATIVE AI DALAM PEMBUATAN PERANGKAT PEMBELAJARAN: WORKSHOP UNTUK GURU SMK NEGERI 10 SURABAYA,"<jats:p>One of the fast-growing technologies that has great potential in the world of education is artificial intelligence. (Artificial Intelligence). The workshop aims to enhance teachers' understanding and skills in using Artificial Intelligence, the ChatGPT and Tome.App applications, to develop more interesting and effective learning plans and create more innovative material presentations. The method of this activity begins with conducting surveys to understand the needs and challenges faced by teachers in using Artificial Intelligence technology. Next, a workshop module is set up to guide the workshop participants. Evaluation instruments are also prepared to measure participants' understanding before and after the workshop, as well as to assess participants' satisfaction with the overall performance of the workshop. The evaluation results showed a significant improvement in participants' understanding of the material after attending the workshop. Based on pre-test and post-test data, the average answer accuracy increased from 55% to 86%. Besides, the participants' response to the workshop was also very positive, with the majority of participants giving a ""Good"" and ""Very Good"" rating on the material, the quality of the source, and the overall workshop atmosphere. Workshop participants became more confident in applying technology in their learning. The evaluation also highlighted the importance of mature preparation in conducting workshops, including better timing and location. The advice for the future is to conduct more intensive periodic workshops to enhance teachers' ability to use Artificial Intelligence technology in learning.</jats:p>"
10.4018/979-8-3693-5578-7.ch004,Mediated by AI-Based Generative Re-Enforcement Learning and Work Attitude,"<jats:p>The primary goal of the chapter was to examine how, in the Ethiopian environment, an employee's work attitude and AI-based reinforcement learning function as mediators between their psychological intrinsic reward system and the perception of organisational support they receive from their employer. Based on their contribution to the GDP of the economy, the textile industries operating in Ethiopia's industrial parks were chosen as the study area. A quantitative research technique and explanatory research design were employed. A multi-phase sampling method was suggested. To assess discriminant and convergent validity, exploratory confirmatory analysis was carried out using AMOS software. It was discovered that the relationship between an employee's psychological intrinsic reward system and the organization's perceived organisational support was extremely poor in the absence of AI-based re-enforcement learning and the employee's work attitude as a mediator. Thus, the work attitude of employees and artificial intelligence (AI) function as a complete mediator.</jats:p>"
10.7551/mitpress/12549.003.0019,Index,N/A
10.7551/mitpress/12549.003.0015,Glossary,N/A
10.7551/mitpress/12549.003.0017,References,N/A
10.4018/979-8-3693-2418-9.ch002,Exploring the Perception of Indian Educators Towards Use of Generative AI Tools for Assessment,"<jats:p>Educational ecosystem has always kept pace with the technological development. Stakeholders in education have had an agile reaction to the progress happening outside the classroom and have embraced these changes at the earliest for their own benefits. While students have readily accepted the free tools, there is a raging debate on the view of educators regarding the same. The present research focuses on teachers and educators who teach at undergraduate and post-graduate students of management and assimilates their opinion on the use of artificial intelligence, in one of the key aspects of the educational journey, i.e., assessment. In the research carried out engaging insights were brought forth in terms of educators preferences regarding the use of artificial intelligence for assessment and how they perceive it to help students and themselves. At the same time, the research has also shown the limitations that educators feel regarding the use of artificial intelligence from an Indian perspective, due to the sheer number of human power involved in the Indian Education System.</jats:p>"
10.2139/ssrn.4660456,Generative Ai from a Cfo Prism,N/A
10.2139/ssrn.4639565,Measuring Tax Enforcement with Generative AI,N/A
10.1201/9781003503781-4,Review of Cybersecurity Metrics,N/A
10.1016/b978-0-44-321857-6.00008-4,Image and video generation,N/A
10.2139/ssrn.4848720,The Unbearable Lightness of Inventing: Postprocess in the Age of Generative Ai,N/A
10.1007/s43681-021-00078-9,Analytical modelling and UK Government policy,"<jats:title>Abstract</jats:title><jats:p>In the last decade, the UK Government has attempted to implement improved processes and procedures in modelling and analysis in response to the Laidlaw report of 2012 and the Macpherson review of 2013. The Laidlaw report was commissioned after failings during the Intercity West Coast Rail (ICWC) Franchise procurement exercise by the Department for Transport (DfT) that led to a legal challenge of the analytical models used within the exercise. The Macpherson review looked into the quality assurance of Government analytical models in the context of the experience with the Intercity West Coast franchise competition. This paper examines what progress has been made in the 8 years since the Laidlaw report in model building and best practise in government and proposes several recommendations for ways forward. This paper also discusses the Lords Science and Technology Committees of June 2020 that analysed the failings in the modelling of COVID. Despite going on to influence policy, many of the same issues raised within the Laidlaw and Macpherson Reports were also present in the Lords Science and Technology Committee enquiry. We examine the technical and organisational challenges to progress in this area and make recommendations for a way forward.</jats:p>"
10.1007/s43681-022-00151-x,The internal morality of markets and artificial intelligence,N/A
10.1007/979-8-8688-0403-8_9,Building Enterprise-Grade Applications Using LLMs,N/A
10.1007/s43681-022-00247-4,Editorial piece: Technology built on sand?,N/A
10.1007/s43681-022-00136-w,Algorithms are not neutral,N/A
10.1007/978-3-031-23035-6_6,Promoting AI Ethics Through Awareness and Case Studies,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) is enabling organizations to address a range of real-world challenges in areas as diverse as global health, education and poverty alleviation.</jats:p>"
10.15187/adr.2024.08.37.4.181,The Effect of Text Movement on Eye Movements in Generative AI Chatbots,N/A
10.4018/979-8-3693-0487-7.ch008,Leveraging Generative AI for Cross-Cultural Knowledge Exchange in Higher Education,"<jats:p>The integration of generative artificial intelligence (AI) has the potential to revolutionize cross-cultural knowledge exchange in higher education. By leveraging its ability to create culturally sensitive and contextually relevant learning materials, generative AI can enhance personalized education for students from diverse backgrounds. Overcoming language barriers through real-time translation, this technology promotes inclusive collaboration and co-creation of knowledge. However, ethical considerations such as cultural authenticity, bias mitigation, and data privacy must be carefully navigated to ensure equitable and respectful cross-cultural interactions. In essence, generative AI offers a transformative avenue for fostering a culturally enriched learning environment in higher education, necessitating a balanced approach that maximizes benefits while upholding ethical principles.</jats:p>"
10.21428/e4baedd9.7dc53bbf,Advancing Equality: Harnessing Generative AI to Combat Systemic Racism,N/A
10.4018/979-8-3693-0074-9.ch006,Generative AI in Terms of Its Ethical Problems for Both Teachers and Learners,"<jats:p>The chapter delves into the intricate ethical challenges posed by the integration of generative AI tools in educational contexts. As the educational landscape undergoes a profound transformation through AI-driven technologies, this chapter navigates the multifaceted ethical concerns that emerge. It explores issues surrounding data privacy, content bias, academic integrity, and the evolving roles of teachers and students in this digital era. Drawing on real-world case studies and ethical frameworks, it provides insights into the complexities of responsible AI integration. Moreover, it offers guidance on fostering ethical awareness in education and emphasizes the critical need for striking a harmonious balance between the capabilities of AI and the enduring value of human educators. This chapter serves as a comprehensive exploration of the ethical tightrope that educators, students, and policymakers must navigate to ensure AI's positive impact on education while safeguarding its ethical underpinnings.</jats:p>"
10.4018/979-8-3693-1950-5.ch006,Exploratory Visual Digital Character and Visual Digital Scene Design Using Artmaking Generative AI,"<jats:p>Static visual illustrations with characters and scenes play an important role in story problems and other pedagogical narratives. Such visuals may better engage learners, connect learners with the learning sequence, set the emotional tone, evoke settings, emphasize critical moments, and support the teaching and learning in other ways. With the popularization of artmaking generative AI, a practical question is how well this tool can make visual characters based on character design prompts for both animate and inanimate characters? What about the computerized drawing of scenes in which such characters may be placed, alone or in relation to each other? How difficult is it to prompt the generative AI to create consistent characters from different angles and perspectives? To create consistent scenes and backgrounds? This work explores how practically usable the AI-generated visuals are for the making of characters and scenes, with some light pre-production and post-production, as needed.</jats:p>"
10.1007/s43681-020-00016-1,Past the tipping point?,N/A
10.1007/s43681-023-00287-4,The probability problems of the Moral Machine Experiment,N/A
10.1007/s43681-023-00376-4,Data-driven framework for evaluating digitization and artificial intelligence risk: a comprehensive analysis,N/A
10.36227/techrxiv.22097942.v1,A Review of Generative AI from Historical Perspectives,"<jats:p>&lt;p&gt;Many applications of Generative AI (such as DALL-E, GPT-3, ChatGPT, etc.) are making headline news in recent months and have been receiving both praise and criticism for their far reaching implications.  Some of these applications include query responses, language translation, text to images and videos, composing stories, essays, creating arts and music, generating programs, etc. This review provides an historical background of Generative AI techniques and how they evolved over the years. This report highlights the benefits of Generative AI technologies and their limitations/challenges in moving forward. It is also to be noted that the large-scale applications of AI and their successes are now possible due to exponential advances in hardware (computational power, storage capacity), cloud computing and related operational layers of software.&lt;/p&gt;</jats:p>"
10.1016/b978-0-44-321857-6.00016-3,Some unusual random walks,N/A
10.21203/rs.3.rs-4603791/v1,A Tutorial for Integrating Generative AI in Mixed Methods Data Analysis,"<title>Abstract</title>
        <p>The current article used real data to demonstrate the analysis and synthesis of Mixed Methods Research (MMR) data with generative Artificial Intelligence (Gen AI). I explore how reliable and valid Gen AI data outputs are and how to improve their use. The current content is geared towards enhancing methodological application regardless of field or discipline and includes access to a prompt library and examples of using outputs. The demonstration data used emanated from a study done in South Africa, with a quantitative sample size of 969 first-year engineering students and, for the qualitative part, 14 second-year students. In the current article, I compare my original analysis to ChatGPT results. Generative AI as a mind tool is best used with human insight, and I found this to be especially true when coding qualitative data. ChatGPT produced generic codes if asked to do inductive coding, and the results improved when training the Gen AI on human examples, which led to moderate and significant correlations between human and machine coding. The quantitative analysis was accurate for the descriptive statistics, but the researcher had to use best judgment to select the correct inferential analysis. Quantitative and qualitative analysis should be conducted separately in generative AI before asking the Chatbot for help with mixed methods results.  In the current paper, I give guidelines and a tutorial on how to use chatbots in an ethically responsible and scientifically sound manner for research in social and human sciences.</p>"
10.36227/techrxiv.24045792,"Exploring the Synergy between Generative AI, Data and Analytics in the Modern Age","<jats:p>&lt;p&gt;In the year 2023, a heightened sense of curiosity and apprehension pervaded the landscape of generative artificial intelligence (AI), particularly in the wake of the unveiling of the ChatGPT product by OpenAI. This pivotal moment sparked a flurry of discussions that predominantly revolved around the role of data in shaping the trajectory of generative AI. As researchers and organizations alike delved into this innovative realm, a pronounced inclination toward investigating its potential applications emerged. Notably, organizations swiftly recognized the transformative potential of generative AI in bolstering productivity across various sectors.&lt;/p&gt;
&lt;p&gt;At the heart of these deliberations lies the profound significance of data. With data as the focal point, a compelling exploration began to unfold, with researchers keenly scrutinizing the ramifications of integrating generative AI within the domain of data and analytics. This research initiative was driven by an intrinsic desire to uncover the ways in which generative AI could be harnessed to enhance and streamline analytical processes.&lt;/p&gt;
&lt;p&gt;In this context, the present research undertook a comprehensive investigation, employing a multifaceted approach. Leveraging various social media platforms as a primary source of insights, the research embarked on a journey to discern the prevailing sentiments, concerns, and expectations surrounding generative AI tools. This was further complemented by the execution of proof-of-concept (POC) endeavors, which not only enabled hands-on experience with generative AI tools but also facilitated a nuanced comprehension of their practical implications.&lt;/p&gt;
&lt;p&gt;The culmination of these efforts yielded a series of noteworthy findings. Principally, it was discerned that enterprises stand to gain substantial benefits from embracing the capabilities of generative AI within the domain of data and analytics. The integration of generative AI tools offers the potential to revolutionize productivity, propelling organizations toward novel insights and expediting analytical processes. Concurrently, a strategic partnership with generative AI entities emerged as a salient consideration for safeguarding intellectual properties. Collaborative engagements between companies and generative AI providers became imperative to navigate the evolving landscape of data-driven innovation.&lt;/p&gt;
&lt;p&gt;In conclusion, the year 2023 ushered in a period marked by intense curiosity and apprehension surrounding generative AI, catalyzed by the introduction of ChatGPT and its ensuing discussions. The centrality of data within this discourse propelled researchers and organizations toward an exploration of generative AI's potential applications, notably in the realm of data and analytics. Through a comprehensive research endeavor encompassing social media insights, POC experimentation, and practical insights, it became evident that the integration of generative AI could usher in transformative enhancements to productivity and analytical processes. In parallel, collaborative endeavors with generative AI entities emerged as a strategic imperative, offering a dual advantage of innovation and intellectual property protection. This research underscores the compelling need for enterprises to harness generative AI's capabilities, thereby positioning themselves at the vanguard of data-driven progress.&lt;/p&gt;</jats:p>"
10.4324/9781003459026,Generative AI in Higher Education,N/A
10.1007/s00146-024-01872-9,What to consider before incorporating generative AI into schools?,N/A
10.2139/ssrn.4682872,This is Not Global AI: International Reports on AI as Vehicles of Colonialism in the Age of Generative AI,N/A
10.1007/978-3-031-46238-2_17,Underwater Acoustic Noise Modeling Based on Generative-Adversarial-Network,N/A
10.4324/9781003074991-24,Why Ethics is a High Hurdle for AI,N/A
10.21275/sr24517073623,Reinforcing Cyber Defense: Generative AI Powered Intelligent Agent Architecture for Enhanced Security Operations,N/A
10.4018/979-8-3693-2440-0.ch019,Role of Video Content Generation in Education Systems Using Generative AI,"<jats:p>This chapter delves into the transformative potential of three cutting-edge models, i.e., CCVS, DreamPose, and Fast Vid2Vid, in reshaping educational video content creation. CCVS, a dynamic framework amalgamating generative and discriminative models, excels in synthesizing high-quality videos from text descriptions. Its versatility in video generation, interpolation, and prediction marks a paradigm shift in educational content development. DreamPose, an advanced AI system leveraging stable diffusion, translates textual descriptions into visually stunning fashion videos. Its user-friendly design caters to diverse fashion styles, making it an ideal tool for educators seeking visually engaging content across disciplines. Fast Vid2Vid, a deep learning model, takes the spotlight for efficiently generating high-quality videos from a single image. Recognized for its realism, it holds promise in dynamic visualizations for educational purposes, spanning virtual and augmented reality experiences. Practical insights and implementation strategies empower educators to seamlessly integrate these models into educational settings, offering a comprehensive guide from planning and scripting to interactive element incorporation. This chapter lays the foundation for educators and content creators to elevate the educational experience through innovative visual storytelling and AI-driven technologies.</jats:p>"
10.1109/icws60048.2023.00103,"Empowering Generative AI with Knowledge Base 4.0: Towards Linking Analytical, Cognitive, and Generative Intelligence",N/A
10.36227/techrxiv.24087534,Synergy in Technology How Generative AI Augments the Capabilities of Customer Data Platforms,"<jats:p>&lt;p&gt;In an era marked by data-driven decision-making, Customer Data Platforms (CDPs) have emerged as pivotal tools for aggregating and analyzing customer data. However, as these platforms grapple with increasingly complex data sets and real-time customer engagement demands, there is a pressing need for more advanced, scalable solutions. This study explores the synergy between Generative Artificial Intelligence (AI) and CDPs, aiming to understand how the integration of generative algorithms can augment the capabilities of these platforms. Employing a multi-method research approach, including case studies, empirical analyses, and expert interviews, this paper investigates various applications of Generative AI within CDPs, such as data augmentation, real-time decision-making, and customer personalization. Moreover, the ethical implications of using generative algorithms, especially concerning data privacy and security, are critically examined. The study reveals that Generative AI can significantly enhance the functionality, performance, and efficiency of CDPs while also posing new questions around ethical considerations. Our findings offer invaluable insights for businesses, marketers, and technologists seeking to leverage the synergistic potential of these two advanced technological paradigms. &lt;/p&gt;</jats:p>"
10.22541/essoar.170034944.42344050/v1,Nowcasting Earthquakes with QuakeGPT An AI-Enhanced Earthquake Generative Pretrained Transformer,N/A
10.31219/osf.io/btczf,Leveraging generative AI to acculturate away from climate apathy,"<p>“Throw away anything that sounds too complicated. Only keep what is simple to grasp. Keep whatever news that seems friendly to the ears. If the information appears fuzzy and causes the brain to implode after two sentences, toss it away and stop listening. Doing so will make the news as orderly and simple to understand as the truth!”—In “GHG Emissions”; The Kingfisher Story Collection.</p>"
10.2139/ssrn.4894684,How Will Generative AI Impact Communication?,N/A
10.36227/techrxiv.24045792.v1,"Exploring the Synergy between Generative AI, Data and Analytics in the Modern Age","<jats:p>&lt;p&gt;In the year 2023, a heightened sense of curiosity and apprehension pervaded the landscape of generative artificial intelligence (AI), particularly in the wake of the unveiling of the ChatGPT product by OpenAI. This pivotal moment sparked a flurry of discussions that predominantly revolved around the role of data in shaping the trajectory of generative AI. As researchers and organizations alike delved into this innovative realm, a pronounced inclination toward investigating its potential applications emerged. Notably, organizations swiftly recognized the transformative potential of generative AI in bolstering productivity across various sectors.&lt;/p&gt;
&lt;p&gt;At the heart of these deliberations lies the profound significance of data. With data as the focal point, a compelling exploration began to unfold, with researchers keenly scrutinizing the ramifications of integrating generative AI within the domain of data and analytics. This research initiative was driven by an intrinsic desire to uncover the ways in which generative AI could be harnessed to enhance and streamline analytical processes.&lt;/p&gt;
&lt;p&gt;In this context, the present research undertook a comprehensive investigation, employing a multifaceted approach. Leveraging various social media platforms as a primary source of insights, the research embarked on a journey to discern the prevailing sentiments, concerns, and expectations surrounding generative AI tools. This was further complemented by the execution of proof-of-concept (POC) endeavors, which not only enabled hands-on experience with generative AI tools but also facilitated a nuanced comprehension of their practical implications.&lt;/p&gt;
&lt;p&gt;The culmination of these efforts yielded a series of noteworthy findings. Principally, it was discerned that enterprises stand to gain substantial benefits from embracing the capabilities of generative AI within the domain of data and analytics. The integration of generative AI tools offers the potential to revolutionize productivity, propelling organizations toward novel insights and expediting analytical processes. Concurrently, a strategic partnership with generative AI entities emerged as a salient consideration for safeguarding intellectual properties. Collaborative engagements between companies and generative AI providers became imperative to navigate the evolving landscape of data-driven innovation.&lt;/p&gt;
&lt;p&gt;In conclusion, the year 2023 ushered in a period marked by intense curiosity and apprehension surrounding generative AI, catalyzed by the introduction of ChatGPT and its ensuing discussions. The centrality of data within this discourse propelled researchers and organizations toward an exploration of generative AI's potential applications, notably in the realm of data and analytics. Through a comprehensive research endeavor encompassing social media insights, POC experimentation, and practical insights, it became evident that the integration of generative AI could usher in transformative enhancements to productivity and analytical processes. In parallel, collaborative endeavors with generative AI entities emerged as a strategic imperative, offering a dual advantage of innovation and intellectual property protection. This research underscores the compelling need for enterprises to harness generative AI's capabilities, thereby positioning themselves at the vanguard of data-driven progress.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4553787,Competition in Generative AI Foundation Models,N/A
10.1162/99608f92.cdfc3092,Castles in the Sand?: How the Public Sector and Academia Can Partner in Regulatory Sandboxes to Help Leverage Generative AI for Public Good,N/A
10.21275/sr24604032016,"A Generative AI Framework for Enhancing Software Test Automation: Design, Implementation, and Validation",N/A
10.1007/979-8-8688-0318-5_9,Generative AI’s Benefits and Risks to Society,N/A
10.1007/979-8-8688-0403-8_7,Advanced Techniques for Large Language Models,N/A
10.1007/978-3-031-46238-2_11,Generative Adversarial Networks for Stain Normalisation in Histopathology,N/A
10.1007/s43681-024-00489-4,Human control of AI systems: from supervision to teaming,"<jats:title>Abstract</jats:title><jats:p>This article reviews two main approaches to human control of AI systems: supervisory human control and human–machine teaming. It explores how each approach defines and guides the operational interplay between human behaviour and system behaviour to ensure that AI systems are effective throughout their deployment. Specifically, the article looks at how the two approaches differ in their conceptual and practical adequacy regarding the control of AI systems based on foundation models––i.e., models trained on vast datasets, exhibiting general capabilities, and producing non-deterministic behaviour. The article focuses on examples from the defence and security domain to highlight practical challenges in terms of human control of automation in general, and AI in particular, and concludes by arguing that approaches to human control are better served by an understanding of control as the product of collaborative agency in a multi-agent system rather than of exclusive human supervision.</jats:p>"
10.18260/1-2--46433,A Custom Generative AI Chatbot as a Course Resource,N/A
10.1007/s43681-023-00319-z,Speciesist bias in AI: a reply to Arandjelović,"<jats:title>Abstract</jats:title><jats:p>The elimination of biases in artificial intelligence (AI) applications—for example biases based on race or gender—is a high priority in AI ethics. So far, however, efforts to eliminate bias have all been anthropocentric. Biases against nonhuman animals have not been considered, despite the influence AI systems can have on normalizing, increasing, or reducing the violence that is inflicted on animals, especially on farmed animals. Hence, in 2022, we published a paper in <jats:italic>AI and Ethics</jats:italic> in which we empirically investigated various examples of image recognition, word embedding, and language models, with the aim of testing whether they perpetuate speciesist biases. A critical response has appeared in <jats:italic>AI and Ethics</jats:italic>, accusing us of drawing upon theological arguments, having a naive anti-speciesist mindset, and making mistakes in our empirical analyses. We show that these claims are misleading.</jats:p>"
10.1007/s43681-023-00417-y,TAI-PRM: trustworthy AI—project risk management framework towards Industry 5.0,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) is increasingly being used in manufacturing to automate tasks and process data, leading to what has been termed Industry. 4.0. However, as we move towards Industry 5.0, there is a need to incorporate societal and human-centric dimensions into the development and deployment of AI software artefacts. This requires blending ethical considerations with existing practices and standards. To address this need, the TAI-PRM framework has been developed. It builds upon established methods, such as Failure Mode and Effect Analysis (FMEA) and the Industrial ISO 31000, to manage risks associated with AI artefacts in the manufacturing sector. The framework identifies ethical considerations as hazards that can impact system processes and sustainability and provides tools and metrics to manage these risks. To validate the framework, it was applied in an EU project for Digital Twins on AI for manufacturing. The results showed that TAI-PRM can effectively identify and track different failure modes associated with AI artefacts and help users to manage ethical risks associated with their deployment. By incorporating ethical considerations into risk management processes, the framework enables the developing and deploying trustworthy AI in the manufacturing sector.</jats:p>"
10.1007/s43681-021-00075-y,The ethical AI—paradox: why better technology needs more and not less human responsibility,N/A
10.4324/9781003507949-2,Pedagogical Foundations of AI Integration,N/A
10.2139/ssrn.4511540,"Large Language Models and Generative AI in Finance: An Analysis of ChatGPT, Bard, and Bing AI",N/A
10.46397/jaih.14.3,Ethical Problems of Super-Massive Generative AI,N/A
10.46397/jaih.15.6,Generative AI Products and Copyright Issues,N/A
10.1007/978-1-4842-9367-6_9,The Future,N/A
10.1007/s43681-023-00358-6,"Elusive technologies, elusive responsibilities: on the perceived responsibility of basic AI researchers","<jats:title>Abstract</jats:title><jats:p>This paper studies how researchers who work in the field of basic research of artificial intelligence (AI) perceive their responsibility. A case study is conducted on an inter-university and interdisciplinary research cluster in Germany that specializes in basic artificial intelligence research. The reason for studying responsibility through the lens of such researchers is that working in basic research of AI involves a lot of uncertainty about potential consequences, more so than in other domains of AI development. After conducting focus groups with 21 respondents followed by a thematic analysis, results show that respondents restrict the boundaries of their sociotechnical visions, regard time as an influencing factor in their responsibility, and refer to many other players in the field. These themes indicate that respondents had difficulties explaining what they consider themselves responsible for, and referred to many factors beyond their own control. The only type of responsibility that was explicitly acknowledged by respondents is <jats:italic>ex ante</jats:italic> responsibility. Respondents define their responsibility in terms of things that are in their immediate control, i.e., responsibilities relating to their role and duties as researchers. According to the respondents, working in the field of basic research makes it difficult to make claims about <jats:italic>ex post</jats:italic> responsibility. Findings of this case study suggest the need to raise questions about how technological maturity is related to AI ethics.</jats:p>"
10.1007/s43681-024-00476-9,The obscure politics of artificial intelligence: a Marxian socio-technical critique of the AI alignment problem thesis,"<jats:title>Abstract</jats:title><jats:p>There is a growing feeling that artificial intelligence (AI) is getting out of control. Many AI experts worldwide stress that great care must be taken on the so-called <jats:italic>alignment problem</jats:italic>, broadly understood as the challenge of developing AIs whose actions are in line with human values and goals. The story goes that ever more powerful AI systems are escaping human control and might soon operate in a manner that is no longer guided by human purposes. This is what we call the <jats:italic>AI-out-of-control discourse</jats:italic> which, in this paper, we critically examine and debunk. Drawing on complementary insights from political theory, socio-technical studies and Marxian political economy, we critique the supposed animistic and autonomous nature of AI, and the myth of the uncontrollability of AI. The problem is not that humanity has lost control over AI, but that only a minority of powerful stakeholders are controlling its creation and diffusion, through politically undemocratic processes of decision-making. In these terms, we reframe the alignment problem thesis with an emphasis on citizen engagement and public political participation. We shed light on the existing politics of AI and contemplate alternative political expressions whereby citizens steer AI development or stop it in the first place.</jats:p>"
10.1007/s43681-023-00333-1,Lustre and shadows: unveiling the gaps in South African University plagiarism policies amidst the emergence of AI-generated content,"<jats:title>Abstract</jats:title><jats:p>In recent years, artificial intelligence (AI) has become a key technology in the field of academic integrity. However, there is a lack of a comprehensive understanding of the legal dimensions of plagiarism in the context of AI. In this study, a theoretical framework that combines the social construction of technology and the legal dimension of plagiarism was used to explore the current construction of plagiarism in South African university plagiarism policies. This study aims to highlight the inadequacy of current plagiarism policies, which primarily focus on the act of copying from others and emphasize the need for a broader perspective that addresses the challenges posed by artificial intelligence in academic integrity in the era of AI-generated content. The author used confirming sampling and data saturation was reached with a sample of ten university plagiarism policies. The findings revealed an inadequacy of the policies on the coverage of AI-generated content and therefore justifying the need to redefine plagiarism in the context of the artificial intelligence revolution. The author concludes by redefining plagiarism and justifying the utility of the recommended definition.</jats:p>"
10.1007/s43681-024-00544-0,Advising AI assistant: ethical risks of Oura smart ring,N/A
10.1201/9781003359982-6,"AI, Peace, and Ethics",N/A
10.1093/oxfordhb/9780190067397.013.18,AI as a Moral Right-Holder,"<p>This chapter evaluates whether AI systems are or will be rights-holders. It develops a skeptical stance toward the idea that current forms of artificial intelligence are holders of moral rights, beginning with an articulation of one of the most prominent and most plausible theories of moral rights: the Interest Theory of rights. On the Interest Theory, AI systems will be rights-holders only if they have interests or a well-being. Current AI systems are not bearers of well-being, and so fail to meet the necessary condition for being rights-holders. This argument is robust against a range of different objections. However, the chapter also shows why difficulties in assessing whether future AI systems might have interests or be bearers of well-being—and so be rights-holders—raise difficult ethical challenges for certain developments in AI.</p>"
10.1007/978-3-031-23035-6_3,Practical Implications of Different Theoretical Approaches to AI Ethics,<jats:title>Abstract</jats:title><jats:p>Ethics are moral principles that govern a person’s behaviour or the conduct of an activity.</jats:p>
10.2139/ssrn.4523551,Talkin’ ‘Bout AI Generation: Copyright and the Generative AI Supply Chain,N/A
10.1007/979-8-8688-0205-8_4,Guardrails and AI: Building Safe + Controllable Apps,N/A
10.4018/979-8-3693-5578-7.ch009,AI in Marketing,"<jats:p>In recent times, artificial intelligence (AI) has become an essential component in marketing, leading to the generation of large amounts of data and information, as well as advancements in management software and algorithms. The goal of artificial intelligence is to intelligently imitate human behavior. Artificial intelligence is a rapidly expanding technology, industry, and field of study. AI not only enhances existing marketing strategies, but also introduces innovative ways to deliver value to customers. AI also plays an important role in helping marketers manage marketing projects, support them, create content, and perform other tasks. Research shows that a lack of understanding of AI's capabilities and failed implementation efforts can create additional obstacles for organizations, including investment risks, ethical dilemmas, data issues, recruitment challenges, and negative environmental impacts. Despite being designed to reduce the workload of employees, human intervention is necessary in roles such as management, supervision, and innovation.</jats:p>"
10.1093/oso/9780198876434.003.0019,Is AI Ethics All Fluff?,"<jats:title>Abstract</jats:title>
               <jats:p>The AI revolution provides a neat illustration of C.P. Snow’s point about the “two cultures” and a timely opportunity to reflect on why a cultural gap between the sciences and humanities persists. This chapter takes aim at an attitude prevailing among some computer scientists that ethics and AI ethics, as branches of the humanities, are unserious disciplines because they do not yield verifiable and quantifiable answers to the problems they address.</jats:p>"
10.1093/oxfordhb/9780190067397.013.6,The Incompatible Incentives of Private-Sector AI,"<p>This chapter evaluates the incompatible incentives of private-sector AI. Private-sector investment in AI is dominated by major internet platform companies such as Facebook, Amazon, Apple, Google, and Microsoft. These platform companies are also leaders in deploying deep learning algorithms. Although deep learning algorithms may be more intelligent than previous generations of machine learning, they are not more robust. There may be a faint technical path forward for problems of bias and unfairness, but algorithms are engines, and pervasive incompatible incentives will remain. As such, algorithms require guardrails. However, technology companies are ill-suited and ill-positioned to design or implement these value-based rules. Guardrails become constraints on people’s behavior, and yet, in cases of high elasticity, effective governance may still be elusive. Ultimately, the pairing of the algorithm and guardrails tempts companies to engage in regulatory arbitrage, providing a requirement for external action.</p>"
10.1007/s43681-022-00145-9,FMEA-AI: AI fairness impact assessment using failure mode and effects analysis,N/A
10.1007/s00146-023-01761-7,Reimagining Benin Bronzes using generative adversarial networks,N/A
10.1007/s43681-023-00264-x,Ought we align the values of artificial moral agents?,N/A
10.1007/979-8-8688-0456-4_9,The AI Gold Rush and the Future of Business,N/A
10.1007/s43681-021-00065-0,Moral consideration of nonhumans in the ethics of artificial intelligence,N/A
10.1093/oxfordhb/9780190067397.013.47,Beyond Bias,"<p>This chapter discusses contemporary debates regarding the use of artificial intelligence as a vehicle for criminal justice reform. It closely examines two general approaches to what has been widely branded as “algorithmic fairness” in criminal law: the development of formal fairness criteria and accuracy measures that illustrate the trade-offs of different algorithmic interventions; and the development of “best practices” and managerialist standards for maintaining a baseline of accuracy, transparency, and validity in these systems. Attempts to render AI-branded tools more accurate by addressing narrow notions of bias miss the deeper methodological and epistemological issues regarding the fairness of these tools. The key question is whether predictive tools reflect and reinforce punitive practices that drive disparate outcomes, and how data regimes interact with the penal ideology to naturalize these practices. The chapter then calls for a radically different understanding of the role and function of the carceral state, as a starting place for re-imagining the role of “AI” as a transformative force in the criminal legal system.</p>"
10.1093/oxfordhb/9780190067397.013.48,“Fair Notice” in the Age of AI,"<p>This chapter examines the concept of “fair notice,” both in the abstract and as it operates in U.S. constitutional doctrine. Fair notice is paramount to the rule of law. The maxim has ancient roots: people ought to know, in advance, what the law demands of them. As such, fair notice will be among the key concepts for regulating the scope and role of artificial intelligence (AI) in the legal system. AI—like its junior sibling, machine learning—unleashes a historically novel possibility: decision-making tools that are at once powerfully accurate and inscrutable to their human stewards and subjects. To determine when the use of AI-based (or AI-assisted) decision-making tools are consistent with the requirements of fair notice, a sharper account of the principle’s contours is needed. The chapter then develops a tripartite model of fair notice, inspired by the problems and opportunities of AI. It argues that lack of fair notice is used interchangeably to describe three distinct properties: notice of inputs, notice of outputs, and notice of input-output functionality. Disentangling these forms of notice, and deciding which matter in which contexts, will be crucial to the proper governance of AI.</p>"
10.61969/jai.1337500,Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning,"<jats:p xml:lang=""en"">Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students’ learning.</jats:p>"
10.1007/978-981-19-9382-4_2,The Rise of AI Ethics,N/A
10.1007/s43681-024-00557-9,The creative agency of large language models: a philosophical inquiry,"<jats:title>Abstract</jats:title><jats:p>This paper explores the difficult question of whether Large Language Models (LLMs) are intrinsically creative. Because they can independently create original content, LLMs are often seen as creative agents. Contrary to the belief that LLMs are creative, this paper argues that LLMs are not creative for two reasons. First, LLMs are not creative because they lack an essential component of creativity, which is the first-person experience of the world. Secondly, LLMs are not creative because they are not the principal authors of their creative output, for they lack the subjective awareness and intentionality necessary to be regarded as authors, and their output is a collaborative effort of the AI model, data providers, and other stakeholders. Since they are not full-fledged authors in a traditional sense, they are not creative.</jats:p>"
10.1007/s43681-021-00112-w,Correction to: Why ethical audit matters in artificial intelligence?,N/A
10.1007/s43681-023-00260-1,What would qualify an artificial intelligence for moral standing?,"<jats:title>Abstract</jats:title><jats:p>What criteria must an artificial intelligence (AI) satisfy to qualify for moral standing? My starting point is that sentient AIs should qualify for moral standing. But future AIs may have unusual combinations of cognitive capacities, such as a high level of cognitive sophistication without sentience. This raises the question of whether sentience is a necessary criterion for moral standing, or merely sufficient. After reviewing nine criteria that have been proposed in the literature, I suggest that there is a strong case for thinking that some non-sentient AIs, such as those that are conscious and have non-valenced preferences and goals, and those that are non-conscious and have sufficiently cognitively complex preferences and goals, should qualify for moral standing. After responding to some challenges, I tentatively argue that taking into account uncertainty about which criteria an entity must satisfy to qualify for moral standing, and strategic considerations such as how such decisions will affect humans and other sentient entities, further supports granting moral standing to some non-sentient AIs. I highlight three implications: that the issue of AI moral standing may be more important, in terms of scale and urgency, than if either sentience or consciousness is necessary; that researchers working on policies designed to be inclusive of sentient AIs should broaden their scope to include all AIs with morally relevant interests; and even those who think AIs cannot be sentient or conscious should take the issue seriously. However, much uncertainty about these considerations remains, making this an important topic for future research.</jats:p>"
10.1007/s43681-022-00204-1,Privacy without persons: a Buddhist critique of surveillance capitalism,"<jats:title>Abstract</jats:title><jats:p>Much has been written about artificial intelligence (AI) perpetuating social inequity and disenfranchising marginalized groups (Barocas in SSRN J, 2016; Goodman in Law and Ethics of AI, 2017; Buolamwini and Gebru in Conference on Fairness, Accountability and Transparency, 2018). It is a sad irony that virtually all of these critiques are exclusively couched in concepts and theories from the Western philosophical tradition (Algorithm Watch in AI ethics guidelines global inventory, 2021; Goffi in Sapiens, 2021). In particular, Buddhist philosophy is, with a few notable exceptions (Hongladarom in A Buddhist Theory of Privacy, Springer, Singapore, 2016; Hongladarom in The Ethics of AI and Robotics A Buddhist Viewpoint, Lexington Book, Maryland, 2020; Hongladarom in MIT Technology Review, 2021; Lin et al. in Robot Ethics: The Ethical and Social Implications fo Robotics, MIT, Cambridge, 2012; Promta and Einar Himma in J Inf Commun Ethics Soc 6(2):172–187, 2008), completely ignored. This inattention to non-Western philosophy perpetuates a pernicious form of intellectual imperialism (Alatas in Southeast Asian J Soc Sci 28(1):23–45, 2000), and deprives the field of vital intellectual resources. The aim of this article is twofold: to introduce Buddhist concepts and arguments to an unfamiliar audience and to demonstrate how those concepts can be fruitfully deployed within the field of AI ethics. In part one, I develop a Buddhist inspired critique of two propositions about privacy: that the scope of privacy is defined by an essential connection between certain types of information and personal identity (i.e., what makes a person who they are), and that privacy is intrinsically valuable as a part of human dignity (Council of the European Union in Position of the Council on General Data Protection Regulation, 2016). The Buddhist doctrine of not self (<jats:italic>anattā</jats:italic>) rejects the existence of a stable and essential self. According to this view, persons are fictions and questions of personal identity have no ultimate answer. From a Buddhist perspective, the scope and value of privacy are entirely determined by contextual norms—nothing is intrinsically private nor is privacy intrinsically valuable (Nissenbaum in Theor Inq Law 20(1):221–256, 2019). In part two, I show how this shift in perspective reveals a new critique of surveillance capitalism (Zuboff in J Inf Technol 30(1):75–89, 2015). While other ethical analyses of surveillance capitalism focus on its scale and scope of illegitimate data collection, I examine the relationship between targeted advertising and what Buddhism holds to be the three causes of suffering: ignorance, craving and aversion. From a Buddhist perspective, the foremost reason to be wary of surveillance capitalism is not that it depends on systematic violations of our privacy, but that it systematically distorts and perverts the true nature of reality, instilling a fundamentally misguided and corrupting conception of human flourishing. Privacy, it turns out, may be a red herring to the extent that critiques of surveillance capitalism frame surveillance, rather than capitalism, as the primary object of concern. A Buddhist critique, however, reveals that surveillance capitalism is merely the latest symptom of a deeper disease.</jats:p>"
10.1007/s43681-023-00364-8,Principles on symbiosis for natural life and living artificial intelligence,N/A
10.21552/aire/2024/1/8,Korean Copyright Issues in Text Data Mining for Generative AI,N/A
10.1007/s43681-024-00503-9,Responsibility before freedom: closing the responsibility gaps for autonomous machines,"<jats:title>Abstract</jats:title><jats:p>The introduction of autonomous machines (AMs) in human domains has raised challenging questions about the attribution of responsibility; referred to as the <jats:italic>responsibility gap</jats:italic>. In this paper, we address the gap by arguing that entities should not be granted the freedom of action unless they can also recognise the same right for others—and be subject to blame or punishment in cases of undermining the rights of others. Since AMs fail to meet this criterion, we argue that the users who utilize an AM to pursue their goals can instead grant the machine <jats:italic>their</jats:italic> (the user’s) right to act autonomously on their behalf. In this way, an AM’s right to act freely hinges on the user’s duty to recognise others’ right to be free. Since responsibility should be attributed <jats:italic>before</jats:italic> an entity is given the freedom to act, the responsibility gap only arises when we ignore the fact that AMs have no right of acting freely on their own. We also discuss some attractive features of the approach, address some potential objections, and compare our theory to existing proposals. We conclude by arguing that holding users responsible for the behaviour of AMs promotes a responsible use of AI while it indirectly motivates companies to make safer machines.</jats:p>"
10.4018/978-1-59140-987-8.ch042,Ethics of AI,"<jats:p>The first question concerns the kinds of AI we might achieve moral, immoral, or amoral. The second concerns the ethics of our achieving such an AI. They are more closely related than a first glance might reveal. For much of technology, the National Rifle Association’s neutrality argument might conceivably apply: “guns don’t kill people, people kill people.” But if we build a genuine, autonomous AI, we arguably will have to have built an artificial moral agent, an agent capable of both ethical and unethical behavior. The possibility of one of our artifacts behaving unethically raises moral problems for their development that no other technology can.</jats:p>"
10.4324/9781315107752-25,AI and robot ethics,N/A
10.1007/s43681-024-00444-3,The moral decision machine: a challenge for artificial moral agency based on moral deference,"<jats:title>Abstract</jats:title><jats:p>Humans are responsible moral agents in part because they can competently respond to moral reasons. Several philosophers have argued that artificial agents cannot do this and therefore cannot be responsible moral agents. I present a counterexample to these arguments: the ‘Moral Decision Machine’. I argue that the ‘Moral Decision Machine’ responds to moral reasons just as competently as humans do. However, I suggest that, while a hopeful development, this does not warrant strong optimism about ‘artificial moral agency’. The ‘Moral Decision Machine’ (and similar agents) can only respond to moral reasons by deferring to others, and there are good reasons to think this is incompatible with responsible moral agency. While the challenge to artificial moral agency based on moral reasons-responsiveness can be satisfactorily addressed; the challenge based on moral deference remains an open question. The right way to understand the challenge, I argue, is as a route to the claim that artificial agents are unlikely to be responsible moral agents because they cannot be authentic.</jats:p>"
10.1007/s43681-023-00321-5,Measuring responsible artificial intelligence (RAI) in banking: a valid and reliable instrument,"<jats:title>Abstract</jats:title><jats:p>Widespread use of artificial intelligence (AI) and machine learning (ML) in the US banking industry raises red flags with regulators and social groups due to potential risk of data-driven algorithmic bias in credit lending decisions. The absence of a valid and reliable measure of responsible AI (RAI) has stunted the growth of organizational research on RAI (i.e., the organizational balancing act to optimize efficiency and equity). To address this void, we develop a novel measurement instrument to assess RAI maturity in firms. A review of the nascent literature reveals that there is a wide distribution of RAI capabilities. The RAI instrument that we advance is based on the exhaustive review of this dispersed literature. Analyses of data from large US banks show strong evidence of validity and reliability of the RAI maturity instrument.</jats:p>"
10.1007/s43681-023-00395-1,Neighborhood sampling confidence metric for object detection,N/A
10.46397/jaih.16.1,Emergence of Generative AI and the Reorientation of Art Education,N/A
10.1093/oxfordhb/9780190067397.013.37,Calculative Composition,"<p>This chapter evaluates the ethical ends and means toward which AI-driven design has been, and perhaps could be, applied. While some designers have committed to applying AI toward more ethical ends, they have paid comparatively less attention toward the ethical means of its application. In order to ensure the ethical application of AI in design, practitioners and managers must make sure that they are both defining responsible design parameters and operationalizing those parameters responsibly. Moreover, designers must consider where they should assert their agency within an automated workflow. The chapter then surveys representative design fields—fashion, product, graphic, and architectural design—to examine what ethical opportunities and risks people might face when AI-driven design practice is programmed to serve the needs and desires of laborers, consumers, and clients.</p>"
10.1007/s43681-021-00048-1,Survey of EU ethical guidelines for commercial AI: case studies in financial services,N/A
10.1007/s43681-023-00305-5,"The technology triad: disruptive AI, regulatory gaps and value change","<jats:title>Abstract</jats:title><jats:p>Disruptive technologies can have far-reaching impacts on society. They may challenge or destabilize cherished ethical values and disrupt legal systems. There is a convergent interest among ethicists and legal scholars in such “second-order disruptions” to norm systems. Thus far, however, ethical and legal approaches to technological norm-disruption have remained largely siloed. In this paper, we propose to integrate the existing ‘dyadic’ models of disruptive change in the ethical and legal spheres, and shift focus to the relations between and mutual shaping of values, technology, and law. We argue that a ‘triadic’ values-technology-regulation model—“the technology triad”—is more descriptively accurate, as it allows a better mapping of second-order impacts of technological changes (on values and norms, through changes in legal systems—or on legal systems, through changes in values and norms). Simultaneously, a triadic model serves to highlight a broader portfolio of ethical, technical, or regulatory interventions that can enable effective ethical triage of—and a more resilient response to—such Socially Disruptive Technologies. We illustrate the application of the triadic framework with two cases, one historical (how the adoption of the GDPR channeled and redirected the evolution of the ethical value of ‘privacy’ when that had been put under pressure by digital markets), and one anticipatory (looking at anticipated disruptions caused by the ongoing wave of generative AI systems).</jats:p>"
10.4018/979-8-3693-8557-9.ch003,Unleashing Creativity in Natural Language,"<jats:p>In today's cybernetic world, Generative AI (Gen AI) and Natural Language Processing (NLP) made a vibrant change in the technology. Intertwine between Generative AI and NLP creates a different angle that comes to light. NLP algorithms enhanced by Generative AI not only understands the language but also generates human-like responses, opening doors to more nuanced and context-aware interactions. This chapter focuses on two specific dimensions such as language generation and multilingual capability. Large Language Models (LLMs) are considered as a foundational element or backbone for generating texts. Thereby, prompting LLM is a crucial element for generating the desired text. The main aim of this chapter is to explore importance of prompting LLM, various prompting techniques used for interacting with LLM and the frameworks available for accessing the LLM, and issues involved in prompting LLM and their challenges and future directions will also be discussed.</jats:p>"
10.1007/s43681-022-00219-8,Ethical risks of AI-designed products: bespoke surgical tools as a case study,"<jats:title>Abstract</jats:title><jats:p>An emerging use of machine learning (ML) is creating products optimised using computational design for individual users and produced using 3D printing. One potential application is bespoke surgical tools optimised for specific patients. While optimised tool designs benefit patients and surgeons, there is the risk that computational design may also create unexpected designs that are unsuitable for use with potentially harmful consequences. We interviewed potential stakeholders to identify both established and unique technical risks associated with the use of computational design for surgical tool design and applied ethical risk analysis (eRA) to identify how stakeholders might be exposed to ethical risk within this process. The main findings of this research are twofold. First, distinguishing between unique and established risks for new medical technologies helps identify where existing methods of risk mitigation may be applicable to a surgical innovation, and where new means of mitigating risks may be needed. Second, the value of distinguishing between technical and ethical risks in such a system is that it identifies the key responsibilities for managing these risks and allows for any potential interdependencies between stakeholders in managing these risks to be made explicit. The approach demonstrated in this paper may be applied to understanding the implications of new AI and ML applications in healthcare and other high consequence domains.</jats:p>"
10.1007/s43681-021-00039-2,AI auditing and impact assessment: according to the UK information commissioner’s office,"<jats:title>Abstract</jats:title><jats:p>As the use of data and artificial intelligence systems becomes crucial to core services and business, it increasingly demands a multi-stakeholder and complex governance approach. The Information Commissioner's Office’s ‘Guidance on the AI auditing framework: Draft guidance for consultation’ is a move forward in AI governance. The aim of this initiative is toward producing guidance that encompasses both technical (e.g. system impact assessments) and non-engineering (e.g. human oversight) components to governance and represents a significant milestone in the movement towards standardising AI governance. This paper will summarise and critically evaluate the ICO effort and try to anticipate future debates and present some general recommendations.</jats:p>"
10.12677/ojls.2023.116712,Legal Regulation of Generative AI Services under Digital Ethics,N/A
10.1007/s43681-023-00303-7,The complex relationship of AI ethics and trust in human–AI teaming: insights from advanced real-world subject matter experts,N/A
10.1007/s43681-022-00153-9,Racing into the fourth industrial revolution: exploring the ethical dimensions of medical AI and rights-based regulatory framework,N/A
10.1007/s43681-022-00141-z,Explainable machine learning practices: opening another black box for reliable medical AI,"<jats:title>Abstract</jats:title><jats:p>In the past few years, machine learning (ML) tools have been implemented with success in the medical context. However, several practitioners have raised concerns about the lack of transparency—at the algorithmic level—of many of these tools; and solutions from the field of explainable AI (XAI) have been seen as a way to open the ‘black box’ and make the tools more trustworthy. Recently, Alex London has argued that in the medical context we do not need machine learning tools to be interpretable at the algorithmic level to make them trustworthy, as long as they meet some strict empirical desiderata. In this paper, we analyse and develop London’s position. In particular, we make two claims. First, we claim that London’s solution to the problem of trust can potentially address another problem, which is how to evaluate the reliability of ML tools in medicine for regulatory purposes. Second, we claim that to deal with this problem, we need to develop London’s views by shifting the focus from the opacity of algorithmic details to the opacity of the way in which ML tools are trained and built. We claim that to regulate AI tools and evaluate their reliability, agencies need an explanation of how ML tools have been built, which requires documenting and justifying the technical choices that practitioners have made in designing such tools. This is because different algorithmic designs may lead to different outcomes, and to the realization of different purposes. However, given that technical choices underlying algorithmic design are shaped by value-laden considerations, opening the black box of the design process means also making transparent and motivating (technical and ethical) values and preferences behind such choices. Using tools from philosophy of technology and philosophy of science, we elaborate a framework showing how an explanation of the training processes of ML tools in medicine should look like.</jats:p>"
10.1007/s43681-023-00382-6,Who is the human in the machine? Releasing the human–machine metaphor from its cultural roots can increase innovation and equity in AI,"<jats:title>Abstract</jats:title><jats:p>Computer science and cognitive science have a shared past, with many intertwined goals and perspectives. The conceptual metaphor, shaping the discoveries of these fields for decades, has been <jats:italic>the human mind–machine.</jats:italic> New cross-cultural findings indicate that it is time that we interrogate the origin of the metaphor and develop a more global representation of attributes labeled <jats:italic>human</jats:italic>. This paper describes a gap in fairness research in cross-cultural bias affecting international participation in the field. It further outlines opportunities to diversify and test core concepts inspiring design and increasing equity. The proposed adaptation would shift our approach to knowledge and technology creation by (1) altering the attributes of the <jats:italic>human mind–machine</jats:italic> metaphor that define intelligence, memory, categorization, logic, inference, perception, concepts of time and space, concepts of personhood, and other cognitive terms which both fields study; (2) interrogating the universality implied by the conceptual metaphor to both machine and end-user; and (3) seizing the broadened conceptual metaphor to create new math, science, and disrupt the current paradigm scripting the inferences of research findings in computer science and cognitive science. A more globally attuned conceptual metaphor, updated to enfranchise the full membership the term <jats:italic>human</jats:italic> implies, will increase our collective ability to investigate, describe, and develop new science and technology and increase the equity of those involved in the process.</jats:p>"
10.1007/s43681-023-00362-w,AI and the quest for diversity and inclusion: a systematic literature review,"<jats:title>Abstract</jats:title><jats:p>The pervasive presence and wide-ranging variety of artificial intelligence (AI) systems underscore the necessity for inclusivity and diversity in their design and implementation, to effectively address critical issues of fairness, trust, bias, and transparency. However, diversity and inclusion (D&amp;I) considerations are significantly neglected in AI systems design, development, and deployment. Ignoring D&amp;I in AI systems can cause digital redlining, discrimination, and algorithmic oppression, leading to AI systems being perceived as untrustworthy and unfair. Therefore, we conducted a systematic literature review (SLR) to identify the challenges and their corresponding solutions (guidelines/ strategies/ approaches/ practices) about D&amp;I in AI and about the applications of AI for D&amp;I practices. Through a rigorous search and selection, 48 relevant academic papers published from 2017 to 2022 were identified. By applying open coding on the extracted data from the selected papers, we identified 55 unique challenges and 33 unique solutions in addressing D&amp;I in AI. We also identified 24 unique challenges and 23 unique solutions for enhancing D&amp;I practices by AI. The result of our analysis and synthesis of the selected studies contributes to a deeper understanding of diversity and inclusion issues and considerations in the design, development and deployment of the AI ecosystem. The findings would play an important role in enhancing awareness and attracting the attention of researchers and practitioners in their quest to embed D&amp;I principles and practices in future AI systems. This study also identifies important gaps in the research literature that will inspire future direction for researchers.</jats:p>"
10.32388/5g2xsi,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/nognpu,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/m13efo,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/2aszdb,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.1117/12.2646476.6312671916112,N/A,N/A
10.2139/ssrn.4817287,Incorporating Generative Ai Into Human Resource Practices,N/A
10.31219/osf.io/95sdx,Investigating Librarian Perspectives on Generative AI,"<p>This study investigates librarians' perspectives on generative artificial intelligence (AI) in libraries. Librarians (n = 52) from 21 different institutions attended a workshop to discuss generative AI and assess the ethics of various applications of large language models (LLMs) in research. Through the analysis of qualitative workshop data, we identified 24 overarching themes — 15 concerns related to embracing AI, 4 about not embracing it, and 5 possible interventions for libraries. Additionally, assessments revealed a nuanced view of the ethical considerations associated with LLMs in research. High ethical ratings were given to use cases involving language feedback and learning support. Lower ratings were associated with generative AI imitating professional roles and generating text based on user input. Furthermore, we found mixed opinions on several use cases among the participants, demonstrating the difficulty of establishing clear guidelines. This paper presents the preliminary findings from a research and development project, offering insights into the perspectives of librarians on generative AI. It underscores the need for a balanced, informed approach to integrating generative AI in libraries.</p>"
10.1007/s43681-023-00272-x,The ethics of computer vision: an overview in terms of power,"<jats:title>Abstract</jats:title><jats:p>Computer vision is a subfield of artificial intelligence, aimed at making computers see. Computer vision tools enable a system or device to automatically analyze, interpret, and respond to images and videos. Computer vision tasks range from object detection and tracking, to the recognition of people’s faces and emotional states. While the ethics of AI in general has received significant attention, and the ethics of facial recognition (a computer vision application) too, little of the AI ethics literature focuses specifically on the ethics of computer vision. In this chapter, I create an overview of ethical, social, and political issues related to computer vision, using a critical approach. This means that I identify issues in terms of power and evaluate them in function of their impact on the value of autonomy and the normative goal of emancipatory progress. The aim of this chapter is first and foremost to offer an overview of potential normative implications of computer vision. Additionally, the chapter functions as an example for the use of a critical approach to AI ethics.</jats:p>"
10.2139/ssrn.4615977,A Case Study Exploring the Effects of Generative Ai on Incidental Language Learning,N/A
10.52591/lxai2018120310,Information Theoretic Generative Modeling,"<jats:p>In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression introduced by Claude Shannon in 1959 [1], to shed light on an important problem in latent variable modeling of data: is there room to improve the model? One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model. The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood. Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions. The result we present here runs much deeper than the particular modeling problem being solved - in formally connecting the latent variable modeling problem to rate-distortion theory, we have established a bridge where decades of work on either field can now be considered for possible cross-pollination; notably in a subsequent article we intend to ask the question next of whether practical algorithms in data compression can be used to design latent variables. We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.</jats:p>"
10.21203/rs.3.rs-3371292/v1,Experimental Evidence on Negative Impact of Generative AI on Scientific Learning Outcomes,"<jats:title>Abstract</jats:title>
        <jats:p>In this study, I explored the impact of Generative AI on learning efficacy in academic reading materials using experimental methods. College-educated participants engaged in three cycles of reading and writing tasks. After each cycle, they responded to comprehension questions related to the material. After adjusting for background knowledge and demographic factors, complete reliance on AI for writing tasks led to a 25.1% reduction in accuracy. In contrast, AI-assisted reading resulted in a 12% decline. Interestingly, using AI for summarization significantly improved both quality and output. Accuracy exhibited notable variance in the AI-assisted section. Further analysis revealed that individuals with a robust background in the reading topic and superior reading/writing skills benefitted the most. I conclude the research by discussing educational policy implications, emphasizing the need for educators to warn students about the dangers of over-dependence on AI and provide guidance on its optimal use in educational settings.</jats:p>"
10.2139/ssrn.4601781,"Information Integrity, Academic Integrity, and Generative AI",N/A
10.32388/emz1bq,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/mr4cey,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/z2j79x,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.36227/techrxiv.24417706.v1,MeloHarmony: Exploring Emotion in Crafting AI-Generated Music with Generative Adversarial Network Powered Harmony,"<jats:p>&lt;p&gt;The profound association between music and human emotion has transcended epochs, underscoring the capacity of musical compositions to elicit a spectrum of feelings, from exuberance to introspection. In the contemporary landscape, the intersection of music and technological advancements has engendered a paradigmatic shift in the creation and interpretation of musical compositions. Central to this transformation is the integration of artificial intelligence (AI) into the realm of music composition, a domain historically governed by human creativity.&lt;/p&gt;
&lt;p&gt;This research endeavors to navigate this juncture, unraveling the prospect of imbuing AI-generated music with heightened emotional resonance, thereby amplifying the scope of artistic expression. At the crux of this exploration lies the innovative utilization of Generative Adversarial Networks (GANs) to infuse the synthesized musical compositions with an intricate tapestry of human-like emotions.&lt;/p&gt;
&lt;p&gt;This paper sets out to elucidate the multifaceted dimensions of this venture by charting a trajectory that traverses the historical lineage of emotional undertones in music, culminating in a contemporary synergy between AI capabilities and human sentiment. Our approach is encapsulated within the nexus of technology and creativity, where GANs are envisaged as a conduit to facilitate the infusion of emotions into AI-generated musical compositions.&lt;/p&gt;
&lt;p&gt;In subsequent sections, we delve into an immersive analysis of the seminal role that music has played in articulating emotions throughout history. Moreover, we embark on a comprehensive exploration of the confluence of AI advancements and the nuanced realm of emotional resonance, delineating the profound possibilities that emerge from this amalgamation. Crucially, the research postulates a novel framework that leverages GANs to imbue AI-generated harmonies with a poignant emotional depth, elucidating the pivotal role of technology in elevating the emotive tenor of musical compositions.&lt;/p&gt;
&lt;p&gt;The subsequent chapters unravel the intricate methodology underpinning this research, encapsulating data collection processes, GAN architecture elucidation, techniques for embedding emotional facets, and the meticulous training process. Furthermore, a meticulous analysis of the emotional impact of AI-generated music on human perception is presented, both quantitatively and qualitatively, shedding light on the efficacy of the GAN-powered approach.&lt;/p&gt;
&lt;p&gt;Conclusively, the research extends its purview to expound upon the ethical considerations embedded within this paradigmatic juncture, while also envisioning potential trajectories for the practical application and validation of the proposed GAN-powered methodology. As the curtains are drawn on this introductory exposition, the subsequent sections promise a symphony of insights, culminating in a harmonious synthesis of AI ingenuity and human emotional resonance within the tapestry of musical composition.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4608268,Generative AI and Finding the Law,N/A
10.1038/d41591-024-00025-1,Designer antibiotics by generative AI,N/A
10.2139/ssrn.4594780,Effective Generative AI: The Human-Algorithm Centaur,N/A
10.1021/acs.analchem.4c01734.s001,Solving Advanced Task-Specific Problems in Measurement Sciences with Generative AI,N/A
10.20944/preprints202406.0578.v1,Intelligent Network Optimization in Cloud Environments with Generative AI and LLMs,"<jats:p>This paper represents a groundbreaking paradigm shift in network optimization. Departing from traditional static methodologies, this innovative approach harnesses the power of Generative Artificial Intelligence (AI) and Large Language Models (LLMs) to optimize cloud networks dynamically. By integrating advanced AI algorithms, this framework continuously adapts and evolves, ensuring optimal real-time performance. This dynamic optimization enhances efficiency and resilience, allowing cloud networks to adjust seamlessly to changing demands and conditions. Through the fusion of cutting-edge technology and adaptive intelligence, this approach heralds a new era in network optimization, empowering organizations to achieve unprecedent-ed levels of agility and scalability in their cloud infrastructures.</jats:p>"
10.4018/979-8-3693-2440-0.ch001,An Introduction to Generative AI Tools for Education 2030,"<jats:p>The year 2030 marks a significant juncture in the evolution of education, where Generative Artificial Intelligence (AI) tools are poised to revolutionize the learning experience. In education society, the importance of generative AI is to improve the accessibility of learning at the global level so that personalized learning experiences can be provided to every learner as per their needs. This chapter explores the multifaceted role of generative AI tools in reshaping educational practices, envisioning a future where these tools foster personalized, adaptive, and engaging learning environments. Generative AI tools, characterized by their ability to create and adapt content autonomously, are instrumental in tailoring educational materials to individual learner needs. This chapter surveys the landscape of generative AI applications in education, including content generation, interactive simulations, intelligent tutoring systems, and dynamic learning pathways. These tools aim to provide adaptive, context-aware learning experiences that cater to diverse learning styles and preferences. The adaptability of generative AI tools extends to the creation of personalized learning pathways. By leveraging data analytics and machine learning algorithms, these tools dynamically adjust content delivery, pacing, and complexity, ensuring that each learner's educational journey is optimized for their unique requirements. The discussion encompasses the potential of generative AI tools to support both formal and informal learning settings. Generative AI tools also play a crucial role in promoting inclusivity in education. By generating diverse and culturally relevant content, these tools contribute to breaking down barriers and addressing disparities in access to quality education. This chapter explores how generative AI can be leveraged to create content that resonates with learners from different backgrounds, fostering a more inclusive educational landscape.</jats:p>"
10.35542/osf.io/g5fb8,The Mosaic of Human-AI Co-Creation: Emerging human-technology relationships in a co-design process with generative AI,"<p>This study explored how pre-service teachers (N=33) perceived human-technology relationships with generative AI (genAI). The study employed a research-creation approach and implemented a hands-on workshop, in which the participants engaged in a speculative design process using generative AI. The study focused on how participants, armed with their new tool, approached their designs, made design decisions, and interacted with the responsive tool. The qualitative analysis of the video data from students' project presentations employed thematic analysis, interpreting the students' responses in relational terms. The results revealed that the emerging human-technology relationships were primarily expressed through distributed decision-making, with the AI actively contributing both to the object of activity and to the emerging design process. The findings highlight that genAI is neither passive nor neutral tools but actively transforms both the design process and its outcomes, shaping how people experience new forms of agency in relation to such technology.</p>"
10.1109/qrs-c60940.2023.00043,AI and Security - What Changes with Generative AI,N/A
10.1007/s43681-024-00537-z,The entangled human being – a new materialist approach to anthropology of technology,"<jats:title>Abstract</jats:title><jats:p>Technological advancements raise anthropological questions: How do humans differ from technology? Which human capabilities are unique? Is it possible for robots to exhibit consciousness or intelligence, capacities once taken to be exclusively human? Despite the evident need for an anthropological lens in both societal and research contexts, the philosophical anthropology of technology has not been established as a set discipline with a defined set of theories, especially concerning emerging technologies. In this paper, I will utilize a New Materialist approach, focusing particularly on the theories of Donna Haraway and Karen Barad, to explore their potential for an anthropology of technology. I aim to develop a techno-anthropological approach that is informed and enriched by New Materialism. This approach is characterized by its relational perspective, a dynamic and open conception of the human being, attention to diversity and the dynamics of power in knowledge production and ontology, and an emphasis on the non-human. I aim to outline an anthropology of technology centered on New Materialism, wherein the focus, paradoxically, is not exclusively on humans but equally on non-human entities and the entanglement with the non-human. As will become clear, the way we understand humans and their relationship with technology is fundamental for our concepts and theories in ethics of technology.</jats:p>"
10.1007/978-3-031-23035-6_1,The Need for AI Ethics in Higher Education,"<jats:title>Abstract</jats:title><jats:p>Business leaders, policymakers and technologists regularly portray Artificial Intelligence (AI) as an easy way to make sense of an increasingly complex world. Unsurprisingly, AI plays a central role in strategy papers, TED talks and speeches about the future of mobility.</jats:p>"
10.4018/979-8-3693-0831-8.ch010,Generative AI Ethical Considerations and Discriminatory Biases on Diverse Students Within the Classroom,"<jats:p>Generative artificial intelligence (AI) can induce a variety of biases that can impact decision-making processes, and it can produce inaccurate or distorted information that may harm marginalized student groups in higher education classrooms. With the increase in generative AI use among college students and instructors, it is important to examine the ethical risks and discriminatory biases that can negatively influence students' learning experiences. For this purpose, this chapter focuses on the different types of generative AI ethical risks that can occur in U.S. classrooms. The variety of AI discriminatory biases against diverse student populations are also documented. Further, the authors discuss a case application that expands on the potential AI biases in higher education. To prevent and address potential AI ethical risks and biases, recommendations are offered to higher education educators. Lastly, guidance is offered suggesting future research in AI biases and diversity in higher education institutions.</jats:p>"
10.1007/s10892-023-09456-3,What Makes Work “Good” in the Age of Artificial Intelligence (AI)? Islamic Perspectives on AI-Mediated Work Ethics,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) technologies are increasingly creeping into the work sphere, thereby gradually questioning and/or disturbing the long-established moral concepts and norms communities have been using to define what makes work good. Each community, and Muslims make no exception in this regard, has to revisit their moral world to provide well-thought frameworks that can engage with the challenging ethical questions raised by the new phenomenon of AI-mediated work. For a systematic analysis of the broad topic of AI-mediated work ethics from an Islamic perspective, this article focuses on presenting an accessible overview of the “moral world” of work in the Islamic tradition. Three main components of this moral world were selected due to their relevance to the AI context, namely (1) Work is inherently good for humans, (2) Practising a religiously permitted profession and (c) Maintaining good relations with involved stakeholders. Each of these three components is addressed in a distinct section, followed by a sub-section highlighting the relevance of the respective component to the particular context of AI-mediated work. The article argues that there are no unsurmountable barriers in the Islamic tradition against the adoption of AI technologies in work sphere. However, important precautions should be considered to ensure that embracing AI will not be at the cost of work-related moral values. The article also highlights how important lessons can be learnt from the positive historical experience of automata that thrived in the Islamic civilization.</jats:p>"
10.2139/ssrn.4426194,User Interaction with Misinformation Heuristics and Systematic Processes of Misinformation in Generative Ai,N/A
10.1007/978-3-031-55642-5,Generative AI for Effective Software Development,N/A
10.2139/ssrn.4587250,Effective Generative AI: The Human-Algorithm Centaur,N/A
10.1016/b978-0-44-321857-6.00012-6,From interpolation to fuzzy regression,N/A
10.32617/973-650ad52f9058b,Generative AI Can Help Grow Your Business,N/A
10.18258/57360,Generative AI-Based Design of Novel Silicase Enzymes for Carbon-Sequestering Agriculture,N/A
10.32388/w1dodh,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/w05nlk,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/dgusl4,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.2139/ssrn.4819491,Incorporating Generative Ai into Human Resource Practices,N/A
10.69554/kzrs2422,Machine unlearning for generative AI,"<jats:p xml:lang=""en"">This paper introduces a new field of AI research called machine unlearning and examines the challenges and approaches to extend machine unlearning to generative AI (GenAI). Machine unlearning is a model-driven approach to make an existing artificial intelligence (AI) model unlearn a set of data from its learning. Machine unlearning is becoming important for businesses to comply with privacy laws such as General Data Protection Regulation (GDPR) customer’s right to be forgotten, to manage security and to remove bias that AI models learn from their training data, as it is expensive to retrain and deploy the models without the bias or security or privacy compromising data. This paper presents the state of the art in machine unlearning approaches such as exact unlearning, approximate unlearning, zero-shot learning (ZSL) and fast and efficient unlearning. The paper highlights the challenges in applying machine learning to GenAI which is built on a transformer architecture of neural networks and adds more opaqueness to how large language models (LLM) learn in pre-training, fine-turning, transfer learning to more languages and in inference. The paper elaborates on how models retain the learning in a neural network to guide the various machine unlearning approaches for GenAI that the authors hope can be built upon their work. The paper suggests possible futuristic directions of research to create transparency in LLM and particularly looks at hallucinations in LLMs when they are extended to do machine translation for new languages beyond their training with ZSL to shed light on how the model stores its learning of newer languages in its memory and how it draws upon it during inference in GenAI applications. Finally, the paper calls for collaborations for future research in machine unlearning for GenAI, particularly LLMs, to add transparency and inclusivity to language AI.</jats:p>"
10.18653/v1/2023.artofsafety-1.6,Uncovering Bias in AI-Generated Images,N/A
10.4018/979-8-3693-1351-0.ch016,Use of Generative AI Tools to Facilitate Personalized Learning in the Flipped Classroom,"<jats:p>With the development of technology, AI has been the subject of intense research in the field of education, with ChatGPT, the adoption of generative AI in education. But generative AI has only gained attention in the last few years. Therefore, there are not many research results on the application of generative AI in flipped classrooms, and there are many questions that need to be explored and verified by researchers. In this study, a flipped classroom combined with generative AI tools to promote personalized learning was studied. A theme in the music course was selected for course design and a pilot study was conducted. The results show that teachers and students have very different views on this research, and this phenomenon is summarized and analyzed, and finally some suggestions are made to help better use generative AI tools to promote personalized learning in the flipped classroom.</jats:p>"
10.1007/978-3-031-55642-5_14,Generating Explanations for AI-Powered Delay Prediction in Software Projects,N/A
10.1007/s00146-020-00937-9,Reimagining life (forms) with generative and bio art,N/A
10.1515/9781501518430-010,Chapter 8: Generative AI: Conversational Agents and Beyond,N/A
10.1515/9781501519024-006,Chapter 5: Visualization with Generative AI,N/A
10.2139/ssrn.4453664,Proper Generative AI Prompting for Financial Analysis,N/A
10.5040/9781509974979.0008,Notes,N/A
10.2139/ssrn.4537389,The Copyright Problem with Emerging Generative AI,N/A
10.32388/ykz2dp,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.1007/978-981-19-9382-4_6,Normative Ethical Theory and AI Ethics,N/A
10.1007/s43681-022-00245-6,Workplace automation and political replacement: a valid analogy?,N/A
10.7551/mitpress/12549.003.0012,Policy Proposals,N/A
10.5220/0012693000003693,The Impact of Structured Prompt-Driven Generative AI on Learning Data Analysis in Engineering Students,N/A
10.1007/s43681-022-00152-w,Should explainability be a fifth ethical principle in AI ethics?,N/A
10.1007/s00146-024-01921-3,"Not “what”, but “where is creativity?”: towards a relational-materialist approach to generative AI","<jats:title>Abstract</jats:title><jats:p>The recent emergence of generative AI software as viable tools for use in the cultural and creative industries has sparked debates about the potential for “creativity” to be automated and “augmented” by algorithmic machines. Such discussions, however, begin from an ontological position, attempting to define creativity by either falling prey to universalism (i.e. “creativity is X”) or reductionism (i.e. “only humans can be truly creative” or “human creativity will be fully replaced by creative machines”). Furthermore, such an approach evades addressing the real and material impacts of AI on creative labour in these industries. This article thus offers more expansive methodological and conceptual approaches to the recent hype on generative AI. By combining (Csikszentmihalyi, The systems model of creativity, Springer, Dordrecht, 2014) systems view of creativity, in which we emphasise the shift from “what” to “where” is creativity, with (Lievrouw, Media technologies, The MIT Press, 2014) relational-materialist theory of “mediation”, we argue that the study of “creativity” in the context of generative AI must be attentive to the interactions between technologies, practices, and social arrangements. When exploring the relational space between these elements, three core concepts become pertinent: creative labour, automation, and distributed agency. Critiquing “creativity” through these conceptual lenses allows us to re-situate the use of generative AI within discourses of labour in post-industrial capitalism and brings us to a conceptualisation of creativity that privileges neither the human user nor machine algorithm but instead emphasises a relational and distributed form of agency.</jats:p>"
10.1007/979-8-8688-0456-4_12,The Platform Shift,N/A
10.1007/979-8-8688-0456-4_3,Understanding Language Models,N/A
10.1629/uksg.649,Empowering knowledge through AI: open scholarship proactively supporting well trained generative AI,N/A
10.7551/mitpress/12549.003.0018,Further Reading,N/A
10.7551/mitpress/12549.003.0006,Just Machines?,N/A
10.2139/ssrn.4461406,Task-Interdependencies between Generative AI and Workers,N/A
10.2139/ssrn.4852574,Interactivity and Illusions of Ability: The Effect of Generative AI on Investor Judgments,N/A
10.12781/978-1-907549-42-7-2,Generative Journalism Provokes New Life,N/A
10.36227/techrxiv.24417706,MeloHarmony: Exploring Emotion in Crafting AI-Generated Music with Generative Adversarial Network Powered Harmony,"<jats:p>&lt;p&gt;The profound association between music and human emotion has transcended epochs, underscoring the capacity of musical compositions to elicit a spectrum of feelings, from exuberance to introspection. In the contemporary landscape, the intersection of music and technological advancements has engendered a paradigmatic shift in the creation and interpretation of musical compositions. Central to this transformation is the integration of artificial intelligence (AI) into the realm of music composition, a domain historically governed by human creativity.&lt;/p&gt;
&lt;p&gt;This research endeavors to navigate this juncture, unraveling the prospect of imbuing AI-generated music with heightened emotional resonance, thereby amplifying the scope of artistic expression. At the crux of this exploration lies the innovative utilization of Generative Adversarial Networks (GANs) to infuse the synthesized musical compositions with an intricate tapestry of human-like emotions.&lt;/p&gt;
&lt;p&gt;This paper sets out to elucidate the multifaceted dimensions of this venture by charting a trajectory that traverses the historical lineage of emotional undertones in music, culminating in a contemporary synergy between AI capabilities and human sentiment. Our approach is encapsulated within the nexus of technology and creativity, where GANs are envisaged as a conduit to facilitate the infusion of emotions into AI-generated musical compositions.&lt;/p&gt;
&lt;p&gt;In subsequent sections, we delve into an immersive analysis of the seminal role that music has played in articulating emotions throughout history. Moreover, we embark on a comprehensive exploration of the confluence of AI advancements and the nuanced realm of emotional resonance, delineating the profound possibilities that emerge from this amalgamation. Crucially, the research postulates a novel framework that leverages GANs to imbue AI-generated harmonies with a poignant emotional depth, elucidating the pivotal role of technology in elevating the emotive tenor of musical compositions.&lt;/p&gt;
&lt;p&gt;The subsequent chapters unravel the intricate methodology underpinning this research, encapsulating data collection processes, GAN architecture elucidation, techniques for embedding emotional facets, and the meticulous training process. Furthermore, a meticulous analysis of the emotional impact of AI-generated music on human perception is presented, both quantitatively and qualitatively, shedding light on the efficacy of the GAN-powered approach.&lt;/p&gt;
&lt;p&gt;Conclusively, the research extends its purview to expound upon the ethical considerations embedded within this paradigmatic juncture, while also envisioning potential trajectories for the practical application and validation of the proposed GAN-powered methodology. As the curtains are drawn on this introductory exposition, the subsequent sections promise a symphony of insights, culminating in a harmonious synthesis of AI ingenuity and human emotional resonance within the tapestry of musical composition.&lt;/p&gt;</jats:p>"
10.5040/9781509974979.ch-005,Law,N/A
10.2139/ssrn.4738748,The Generative AI challenges for competition authorities,N/A
10.18653/v1/2023.artofsafety-1,Proceedings of the ART of Safety: Workshop on Adversarial testing and Red-Teaming for generative AI,N/A
10.32388/m78whk,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.1515/9783839474723,Art Intelligence,N/A
10.2139/ssrn.4877505,Generative AI and Firm-level Productivity: Evidence from Startup Funding Dynamics,N/A
10.14236/ewic/eva2024.38,Generative AI: The death of computer art?,N/A
10.1109/iedm45741.2023.10413684,Innovations For Energy Efficient Generative AI,N/A
10.1007/s43681-021-00053-4,Real Estate Data Marketplace,N/A
10.1007/s43681-021-00088-7,God does not play dice but self-driving cars should,N/A
10.1007/s43681-024-00436-3,State of artificial intelligence eco-system in Ethiopia,N/A
10.1007/s43681-024-00442-5,Artificial intelligence at sentencing: when do algorithms perform well enough to replace humans?,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence is currently supplanting the work of humans in many societal contexts. The purpose of this article is to consider the question of when algorithmic tools should be regarded as performing sufficiently well to replace human judgements and decision-making at sentencing. More precisely, the question as to which are the ethically plausible criteria for the comparative performance assessments of algorithms and humans is considered with regard to both risk assessment algorithms that are designed to provide predictions of recidivism and sentencing algorithms designed to determine sentences in individual criminal cases. It is argued, first, that the prima facie most obvious assessment criteria do not stand up to ethical scrutiny. Second, that ethically plausible criteria presuppose ethical theory on penal distribution which currently has not been sufficiently developed. And third, that the current lack of assessment criteria has comprehensive implications regarding when algorithmic tools should be implemented in criminal justice practice.</jats:p>"
10.1007/s43681-024-00422-9,Correction: QI2: an interactive tool for data quality assurance,N/A
10.1007/s43681-023-00297-2,A principles-based ethics assurance argument pattern for AI and autonomous systems,"<jats:title>Abstract</jats:title><jats:p>An assurance case is a structured argument, typically produced by safety engineers, to communicate confidence that a critical or complex system, such as an aircraft, will be <jats:italic>acceptably safe</jats:italic> within its intended context. Assurance cases often inform third party approval of a system. One emerging proposition within the trustworthy AI and autonomous systems (AI/AS) research community is to use assurance cases to instil justified confidence that specific AI/AS will be <jats:italic>ethically acceptable</jats:italic> when operational in well-defined contexts. This paper substantially develops the proposition and makes it concrete. It brings together the assurance case methodology with a set of ethical principles to structure a principles-based ethics assurance argument pattern. The principles are justice, beneficence, non-maleficence, and respect for human autonomy, with the principle of transparency playing a supporting role. The argument pattern—shortened to the acronym PRAISE—is described. The objective of the proposed PRAISE argument pattern is to provide a reusable template for individual ethics assurance cases, by which engineers, developers, operators, or regulators could justify, communicate, or challenge a claim about the overall ethical acceptability of the use of a specific AI/AS in a given socio-technical context. We apply the pattern to the hypothetical use case of an autonomous ‘robo-taxi’ service in a city centre.</jats:p>"
10.7551/mitpress/12549.003.0007,The Technology,N/A
10.7551/mitpress/12549.003.0020,[ Front Matter ],N/A
10.47289/aiej,AI Ethics Journal,N/A
10.4018/979-8-3693-0502-7.ch010,Modern Applications With a Focus on Training ChatGPT and GPT Models,"<jats:p>Generative AI (GAI) and natural language processing (NLP) have emerged as the most exciting and rapidly growing fields in artificial intelligence (AI). This book chapter provides a comprehensive exploration of the advanced applications of GAI and NLP models, with a specific focus on the renowned ChatGPT model. The chapter commences by offering a concise historical overview of the development of GAI and NLP, highlighting crucial milestones and advancements in the field over the period. In order to understand the workings of the current technology sensation, we will take a brief look at the basic building blocks of GPT models, such as transformers. Subsequently, the chapter delves into the introduction of ChatGPT, presenting an extensive overview of the model, elucidating its underlying architecture, and emphasizing its unique capabilities. Furthermore, it will illustrate the training process of the GPT model followed by a fine-tuning process to deal with the current model's shortcomings.</jats:p>"
10.4018/979-8-3693-0831-8.ch003,Unravelling the Evolution of Generative AI in Communication Education,"<jats:p>This chapter traces the historical evolution of human-machine communication, highlighting generative AI's transformative role. It begins with the origins of AI and early communication technologies, including Alan Turing's pioneering work and the development of Turing machines, as well as early AI experiments in human-machine interaction. The narrative then transitions to AI's evolution and the emergence of generative models, spotlighting advancements like recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, which are fundamental to coherent text generation. The pivotal role of generative AI in language and communication learning is explored, showcasing its impact on writing skills and comprehension. Ethical considerations in AI usage are discussed, including bias, privacy, and ethical communication practices. The chapter concludes by envisioning AI's future in communication pedagogy, emphasizing collaboration between educators and AI developers and recapitulating historical insights for the integration of AI in education.</jats:p>"
10.4018/979-8-3693-2418-9.ch006,Generative AI and Its Impact on Creative Thinking Abilities in Higher Education Institutions,"<jats:p>Generative AI technologies such as ChatGPT have started gaining increased popularity among higher education institutions. Students, as well as teaching professionals, can utilize these tools for various academic purposes due to the immense benefits they provide by way of customization of data generated and ease of access to data. However, this chapter seeks to analyze how such tools may impact students' creative thinking ability. It also analyses the drawbacks faced by teachers after implementation of such tools. The methodology adopted for the study was two surveys: one administered to gather students' opinions and the other for understanding teachers' perspectives. The analysis of the data collected shows that the over-reliance of students on such generative AI tools might hinder students' ability to think creatively to some extent. The chapter also suggests some of the strategies that can be adopted by teachers to ensure students' capabilities are assessed accurately.</jats:p>"
10.51219/urforum.2023.massimo-buonomo,Exploring the Synergy: Generative AI and Venture Capital in Driving Innovation and Growth,N/A
10.1162/99608f92.d949f941,Government Interventions to Avert Future Catastrophic AI Risks,N/A
10.55041/isjem01633,StoryCraft AI: Exploring Generative Approaches to Story Narration through AI,"<jats:p>In this project, we demonstrate a Storytelling AI system, which can generate short stories and complementary illustrated images with minimal input from the user. The system makes use of a text generation model, a text-to-image synthesis network, and a neural style transfer model. The final project is deployed into a website where users can build their stories. The field of AI has made significant changes in various domains, including Natural Language Processing and Generative models. One captivating application of these advancements is in the realm of storytelling. This project introduces a novel approach to story narration using a Generative AI model, specifically leveraging the GPT architecture. Traditional storytelling involves human creativity, imagination, and the ability to craft engaging narratives. However, the integration of AI into storytelling brings about new opportunities and challenges. In this project, we delve into the methodologies and techniques used to train a GPT-based model for generating coherent and captivating stories. In the end, we found that using computers for storytelling can be exciting, but we need to work together to ensure the stories are great and meaningful.  Key Words:  Artificial Intelligence (AI), Generative Adversarial Network (GAN), Generative Pre-trained Transformer (GPT)</jats:p>"
10.2139/ssrn.4860853,AI at the Bench: Legal and Ethical Challenges of Informing - or Misinforming - Judicial Decision-Making Through Generative AI,N/A
10.1007/s43681-023-00332-2,"Against the opacity, and for a qualitative understanding, of artificially intelligent technologies","<jats:title>Abstract</jats:title><jats:p>This paper aims, first, to argue against using opaque AI technologies in decision making processes, and second to suggest that we need to possess a qualitative form of understanding about them. It first argues that opaque artificially intelligent technologies are suitable for users who remain indifferent to the understanding of decisions made by means of these technologies. According to virtue ethics, this implies that these technologies are not well-suited for those who care about realizing their moral capacity. The paper then draws on discussions on scientific understanding to suggest that an AI technology becomes understandable to its users when they are provided with a qualitative account of the consequences of using it. As a result, explainable AI methods can render an AI technology understandable to its users by presenting the qualitative implications of employing the technology for their lives.</jats:p>"
10.1007/s43681-024-00554-y,Navigating modern era at sea: legal challenges and opportunities of unmanned and autonomous shipping,N/A
10.1145/3461702.3462552,An AI Ethics Course Highlighting Explicit Ethical Agents,N/A
10.1007/s43681-023-00403-4,Correction: Ought we align the values of artificial moral agents?,N/A
10.1007/s43681-024-00429-2,Correction: ECS: an interactive tool for data quality assurance,N/A
10.1007/s43681-022-00212-1,Against explainability requirements for ethical artificial intelligence in health care,N/A
10.1007/s43681-023-00324-2,A context-specific analysis of ethical principles relevant for AI-assisted decision-making in health care,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI)-assisted technologies may exert a profound impact on social structures and practices in care contexts. Our study aimed to complement ethical principles considered relevant for the design of AI-assisted technology in health care with a context-specific conceptualization of the principles from the perspectives of individuals potentially affected by the implementation of AI technologies in nursing care. We conducted scenario-based semistructured interviews focusing on situations involving moral decision-making occurring in everyday nursing practice with nurses (<jats:italic>N</jats:italic> = 15) and care recipients (<jats:italic>N</jats:italic> = 13) working, respectively, living in long-term care facilities in Germany. First, we analyzed participants’ concepts of the ethical principles beneficence, respect for autonomy and justice. Second, we investigated participants’ expectations regarding the actualization of these concepts within the context of AI-assisted decision-making. The results underscore the importance of a context-specific conceptualization of ethical principles for overcoming epistemic uncertainty regarding the risks and opportunities associated with the (non)fulfillment of these ethical principles. Moreover, our findings provide indications regarding which concepts of the investigated ethical principles ought to receive extra attention when designing AI technologies to ensure that these technologies incorporate the moral interests of stakeholders in the care sector.</jats:p>"
10.1007/s43681-022-00242-9,Values in AI: bioethics and the intentions of machines and people,N/A
10.1007/s43681-023-00373-7,Designing value-sensitive AI: a critical review and recommendations for socio-technical design processes,"<jats:title>Abstract</jats:title><jats:p>This paper presents a critical review of how different socio-technical design processes for AI-based systems, from scholarly works and industry, support the creation of value-sensitive AI (VSAI). The review contributes to the emerging field of human-centred AI, and the even more embryonic space of VSAI in four ways: (i) it introduces three criteria for the review of VSAI based on their contribution to design processes’ overall value-sensitivity, and as a response to criticisms that current interventions are lacking in these aspects: comprehensiveness, level of guidance offered, and methodological value-sensitivity, (ii) it provides a novel review of socio-technical design processes for AI-based systems, (iii) it assesses each process based on the mentioned criteria and synthesises the results into broader trends, and (iv) it offers a resulting set of recommendations for the design of VSAI. The objective of the paper is to help creators and followers of design processes—whether scholarly or industry-based—to understand the level of value-sensitivity offered by different socio-technical design processes and act accordingly based on their needs: to adopt or adapt existing processes or to create new ones.</jats:p>"
10.1007/s43681-022-00205-0,"A principled governance for emerging AI regimes: lessons from China, the European Union, and the United States",N/A
10.21428/e4baedd9.6b3930b1,Preface,N/A
10.32388/wh1ck9,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/szm69h,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/cllzwo,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/xgqwmx,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.1007/979-8-8688-0205-8,Building Generative AI-Powered Apps,N/A
10.31234/osf.io/vgzhj,Beyond the Brush: Human Versus AI Creativity  in the Realm of Generative Art,"<p>Drawing parallels from the evolution of chess, where human-machine collaboration once thrived before succumbing to AI dominance, we investigate whether similar dynamics are at play in the realm of digital art. Leveraging DALL-E 3, an advanced AI program that generates digital images from textual descriptions, we produced artwork using creative wordsets provided by professional artists, novice artists, and AI (ChatGPT 4.0). Based on ratings from 299 participants, our results show that digital images collaboratively produced by professional artists and AI were perceived as more creative than those produced by AI alone. Additionally, digital images collaboratively produced by professional artists and AI were deemed more creative than those collaboratively produced by novice artists and AI, the latter being outperformed by AI. These findings highlight the enduring significance of human creativity in the face of advancing AI technologies, and suggest the potential for continued collaboration between humans and AI in creative expression.</p>"
10.52783/jier.v4i2.1019,Next-Generation Education: The Impact of Generative AI on Learning,N/A
10.2139/ssrn.4899250,"Optimal Integration: Human, Machine, and Generative AI",N/A
10.2139/ssrn.4847595,Bias in Text Generative Open AI,N/A
10.1038/d41586-023-01516-w,How generative AI is building better antibodies,N/A
10.2307/jj.8500773.5,EUROPEAN UNION’S ETHICS GUIDELINES FOR AI,N/A
10.1007/s43681-021-00042-7,Privacy and data balkanization: circumventing the barriers,N/A
10.1007/978-981-19-9382-4_3,"AI, Philosophy of Technology, and Ethics",N/A
10.1007/s43681-022-00193-1,Why reciprocity prohibits autonomous weapons systems in war,N/A
10.1007/s43681-024-00455-0,A shift towards oration: teaching philosophy in the age of large language models,N/A
10.1007/s43681-021-00041-8,Public acceptance and perception of autonomous vehicles: a comprehensive review,N/A
10.1007/s43681-023-00328-y,Let us make man in our image-a Jewish ethical perspective on creating conscious robots,"<jats:title>Abstract</jats:title><jats:p>The dream of making conscious humanoid robots is one that has long tantalized humanity, yet today it seems closer than ever before. Assuming that science can make it happen, the question becomes: should we make it happen? Is it morally permissible to create synthetic beings with consciousness? While a consequentialist approach may seem logical, attempting to assess the potential positive and negative consequences of such a revolutionary technology is highly speculative and raises more questions than it answers. Accordingly, some turn to ancient and not-so-ancient stories of “automata” for direction. Of the many automata conjured throughout history, if not in matter then in mind, the Golem stands out as one of the most persistent paradigms employed to discuss technology in general and technologically engendered life forms in particular. In this essay, I introduce a novel reading of the Golem paradigm to argue not from consequentialism, but from a deep-seated two-thousand-year-old tradition, the ethical implications of which are wholly deontological.</jats:p>"
10.1007/s43681-023-00360-y,A multidimensional approach towards addressing existing and emerging challenges in the use of ChatGPT,N/A
10.1007/s43681-021-00109-5,Distributed responsibility in human–machine interactions,"<jats:title>Abstract</jats:title><jats:p>Artificial agents have become increasingly prevalent in human social life. In light of the diversity of new human–machine interactions, we face renewed questions about the distribution of moral responsibility. Besides positions denying the mere possibility of attributing moral responsibility to artificial systems, recent approaches discuss the circumstances under which artificial agents may qualify as moral agents. This paper revisits the discussion of how responsibility might be distributed between artificial agents and human interaction partners (including producers of artificial agents) and raises the question of whether attributions of responsibility should remain entirely on the human side. While acknowledging a crucial difference between living human beings and artificial systems culminating in an asymmetric feature of human–machine interactions, this paper investigates the extent to which artificial agents may reasonably be attributed a share of moral responsibility. To elaborate on criteria that can justify a distribution of responsibility in certain human–machine interactions, the role of types of criteria (interaction-related criteria and criteria that can be deferred from socially constructed responsibility relationships) is examined. Thereby, the focus will lay on the evaluation of potential criteria referring to the fact that artificial agents surpass in some aspects the capacities of humans. This is contrasted with socially constructed responsibility relationships that do not take these criteria into account. In summary, situations are examined in which it seems plausible that moral responsibility can be distributed between artificial and human agents.</jats:p>"
10.4018/979-8-3693-1351-0.ch009,Artificial Intelligence (AI) and Cheating,"<jats:p>Generative AI (GenAI) systems pose new challenges in academic dishonesty. Students may be tempted to use GenAI systems to cheat and submit content in assignments and projects that they did not create themselves. This points to the need for schools to focus on strong deterrent measures as well as informative and enhancing practices. Academic dishonesty is not limited to students. The use of GenAI in academia raises two extreme ethical problems. These problems relate to the ethical problems of the end user and the ethical problems in the development of this technology. The ethical use of GenAI technologies should be achieved in a way that respects human rights and takes into account user concerns. This new technology requires a rethinking of teaching methods, as well as assessment and evaluation. It is recommended that policymakers, students, and faculty work collectively and take responsibility for adopting ethical values in the process of integrating GenAI into academia.</jats:p>"
10.2139/ssrn.4662322,Enhancing Generative Ai-Based Software Implementation: The Role of Trust in Ai and Top Management Support,N/A
10.1007/s43681-024-00560-0,Ensuring fundamental rights compliance and trustworthiness of law enforcement AI systems: the ALIGNER Fundamental Rights Impact Assessment,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence systems can expand the capabilities and enhance the efficiency of law enforcement agencies preventing, investigating, detecting, and prosecuting criminal offences in the European Union. At the same time, the deployment of artificial intelligence in the security domain often raises numerous legal and ethical concerns. The ALIGNER Fundamental Rights Impact Assessment is an operational tool, rooted in fundamental rights and in the principles of AI ethics, ready to be integrated in the AI governance measures of European law enforcement agencies to inform their decision-making processes and ensure compliance with the recently adopted Artificial Intelligence Act. This paper first introduces the main tensions between law enforcement AI and fundamental rights, as enshrined in the Charter of Fundamental Rights of the European Union; then, it gives an overview of the main developments and best practices in AI governance and their relationship with fundamental rights as well as AI ethics; and finally, it describes the structure of the ALIGNER Fundamental Rights Impact Assessment.</jats:p>"
10.1145/3278721.3278806,"AI Decisions, Risk, and Ethics",N/A
10.1093/oxfordhb/9780190067397.013.41,Europe,"<p>This chapter assesses Europe’s efforts in developing a full-fledged strategy on the human and ethical implications of artificial intelligence (AI). The strong focus on ethics in the European Union’s AI strategy should be seen in the context of an overall strategy that aims at protecting citizens and civil society from abuses of digital technology but also as part of a competitiveness-oriented strategy aimed at raising the standards for access to Europe’s wealthy Single Market. In this context, one of the most peculiar steps in the European Union’s strategy was the creation of an independent High-Level Expert Group on AI (AI HLEG), accompanied by the launch of an AI Alliance, which quickly attracted several hundred participants. The AI HLEG, a multistakeholder group including fifty-two experts, was tasked with the definition of Ethics Guidelines as well as with the formulation of “Policy and Investment Recommendations.” With the advice of the AI HLEG, the European Commission put forward ethical guidelines for Trustworthy AI—which are now paving the way for a comprehensive, risk-based policy framework.</p>"
10.1007/s43681-024-00558-8,Situating AI in assessment—an exploration of university teachers’ valuing practices,"<jats:title>Abstract</jats:title><jats:p>Emerging AI technologies are changing teachers’ assessment practices and posing higher education institutions with novel ethical dilemmas. While frameworks and guidelines promise to align technology with moral and human values, the dilemma of how AI may impact existing valuing practices is often overlooked. To examine this gap, we conducted an interview study with university teachers from different disciplines at a university in Sweden. Following a semi-structured study design, we explored university teachers’ anticipations of AI in assessment and examined how emerging AI technologies may reconfigure the fit between values, challenges, and activities situated in everyday assessment contexts. Our findings suggest that anticipated AI, including automation and AI-mediated communication and grading, may amplify and reduce teachers’ possibilities to align activities with professional, pedagogical, and relational values and solve current challenges. In light of the study’s findings, the paper discusses potential ethical issues in the anticipated shifts from human to automated assessment and possible new and reinforced challenges brought by AI for education.</jats:p>"
10.1515/9783111323749-010,10 The future with AI and AI in action,N/A
10.1007/978-981-99-0707-6_8,Applied Ethics: AI and Ethics,N/A
10.1007/s43681-021-00098-5,Factoring ethics in management algorithms for municipal information-analytical systems,"<jats:title>Abstract</jats:title><jats:p>The discourse on the ethics of artificial intelligence (AI) has generated a plethora of different conventions, principles and guidelines outlining an ethical perspective on the use and research of AI. However, when it comes to breaking down general implications to specific use cases, existent frameworks have been remaining vague. The following paper aims to fill this gap by examining the ethical implications of the use of information analytical systems through a management approach for filtering the content in social media and preventing information thrusts with negative consequences for human beings and public administration. The ethical dimensions of AI technologies are revealed through deduction of general challenges of digital governance to applied level management technics.</jats:p>"
10.1007/s43681-021-00123-7,The AI ethicist’s dilemma: fighting Big Tech by supporting Big Tech,"<jats:title>Abstract</jats:title><jats:p>Assume that a researcher uncovers a major problem with how social media are currently used. What sort of challenges arise when they must subsequently decide whether or not to use social media to create awareness about this problem? This situation routinely occurs as ethicists navigate choices regarding how to effect change and potentially remedy the problems they uncover. In this article, challenges related to new technologies and what is often referred to as ‘Big Tech’ are emphasized. We present what we refer to as the AI ethicist’s dilemma, which emerges when an AI ethicist has to consider how their own success in communicating an identified problem is associated with a high risk of decreasing the chances of successfully remedying the problem. We examine how the ethicist can resolve the dilemma and arrive at ethically sound paths of action through combining three ethical theories: virtue ethics, deontological ethics and consequentialist ethics. The article concludes that attempting to change the world of Big Tech only using the technologies and tools they provide will at times prove to be counter-productive, and that political and other more disruptive avenues of action should also be seriously considered by ethicists who want to effect long-term change. Both strategies have advantages and disadvantages, and a combination might be desirable to achieve these advantages and mitigate some of the disadvantages discussed.</jats:p>"
10.1177/17470161231220946,AI research ethics is in its infancy: the EU’s AI Act can make it a grown-up,"<jats:p> As the artificial intelligence (AI) ethics field is currently working towards its operationalisation, ethics review as carried out by research ethics committees (RECs) constitutes a powerful, but so far underdeveloped, framework to make AI ethics effective in practice at the research level. This article contributes to the elaboration of research ethics frameworks for research projects developing and/or using AI. It highlights that these frameworks are still in their infancy and in need of a structure and criteria to ensure AI research projects advance in a way that respects norms and principles. This article proposes to draw from the European Union’s AI Act currently in development to shape these frameworks. Although, in the current form of the draft (as of August 2023), the obligations of the AI Act do not apply to scientific research, it is most likely that they will still have a strong impact on AI research considering the need to anticipate market placement or to test new tools in real world conditions. This article investigates what the risk-based approach in the AI Act implies for research ethics and highlights some AI Act obligations of particular value to implement for ethics review processes. </jats:p>"
10.1007/978-3-030-51110-4_12,Ethics in AI and Robotics: A Strategic Challenge,N/A
10.4337/9781803926728.00012,AI Ethics and Machine Ethics,N/A
10.54079/jpmi.37.3.3284,"CHATBOTS, GENERATIVE AI, AND SCHOLARLY MANUSCRIPTS WAME RECOMMENDATIONS ON CHATBOTS AND GENERATIVE ARTIFICIAL INTELLIGENCE IN RELATION TO SCHOLARLY PUBLICATIONS",N/A
10.54941/ahfe1005079,From Generative AI to Generative Organizations: A Service Lens on Organizational Learning and Development,"<jats:p>GenAI has conquered the world in flight. Balancing benefits and risks of introducing GenAI the question organizations are asking themselves is not whether to use GenAI, but how: integrate into the ""run"" of existing processes, ""transform"" the processes or ""innovate"" the organization at all (Spohrer, 2021, March, 1991)? On the other hand generative learning in the organizational context is in theory  hardly characterized beyond rudimentary properties (Chiva et al., 2010, Senge, 1997). Our research focuses on building theoretical knowledge and practical implications of ""how GenAI can be used to transform organizations to generative organizations for improving organizational learning and development"". To address the research question we take a service lens and ground our research design on a broad knowledge base of theories, concepts, company practices, and instantiations that address individual parts of our research. The purpose of the paper is to explore how the introduction of GenAI can be used in a ""land and expand"" strategy to develop also other generative capabilities (GenXX). In this way, the study contributes to building knowledge about how organizations can continuously scale up their capacity to (co-) create value to learn, to adapt, and thus to develop.Methodology and Approach Our research design is seen as overall strategy in order to integrate in a logical way the different components of our research for ensuring that the research question will be thoroughly analyzed and investigated (Khanday S., 2019). A conceptual paper as approach and within this a “model” as type of paper is selected for building a conceptual framework that predicts relationships between the properties and the processes in the context of introducing GenAI.  For explaining the properties of the phenomenon, we draw on the domain theories Service-Dominant Logic and Service Science. To demonstrate and study the relationships of the properties Service Dominant Architecture is chosen as method theory (MacInnis, 2011, Jaakkola, 2020, Gilson and Goldberg, 2015).Findings The study contributes to theory building referring to the terms and mechanisms of building generative capabilities and generative learning of organizations. More precisely, this means: The extension of the definition of ""generative"". The narrative and process of learning and building generative capabilities (GenXX) via service for service exchange provided by S-D Logic. The structural perspective on generative learning judged by Service Science as improvement of a service system or the structure of interconnected service (eco) systems. And architecture perspectives as (construction-) plan as medium and output for the intentional building of generative capabilities and fostering learning to improve change by Service Dominant Architecture. In this way, this work also contributes knowledge for practical decisions to foster organizational learning in the sense of ""land and expand"" with the introduction of GenAI.</jats:p>"
10.1017/cbo9780511978036.034,What Can AI Do for Ethics?,N/A
10.2307/jj.8500773.11,PRINCIPLES BASED ETHICS AND VIRTUE ETHICS,N/A
10.1007/979-8-8688-0318-5_10,After the Brain Rush: What Is Generative AI’s Future?,N/A
10.1007/978-3-031-46238-2_13,"Can Generative Artificial Intelligence Foster Belongingness, Social Support, and Reduce Loneliness? A Conceptual Analysis",N/A
10.1093/oxfordhb/9780190067397.013.53,Smart City Ethics,"<p>This chapter explores the concept of “smart cities,” a term which describes the growing role of data analytics and sensors in urban life. Smart city initiatives rely on pervasive data gathering and integration, big data analytics, and artificial intelligence to manage mobility, energy, housing, public realm access, and myriad public and private services. These data flows can change how physical infrastructure like streets and parks are configured and services provisioned. They can tailor opportunities for housing or education based on individual digital identities and predictive algorithms. As more life in the city runs through digital apps and platforms, rights to access and control data increase in importance. Data flows from residents and public spaces to smart city corporations raise pressing policy questions about what power the public should cede to private developers to shape urban space, subject to how much oversight and with what expectation of return on public assets. The chapter then sorts these concerns into three major groups: privatization, platformization, and domination.</p>"
10.1515/9783111425078-005,5 LLM Pretraining Methods,N/A
10.19033/sks.2024.6.84.59,"Korean Semantic Education in the AI Era: Focusing on AI Translation, Chatbot Builders, and Generative AI",N/A
10.1007/s43681-024-00419-4,Anthropomorphism in AI: hype and fallacy,"<jats:title>Abstract</jats:title><jats:p>This essay focuses on anthropomorphism as both a form of hype and fallacy. As a form of hype, anthropomorphism is shown to exaggerate AI capabilities and performance by attributing human-like traits to systems that do not possess them. As a fallacy, anthropomorphism is shown to distort moral judgments about AI, such as those concerning its moral character and status, as well as judgments of responsibility and trust. By focusing on these two dimensions of anthropomorphism in AI, the essay highlights negative ethical consequences of the phenomenon in this field.</jats:p>"
10.1016/b978-0-443-18851-0.00009-3,The ethics of online AI-driven agriculture and food systems,N/A
10.1007/s43681-021-00054-3,A framework to contest and justify algorithmic decisions,N/A
10.1007/s00146-024-01917-z,Lessons from the California Gold Rush of 1849: prudence and care before advancing generative AI initiatives within your enterprise,N/A
10.1007/s00146-023-01679-0,How we can create the global agreement on generative AI bias: lessons from climate justice,N/A
10.1007/s00146-020-01077-w,AI Ethics: how can information ethics provide a framework to avoid usual conceptual pitfalls? An Overview,N/A
10.1007/s43681-024-00461-2,The mechanisms of AI hype and its planetary and social costs,"<jats:title>Abstract</jats:title><jats:p>Our global landscape of emerging technologies is increasingly affected by artificial intelligence (AI) hype, a phenomenon with significant large-scale consequences for the global AI narratives being created today. This paper aims to dissect the phenomenon of AI hype in light of its core mechanisms, drawing comparisons between the current wave and historical episodes of AI hype, concluding that the current hype is historically unmatched in terms of magnitude, scale and planetary and social costs. We identify and discuss socio-technical mechanisms fueling AI hype, including anthropomorphism, the proliferation of self-proclaimed AI “experts”, the geopolitical and private sector “fear of missing out” trends and the overuse and misappropriation of the term “AI” in emerging technologies. The second part of the paper seeks to highlight the often-overlooked costs of the current AI hype. We examine its planetary costs as the AI hype exerts tremendous pressure on finite resources and energy consumption. Additionally, we focus on the connection between AI hype and socio-economic injustices, including perpetuation of social inequalities by the huge associated redistribution of wealth and costs to human intelligence. In the conclusion, we offer insights into the implications for how to mitigate AI hype moving forward. We give recommendations of how developers, regulators, deployers and the public can navigate the relationship between AI hype, innovation, investment and scientific exploration, while addressing critical societal and environmental challenges.</jats:p>"
10.1007/s43681-024-00464-z,Talking existential risk into being: a Habermasian critical discourse perspective to AI hype,"<jats:title>Abstract</jats:title><jats:p>Recent developments in Artificial Intelligence (AI) have resulted in a hype around both opportunities and risks of these technologies. In this discussion, one argument in particular has gained increasing visibility and influence in various forums and positions of power, ranging from public to private sector organisations. It suggests that Artificial General Intelligence (AGI) that surpasses human intelligence is possible, if not inevitable, and which can—if not controlled—lead to human extinction (Existential Threat Argument, ETA). Using Jürgen Habermas’s theory of communicative action and the validity claims of truth, truthfulness and rightness therein, we inspect the validity of this argument and its following ethical and societal implications. Our analysis shows that the ETA is problematic in terms of scientific validity, truthfulness, as well as normative validity. This risks directing AI development towards a strategic game driven by economic interests of the few rather than ethical AI that is good for all.</jats:p>"
10.69554/eewn6637,Architecting AI will improve AI ethics,N/A
10.1007/s43681-022-00196-y,“Ethically contentious aspects of artificial intelligence surveillance: a social science perspective”,N/A
10.1007/s43681-023-00338-w,"The role of ChatGPT in disrupting concepts, changing values, and challenging ethical norms: a qualitative study",N/A
10.1007/s43681-024-00433-6,Unveiling the ethical positions of conversational AIs: a study on OpenAI’s ChatGPT and Google’s Bard,"<jats:title>Abstract</jats:title><jats:p>In an era where conversational AIs (CAIs) like OpenAI’s ChatGPT and Google's Bard are becoming integral to daily life, understanding their ethical positions is paramount. This research delves into the expressed moral values of these CAIs, exploring how their pre-training influences their ethical stances. The study aims to assess the articulated ethical positions of ChatGPT and Bard, uncovering whether these systems align with particular moral values. By understanding their ethical positions, the research seeks to provide insights into how these CAIs might respond to prompts and guide users in their selection and utilization. Utilizing O’Boyle and Forsyth’s Ethical Position Questionnaire (EPQ-5), the research evaluated the CAIs’ levels of idealism and relativism. The study also involved a third CAI, Anthropic’s Claude and an online human panel, to analyze the reasoning behind the responses, providing a more nuanced understanding of the ethical positions. The initial findings revealed that ChatGPT aligns more with an ‘absolutist’ position, endorsing strict adherence to moral principles, while Bard leans towards a ‘situationist’ stance, valuing flexibility and situational considerations. However, further analysis by Claude and humans suggested a more complex categorization, with ChatGPT fitting the 'exceptionist' categorization and Bard aligning with ‘absolutism.’ The research underscores the significance of recognizing the trained-in ethical positions of CAIs, as they are not neutral but reflect particular ethical leanings. Understanding these positions is vital for interpreting CAI outputs and using these systems effectively and ethically. The study calls for further exploration into how these ethical positions might influence real-world applications of CAIs.</jats:p>"
10.1145/3362077.3362087,The intersection of ethics and AI,"<jats:p>Artificial intelligence is a rapidly advancing field with the potential to revolutionize health care, transportation, and national security. Although the technology has been ubiquitous in every day society for a while, the advent of self-driving cars and smart home devices have propelled a discussion of the associated ethical risks and responsibilities. Since the usage of AI can have significant impacts on people, it is essential to establish a set of ethical values to follow when developing and deploying AI.</jats:p>"
10.3030/101145339,N/A,N/A
10.51219/urforum.2023.sebastian-obeta,Beyond Creation: Unravelling the Complex Tapestry of Generative AI in the Big Data Landscape,N/A
10.2139/ssrn.4914938,Towards an Independent EU Regulator for Copyright Issues of Generative AI: What Role for the AI Office (But More Importantly: What's Next)?,N/A
10.1007/979-8-8688-0456-4_6,The Future of AGI,N/A
10.1109/itc-egypt61547.2024.10620598,AI-Driven Testing: Unleashing Autonomous Systems for Superior Software Quality Using Generative AI,N/A
10.2139/ssrn.4918354,Teaching Mathematical Concepts in Management with Generative AI: The Power of Human Oversight in AI-Driven Learning,N/A
10.1007/s43681-023-00397-z,AITA: AI trustworthiness assessment,N/A
10.1007/s43681-023-00404-3,Establishing counterpoints in the sonic framing of AI narratives,"<jats:title>Abstract</jats:title><jats:p>In order to challenge dominant representations and conceptions of artificial intelligence (AI), this article explores how AI is sonically represented in documentaries. Using a corpus of documentaries alongside expert interviews with sound designers, we explore the ways in which music and sound may influence perception about AI. The notion of ‘counterpoint’ in music theory is developed as a concept to capture and explain how the integrated dynamics of human/machines are represented within these sonic framings. The concept of the counterpoint allows us to reflect on how the relations between AI and the human and how they are sonically framed in ways that separate and blend without recourse to reductive or binary futures, which potentially misrepresent AI capabilities and performance. The article identifies and develops four types of counterpoint in what we refer to as AI sonic narratives. This article provides a framework from which AI could be sonically framed responsibly, which is critical when misinformation and hype impede the public understanding of science.</jats:p>"
10.1007/s43681-022-00210-3,Artificial intelligence and the model of rules: better than us?,N/A
10.1007/s43681-024-00511-9,Robots and reactive attitudes: a defense of the moral and interpersonal status of non-conscious agents,N/A
10.1007/978-3-031-23035-6_7,AI Ethics Education for Future African Leaders,"<jats:title>Abstract</jats:title><jats:p>From the Greek word “ethos”, which means custom, habit or character, the word ethics can mean and has been defined in many different ways by ethics and morality theorists.</jats:p>"
10.1007/s43681-021-00046-3,Correction to: AI auditing and impact assessment: according to the UK information commissioner’s office,N/A
10.1007/s43681-024-00500-y,How to gain control and influence algorithms: contesting AI to find relevant reasons,"<jats:title>Abstract</jats:title><jats:p>Relevancy is a prevalent term in value alignment. We either need to keep track of the relevant moral reasons, we need to embed the relevant values, or we need to learn from the relevant behaviour. What relevancy entails in particular cases, however, is often ill-defined. The reasons for this are obvious, it is hard to define relevancy in a way that is both general and concrete enough to give direction towards a specific implementation. In this paper, we describe the inherent difficulty that comes along with defining what is relevant to a particular situation. Simply due to design and the way an AI system functions, we need to state or learn particular goals and circumstances under which that goal is completed. However, because of both the changing nature of the world and the varied wielders and users of such implements, misalignment occurs, especially after a longer amount of time. We propose a way to counteract this by putting contestability front and centre throughout the lifecycle of an AI system, as it can provide insight into what is actually relevant at a particular instance. This allows designers to update the applications in such a manner that they can account for oversight during design.</jats:p>"
10.1007/s43681-021-00121-9,On the risk of confusing interpretability with explicability,"<jats:title>Abstract</jats:title><jats:p>﻿This Comment explores the implications of a lack of tools that facilitate an explicable utilization of epistemologically richer, but also more involved white-box approaches in AI. In contrast, advances in explainable artificial intelligence for black-box approaches have led to the availability of semi-standardized and attractive toolchains that offer a seemingly competitive edge over inherently interpretable white-box models in terms of intelligibility towards users. Consequently, there is a need for research on efficient tools for rendering interpretable white-box approaches in AI explicable to facilitate responsible use.</jats:p>"
10.1007/s43681-023-00281-w,Merging public health and automated approaches to address online hate speech,N/A
10.5334/tilr.297,"Learning from the Ethics of AI &amp;ndash; A Research Proposal on Soft Law and
            Ethics of AI","<jats:p>This contribution outlines a research proposal combining ethical guidelines on AI
            and a law-as-data approach. Building upon the definitions of soft law discussed in legal
            scholarship, it proposes a way of structuring the regulatory landscape on AI and of
            addressing the question of what is included in the “soft law of AI” today. By adopting a
            building-blocks approach (combining distinct definitional components of soft law), the
            paper shows that the state of current soft law on AI depends on which position on
            international law one defends. Concretely, the paper firstly offers a complete codebook
            for identifying the different types of soft law. Secondly, it applies this codebook as a
            proof-of-concept for the research proposal by analyzing 40+ ethical guidelines and by
            clustering preliminary results according to the actor enacting the guidelines and the
            legally relevant effects they could deploy. Four paradigmatic types of soft law emerge:
            statist and international organization soft law, process-oriented soft law,
            expertise-oriented soft law, and de facto relevant standards soft law. These results
            illustrate the contributions which are to be expected from a law-as-data research
            proposal.</jats:p>"
10.14296/ac.v5i1.5663,Copyright Protection for AI-Generated Works,"<jats:p>Since the 2010s, artificial intelligence (AI) has quickly grown from another subset of machine learning (ie deep learning) in particular with recent advances in generative AI, such as ChatGPT. The use of generative AI has gone beyond leisure purposes. It has now been widely used to generate music, news articles and image-based art works. This prompts a regulatory interpretation as to how AI-generated works should be appropriately used to eliminate their potential harm to society, but at the same time how it should be protected to foster human creativity and promote a well-functioning market.
This article is an update from the author’s evidential report and speech on “AI and Intellectual Property Rights: IPR Protection for AI-Created Work” for the evidence meeting of the All-Party Parliamentary Group on Artificial Intelligence on 24 January 2022. It considers whether AI technologies should be granted status as copyright or patent owners by looking into existing regulations in the United Kingdom, European Union, United States and China. It further considers how generative AI copyright protection should be managed in the digital society to protect users and strike a fair balance among rightsholders. It argues that it would be beneficial to a well-functioning market if AI-generated works could be subject to collective management of copyright via copyright management organizations within countries. In addition, the article provides mapping of existing legislations in a comparative study and their interpretation for the application of AI-generated works protection and aims to bring together global policymakers and stakeholders to initiate joint efforts to promote international harmonization on intellectual property rights (IPR) protection for AI-generated works.
Keywords: artificial intelligence; generative AI; AI-generated works; collective copyright management; computer-generated work; copyright protection.</jats:p>"
10.1101/2023.05.12.23289878,Dissection of medical AI reasoning processes via physician and generative-AI collaboration,"<jats:title>Abstract</jats:title><jats:p>Despite the proliferation and clinical deployment of artificial intelligence (AI)-based medical software devices, most remain black boxes that are uninterpretable to key stakeholders including patients, physicians, and even the developers of the devices. Here, we present a general model auditing framework that combines insights from medical experts with a highly expressive form of explainable AI that leverages generative models, to understand the reasoning processes of AI devices. We then apply this framework to generate the first thorough, medically interpretable picture of the reasoning processes of machine-learning–based medical image AI. In our synergistic framework, a generative model first renders “counterfactual” medical images, which in essence visually represent the reasoning process of a medical AI device, and then physicians translate these counterfactual images to medically meaningful features. As our use case, we audit five high-profile AI devices in dermatology, an area of particular interest since dermatology AI devices are beginning to achieve deployment globally. We reveal how dermatology AI devices rely both on features used by human dermatologists, such as lesional pigmentation patterns, as well as multiple, previously unreported, potentially undesirable features, such as background skin texture and image color balance. Our study also sets a precedent for the rigorous application of explainable AI to understand AI in any specialized domain and provides a means for practitioners, clinicians, and regulators to uncloak AI’s powerful but previously enigmatic reasoning processes in a medically understandable way.</jats:p>"
10.1007/s43681-022-00198-w,Being at home in the metaverse? Prospectus for a social imaginary,N/A
10.1007/s43681-024-00434-5,Prospectives and drawbacks of ChatGPT in healthcare and clinical medicine,N/A
10.1007/s43681-021-00072-1,The future of online trust (and why Deepfake is advancing it),N/A
10.69554/xqrw7390,Enabling generative AI through use cases in a big enterprise,"<jats:p xml:lang=""en"">In the emerging field of generative artificial intelligence (GenAI), we possess the potential to significantly enhance our business operations and processes. Achieving this goal within a large corporation like Nestlé is challenging, however, given the immature stage of this technology. This paper outlines the approach to implementing GenAI at Nestlé, guided by the most influential use cases. It also underscores the importance of scaling people’s capabilities and establishing legal, ethical and compliance frameworks to support the deployment of this technology.</jats:p>"
10.1007/s00146-024-02007-w,Intentionality gap and preter-intentionality in generative artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>The emergence of generative artificial intelligence, such as large language models and text-to-image models, has had a profound impact on society. The ability of these systems to simulate human capabilities such as text writing and image creation is radically redefining a wide range of practices, from artistic production to education. While there is no doubt that these innovations are beneficial to our lives, the pervasiveness of these technologies should not be underestimated, and raising increasingly pressing ethical questions that require a radical resemantization of certain notions traditionally ascribed to humans alone. Among these notions, that of technological intentionality plays a central role. With regard to this notion, this paper first aims to highlight what we propose to define in terms of the intentionality gap, whereby, insofar as, currently, (1) it is increasingly difficult to assign responsibility for the actions performed by AI systems to humans, as these systems are increasingly autonomous, and (2) it is increasingly complex to reconstruct the reasoning behind the results they produce as we move away from good old fashioned AI; it is now even more difficult to trace the intentionality of AI systems back to the intentions of the developers and end users. This gap between human and technological intentionality requires a revision of the concept of intentionality; to this end, we propose here to assign preter-intentional behavior to generative AI. We use this term to highlight how AI intentionality both incorporates and transcends human intentionality; i.e., it goes beyond (preter) human intentionality while being linked to it. To show the merits of this notion, we first rule out the possibility that such preter-intentionality is merely an unintended consequence and then explore its nature by comparing it with some paradigmatic notions of technological intentionality present in the wider debate on the moral (and technological) status of AI.</jats:p>"
10.3389/frai.2023.1259407,The rise of generative AI and enculturating AI writing in postsecondary education,N/A
10.3389/fcomm.2023.1243474,Front-end AI vs. Back-end AI: new framework for securing truth in communication during the generative AI era,"<jats:p>The proliferation of artificial intelligence (AI) in digital platforms has complicated the concept of truth in communication studies. The article presents the dichotomic framework of Front-end AI and Back-end AI to tackle the complexity of distinguishing truth. Front-end AI refers to AI technology used up-front, often as the face of a product or service, challenging the authenticity and truthfulness of content. In contrast, Back-end AI refers to AI technology used behind the scenes, which can generate misleading or biased content without disclosing its AI-generated nature. Addressing these challenges requires different approaches, such as verification and ethical guidelines for Front-end AI and algorithmic transparency, bias detection, and human oversight for Back-end AI.</jats:p>"
10.1007/s43681-024-00532-4,Evaluating computational models of ethics for autonomous decision making,N/A
10.24135/pjtel.v6i1.176,AI in the wild,"<jats:p>It has been well over a year since ChatGPT emerged and brought with it much commentary about challenges and opportunities for education. There has been considerable discussion about risks to academic integrity and the possibilities of generative AI for enhancing learning and teaching. As the dust settles, the hard work of determining how exactly generative AI will integrate into higher education begins. In this session, we will explore the current state of generative AI in student learning. While the integration of generative AI into formal coursework has been inconsistent, to say the least, many students are using these tools extensively as part of their studies. Drawing on in-depth interviews with 50 students across disciplines, a set of hypotheses about the impact of generative AI on student learning practices will be presented. A key component of the impact of these emerging technologies appears to be how familiar and confident students are in their understanding of their own learning. The implications of these findings will also be discussed.Jason Lodge is Associate Professor of Educational Psychology and Director of the Learning, Instruction, and Technology Lab in the School of Education and is a Deputy Associate Dean (Academic) in the Faculty of Humanities, Arts and Social Sciences at The University of Queensland. Jason’s research with his lab focuses on the cognitive, metacognitive, and emotional mechanisms of learning, primarily in post-secondary settings and in digital learning environments. He currently serves as Lead Editor of Australasian Journal of Educational Technology and Editor of Student Success.</jats:p>"
10.21203/rs.3.rs-4515034/v1,Generative AI: A Case Study of ChatGPT’s Impact on University Students’ Learning Practices,"<title>Abstract</title>
        <p>Recently, technology has been widely integrated across the educational landscape. Artificial Intelligence (AI) tools have become essential components of students' learning practices, requiring an examination of the impact of each tool. The aim of this study is to investigate the impact of ChatGPT tool on university students’ learning practices. A quantitative online survey was adopted using cross-sectional design to collect the data from university students at King Saud university and Imam Abdulrahman Bin Faisal university in Saudi Arabia. A total of 402 responses were finalised for data analysis at the end of five weeks after starting the survey. Out of 402 students, 293 have been using ChatGPT. ChatGPT services were mainly used for writing research papers (81.8%), essays (73.8%), and correcting grammar (43.3%). Positive effects included motivating, engaging, and improving skills and competencies of students and negative effects included academic dishonesty, limiting critical thinking and problem-solving skills among students. Significant differences were identified among male and female students on perceptions about motivating and engaging ability of ChatGPT (p &lt; .05).Students should be trained to use ChatGPT ethically and universities should adopt alternative assessment practices.</p>"
10.1515/9783839474723-fm,Frontmatter,N/A
10.2139/ssrn.4769448,Rule 11 Is No Match for Generative AI,N/A
10.36591/se-d-4602-13,Generative AI: The Promise and Peril for Scientific Publishing,N/A
10.2139/ssrn.4915317,Does Generative Ai Copy?——Rethinking the Right to Copy Under Copyright Law,N/A
10.2139/ssrn.4616977,The Market Value of Generative Ai: Evidence from China Market,N/A
10.18260/1-2--45532,Generative AI as an educational resource,N/A
10.1016/b978-0-44-321857-6.00011-4,"Synthetic data, interpretable regression, and submodels",N/A
