DOI,Title,Abstract
10.1007/s43681-024-00521-7,Ethics and the use of generative AI in professional editing,"<jats:title>Abstract</jats:title><jats:p>Generative artificial intelligence (GnAI) has garnered significant attention worldwide across diverse industries, including in book publishing. To date, more attention has been paid to its potential in creative collaboration and less to the editorial possibilities of its application. Interest has accelerated since the breakthrough of a new Large Language Model in late 2022. This paper engages with the ethical and industrial implications of using GnAI in a creative context, namely literary publishing. It raises crucial questions about intellectual property, trust, the author–editor relationship and publishing professionals’ evolving roles in shaping quality literature. Using a published story as a test case, we compare edits using GnAI with those by professional editors over multiple drafts and at different stages of editorial development. We consider the potential ethical implications of the use of GnAI in literary fiction editing, highlighting the principles and practices that underpin professional editing to consider how these may or may not translate in the use of GnAI. This is followed by a discussion of the risks and opportunities in using GnAI in editing literary texts in the trade publishing context.</jats:p>"
10.1007/s43681-023-00315-3,Does the sun rise for ChatGPT? Scientific discovery in the age of generative AI,N/A
10.2139/ssrn.4784555,The Ethics of Generative Ai in Social Science Research: A Qualitative Approach for Community-Based Ai Research Ethics,N/A
10.1007/s43681-024-00519-1,A powerful potion for a potent problem: transformative justice for generative AI in healthcare,"<jats:title>Abstract</jats:title><jats:p>Generative Artificial Intelligence (AI), as a transformative technology, holds significant promise for applications in healthcare. At the same time, the datafication, AI integration, and commodification of health have opened the floodgates for ethical issues, including those related to fairness, access, beneficence, democracy, solidarity, inclusion, and societal harms. As further the digitalization, innovation, and disruption of healthcare is inevitable, the paper maps out how power, equity, access, identity, participation, and knowledge contribute to creating social injustice issues. It also discusses that current justice approaches—distributive justice, representational justice, restorative justice, and capabilities-centered justice—do not have enough impact to prevent or remedy the many harms and injustices that AI has already created in healthcare or will continue to do so. The paper proposes that a transformative justice approach is needed for generative AI as a transformative technology, focused on (1) peace, emancipation, and eliminating the root causes of injustice, (2) holistic conflict resolution, (3) human rights-based approaches, and (4) the empowerment of agency and actors.</jats:p>"
10.1201/9781032654829-2,"Generative Artificial Intelligence: Introduction, Application, Trends, and Ethics",N/A
10.4018/979-8-3693-8557-9.ch009,Generative AI for Cybersecurity,"<jats:p>The intersection of cybersecurity and generative artificial intelligence (AI) has become a crucial frontier as the digital landscape changes. By examining the interaction between AI-powered attacks and defence mechanisms and concentrating on applications like anomaly detection, synthetic data generation, automated incident response, and forensics, the chapter examines the potential of generative artificial intelligence (AI) in redefining conventional cybersecurity paradigms. In order to reduce the hazards associated with deepfakes and synthetic media, the chapter discusses the examination of adversarial machine learning techniques and strategies. Along with offering guidance on incorporating AI into security operations, encouraging human-AI cooperation, and building strong AI skills, it also discusses the ethical ramifications of AI-driven security procedures. It also serves as a comprehensive guide for security professionals, researchers, and decision-makers, offering a holistic understanding of the synergies between AI and cybersecurity.</jats:p>"
10.1007/s43681-023-00405-2,Ethical considerations and policy interventions concerning the impact of generative AI tools in the economy and in society,N/A
10.2139/ssrn.4735389,The Legal Ethics of Generative AI,N/A
10.2139/ssrn.4703377,The Ethics of Generative AI in Social-Scientific Research: A Qualitative Approach for Community-Based AI Ethics,N/A
10.1007/s43681-024-00439-0,Generative AI can fabricate advanced scientific visualizations: ethical implications and strategic mitigation framework,N/A
10.4018/979-8-3693-8557-9.ch002,Generative AI,"<jats:p>For nearly 50 years, artificial intelligence (AI) has been a technology and a field of study. Its advancement had been conflicting and differed after some time. Due to the development of intriguing AI applications like Alpha Go and Alpha Fold, large data sets, advancements in the internet and semiconductors, the most significant development in machine learning and deep learning occurred in the 2000s. Generative AI models have recently gained a lot of popularity. AI has become a household name thanks to massive language models like GPT (generative pre-trained transformer) and tools like Midjourney and ChatGPT. Microsoft and Google have been incorporating these models into their enterprise and search offerings as generative AI has had a significant impact on everyday life. The part dug into the potential outcomes and difficulties related to the condition of generative computer-based intelligence around then and investigated its future. Additionally, it offered recommendations for generative AI-using businesses.</jats:p>"
10.1007/s43681-024-00445-2,Addressing diversity in hiring procedures: a generative adversarial network approach,N/A
10.37307/j.2196-9817.2023.05.09,Code of Ethics for “Empathetic” Generative AI,N/A
10.1007/s43681-024-00443-4,AI hype as a cyber security risk: the moral responsibility of implementing generative AI in business,"<jats:title>Abstract</jats:title><jats:p>This paper examines the ethical obligations companies have when implementing generative Artificial Intelligence (AI). We point to the potential cyber security risks companies are exposed to when rushing to adopt generative AI solutions or buying into “AI hype”. While the benefits of implementing generative AI solutions for business have been widely touted, the inherent risks associated have been less well publicised. There are growing concerns that the race to integrate generative AI is not being accompanied by adequate safety measures. The rush to buy into the hype of generative AI and not fall behind the competition is potentially exposing companies to broad and possibly catastrophic cyber-attacks or breaches. In this paper, we outline significant cyber security threats generative AI models pose, including potential ‘backdoors’ in AI models that could compromise user data or the risk of ‘poisoned’ AI models producing false results. In light of these the cyber security concerns, we discuss the moral obligations of implementing generative AI into business by considering the ethical principles of beneficence, non-maleficence, autonomy, justice, and explicability. We identify two examples of ethical concern, <jats:italic>overreliance</jats:italic> and <jats:italic>over-trust</jats:italic> in generative AI, both of which can negatively influence business decisions, leaving companies vulnerable to cyber security threats. This paper concludes by recommending a set of checklists for ethical implementation of generative AI in business environment to minimise cyber security risk based on the discussed moral responsibilities and ethical concern.</jats:p>"
10.55092/let20240002,Generative AI in finance: risks and potential solutions,N/A
10.4018/979-8-3693-8557-9.ch011,Innovations in Cloud Storage,"<jats:p>As the demand for efficient and scalable data storage solutions continues to rise, the integration of artificial intelligence (AI) technologies with cloud storage systems has emerged as a promising avenue for innovation. This chapter explores the intersection of AI and cloud storage, investigating how AI-driven approaches are revolutionizing traditional cloud storage practices and unlocking new opportunities for enhanced performance and functionality. Through a comprehensive review of literature and case studies, this study examines the role of AI in optimizing data management, improving security measures, and enabling intelligent data processing within cloud storage environments. Furthermore, this chapter delves into the implications of AI-driven innovation for businesses and organizations, highlighting the potential benefits of increased efficiency, cost savings, and competitive advantage.</jats:p>"
10.1007/s43681-024-00546-y,Generative AI can effectively manipulate data,N/A
10.1007/s43681-024-00497-4,Anticipating impacts: using large-scale scenario-writing to explore diverse implications of generative AI in the news environment,"<jats:title>Abstract</jats:title><jats:p>The tremendous rise of generative AI has reached every part of society—including the news environment. There are many concerns about the individual and societal impact of the increasing use of generative AI, including issues such as disinformation and misinformation, discrimination, and the promotion of social tensions. However, research on anticipating the impact of generative AI is still in its infancy and mostly limited to the views of technology developers and/or researchers. In this paper, we aim to broaden the perspective and capture the expectations of three stakeholder groups (news consumers; technology developers; content creators) about the potential negative impacts of generative AI, as well as mitigation strategies to address these. Methodologically, we apply scenario-writing and use participatory foresight in the context of a survey (n = 119) to delve into cognitively diverse imaginations of the future. We qualitatively analyze the scenarios using thematic analysis to systematically map potential impacts of generative AI on the news environment, potential mitigation strategies, and the role of stakeholders in causing and mitigating these impacts. In addition, we measure respondents' opinions on a specific mitigation strategy, namely transparency obligations as suggested in Article 52 of the draft EU AI Act. We compare the results across different stakeholder groups and elaborate on different expected impacts across these groups. We conclude by discussing the usefulness of scenario-writing and participatory foresight as a toolbox for generative AI impact assessment.</jats:p>"
10.1136/jme-2023-108909,Ethics of generative AI,N/A
10.2139/ssrn.4715603,"Performance, Skills, Ethics, Generative AI Adoption, and the Philippines",N/A
10.2139/ssrn.4737492,"Generative AI for Emerging Researchers: The Promises, Ethics, and Risks",N/A
10.2139/ssrn.4413206,ChatGPT and Generative AI Systems as Military Ethics Advisors,N/A
10.36315/2024v2end018,Ethics of generative AI use in higher education: A focus group study,N/A
10.4018/979-8-3693-8557-9.ch004,Generative AI,"<jats:p>This study provides a comprehensive view of the state of generative AI today, touching on its uses, foundational models, obstacles, prospects, and potential future courses of action. Autoregressive models like Transformers, GANs, and Variational Autoencoders (VAEs) are the backbone of generative AI. Generated AI still has a way to go before fully realizing its potential. Problems with model interpretability, training stability, and generated content bias are all examples of such challenges. Computer scientists, psychologists, and ethicists must work together to find solutions to these problems. Generative AI does, however, offer tremendous potential. Artists, designers, and storytellers have new tools at their fingertips. Improving the robustness of models, granting greater control over generated outputs, and investigating uses in interactive storytelling and real-time content production are all potential future areas for generative AI.</jats:p>"
10.4018/979-8-3693-8557-9.ch006,Ethical Considerations for Generative AI in Social Science Research,"<jats:p>Social science research embodies the inquiry into people as individuals and their interpersonal interactions with each other in communities and varied societies, with due consideration for their natural, technological, and constructed environments. Due to (a) the nature and composition of qualitative, quantitative, and mixed methods research designs coupled with (b) the apparent expectations of responsible behavior from researchers (human beings), room exists for research misconduct or unethical practices. The prevalence and acceptance of generative artificial intelligence (AI) technology such as ChatGPT propagate at a hyper-accelerated pace based on its potential for ease of work in many sectors, including research, particularly academic research. Journal reviewers, editors, and publishers do not possess sufficient tools to differentiate between human-written and partially or wholly AI-authored manuscripts submitted for journal publication.</jats:p>"
10.2139/ssrn.4478397,Ethics of Generative AI and Manipulation: A Design-Oriented Research Agenda,N/A
10.4018/979-8-3693-8557-9.ch001,An Introduction to Generative AI,"<jats:p>The chapter serves as an intricate exploration into the domain of Generative AI, offering a comprehensive understanding of its fundamental principles, diverse methodologies, and wide-ranging applications. Beginning with a clear definition and overview, it progresses to clarify key concepts such as probability distributions and sampling while providing an overview of various generative models, including autoregressive models, variational autoencoders, and generative adversarial networks. Architectural components, training methods, and optimization techniques are thoroughly examined, alongside an in-depth analysis of challenges and considerations in model design and selection. Through real-world examples, the chapter showcases the transformative potential of Generative AI across domains such as image generation, text processing, music composition, creative arts, etc. Addressing current limitations and outlining future directions offers valuable insights into the growing landscape of Generative AI, positioning it as a catalyst for innovation and creative expression.</jats:p>"
10.1007/s10676-024-09745-x,Ethics of generative AI and manipulation: a design-oriented research agenda,"<jats:title>Abstract</jats:title><jats:p>Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies.</jats:p>"
10.1007/s43681-024-00440-7,Engaging the many-hands problem of generative-AI outputs: a framework for attributing credit,"<jats:title>Abstract</jats:title><jats:p>The recent wave of generative AI (GenAI) systems like Stable Diffusion or ChatGPT that can produce images, text and code from human prompts raises controversial issues about creatorship, originality, creativity and copyright. This paper focuses on creatorship: who creates and should be credited with the outputs made with the help of GenAI? There is currently significant moral, legal and regulatory uncertainty around these questions. We develop a novel framework, called CCC (collective-centered creation), that helps resolve this uncertainty. According to CCC, GenAI outputs are created by collectives in the first instance. Claims to creatorship come in degrees and depend on the nature and significance of individual contributions made by the various agents and entities involved, including users, GenAI systems, developers, producers of training data and others. We demonstrate how CCC can help navigate a range of ongoing controversies around the responsible development and deployment of GenAI technologies and help more accurately attribute credit where it is due.</jats:p>"
10.1007/s40889-023-00179-5,Why we should (not) worry about generative AI in medical ethics teaching,N/A
10.1145/3600211.3607545,The Generative AI Deployment Rush: How to Democratize the Politics of Pace,N/A
10.1007/s43681-024-00451-4,Safeguarding human values: rethinking US law for generative AI’s societal impacts,"<jats:title>Abstract</jats:title><jats:p>Our interdisciplinary study examines the effectiveness of US law in addressing the complex challenges posed by generative AI systems to fundamental human values, including physical and mental well-being, privacy, autonomy, diversity, and equity. Through the analysis of diverse hypothetical scenarios developed in collaboration with experts, we identified significant shortcomings and ambiguities within the existing legal protections. Constitutional and civil rights law currently struggles to hold AI companies responsible for AI-assisted discriminatory outputs. Moreover, even without considering the liability shield provided by Section 230, existing liability laws may not effectively remedy unintentional and intangible harms caused by AI systems. Demonstrating causal links for liability claims such as defamation or product liability proves exceptionally difficult due to the intricate and opaque nature of these systems. To effectively address these unique and evolving risks posed by generative AI, we propose a “Responsible AI Legal Framework”  that adapts to recognize new threats and utilizes a multi-pronged approach. This framework would enshrine fundamental values in legal frameworks, establish comprehensive safety guidelines, and implement liability models tailored to the complexities of human-AI interactions. By proactively mitigating unforeseen harms like mental health impacts and privacy breaches, this framework aims to create a legal landscape capable of navigating the exciting yet precarious future brought forth by generative AI technologies.</jats:p>"
10.1007/978-1-4842-9579-3_5,Prototyping with Generative AI,N/A
10.1007/978-1-4842-9579-3_3,Generative AI with Personalities,N/A
10.2139/ssrn.4735776,Between Copyright and Computer Science: The Law and Ethics of Generative AI,N/A
10.1109/mitp.2023.3267140,What If Ethics Got in the Way of Generative AI?,N/A
10.1145/3600211.3604744,The ELIZA Defect,N/A
10.1007/978-1-4842-9579-3_11,Dilemmas Interacting with Generative AI,N/A
10.1007/978-1-4842-9579-3_7,Generative AI Form and Composition,N/A
10.1007/978-3-031-46238-2_25,The Economics of Generative AI,N/A
10.1136/jme-2023-109834,Generative AI and medical ethics: the state of play,N/A
10.1515/9783839467664-015,"Science in the era of ChatGPT, large language models and generative AI",N/A
10.1007/978-1-4842-9367-6_3,AI Fundamentals,N/A
10.14361/9783839467664-015,"Science in the era of ChatGPT, large language models and generative AI",N/A
10.1787/bf3c0c60-en,G7 Hiroshima Process on Generative Artificial Intelligence (AI),N/A
10.1007/978-1-4842-9367-6_2,Data,N/A
10.1007/979-8-8688-0403-8_1,Introduction to Generative AI,N/A
10.1057/s41599-024-03526-z,AI and ethics: Investigating the first policy responses of higher education institutions to the challenge of generative AI,"<jats:title>Abstract</jats:title><jats:p>This article addresses the ethical challenges posed by generative artificial intelligence (AI) tools in higher education and explores the first responses of universities to these challenges globally. Drawing on five key international documents from the UN, EU, and OECD, the study used content analysis to identify key ethical dimensions related to the use of generative AI in academia, such as accountability, human oversight, transparency, or inclusiveness. Empirical evidence was compiled from 30 leading universities ranked among the top 500 in the Shanghai Ranking list from May to July 2023, covering those institutions that already had publicly available responses to these dimensions in the form of policy documents or guidelines. The paper identifies the central ethical imperative that student assignments must reflect individual knowledge acquired during their education, with human individuals retaining moral and legal responsibility for AI-related wrongdoings. This top-down requirement aligns with a bottom-up approach, allowing instructors flexibility in determining how they utilize generative AI especially large language models in their own courses. Regarding human oversight, the typical response identified by the study involves a blend of preventive measures (e.g., course assessment modifications) and soft, dialogue-based sanctioning procedures. The challenge of transparency induced the good practice of clear communication of AI use in course syllabi in the first university responses examined by this study.</jats:p>"
10.4018/979-8-3693-2643-5.ch016,Bane and Boon of Hallucinations in the Context of Generative AI,"<jats:p>The phenomenon of hallucinations takes place when generative artificial intelligence systems, such as large language models (LLMs) like ChatGPT, generate outputs that are illogical, factually incorrect, or otherwise unreal. In generative artificial intelligence, hallucinations have the ability to unlock creative potential, but they also create challenges for producing accurate and trustworthy AI outputs. Both concerns will be covered in this abstract. Artificial intelligence hallucinations can be caused by a variety of factors. There is a possibility that the model will show an inaccurate response to novel situations or edge cases if the training data is insufficient, incomplete, or biassed. It is common for generative artificial intelligence to generate content in response to cues, regardless of the model's “understanding” or the quality of its output.</jats:p>"
10.1007/978-3-031-46238-2_2,Creating Ad Campaigns Using Generative AI,N/A
10.35542/osf.io/7dr9j,Critical Thinking and Ethics in the Age of Generative AI in Education,"<p>This report is an invitation for educators, policymakers, technologists, and learners to consider how generative AI can contribute to the future of education?. It aims to lay down a foundation upon which we can start building an educational ecosystem that is dynamic, inclusive, and profoundly human, despite being significantly aided by artificial intelligence?.</p>"
10.1007/s10916-023-01987-4,"Generative AI in Medical Imaging: Applications, Challenges, and Ethics",N/A
10.4018/979-8-3693-8557-9,"Generative AI and Implications for Ethics, Security, and Data Management",N/A
10.1007/s00146-024-01889-0,"Generative AI and human–robot interaction: implications and future agenda for business, society and ethics",N/A
10.1201/9781032654829-3,The Mechanics and Validation of Generative AI Outcomes,N/A
10.1016/j.applanim.2023.106107,Preserving ethics and integrity of scientific writing and reviewing after the advent of generative AI and AI-assisted technologies,N/A
10.1007/s43681-024-00552-0,"Overcoming AI ethics, towards AI realism",N/A
10.21428/e4baedd9.777b7123,Generative AI and the Future of Inequality,N/A
10.1007/978-1-4842-9367-6_5,Large Language Models,N/A
10.4018/979-8-3693-8557-9.ch007,Harmonizing Ethics With Artificial Intelligence,"<jats:p>Artificial intelligence (AI) significantly transforms human existence, yet ethical concerns in AI development remain unaddressed. This chapter highlights the critical connection between AI and ethics, emphasizing the need to align AI with human values. Despite ongoing ethical discussions, a comprehensive understanding of AI's ethical implications is lacking. This study employs a systematic literature review approach to provide practical tools for AI developers, enabling them to navigate ethical dimensions of their creations. By examining the societal impact of AI and offering a checklist for assessing ethical practicality, the study promotes ethically sound practices in AI development. The chapter explores ethical issues in AI deployment, identifying crucial societal concerns requiring ethical decision-making. Ultimately, it advocates for a compassionate approach to AI development, prioritizing ethical considerations to safeguard humanity's interests.</jats:p>"
10.1007/s43681-024-00570-y,"Correction: Overcoming AI ethics, towards AI realism",N/A
10.1007/978-1-4842-9367-6_6,Auto Code Generation,N/A
10.1007/978-1-4842-9367-6_4,Core Generative AI Technology,N/A
10.1007/978-3-031-46238-2_1,Generative AI as a Supportive Tool for Scientific Research,N/A
10.1007/978-3-031-46238-2_29,Generative AI for Fire Safety,N/A
10.4018/979-8-3693-8557-9.ch010,Guardians of the Digital Realm,"<jats:p>This chapter explores the burgeoning role of generative artificial intelligence (AI) in the realm of cybersecurity. As our digital world expands, so do the threats posed by malicious actors. In response, the emergence of generative AI technologies presents a promising avenue for bolstering cybersecurity defenses. This chapter examines the various applications of generative AI in fortifying digital security, including its use in threat detection, anomaly identification, and vulnerability assessment. By harnessing the power of machine learning and neural networks, generative AI systems exhibit remarkable capabilities in predicting, pre-empting, and mitigating cyber threats. Moreover, this chapter delves into the ethical considerations and potential challenges associated with deploying generative AI in cybersecurity contexts, emphasizing the importance of responsible development and deployment practices. Ultimately, this exploration highlights the pivotal role of generative AI as guardians of the digital realm, ushering in a new era of enhanced cybersecurity measures.</jats:p>"
10.36227/techrxiv.23968809.v1,"Synergizing Generative AI and Cybersecurity: Roles of Generative AI Entities, Companies, Agencies, and Government in Enhancing Cybersecurity","<jats:p>&lt;p&gt;The digital landscape of the modern world has witnessed a remarkable evolution over the past few decades, with technological advancements permeating every facet of our lives. While these innovations have brought forth unprecedented convenience and connectivity, they have also exposed society to new vulnerabilities. Cybercrimes have surged in both frequency and sophistication, punctuating the digital era with high-profile incidents that have shaken industries and nations. Recent history serves as a stark reminder of the potential havoc that cybercriminals can unleash upon critical infrastructure, as exemplified by the notorious Colonial Pipeline breach, where a malevolent actor manipulated digital systems to demand ransom in the form of cryptocurrency.&lt;/p&gt;
&lt;p&gt;Traditionally, cybercriminal endeavors necessitated a specialized skill set and an in-depth understanding of intricate technological nuances. However, the landscape has transformed dramatically with the emergence of Generative Artificial Intelligence (AI). Previously the domain of highly specialized engineers, the tools required to orchestrate cybercrimes have become increasingly accessible due to the proliferation of advanced AI models such as ChatGPT and other modern Large Language Models (LLMs). These AI-driven capabilities have lowered the entry barrier for potential wrongdoers, enabling individuals with even basic technical aptitude to partake in cybercriminal activities.&lt;/p&gt;
&lt;p&gt;A cursory glance at contemporary news headlines underscores the growing ubiquity of cybercrimes. A relentless surge in cyberattacks serves as an alarming indication of the escalating threat posed by malicious actors in the digital realm. As each year unfolds, instances of cybercrime proliferate, impacting individuals, corporations, and governments alike. This trend signals a pressing need to comprehend the intersection between Generative AI and cybersecurity – a convergence that holds the potential to reshape the dynamics of digital malfeasance and defense.&lt;/p&gt;
&lt;p&gt;This research paper embarks on a journey to explore the intricate relationship between Generative AI and cybersecurity. Delving into the realm of AI-driven creativity and manipulation, we examine how the advent of Generative AI technologies has facilitated a paradigm shift in the landscape of cyber threats. As we navigate through this exploration, we unravel the challenges and opportunities that arise from this dynamic interplay. By delving into case studies, examining emerging trends, and scrutinizing potential countermeasures, this paper aims to shed light on the novel dimensions of cybersecurity in the era of Generative AI. Through a comprehensive analysis, we aim to equip readers with an informed understanding of the evolving cybersecurity landscape and the critical role that Generative AI plays therein.&lt;/p&gt;</jats:p>"
10.1007/978-3-031-46238-2_8,Generative AI Use in the Construction Industry,N/A
10.1145/3589335.3641295,"AI Driven Online Advertising: Market Design, Generative AI, and Ethics",N/A
10.1007/979-8-8688-0403-8_4,Bridging Text and Audio in Generative AI,N/A
10.36227/techrxiv.23968809,"Synergizing Generative AI and Cybersecurity: Roles of Generative AI Entities, Companies, Agencies, and Government in Enhancing Cybersecurity","<jats:p>&lt;p&gt;The digital landscape of the modern world has witnessed a remarkable evolution over the past few decades, with technological advancements permeating every facet of our lives. While these innovations have brought forth unprecedented convenience and connectivity, they have also exposed society to new vulnerabilities. Cybercrimes have surged in both frequency and sophistication, punctuating the digital era with high-profile incidents that have shaken industries and nations. Recent history serves as a stark reminder of the potential havoc that cybercriminals can unleash upon critical infrastructure, as exemplified by the notorious Colonial Pipeline breach, where a malevolent actor manipulated digital systems to demand ransom in the form of cryptocurrency.&lt;/p&gt;
&lt;p&gt;Traditionally, cybercriminal endeavors necessitated a specialized skill set and an in-depth understanding of intricate technological nuances. However, the landscape has transformed dramatically with the emergence of Generative Artificial Intelligence (AI). Previously the domain of highly specialized engineers, the tools required to orchestrate cybercrimes have become increasingly accessible due to the proliferation of advanced AI models such as ChatGPT and other modern Large Language Models (LLMs). These AI-driven capabilities have lowered the entry barrier for potential wrongdoers, enabling individuals with even basic technical aptitude to partake in cybercriminal activities.&lt;/p&gt;
&lt;p&gt;A cursory glance at contemporary news headlines underscores the growing ubiquity of cybercrimes. A relentless surge in cyberattacks serves as an alarming indication of the escalating threat posed by malicious actors in the digital realm. As each year unfolds, instances of cybercrime proliferate, impacting individuals, corporations, and governments alike. This trend signals a pressing need to comprehend the intersection between Generative AI and cybersecurity – a convergence that holds the potential to reshape the dynamics of digital malfeasance and defense.&lt;/p&gt;
&lt;p&gt;This research paper embarks on a journey to explore the intricate relationship between Generative AI and cybersecurity. Delving into the realm of AI-driven creativity and manipulation, we examine how the advent of Generative AI technologies has facilitated a paradigm shift in the landscape of cyber threats. As we navigate through this exploration, we unravel the challenges and opportunities that arise from this dynamic interplay. By delving into case studies, examining emerging trends, and scrutinizing potential countermeasures, this paper aims to shed light on the novel dimensions of cybersecurity in the era of Generative AI. Through a comprehensive analysis, we aim to equip readers with an informed understanding of the evolving cybersecurity landscape and the critical role that Generative AI plays therein.&lt;/p&gt;</jats:p>"
10.1201/9781003503781-3,"Introduction to Generative AI, Natural Language Processing, and the Digital Person",N/A
10.21428/e4baedd9.cf3e35e5,"Generative AI and Creative Learning: Concerns, Opportunities, and Choices",N/A
10.1007/978-3-031-46238-2_15,Generative AI to Understand Complex Ecological Interactions,N/A
10.4018/979-8-3693-8557-9.ch008,AI-Driven Territorial Intelligence,"<jats:p>Artificial intelligence (AI) and territorial intelligence (TI) are currently two key fields. AI enables the development of systems capable of performing tasks that generally require human intelligence, such as pattern recognition, classification, and prediction. Business intelligence applied to territories is at the heart of research into sustainable territorial development, as part of the paradigm of intelligent territories. So how can digital technology be used to develop territorial intelligence and help improve the digital sovereignty of regions? The study traces the many challenges posed by the application of AI and IT in real-life scenarios and identifies opportunities for combining the two fields via a hybrid approach aimed at decision-making in digital territorial management.</jats:p>"
10.5840/ijap2023372202,"Plagiarism, Academic Ethics, and the Utilization of Generative AI in Academic Writing","<jats:p>In the wake of ChatGPT’s release, academics and journal editors have begun making important decisions about whether and how to integrate generative artificial intelligence (AI) into academic publishing. Some argue that AI outputs in scholarly works constitute plagiarism, and so should be disallowed by academic journals. Others suggest that it is acceptable to integrate AI output into academic papers, provided that its contributions are transparently disclosed. By drawing on Taylor’s work on academic norms, this paper argues against both views. Unlike “traditional” forms of plagiarism, use of generative AI can be consistent with the norms that should underlie academic research. In these cases, its use should neither be prohibited nor required to be disclosed. However, some careless uses of generative AI do threaten to undermine the quality of academic research by mischaracterizing existing literature. This, not “AI plagiarism,” is the real concern raised by ChatGPT and related technologies.</jats:p>"
10.1007/979-8-8688-0205-8_1,Introduction to Generative AI,N/A
10.1007/978-1-4842-9367-6_1,Introduction to Generative AI,N/A
10.1007/978-1-4842-9367-6_7,The Transformation of Business,N/A
10.61969/jai.1385915,Generative AI Professional Development Needs for Teacher Educators,"<jats:p xml:lang=""en"">This study presents findings from a professional development (PD) webinar aimed at sensitizing and gathering teacher educators’ knowledge of Generative Artificial Intelligence (GAI). The primary objective of the webinar was to deepen teacher educators’ understanding and applications of GAI within the context of teacher education in Ghana and to identify areas requiring additional development. Three hundred and seven participants from a diverse group, including teacher educators, administrators, and in-service teachers participated in the PD session. The session was conducted online via Zoom. The video and audio recordings were transcribed and analyzed thematically using MAXQDA version 2022.4. Findings indicate a diverse range of familiarity with GAI among participants. While some expressed knowledge of GAI tools, others were learning about GAI for the first time. Further, the findings showed an increasing curiosity among participants for the inspiring functions of GAI in education, such as automatic scoring, academic writing, assisting teachers with image generation for their classroom practices, etc. The participants demonstrated a willingness to include GAI in their classroom practices and support their students. However, they also identified infrastructural gaps, such as the expense of premium GAI tools, training on GAI promptings, and ethical issues such as transparency, as potential barriers to the successful implementation of GAI in teacher education. Therefore, the study suggests that institutional support should be provided to teacher educators. This support would expand their access to various GAI tools and features. The study further recommends integrating GAI, including explainable GAI and prompt engineering, as a core component of teacher education and continuous professional development programs. Additionally, it emphasizes the importance of strengthening educators' skills in innovative assessment practices.</jats:p>"
10.1007/978-3-031-46238-2_23,AI Deep Learning Generative Models for Drug Discovery,N/A
10.1007/978-1-4842-9579-3,Creative Prototyping with Generative AI,N/A
10.1201/9781032654829-1,Generative AI and Social Engines of Hate,N/A
10.21428/e4baedd9,An MIT Exploration of Generative AI,N/A
10.2139/ssrn.4864236,AI Justice: Harnessing Generative AI in Legal Services,N/A
10.1515/9783111425078-010,10 Future Directions and Open Problems in Generative AI,N/A
10.1145/3600211.3604686,The Ethical Implications of Generative Audio Models: A Systematic Literature Review,N/A
10.55982/openpraxis.16.1.654,"GenAI et al.: Cocreation, Authorship, Ownership, Academic Ethics and Integrity in a Time of Generative AI",N/A
10.1515/9783111425078-008,8 Exploring the Applications on Generative AI and LLM,N/A
10.1007/979-8-8688-0318-5_4,Generative AI Consulting,N/A
10.1007/979-8-8688-0318-5_5,Generative AI Software,N/A
10.1007/s43681-020-00013-4,You cannot have AI ethics without ethics,N/A
10.1007/979-8-8688-0318-5_7,Generative AI Hardware,N/A
10.1201/9781032654829-7,Unwanted Psychological Diagnoses: Discriminative Dangers of Generative AI,N/A
10.37305/jkba.2023.06.24.1.1,Ethical Issues Posed by ‘Generative-AI’ (G-AI) - Response strategies for ‘Good AI Society’,N/A
10.22318/icls2024.651304,Exploring AI Ethics through Educational Scenarios with AI Generative Arts Apps,N/A
10.47142/gec.10.1,Research on self-assessment items for teaching writing ethics in the age of generative AI,N/A
10.1515/9783111425078-004,4 Importance of Prompt Engineering in Generative AI Models,N/A
10.1515/9783111425078-009,9 Bias and Fairness in Generative AI,N/A
10.1007/s10676-024-09776-4,An Ellulian analysis of propaganda in the context of generative AI,N/A
10.54254/2755-2721/87/20241543,Decision tree C4.5 algorithm for generative AI technology ethics--Based on the results of the questionnaire,"<jats:p>With the development of AI technology, generative AI has gradually entered the life of the public, for example, the explosion of CHAT-GPT has allowed more people to see the huge potential and obvious advantages of generative AI. However, in the process of generative AI operation, events that violate social responsibility and ethics often occur, which makes the research on the scientific and technological ethics of generative AI more urgent. In the past literature and research, many industry experts have analysed the impact of generative AI on specific industries, but everyone is or will be a user of generative AI, so we should pay attention to the study of the people's scientific and technological ethical issues of generative AI after putting aside the industry background, so this paper collects primary data by means of questionnaire surveys to find out the public's awareness of generative AI and their perception of generative AI. and attitudes towards generative AI, and using the decision tree C4.5 algorithm with Python as the tool, it is used to respond to people's awareness of generative AI and the public's perception of the relationship between the various factors of the ethical issues of </jats:p>"
10.1117/12.3011173.84d02228-3453-ee11-a99c-00505691c5e1,N/A,N/A
10.1080/23735082.2023.2261131,"Towards social generative AI for education: theory, practices and ethics",N/A
10.4018/979-8-3693-6517-5.ch003,Do Generative Large Language Models Need Billions of Parameters?,"<jats:p>This chapter presents novel systems and methodologies for the development of efficient large language models (LLMs). It explores the trade-offs between model size, performance, and computational resources, with the aim of maximizing the efficiency of these AI systems. The research explores novel methods that allow different parts of the model to share parameters, reducing the total number of unique parameters required. This approach ensures that the model remains compact without sacrificing its ability to learn and represent complex language structures. This study provides valuable insights and tools for creating more efficient and effective LLMs, contributing to a more sustainable and accessible future for AI language modeling.</jats:p>"
10.1007/978-1-4842-9579-3_13,AI and the Future of Creative Work,N/A
10.1007/s43681-024-00505-7,Proposing Central Asian AI ethics principles: a multilevel approach for responsible AI,N/A
10.1007/979-8-8688-0318-5_6,Generative AI Cloud Platforms,N/A
10.1007/979-8-8688-0318-5_2,What Is Generative AI?,N/A
10.4018/979-8-3693-1950-5.ch003,Generative AI and Generative Pre-Trained Transformer Applications,"<jats:p>Generative AI, such as generative pre-trained transformer (GPT), has seen rapid advancements in recent years, offering a wide range of applications, but it also presents several challenges and opportunities. GPT can automate content generation for various industries, including journalism, marketing, and entertainment, reducing the need for manual content creation. Generative AI can personalize content and recommendations in e-commerce, streaming services, and more, enhancing user experiences. Generative AI, such as GPT, offers immense potential across various sectors but requires careful management to address bias, ethics, and quality control challenges. As technology evolves, finding the right balance between control and creativity will be crucial for maximizing its benefits while minimizing risks. Based on the above, the authors systematically review the bibliometric literature on how Generative AI and generative pre-trained transformer applications challenge opportunities using the Scopus database by analysing 49 academic and/or scientific documents.</jats:p>"
10.1007/s43681-022-00150-y,Towards AI ethics’ institutionalization: knowledge bridges from business ethics to advance organizational AI ethics,"<jats:title>Abstract</jats:title><jats:p>This paper proposes to generate awareness for developing Artificial intelligence (AI) ethics by transferring knowledge from other fields of applied ethics, particularly from business ethics, stressing the role of organizations and processes of institutionalization. With the rapid development of AI systems in recent years, a new and thriving discourse on AI ethics has (re-)emerged, dealing primarily with ethical concepts, theories, and application contexts. We argue that business ethics insights may generate positive knowledge spillovers for AI ethics, given that debates on ethical and social responsibilities have been adopted as voluntary or mandatory regulations for organizations in both national and transnational contexts. Thus, business ethics may transfer knowledge from five core topics and concepts researched and institutionalized to AI ethics: (1) stakeholder management, (2) standardized reporting, (3) corporate governance and regulation, (4) curriculum accreditation, and as a unified topic (5) AI ethics washing derived from greenwashing. In outlining each of these five knowledge bridges, we illustrate current challenges in AI ethics and potential insights from business ethics that may advance the current debate. At the same time, we hold that business ethics can learn from AI ethics in catching up with the digital transformation, allowing for cross-fertilization between the two fields. Future debates in both disciplines of applied ethics may benefit from dialog and cross-fertilization, meant to strengthen the ethical depth and prevent ethics washing or, even worse, ethics bashing.</jats:p>"
10.1162/99608f92.88b4cc98,Future Shock: Generative AI and the International AI Policy and Governance Crisis,N/A
10.1201/9781003499527-6,Generative AI and Other Types of AI,N/A
10.53593/n4121a,Generative AI in Economics: Teaching Economics and AI Literacy,N/A
10.1162/99608f92.91162b2e,Data Protection and Generative AI: An Inconclusive Answer,N/A
10.4018/979-8-3693-1351-0.ch014,Empowering Faculty Vitality and Mitigating Burnout Through Generative AI in Higher Education,"<jats:p>In today's dynamic landscape of higher education, the pervasive issue of faculty burnout has emerged as a pressing concern, casting a shadow over the well-being of educators and the overall quality of instruction. This chapter proposal embarks on an exploration of how generative artificial intelligence (AI) can act as a transformative force within higher education, specifically focusing on its potential to empower faculty members, enhance pedagogical practices, and mitigate the alarming prevalence of burnout. The chapter's central objectives are multifaceted, commencing with a comprehensive examination of the multifaceted phenomenon of faculty burnout. This includes an analysis of the contributing factors such as the escalating workloads, the shift towards online and blended learning modalities, and the overwhelming administrative duties that educators must shoulder. The aim here is to illuminate the multifaceted nature of burnout, thereby fostering an enhanced understanding of its urgency and the critical need for sustainable solutions.</jats:p>"
10.1201/9781032669182-7,Generative AI and Other Types of AI,N/A
10.31584/psumj.2023259047,"Generative AI, ChatGPT and Medicine",N/A
10.1515/9783111425078-011,11 Optimizing Sustainable Project Management Life Cycle Using Generative AI Modeling,N/A
10.1007/979-8-8688-0282-9_8,Sense-Based Generative AI Demystified,N/A
10.1007/979-8-8688-0318-5_3,Generative AI Customer End Uses,N/A
10.1002/tl.20626,An ethics module on academic integrity and generative AI,"<jats:title>Abstract</jats:title><jats:p>This article explores the intersection between academic integrity and generative AI (GenAI). It presents a tested framework for a versatile 3‐h module applicable to various disciplines. Since ChatGPT's emergence, GenAI's impact on academic integrity has raised concerns, challenged established norms, and blurred lines of authorship. Engaging students in this topic encourages critical reflection and ethical use of these technologies. This approach draws on experiential learning and student–faculty partnership approaches to activities and assessments, providing students with a platform to not only navigate the responsible application of GenAI in assignments but also foster a dialogue between students and faculty on crafting effective policies for GenAI use.</jats:p>"
10.1515/9783111425078-013,13 Generative AI and LLM: Case Study in E-Commerce,N/A
10.1007/978-1-4842-9579-3_6,Building Blocks,N/A
10.1016/c2023-0-00094-2,Synthetic Data and Generative AI,N/A
10.1093/oxfordhb/9780190067397.013.2,The Ethics of the Ethics of AI,"<p>This chapter discusses several challenges for doing the ethics of artificial intelligence (AI). The challenges fall into five major categories: conceptual ambiguities within philosophy and AI scholarship; the estimation of AI risks; implementing machine ethics; epistemic issues of scientific explanation and prediction in what can be called computational data science (CDS), which includes “big data” science; and oppositional versus systemic ethics approaches. The chapter then argues that these ethical problems are not likely to yield to the “common approaches” of applied ethics. Primarily due to the transformational nature of artificial intelligence within science, engineering, and human culture, novel approaches will be needed to address the ethics of AI in the future. Moreover, serious barriers to the formalization of ethics will be needed to overcome to implement ethics in AI.</p>"
10.1007/s43681-023-00367-5,AI ethics and ordoliberalism 2.0: towards a ‘Digital Bill of Rights’,N/A
10.4018/979-8-3693-5578-7.ch001,Generative AI and Business Strategy,"<jats:p>A universal technology appears that sparks creativity to the point where several sectors undergo complete revolution. The Generative AI is one of these uncommon general-purpose technologies and is poised to bring about exponential transformation and is now consistently supported by research. It has the power to change the competitive landscape and propel businesses towards explosive growth. Businesses will either leap ahead of this technological transformation or fall behind as they race to comprehend its ramifications. In Generative AI and business transformation, potential for strategic and useful business applications are identified by analyzing the platforms, big language models, and technology of generative AI. This chapter focuses on Generative AI role in influencing business strategy.</jats:p>"
10.1007/978-1-4842-9579-3_4,Creative Companion,N/A
10.1007/979-8-8688-0282-9_3,The Business Case for Generative AI,N/A
10.21428/e4baedd9.d562223c,The Impact of Generative AI on Labor Market Matching,N/A
10.1201/9781032654829-4,Generative AI for Hate Speech Detection: Evaluation and Findings,N/A
10.1007/978-1-4842-9579-3_12,Use Cases,N/A
10.1515/9783111425078-012,12 Generative AI and LLM: Case Study in Finance,N/A
10.1515/9783111425078-002,2 Early Roots of Generative AI Models and LLM: A Diverse Landscape,N/A
10.1007/978-3-031-46238-2_18,How Generative AI Is Transforming Medical Imaging: A Practical Guide,N/A
10.1007/979-8-8688-0282-9_15,The Evolving World of Generative AI,N/A
10.2139/ssrn.4858973,Risks from Generative AI,N/A
10.2196/preprints.55957,Toward Clinical Generative AI: Conceptual Framework (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>Clinical decision-making is a crucial aspect of health care, involving the balanced integration of scientific evidence, clinical judgment, ethical considerations, and patient involvement. This process is dynamic and multifaceted, relying on clinicians’ knowledge, experience, and intuitive understanding to achieve optimal patient outcomes through informed, evidence-based choices. The advent of generative artificial intelligence (AI) presents a revolutionary opportunity in clinical decision-making. AI’s advanced data analysis and pattern recognition capabilities can significantly enhance the diagnosis and treatment of diseases, processing vast medical data to identify patterns, tailor treatments, predict disease progression, and aid in proactive patient management. However, the incorporation of AI into clinical decision-making raises concerns regarding the reliability and accuracy of AI-generated insights. To address these concerns, 11 “verification paradigms” are proposed in this paper, with each paradigm being a unique method to verify the evidence-based nature of AI in clinical decision-making. This paper also frames the concept of “clinically explainable, fair, and responsible, clinician-, expert-, and patient-in-the-loop AI.” This model focuses on ensuring AI’s comprehensibility, collaborative nature, and ethical grounding, advocating for AI to serve as an augmentative tool, with its decision-making processes being transparent and understandable to clinicians and patients. The integration of AI should enhance, not replace, the clinician’s judgment and should involve continuous learning and adaptation based on real-world outcomes and ethical and legal compliance. In conclusion, while generative AI holds immense promise in enhancing clinical decision-making, it is essential to ensure that it produces evidence-based, reliable, and impactful knowledge. Using the outlined paradigms and approaches can help the medical and patient communities harness AI’s potential while maintaining high patient care standards.</p>
                </sec>"
10.1007/s43681-023-00340-2,"Primary recognition, morality and AI","<jats:title>Abstract</jats:title><jats:p>This paper aims to show that the experience of ‘primary recognition’ (O’Hara in Moral certainty and the foundations of morality, Palgrave Macmillan, London, 2018) can be extended to human AI interactions. That is, I argue that human beings can (and do) experience non-rational, reflex moral responses to AI and social robots that fit O’Hara’s description of primary recognition. I give two plausible examples, one involving a military mine-sweeping robot and the other, a toy dinosaur called a ‘Pleo’. These experiences of primary recognition do not, however, settle the question of whether any particular AI can be considered a true moral patient or a ‘person’.</jats:p>"
10.1007/s43681-023-00409-y,How to design an AI ethics board,"<jats:title>Abstract</jats:title><jats:p>The development and deployment of artificial intelligence (AI) systems poses significant risks to society. To reduce these risks to an acceptable level, AI companies need an effective risk management process and sound risk governance. In this paper, we explore a particular way in which AI companies can improve their risk governance: by setting up an AI ethics board. We identify five key design choices: (1) What responsibilities should the board have? (2) What should its legal structure be? (3) Who should sit on the board? (4) How should it make decisions? (5) And what resources does it need? We break each of these questions down into more specific sub-questions, list options, and discuss how different design choices affect the board’s ability to reduce societal risks from AI. Several failures have shown that designing an AI ethics board can be challenging. This paper provides a toolbox that can help AI companies to overcome these challenges.</jats:p>"
10.1007/s43681-021-00129-1,AI ethics and systemic risks in finance,"<jats:title>Abstract</jats:title><jats:p>The paper suggests that AI ethics should pay attention to morally relevant systemic effects of AI use. It draws the attention of ethicists and practitioners to systemic risks that have been neglected so far in professional AI-related codes of conduct, industrial standards and ethical discussions more generally. The paper uses the financial industry as an example to ask: how can AI-enhanced systemic risks be ethically accounted for? Which specific issues does AI use raise for ethics that takes systemic effects into account? The paper (1) relates the literature about <jats:italic>AI ethics</jats:italic> to the <jats:italic>ethics of systemic risks</jats:italic> to clarify the moral relevance of AI use with respect to the imposition of systemic risks, (2) proposes a theoretical framework based on <jats:italic>the ethics of complexity</jats:italic> and (3) applies this framework to <jats:italic>discuss implications for AI ethics</jats:italic> concerned with AI-enhanced systemic risks.</jats:p>"
10.1201/9781003442240-6,Computational Creativity and Generative AI,N/A
10.1162/99608f92.fbdf6128,Carbon Emissions in the Tailpipe of Generative AI,N/A
10.21428/e4baedd9.1729053f,Implementing Generative AI in U.S. Hospital Systems,N/A
10.1007/979-8-8688-0282-9_4,The World of Text-Based Generative AI,N/A
10.1007/978-1-4842-9579-3_10,Uncanny by Nature,N/A
10.1007/978-3-031-55642-5_6,Advancing Requirements Engineering Through Generative AI: Assessing the Role of LLMs,N/A
10.21428/e4baedd9.a1f6a281,Generative AI from Theory to Practice: A Case Study of Financial Advice,N/A
10.1007/s43681-020-00002-7,Emerging challenges in AI and the need for AI ethics education,N/A
10.1007/s43681-021-00040-9,Philosophical foundations for digital ethics and AI Ethics: a dignitarian approach,"<jats:title>Abstract</jats:title><jats:p><jats:italic>AI Ethics</jats:italic>is a burgeoning and relatively new field that has emerged in response to growing concerns about the impact of artificial intelligence (AI) on human individuals and their social institutions. In turn, AI ethics is a part of the broader field of digital ethics, which addresses similar concerns generated by the development and deployment of new digital technologies. Here, we tackle the important worry that digital ethics in general, and AI ethics in particular, lack adequate philosophical foundations. In direct response to that worry, we formulate and rationally justify some basic concepts and principles for digital ethics/AI ethics, all drawn from a broadly Kantian theory of human dignity. Our argument, which is designed to be relatively compact and easily accessible, is presented in ten distinct steps: (1) what “digital ethics” and “AI ethics” mean, (2) refuting the dignity-skeptic, (3) the metaphysics of human dignity, (4) human happiness or flourishing, true human needs, and human dignity, (5) our moral obligations with respect to all human real persons, (6) what a natural automaton or natural machine is, (7) why human real persons are not natural automata/natural machines: because consciousness is a form of life, (8) our moral obligations with respect to the design and use of artificial automata or artificial machines, aka computers, and digital technology more generally, (9) what privacy is, why invasions of digital privacy are morally impermissible, whereas consensual entrances into digital privacy are either morally permissible or even obligatory, and finally (10) dignitarian morality versus legality, and digital ethics/AI ethics. We conclude by asserting our strongly-held belief that a well-founded and generally-accepted dignitarian digital ethics/AI ethics is of global existential importance for humanity.</jats:p>"
10.21428/e4baedd9.81164b06,Generative AI and K-12 Education: An MIT Perspective,N/A
10.1007/979-8-8688-0282-9_7,Advanced Applications of Text-Based Generative AI,N/A
10.1007/979-8-8688-0282-9_1,Introduction to the World of Generative AI,N/A
10.2196/55957,Toward Clinical Generative AI: Conceptual Framework,"<jats:p>Clinical decision-making is a crucial aspect of health care, involving the balanced integration of scientific evidence, clinical judgment, ethical considerations, and patient involvement. This process is dynamic and multifaceted, relying on clinicians’ knowledge, experience, and intuitive understanding to achieve optimal patient outcomes through informed, evidence-based choices. The advent of generative artificial intelligence (AI) presents a revolutionary opportunity in clinical decision-making. AI’s advanced data analysis and pattern recognition capabilities can significantly enhance the diagnosis and treatment of diseases, processing vast medical data to identify patterns, tailor treatments, predict disease progression, and aid in proactive patient management. However, the incorporation of AI into clinical decision-making raises concerns regarding the reliability and accuracy of AI-generated insights. To address these concerns, 11 “verification paradigms” are proposed in this paper, with each paradigm being a unique method to verify the evidence-based nature of AI in clinical decision-making. This paper also frames the concept of “clinically explainable, fair, and responsible, clinician-, expert-, and patient-in-the-loop AI.” This model focuses on ensuring AI’s comprehensibility, collaborative nature, and ethical grounding, advocating for AI to serve as an augmentative tool, with its decision-making processes being transparent and understandable to clinicians and patients. The integration of AI should enhance, not replace, the clinician’s judgment and should involve continuous learning and adaptation based on real-world outcomes and ethical and legal compliance. In conclusion, while generative AI holds immense promise in enhancing clinical decision-making, it is essential to ensure that it produces evidence-based, reliable, and impactful knowledge. Using the outlined paradigms and approaches can help the medical and patient communities harness AI’s potential while maintaining high patient care standards.</jats:p>"
10.1007/s00146-023-01817-8,Generative AI and photographic transparency,N/A
10.1007/s43681-024-00551-1,Opinion piece: on the ethics of a pending AI crisis in business,N/A
10.1007/s43681-022-00209-w,The uselessness of AI ethics,"<jats:title>Abstract</jats:title><jats:p>As the awareness of AI’s power and danger has risen, the dominant response has been a turn to ethical principles. A flood of AI guidelines and codes of ethics have been released in both the public and private sector in the last several years. However, these are<jats:italic>meaningless principles</jats:italic>which are contested or incoherent, making them difficult to apply; they are<jats:italic>isolated principles</jats:italic>situated in an industry and education system which largely ignores ethics; and they are<jats:italic>toothless principles</jats:italic>which lack consequences and adhere to corporate agendas. For these reasons, I argue that AI ethical principles are useless, failing to mitigate the racial, social, and environmental damages of AI technologies in any meaningful sense. The result is a gap between high-minded principles and technological practice. Even when this gap is acknowledged and principles seek to be “operationalized,” the translation from complex social concepts to technical rulesets is non-trivial. In a zero-sum world, the dominant turn to AI principles is not just fruitless but a dangerous distraction, diverting immense financial and human resources away from potentially more effective activity. I conclude by highlighting alternative approaches to AI justice that go beyond ethical principles: thinking more broadly about systems of oppression and more narrowly about accuracy and auditing.</jats:p>"
10.1007/s43681-024-00485-8,AI for all: Diversity and Inclusion in AI,N/A
10.1162/99608f92.ad8ebbd4,"Toward a Theory of AI Errors: Making Sense of Hallucinations, Catastrophic Failures, and the Fallacy of Generative AI",N/A
10.1007/s43681-021-00071-2,AI ethics: A framework for measuring embodied carbon in AI systems,"<jats:title>Abstract</jats:title><jats:p>This paper outlines the ethical implications of AI from a climate perspective. So far, much of the discussion around AI ethics have focused on bias, unexplainable outcomes, privacy and other social impacts of such systems. The role and contribution of AI towards climate change and the ethical implications of its contribution to an unjust distribution of impact on the planet, humans and flora and fauna have not yet been covered in detail within the technical community. Within this paper, we aim to raise some of the issues of AI associated with climate justice and we propose a framework that will allow the AI and ICT industries to measure their true impact on the planet, propose an organisational structure to take this work forward and propose future research areas for this important topic.</jats:p>"
10.1007/s43681-022-00156-6,The ethics of AI business practices: a review of 47 AI ethics guidelines,N/A
10.1515/9783111425078-003,3 Generative AI Models and LLM: Training Techniques and Evaluation Metrics,N/A
10.1515/9783111425078-001,1 Unveiling the Power of Generative AI: A Journey into Large Language Models,N/A
10.1145/3600211.3604716,Diffusing the Creator: Attributing Credit for Generative AI Outputs,N/A
10.1007/979-8-8688-0083-2_2,Exploring Generative AI and Its Transformative Power,N/A
10.37736/kjlr.2023.08.14.4.03,Undergraduates’ Awareness of the Ethics of Generative AI Utilization in College Writing,"<jats:p>This study examines the utilization of generative AI among undergraduate students and their awareness of ethics in college writing. It also identifies the demand for ethics education related to writing using generative AI. A survey was conducted on 895 undergraduate students taking college writing courses at A University.
The study found that first, undergraduate students who have had experience with university writing tasks employed generative AI used ChatGPT the most; they used it substantially to generate ideas and collect data when writing general reports. In addition, more than half of the experienced users were satisfied with generative AI. Undergraduate students comply with writing ethics when performing college writing and are well aware of the problems in using generative AI, but they have low awareness of citation methods in writing when using generative AI. There was a high educational demand for citation methods and writing ethics when performing writing using generative AI. Moreover, there were some differences in the level of awareness and educational needs depending on the undergraduate major and the experience of using generative AI.
Accordingly, this study suggested the following measures in college writing subjects: education on how to comply with writing ethics and citation when using generative AI for writing, education that reflects the differences in students’ awareness and educational needs by major category, and education on how to use ChatGPT in generating ideas and collecting data when teaching how to write a report.</jats:p>"
10.2139/ssrn.4878339,Generative AI as Digital Media,N/A
10.3386/w32690,How will Generative AI impact Communication?,N/A
10.1007/978-1-4842-9579-3_1,Generating Creativity from Negativity,N/A
10.1007/978-1-4842-9579-3_9,The Master of Mashup,N/A
10.1162/99608f92.fad6d25c,Future Shock: Grappling With the Generative AI Revolution,N/A
10.61700/qs6nzkddi20lw469,Using Generative AI for Qualitative Analysis,"<jats:p>This comprehensive workshop is aimed at equipping researchers with the skills to utilize AI appropriately in their qualitative analysis. Covering a range of topics from understanding AI software tools to ethical considerations and troubleshooting, this seminar provides a unique opportunity to enhance research capabilities and gain a competitive edge in the academic field.</jats:p>"
10.1007/s43681-020-00015-2,AI ethics and its impact on knowledge management,N/A
10.1093/oxfordhb/9780190067397.013.35,Perspectives on Ethics of AI,"<p>This chapter investigates the philosophical aspects of AI and ethics. As the world today becomes increasingly populated by intelligent, socially interactive artefacts—devices that are not just instruments of human action but designed to be a kind of social actor in their own right—people will need to grapple with challenging questions concerning the status and moral standing of these machinic others. In formulating responses to these questions, one can obviously deploy the standard properties approach. This method has considerable historical precedent behind it and constitutes what can be called the default setting for addressing questions concerning moral standing. Indeed, a good deal of the current work in moral machines, machine ethics, AI ethics, and the ethics of AI follow this procedure. However, this approach, for all its advantages, also has considerable difficulties. The chapter therefore proposes an alternative approach to addressing AI ethics and the ethics of AI that circumvents many of the problems encountered in the properties approach by arranging for an ethics that is relational, radically empirical, and altruistic. This other way of thinking is informed by and follows from recent innovations in moral philosophy.</p>"
10.1007/s43681-020-00008-1,Lessons learned from AI ethics principles for future actions,N/A
10.1007/978-3-031-55642-5_5,Requirements Engineering Using Generative AI: Prompts and Prompting Patterns,N/A
10.1007/978-1-4842-9579-3_2,Being Creative with Machines,N/A
10.1007/s43681-024-00524-4,Operationalizing responsible AI principles through responsible AI capabilities,"<jats:title>Abstract</jats:title><jats:p>Responsible artificial intelligence (RAI) has emerged in response to growing concerns about the impact of AI. While high-level principles have been provided, operationalizing these principles poses challenges. This study, grounded in recent RAI literature in organizational contexts and dynamic capability theory, and informed by literature on RAI principles and expert interviews in organizations deploying AI systems, (1) problematizes the high-level principles and low-level requirements and underscores the need for mid-level norms by adopting dynamic capability as a theoretical lens, and (2) develops five themes to capture firms’ RAI capability, including (i) understandable AI model, (ii) bias remediation, (iii) responsiveness, (iv) harmless, and vi) common good. As our contribution to the field of information systems (IS), this study extends the emerging literature on operationalizing RAI and dynamic capabilities, empirically elucidating the capabilities needed by firms. For IS practice, we provide organizations deploying AI with novel insights to aid in the responsible implementation of AI.</jats:p>"
10.1007/s43681-021-00082-z,Correction to: Philosophical foundations for digital ethics and AI Ethics: a dignitarian approach,N/A
10.1515/9783111425078-014,Index,N/A
10.1007/978-3-031-46238-2_24,3D Generative Network,N/A
10.1515/9783111425078-fm,Frontmatter,N/A
10.1201/9781032669182-8,"Generative AI: Types, Skills, Opportunities, and Challenges",N/A
10.1007/978-1-4842-9579-3_8,The Art of the Prompt,N/A
10.1007/978-3-031-54252-7_1,Foundations of Generative AI,N/A
10.1515/9783111425078-toc,Contents,N/A
10.1515/9783111425078-202,Preface,N/A
10.1007/s43681-024-00437-2,Action-guidance and AI ethics: the case of fair machine learning,"<jats:title>Abstract</jats:title><jats:p>A prominent approach to implementing AI ethics involves translating ethical principles, such as fairness and transparency, into practical frameworks and tools that responsible agents, such as ML developers, can use to ensure that machine learning systems act according to the relevant principles. Fair machine learning research exemplifies this approach by producing frameworks and software toolkits that responsible agents could apply to align machine learning systems with principles such as fairness, equality, and justice. However, the application of available frameworks and tools has proven challenging both due to ambiguous operationalization of the relevant principles and many real-life obstacles that agents face in the context of machine learning system design and development, such as lack of access to proper evaluation data. This article conceptualizes these problems as instances of a more general “action-guidance gap” in AI ethics. The article addresses the action-guidance gap by outlining a philosophical account of action-guidance that can be used to identify and address problems related to the specification and practical implementation of AI ethics principles. Centering on fair machine learning practice as a case example, the article presents a set of detailed requirements for action-guidance in fair machine learning practice which explain problems that previous studies have identified with regard to the real-life application of fair machine learning frameworks and tools. Paving a way forward, the article presents theoretical and practical lessons for ensuring action-guidance in fairness-sensitive design, with implications for AI ethics more generally.</jats:p>"
10.1007/978-3-031-46238-2_26,Plant Data Generation with Generative AI: An Application to Plant Phenotyping,N/A
10.1093/oxfordhb/9780190067397.013.46,Ethics of AI in Law,"<p>This chapter studies some of the most important ethical topics involving the use of artificial intelligence (AI) within the legal system itself and examines how central legal values might unintentionally or intentionally change with increased use of AI in law. Ethical issues surrounding AI use in law often share a common theme. As AI becomes increasingly integrated within the legal system, how can society ensure that core legal values are preserved? Among the most important of these legal values are: equal treatment under the law; legal results arising from law, principle, and facts rather than social status or power; procedural fairness and due process; fairness in design and application of the law; transparency in legal substance and process; adequate access to justice for all; integrity and honesty in creation and application of law; and judicial, legislative, and administrative efficiency. The central ethical challenge is to identify the ways in which the use of AI may be shifting core legal values and to ensure that these crucial values are preserved in the technological transition. A more positive view also identifies the ways in which AI technology can not only preserve central values, but rather, can foster and enhance these values to the betterment of the legal system and society overall.</p>"
10.2139/ssrn.4910877,African AI Ethics?—The Role of AI Ethics Initiatives in Africa,N/A
10.1117/12.3011374.61c8834a-a454-ee11-a99c-00505691c5e1,N/A,N/A
10.1093/oxfordhb/9780190067397.013.27,Perspectives on Ethics of AI,"<p>This chapter describes a computational view of the function of ethics in human society and discusses its application to three diverse examples. First, autonomous vehicles are individually embodied intelligent systems that act as members of society. The ethical knowledge needed by such an agent is not how to choose the lesser evil when confronted by a Deadly Dilemma, but how to recognize the upstream decision point that makes it possible to avoid the Deadly Dilemma entirely. Second, disembodied distributed intelligent systems like Google and Facebook provide valuable services while collecting, aggregating, and correlating vast amounts of information about individual users. With inadequate controls, these corporate systems can invade privacy and do substantial damage through either correct or incorrect inferences. Third, acceptance of the legitimacy of the society by its individual members depends on a general perception of fairness. Rage about unfairness can be directed at individual free-riders or at systematic inequality across the society. Ultimately, the promise of a computational approach to ethical knowledge is not simply ethics for computational devices such as robots. It also promises to help people understand the pragmatic value of ethics as a feedback mechanism that helps intelligent creatures, human and nonhuman, live together in thriving societies.</p>"
10.1515/9781501519765-008,Chapter 4: Generative AI,N/A
10.1007/s43681-023-00347-9,Against the substitutive approach to AI in healthcare,N/A
10.1007/978-1-4842-9367-6,Generative AI,N/A
10.1007/s43681-021-00090-z,Using edge cases to disentangle fairness and solidarity in AI ethics,N/A
10.15833/kafeiam.30.4.1261,Meta-analysis of learning effectiveness using generative AI,N/A
10.1007/s43681-022-00243-8,Correction: AI ethics: the case for including animals,N/A
10.1016/b978-0-44-321857-6.00003-5,Copyright,N/A
10.1201/9781003503781,Generative AI,N/A
10.1007/s43681-021-00080-1,From computer ethics and the ethics of AI towards an ethics of digital ecosystems,"<jats:title>Abstract</jats:title><jats:p>Ethical, social and human rights aspects of computing technologies have been discussed since the inception of these technologies. In the 1980s, this led to the development of a discourse often referred to as computer ethics. More recently, since the middle of the 2010s, a highly visible discourse on the ethics of artificial intelligence (AI) has developed. This paper discusses the relationship between these two discourses and compares their scopes, the topics and issues they cover, their theoretical basis and reference disciplines, the solutions and mitigations options they propose and their societal impact. The paper argues that an understanding of the similarities and differences of the discourses can benefit the respective discourses individually. More importantly, by reviewing them, one can draw conclusions about relevant features of the next discourse, the one we can reasonably expect to follow after the ethics of AI. The paper suggests that instead of focusing on a technical artefact such as computers or AI, one should focus on the fact that ethical and related issues arise in the context of socio-technical systems. Drawing on the metaphor of ecosystems which is widely applied to digital technologies, it suggests preparing for a discussion of the ethics of digital ecosystems. Such a discussion can build on and benefit from a more detailed understanding of its predecessors in computer ethics and the ethics of AI.</jats:p>"
10.1007/s43681-020-00024-1,Correction to: AI ethics and its impact on knowledge management,N/A
10.21428/92fbeb44.1aaaf7ab,Human-AI Partnerships in Generative Music,N/A
10.1201/9781003318538-10,"Exploring the Potential of ChatGPT, Responsible AI, Explainable AI and Generative AI",N/A
10.1007/s00146-023-01719-9,"Generative AI, generating precariousness for workers?",N/A
10.1007/s00146-023-01773-3,Generative AI and human labor: who is replaceable?,N/A
10.21428/e4baedd9.82175d26,Generative AI in the Era of 'Alternative Facts',N/A
10.4018/979-8-3693-5578-7.ch011,Revolutionizing Tourism,"<jats:p>This chapter aims to explore the potential of generative (AI) in improving aspects of tourism experiences. Areas focused on include personalized recommendation systems, content generation for marketing purposes, virtual tour guides, and adaptive travel planning within the tourism industry. It focuses on customization approaches, incorporation of AR and VR technologies, and creation of real-time decision support systems. Moreover, its objective is to understand the ethical utilization of AI in the tourism industry, concerns regarding data privacy, and the incorporation of AI into current tourism infrastructure. It helps to delineate forthcoming research paths and emerging patterns, highlighting the possible influence of AI on the tourism sector and its future. Furthermore, it seeks to elucidate the socio-economic impacts of using AI in the tourist industry, examining its capacity to transform employment patterns, visitor characteristics, and methods for developing destinations. Finally, it promotes interdisciplinary discussion in influencing the future of tourism.</jats:p>"
10.17918/00001845,Image Forensics and Anti-Forensics for Generative AI,N/A
10.1007/978-3-031-46238-2,Applications of Generative AI,N/A
10.1007/979-8-8688-0403-8_6,Generative Large Language Models,N/A
10.1007/978-3-031-55642-5_11,How Can Generative AI Enhance Software Management? Is It Better Done than Perfect?,N/A
10.1007/978-1-4842-9994-4_1,Introduction to Generative AI,N/A
10.21428/e4baedd9.5aaf489a,Generative AI for Pro-Democracy Platforms,N/A
10.1007/978-3-031-46238-2_19,Generative AI in Medical Imaging and Its Application in Low Dose Computed Tomography (CT) Image Denoising,N/A
10.1007/978-3-031-64087-2_11,From Large Language Models to Generative AI Systems,N/A
10.21428/e4baedd9.2d7598a2,Intelligence as Agency: Evaluating the Capacity of Generative AI to Empower or Constrain Human Action,N/A
10.21428/e4baedd9.567bfd15,"Generative AI for Trustworthy, Open, and Equitable Scholarship",N/A
10.4324/9781003482918-15,Generative AI and the implications for authentic assessment,N/A
10.1162/99608f92.ec74a002,Can ChatGPT Plan Your Retirement?: Generative AI and Financial Advice,N/A
10.1007/979-8-8688-0473-1_1,Introduction to Generative AI,N/A
10.4018/979-8-3693-3278-8.ch005,AI Generative Models for the Fashion Industry,"<jats:p>Fashion designers and brands use GANs to create new and unique patterns, styles, and textures. GANs consist of a generator and a discriminator, which work together to produce high-quality, realistic outputs. VAEs are another type of generative model that is applied to generate new fashion designs. VAEs are known for their ability to generate diverse outputs by sampling from a learned latent space. Fashion designers can use VAEs to explore different design variations and styles. StyleGAN and its successor, StyleGAN2, are advancements of GANs that specifically focus on generating high-resolution and realistic images with control over different style elements. These models have been employed in fashion to create detailed and visually appealing designs. These AI generative models have the potential to revolutionize the fashion industry by facilitating creativity and providing new avenues for artistic expression. However, it's essential to consider ethical implications, intellectual property rights, and the responsible use of AI technologies in the context of fashion design.</jats:p>"
10.1007/s43681-023-00357-7,How to measure value alignment in AI,N/A
10.1007/s10805-023-09492-6,Detection of GPT-4 Generated Text in Higher Education: Combining Academic Judgement and Software to Identify Generative AI Tool Misuse,N/A
10.1007/s43681-024-00420-x,On monitorability of AI,"<jats:title>Abstract</jats:title><jats:p>Artificially intelligent (AI) systems have ushered in a transformative era across various domains, yet their inherent traits of unpredictability, unexplainability, and uncontrollability have given rise to concerns surrounding AI safety. This paper aims to demonstrate the infeasibility of accurately monitoring advanced AI systems to predict the emergence of certain capabilities prior to their manifestation. Through an analysis of the intricacies of AI systems, the boundaries of human comprehension, and the elusive nature of emergent behaviors, we argue for the impossibility of reliably foreseeing some capabilities. By investigating these impossibility results, we shed light on their potential implications for AI safety research and propose potential strategies to overcome these limitations.</jats:p>"
10.1007/s43681-024-00534-2,"Author Correction: AI hype, promotional culture, and affective capitalism",N/A
10.1007/s43681-023-00341-1,"Participation, prediction, and publicity: avoiding the pitfalls of applying Rawlsian ethics to AI","<jats:title>Abstract</jats:title><jats:p>Given the popularity of John Rawls’ theory of justice as fairness as an ethical framework in the artificial intelligence (AI) field, this article examines how the theory fits with three different conceptual applications of AI technology. First, the article discusses a proposition by Ashrafian to let an AI agent perform the deliberation that produces a Rawlsian social contract governing humans. The discussion demonstrates the inviability of such an application as it contradicts foundational aspects of Rawls’ theories. An exploration of more viable applications of Rawlsian theory in the AI context follows, introducing the distinction between <jats:italic>intrinsic</jats:italic> and <jats:italic>extrinsic</jats:italic> theoretical adherence, i.e., the difference between approaches integrating Rawlsian theory in the system design and those situating AI systems in Rawls-consistent policy/legislative frameworks. The article uses emerging AI legislation in the EU and the U.S. as well as Gabriel’s argument for adopting Rawls’ <jats:italic>publicity</jats:italic> criterion in the AI field as examples of extrinsic adherence to Rawlsian theory. A discussion of the epistemological challenges of predictive AI systems then illustrates some implications of intrinsic adherence to Rawlsian theory. While AI systems can make short-term predictions about human behavior with intrinsic adherence to Rawls’ theory of justice as fairness, long-term, large-scale predictions results do not adhere to the theory, but instead constitute the type of utilitarianism Rawls vehemently opposed. The article concludes with an overview of the implications of these arguments for policymakers and regulators.</jats:p>"
10.1093/oxfordhb/9780190067397.013.39,Perspectives and Approaches in AI Ethics,"<p>This chapter focuses on Chinese, Japanese, and South Korean perspectives on and approaches to AI and robots, which can be tools and partners in the AI ethics debate. Each country, in its own way, debates its movement across the tool-partner spectrum. To date, South Korean policy makes a stand against partner AI and robots, while popular culture explores the idea. Chinese policy is headed in the direction of a tool-oriented AI and robotics ethical guidelines, while local practices and culture experiment with the idea of physical and spiritual partnership. Meanwhile, Japan’s social principles are also moving in the tool direction, but its society actively seeks and creates partner-like AI and robots. The chapter then considers two cross-cutting AI and robotics-related ethical issues: the Anthropomorphized Tools Paradox and female objectification. These issues underscore the question of “antisocial” technology. It is clear that both the Anthropomorphized Tools Paradox and female objectification in technology fall under “antisocial” development.</p>"
10.1016/j.joms.2023.09.015,Generative Artificial Intelligence (AI) and Medical Ethics: A Symbiotic Dance for the Future,N/A
10.4018/979-8-3693-1950-5.ch005,Children's Book Visualizations From an Artmaking Generative AI,"<jats:p>How would a well-known artmaking generative AI (A-GAI) go about illustrating children's books? This chapter uses multiple prompting approaches to elicit children's book illustrations for known stories and more original plots to identify some early visual patterns from Deep Dream Generator. The research explores a variety of questions around the quality of the output images in conveying visual meaning in the children's book space (albeit using fictional prompts). This exploratory work provides some early and preliminary insights into this space, including in how to set up a light experiment using an A-GAI.</jats:p>"
10.1007/978-3-031-55642-5_10,Transforming Software Development with Generative AI: Empirical Insights on Collaboration and Workflow,N/A
10.1108/ijoes-04-2024-0112,Ethical dimensions of generative AI: a cross-domain analysis using machine learning structural topic modeling,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>The purpose of this study is to comprehensively examine the ethical implications surrounding generative artificial intelligence (AI).</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>Leveraging a novel methodological approach, the study curates a corpus of 364 documents from Scopus spanning 2022 to 2024. Using the term frequency-inverse document frequency (TF-IDF) and structural topic modeling (STM), it quantitatively dissects the thematic essence of the ethical discourse in generative AI across diverse domains, including education, healthcare, businesses and scientific research.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>The results reveal a diverse range of ethical concerns across various sectors impacted by generative AI. In academia, the primary focus is on issues of authenticity and intellectual property, highlighting the challenges of AI-generated content in maintaining academic integrity. In the healthcare sector, the emphasis shifts to the ethical implications of AI in medical decision-making and patient privacy, reflecting concerns about the reliability and security of AI-generated medical advice. The study also uncovers significant ethical discussions in educational and financial settings, demonstrating the broad impact of generative AI on societal and professional practices.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title>
<jats:p>This study provides a foundation for crafting targeted ethical guidelines and regulations for generative AI, informed by a systematic analysis using STM. It highlights the need for dynamic governance and continual monitoring of AI’s evolving ethical landscape, offering a model for future research and policymaking in diverse fields.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>The study introduces a unique methodological combination of TF-IDF and STM to analyze a large academic corpus, offering new insights into the ethical implications of generative AI across multiple domains.</jats:p>
</jats:sec>"
10.1007/s43681-022-00229-6,Reexamining computer ethics in light of AI systems and AI regulation,"<jats:title>Abstract</jats:title><jats:p>This article argues that the emergence of AI systems and AI regulation showcases developments that have significant implications for computer ethics and make it necessary to reexamine some key assumptions of the discipline. Focusing on design- and policy-oriented computer ethics, the article investigates new challenges and opportunities that occur in this context. The main challenges concern how an AI system’s technical, social, political, and economic features can hinder a successful application of computer ethics. Yet, the article demonstrates that features of AI systems that potentially interfere with successfully applying some approaches to computer ethics are (often) only contingent, and that computer ethics can influence them. Furthermore, it shows how computer ethics can make use of how power manifests in an AI system’s technical, social, political, and economic features to achieve its goals. Lastly, the article outlines new interdependencies between policy- and design-oriented computer ethics, manifesting as either conflicts or synergies.</jats:p>"
10.21606/drs.2024.731,Communicating the use of generative AI to design students: Fostering ethics rather than teaching it,N/A
10.31235/osf.io/ejqan,Adapting to AI: How Will Generative AI Affect Work? How Should We Respond?,<p>A discussion paper directed towards nonspecialist educators who are contemplating the fallout of Generative AI on university curricula in sociology and similar fields.</p>
10.21428/e4baedd9.0d255ab6,Bringing Worker Voice into Generative AI,N/A
10.31224/3230,"Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol and the NIST-approved Quantum-Resistant Cryptographic Algorithms",N/A
10.1201/9781003499527-7,"Generative AI: Types, Skills, Opportunities and Challenges",N/A
10.1101/2024.03.08.24303960,Generative AI Guidelines in Korean Medical Journals: A Survey Using Human-AI Collaboration,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>Generative artificial intelligence (GAI) tools, such as large language models, have the potential to revolutionize medical research and writing, but their use also raises important ethical and practical concerns. This study examines the prevalence and content of GAI guidelines among Korean medical journals to assess the current landscape and inform future policy development.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Top 100 Korean medical journals by H-index were surveyed. Author guidelines were collected and screened by a human author and AI chatbot to identify GAI-related content. Key components of GAI policies were extracted and compared across journals. Journal characteristics associated with GAI guideline adoption were also analyzed.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Only 18% of the surveyed journals had GAI guidelines, which is much lower than previously reported international journals. However, adoption rates increased over time, reaching 57.1% in the first quarter of 2024. Higher-impact journals were more likely to have GAI guidelines. All journals with GAI guidelines required authors to declare GAI use, and 94.4% prohibited AI authorship. Key policy components included emphasizing human responsibility (72.2%), discouraging AI-generated content (44.4%), and exempting basic AI tools (38.9%).</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>While GAI guideline adoption among Korean medical journals is lower than global trends, there is a clear increase in implementation over time. The key components of these guidelines align with international standards, but greater standardization and collaboration are needed to ensure responsible and ethical use of GAI in medical research and writing.</jats:p></jats:sec><jats:sec><jats:title>Abstract Figure</jats:title><jats:fig id=""ufig1"" position=""float"" fig-type=""figure"" orientation=""portrait""><jats:graphic xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""24303960v1_ufig1"" position=""float"" orientation=""portrait"" /></jats:fig></jats:sec>"
10.2139/ssrn.4912701,"Generative AI, Copyright and the AI Act",N/A
10.1007/s43681-023-00316-2,Technical challenges and perception: does AI have a PR issue?,"<jats:title>Abstract</jats:title><jats:p>Increasingly, models have been highlighted that not only disadvantage society but those whom the model was originally designed to benefit. An increasing number of legal challenges around the world illustrates this. A surge of recent work has focussed on the technical, legal or regulatory challenges but not necessarily the real-world day to day challenges for practitioners such as data collection or fairness by design. Since the publication of the Holstein et al.’s study in 2019, additional legislation, regulation and multiple bodies have been created to address practitioner challenge. This study asks what, if anything, has improved for practitioners between 2019 and 2022. Study 1 conducts an investigation into real-world needs within industry and asks whether practitioners are now able to mitigate challenges in a more robust manner. A further pilot study on the perception of AI examines whether perception of AI impacts practitioner work. The results show increasing and continuing interdisciplinary issues. Where increased regulation and legislation might have seemed reasonable, the result for practitioners is indecision and overwhelm. Based on these findings, we highlight directions for future research in this area. The most problematic area being human factors.</jats:p>"
10.1007/979-8-8688-0473-1_4,Building a Generative AI Application,N/A
10.1162/99608f92.2c8e7e81,Institutional Efforts to Help Academic Researchers Implement Generative AI in Research,N/A
10.4018/979-8-3693-3278-8.ch007,Foundations of Generative AI,"<jats:p>The chapter delves into the foundations of generative artificial intelligence (AI), offering an introductory overview and a nuanced understanding of its basic principles, history, and evolution. It navigates through core technologies underpinning generative AI, including neural networks, machine learning models, and key algorithms. The introduction traces generative AI's roots, unraveling its historical trajectory. It progresses to elucidate fundamental concepts, exploring neural networks' structures, functionalities, and applications. The study examines diverse machine learning models and pivotal algorithms crucial to generative AI, shedding light on their roles in generating innovative outputs. This abstract encapsulates a comprehensive journey through generative AI's core elements, serving as a foundational guide for understanding its origins, principles, and technologies.</jats:p>"
10.4018/979-8-3693-5578-7.ch007,Financial Frontiers,"<jats:p>Fintech players have always adopted the latest technology that evolve in the market to stay ahead in this competitive industry. In that adoption curve, we are now standing in the place where we are going to witness the adoption of the Generative AI as the next technological phenomenon that will shape the future of Fintech industry. In this chapter, the authors explore the following areas to getter a deeper insight into how Generative AI tends to play in Fintech industry: 1) Generative AI technologies involved in Fintech, 2) application of Generative AI in Fintech, 3) benefits of adopting Generative AI in Fintech, 4) case studies where Generative AI is adopted, 5) challenges and risks in adopting Generative AI, 6) mitigation strategies for the above said challenges and risks, and 7) future trends and developments.</jats:p>"
10.1007/s43681-022-00200-5,Trust and trustworthiness in AI ethics,"<jats:title>Abstract</jats:title><jats:p>Due to the extensive progress of research in artificial intelligence (AI) as well as its deployment and application, the public debate on AI systems has also gained momentum in recent years. With the publication of the <jats:italic>Ethics Guidelines for Trustworthy AI</jats:italic> (2019), notions of trust and trustworthiness gained particular attention within AI ethics-debates; despite an apparent consensus that AI should be trustworthy, it is less clear what trust and trustworthiness entail in the field of AI. In this paper, I give a detailed overview on the notion of trust employed in AI Ethics Guidelines thus far. Based on that, I assess their overlaps and their omissions from the perspective of practical philosophy. I argue that, currently, AI ethics tends to overload the notion of trustworthiness. It thus runs the risk of becoming a buzzword that cannot be operationalized into a working concept for AI research. What is needed, however, is an approach that is also informed with findings of the research on trust in other fields, for instance, in social sciences and humanities, especially in the field of practical philosophy. This paper is intended as a step in this direction.</jats:p>"
10.1007/s43681-024-00450-5,Ethical implications of AI in the Metaverse,"<jats:title>Abstract</jats:title><jats:p>This paper delves into the ethical implications of AI in the Metaverse through the analysis of real-world case studies, including Horizon Worlds, Decentraland, Roblox, Sansar, and Rec Room. The examination reveals recurring concerns related to content moderation, emphasising the need for a human-AI hybrid approach to strike a balance between creative freedom and user safety. Privacy and data protection emerge as crucial considerations, highlighting the importance of transparent communication and user data control for responsible AI implementation. Additionally, promoting inclusivity and diversity is emphasised, calling for transparent governance, diverse representation, and collaboration with ethics experts to ensure equitable AI practices. By addressing these specific ethical challenges, we can pave the way towards a responsible and user-centric Metaverse, maximising its potential while safeguarding user well-being and rights.</jats:p>"
10.1007/s43681-024-00463-0,Conversational hyperconvergence: an onlife evolution model for conversational AI agency,N/A
10.31234/osf.io/xj54a,Are Generative AIs like Midjourney and Sora creative? The “Generative AI Mary’s room” thought experiment.,"<p>This article examines the debate of whether generative AI systems like Midjourney and Sora can be considered truly creative or are merely remixing existing data. It reviews Finke, Ward, and Smith's creative cognition approach which proposed constraints on human creativity. A new thought experiment, the ""Generative AI Mary's Room"", is proposed to explore whether a person with comprehensive artistic knowledge but no real-world experience could produce original creative works. The argument is that one who believes generative AI is not creative because it works based solely on previous artistic data, should not believe that Mary would be creative either, otherwise, they are inferring mysterious properties to the human mind.</p>"
10.1007/s43681-024-00484-9,Addressing corrigibility in near-future AI systems,"<jats:title>Abstract</jats:title><jats:p>When we discuss future advanced autonomous AI systems, one of the worries is that these systems will be capable enough to resist external intervention, even when such intervention is crucial, for example, when the system is not behaving as intended. The rationale behind such worries is that such intelligent systems will be motivated to resist attempts to modify or shut them down so they can preserve their objectives. To mitigate and face these worries, we want our future systems to be corrigible, i.e., to tolerate, cooperate or assist many forms of outside correction. One important reason for considering corrigibility as an important safety property is that we already know how hard it is to construct AI agents with a generalized enough utility function; and the more advanced and capable the agent is, the more it is unlikely that a complex baseline utility function built into it will be perfect from the start. In this paper, we try to achieve corrigibility in (at least) systems based on known or near-future (imaginable) technology, by endorsing and integrating different approaches to building AI-based systems. Our proposal replaces the attempts to provide a corrigible utility function with the proposed corrigible software architecture; this takes the agency off the RL agent – which now becomes an RL solver – and grants it to the system as a whole.</jats:p>"
10.1007/s43681-023-00352-y,The consequences of AI hype,N/A
10.1515/9783111425078-204,About the Editors,N/A
10.1007/s43681-024-00526-2,The ethics of envisioning spam free email inboxes,N/A
10.1515/9783111425078-205,List of Contributors,N/A
10.1007/978-3-031-46238-2_27,Generative Models for Missing Data,N/A
10.1007/s43681-024-00499-2,Algorithm evaluation without autonomy,"<jats:title>Abstract</jats:title><jats:p>In <jats:italic>Algorithms &amp; Autonomy</jats:italic>, Rubel, Castro, and Pham (hereafter RCP), argue that the concept of autonomy is especially central to understanding important moral problems about algorithms. In particular, autonomy plays a role in analyzing the version of social contract theory that they endorse. I argue that although RCP are largely correct in their diagnosis of what is wrong with the algorithms they consider, those diagnoses can be appropriated by moral theories RCP see as in competition with their autonomy based theory. Most notably, proponents of consequentialism and virtue ethics can appropriate RCPs insights. This is good news because RCP’s social contract theory is vulnerable to a well known class of counterexamples. The most significant contribution of RCP, if I am right, is in their identification, presentation, and evaluation of concrete cases involving algorithms and not in the more controversial claims about theoretical ethics that RCP themselves see as central to what they are doing.</jats:p>"
10.2139/ssrn.4944690,Generative AI Regulation in the UK,N/A
10.31235/osf.io/eh9sk,Predicting the Self with Generative AI,"<p>Here I describe a first attempt to use generative artificial intelligence to build an oracle that predicts future self-descriptions from past self-descriptions. I did this as a small step toward addressing the bigger issue of how predictable we should expect individuals' lives to be. To address that vexing issue, I call for the construction and comparative testing of many general identity forecasting systems.</p>"
10.2139/ssrn.4887072,How Will Generative Ai Impact Communication?,N/A
10.54675/ewzm9535,Guidance for generative AI in education and research,N/A
10.1007/979-8-8688-0403-8,Generative Artificial Intelligence,N/A
10.1016/b978-0-44-321857-6.00024-2,Index,N/A
10.1201/9781032654829,Regulating Hate Speech Created by Generative AI,N/A
10.1148/ryai.07262023.podcast,Roundtable Discussion Series - Generative AI (Part 2),N/A
10.54377/aac6-c1f6,The challenges ahead for generative AI,N/A
10.31219/osf.io/2c48n,Generative AI and Research Integrity,"<p>GPT based text generators like ChatGPT or Microsoft Copilot have rapidly become a ""cultural sensation"". This document provides scientific background  and guidance on how to think critically and mindfully about these tools in academic writing and research.</p>"
10.31228/osf.io/bzsn4,Generative AI and Finding the Law,"<p>Legal information science requires, among other things, principles and theories. The article states six principles or considerations that any discussion of generative AI large language models and their role in finding the law must include. The article concludes that law librarianship will increasingly become legal information science and require new paradigms. In addition to the six principles, the article applies ecological holistic media theory to understand the relationship of the legal community’s cognitive authority, institutions, techné (technology, medium and method), geopolitical factors, and the past and future to understand the changes in this information milieu. The article also explains generative AI, and finally, presents some examples of generative AI responses to various legal research problems and the issues that present themselves in such circumstances.</p>"
10.3386/w31161,Generative AI at Work,N/A
10.1007/978-3-031-46238-2_9,Generative AI Applications in the Health and Well-Being Domain: Virtual and Robotic Assistance and the Need for Niche Language Models (NLMs),N/A
10.1148/ryai.06302023.podcast,Roundtable Discussion Series - Generative AI (Part 1),N/A
10.2139/ssrn.4860325,Generative AI and Remix: Difference and Repetition,N/A
10.1016/b978-0-44-321857-6.00023-0,Glossary,N/A
10.2139/ssrn.4886590,How will Generative AI impact Communication?,N/A
10.4018/979-8-3693-1565-1.ch007,For Better or for Worse?,"<jats:p>This chapter explores how the social implications of AI are being posited, often sensationalized as a threat to humanity, rather than being framed in something humanly designed that ought to remain within the control of its maker, transparent in terms of capacity to undertake complex decision making and which most importantly is accountable for every individual action made in terms of design and programming. The aims of the chapter are threefold, namely, to consider global ethics and the impact that AI could potentially have in terms of increasing societal inequalities in terms of existing infrastructure; to provide an insight into the developmental and progressive use of AI across organizational infrastructures such as global medicine and health and the military; finally, to embed the concept of ethical AI and the potential for its praxis across all areas of its integration.</jats:p>"
10.1007/s43681-022-00191-3,Correction: Operationalising AI governance through ethics-based auditing: an industry case study,N/A
10.1007/s43681-024-00462-1,AI ethics should be mandatory for schoolchildren,"<jats:title>Abstract</jats:title><jats:p>As society increasingly integrates artificial intelligence (AI) into its fabric, AI ethics education in primary schools becomes necessary. Drawing parallels between the integration of foundational subjects such as languages and mathematics and the pressing need for AI literacy, we argue for mandatory, age-appropriate AI education focusing on technical proficiency and ethical implications. Analogous to how sex and drug education prepare youth for real-world challenges and decisions, AI education is crucial for equipping students to navigate an AI-driven future responsibly. Our study delineates the ethical pillars, such as data privacy and unbiased algorithms, essential for students to grasp, and presents a framework for AI literacy integration in elementary schools. What is needed is a comprehensive, dynamic, and evidence-based approach to AI education, to prepare students for an AI-driven future.</jats:p>"
10.1007/s43681-021-00051-6,Two arguments against human-friendly AI,N/A
10.1007/s43681-024-00522-6,Sustainable AI and the third wave of AI ethics: a structural turn,"<jats:title>Abstract</jats:title><jats:p>With the introduction of the concept of Sustainable AI, considerations of the environmental impact of the technology have begun to enter AI ethics discussions. This, Aimee van Wynsberghe suggests, constitutes a new “third wave of AI ethics” which yet needs to be ushered in. In this paper, we ask what is entailed by Sustainable AI that should warrant such special accentuation. Do we find simply run-of-the-mill AI ethics applied to an environmental context? Or does Sustainable AI constitute a true a “game-changer”? We engage in a discussion about what the “waves of AI ethics” ought to mean and the criteria for labelling a wave as such. We argue that the third wave of AI ethics rests on a turn towards a structural approach for uncovering ethical issues on a broader scale, often paired with an analysis of power structures that prevent the uncovering of these issues.</jats:p>"
10.2139/ssrn.4484578,Task-Interdependencies between Generative Ai and Workers,N/A
10.1016/b978-0-44-321857-6.00004-7,Contents,N/A
10.31235/osf.io/rwtzs,Can Generative AI Improve Social Science?,"<p>Artificial intelligence that can produce realistic text,images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might transform social science research. In the first section of this article, I assess the potential of Generative AI to improve online experiments, agent-based models, and automated content analyses.I also discuss whether these tools may help social scientists perform literature reviews, identify novel research questions, and develop hypotheses to explain them. Next, I evaluate whether Generative AI can help social scientists with more mundane tasks such as acquiring advancedprogramming skills or writing more effective prose. In the second section of this article I discuss the limitations of Generative AI as well as how these tools might be employed by researchers in an ethical manner. I discuss how bias in the processes and data used to train these tools can negatively impact social science research as well as a rangeof other challenges related to accuracy, reproducibility,interpretability, and efficiency. I conclude by highlighting the need for increased collaboration between social scientists and artificial intelligence researchers--- not only to ensure that such tools are used in a safe and ethical manner, but also because the progress of artificial intelligence may require deeper understanding of theories of human behavior</p>"
10.21275/sr231021200626,Process Automation 2.0 with Generative AI Framework,N/A
10.1007/s43681-024-00478-7,ACESOR: a critical engagement in systems of oppression AI assessment tool,N/A
10.1007/s43681-024-00510-w,"Biden’s Executive Order on AI: strengths, weaknesses, and possible reform steps",N/A
10.4324/9781003260127-2,Generative AI in Court,N/A
10.1007/s00146-024-02028-5,Effects of generative AI on service occupations with social interaction,N/A
10.1002/aaai.12155,Generative AI: An AI paradigm shift in the making?,"<jats:title>Abstract</jats:title><jats:p>It is sometimes difficult to evaluate progress in Generative AI, that is, image generation and large language models. This may be because they represent a paradigm shift in AI, and the traditional ways of developing, evaluating, understanding, and deploying AI systems no longer apply. Instead, we need to develop new such approaches, possibly by extending those currently in use in cognitive neuroscience and psychology. In this manner, a new AI paradigm can be created, providing a significant leap in AI research and practice.</jats:p>"
10.1056/aip2400611,AI as an Ecosystem — Ensuring Generative AI Is Safe and Effective,N/A
10.1007/s43681-023-00371-9,Using structured ethical techniques to facilitate reasoning in technology ethics,N/A
10.1016/b978-0-44-321857-6.00010-2,Shape classification and synthetization via explainable AI,N/A
10.1016/b978-0-44-321857-6.00018-7,Synthetic terrain generation and AI-generated art,N/A
10.1007/s43681-024-00506-6,Book Review: Ethics of Privacy and Surveillance by Carissa Veliz,N/A
10.1007/978-3-031-46238-2_20,Generating 3D Reconstructions Using Generative Models,N/A
10.2139/ssrn.4897356,"AI4Tech: X-AI Enabling X-Tech with Human-like, Generative, Decentralized, Humanoid and Metaverse AI",N/A
10.1007/s43681-021-00106-8,Towards an ethics of AI in Africa: rule of education,N/A
10.1007/s43681-021-00130-8,‘Data dregs’ and its implications for AI ethics: Revelations from the pandemic,N/A
10.1007/s43681-023-00331-3,Operationalising AI ethics through the agile software development lifecycle: a case study of AI-enabled mobile health applications,"<jats:title>Abstract</jats:title><jats:p>Although numerous ethical principles and guidelines have been proposed to guide the development of artificial intelligence (AI) systems, it has proven difficult to translate these principles into actionable practices beyond mere adherence to ethical ideas. This is particularly challenging in the context of AI systems for healthcare, which requires balancing the potential benefits of the solution against the risks to patients and the wider community, including minorities and underserved populations. To address this challenge, we propose a shift from one-size-fits-all ethical principles to contextualized case-based ethical frameworks. This study uses an AI-enabled mHealth application as a case study. Our framework is built on existing ethical guidelines and principles, including the AI4People framework, the EU High-Level Expert Group on trustworthy AI, and wider human rights considerations. Additionally, we incorporate relational perspectives to address human value concerns and moral tensions between individual rights and public health. Our approach is based on ”ethics by design,” where ethical principles are integrated throughout the entire AI development pipeline, ensuring that ethical considerations are not an afterthought but implemented from the beginning. For our case study, we identified 7 ethical principles: fairness, agility, precision, safeguarding humanity, respect for others, trust and accountability, and robustness and reproducibility. We believe that the best way to mitigate and address ethical consequences is by implementing ethical principles in the software development processes that developers commonly use. Finally, we provide examples of how our case-based framework can be applied in practice, using examples of AI-driven mobile applications in healthcare.</jats:p>"
10.1007/978-3-031-46238-2_6,Image Rendering with Generative Adversarial Networks,N/A
10.4018/979-8-3693-0831-8.ch001,Generative AI in Higher Education,"<jats:p>This chapter provides a comprehensive exploration of generative artificial intelligence (AI), particularly focusing on its implications and applications in higher education. It discusses the evolution and fundamental concepts of AI, including large language models and their development, emphasizing the intricate processes involved in creating and refining these models. The chapter delves into the ethical considerations and potential biases inherent in AI systems, highlighting the importance of responsible AI development. Moreover, the chapter examines the transformative potential of generative AI in enhancing learning, creativity, and information processing in higher education settings.</jats:p>"
10.4018/979-8-3693-5578-7.ch010,Generative AI for Customized Public Policy in Maritime Transport,"<jats:p>Maritime transport is crucial for global trade, transporting over 90% of global goods by volume and facing challenges such as environmental impacts and security threats. Public policy is the key to addressing these issues through regulations that enhance safety, security, and sustainability. Integrating this sector with generative AI can significantly improve predictive maintenance, route optimisation, and accident prevention. AI technologies can process vast amounts of data for real-time decision-making, which is vital for the efficiency and safety of maritime operations. AI can lead to autonomous ships, optimise logistics, and reduce human error. However, integrating AI poses technical barriers such as cybersecurity, significant initial costs, and social hurdles such as job displacement fears. Effective public policy must evolve to address these challenges and promote AI integration while considering ethical and legal implications.</jats:p>"
10.1007/978-1-4842-9994-4_8,Diffusion Model and Generative AI for Images,N/A
10.4324/9781003482918-1,Using generative AI effectively in higher education,N/A
10.31968/hae.2024.05.38.445,Generative AI and Its Application Trends in the Field of History,N/A
10.1007/978-3-031-23035-6_2,Teaching Ethics Applied to AI from a Cultural Standpoint: What African “AI Ethics” for Africa?,"<jats:title>Abstract</jats:title><jats:p>Ethics applied to Artificial Intelligence (AI), improperly called AI ethics, is mainly addressed through a Western perspective focusing on continental philosophy. As a result, discussions on ethics applied to AI are shaped by the West.</jats:p>"
10.4018/979-8-3693-0831-8.ch002,Some Emerging Communication Roles for Generative AI,"<jats:p>Understanding generative AI (GAI) from a communication perspective is challenging given the breadth of issues and perspectives that have emerged. To help distill these ideas and focus our conversation, this chapter proposes thinking about GAI in terms of the different communication roles that it fulfills. The roles of expert, copilot, interlocutor, and channel are presented with illustrations of each role taken from other contributions to this volume. Across the academy, scholars have mobilized to understand generative artificial intelligence (GAI) and foresee how GAI may alter the horizon of higher education.</jats:p>"
10.1007/s43681-023-00380-8,"Equity, autonomy, and the ethical risks and opportunities of generalist medical AI",N/A
10.1007/s43681-022-00226-9,Rawlsian AI fairness loopholes,"<jats:title>Abstract</jats:title><jats:p>Researchers and industry developers in artificial intelligence (AI) and natural language processing (NLP) have uniformly adopted a Rawlsian definition of fairness. On this definition, a technology is fair if performance is maximized for the least advantaged. We argue this definition has considerable loopholes, which can be used to legitimize common practices in AI/NLP research that actively contributes to social and economic inequalities. Such practices include what we shall refer to as Subgroup Test Ballooning and Snapshot-Representative Evaluation. Subgroup Test Ballooning refers to the practice of initially tailoring a technology to a specific target group of technology-ready early adopters to collect feedback faster. Snapshot-Representative Evaluation refers to the practice of evaluating a technology on a representative sample of current end users. Both strategies may contribute to social and economic inequalities but are commonly justified using arguments familiar from political economics and grounded in Rawlsian fairness. We discuss an egalitarian alternative to Rawlsian fairness, as well as, more generally, the roadblocks on the path toward globally and socially fair AI/NLP research and development.</jats:p>"
10.1007/s43681-020-00011-6,Beyond the promise: implementing ethical AI,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) applications can and do have unintended negative consequences for businesses if not implemented with care. Specifically, faulty or biased AI applications risk compliance and governance breaches and damage to the corporate brand. These issues commonly arise from a number of pitfalls associated with AI development, which include rushed development, a lack of technical understanding, and improper quality assurance, among other factors. To mitigate these risks, a growing number of organisations are working on ethical AI principles and frameworks. However, ethical AI principles alone are not sufficient for ensuring responsible AI use in enterprises. Businesses also require strong, mandated governance controls including tools for managing processes and creating associated audit trails to enforce their principles. Businesses that implement strong governance frameworks, overseen by an ethics board and strengthened with appropriate training, will reduce the risks associated with AI. When applied to AI modelling, the governance will also make it easier for businesses to bring their AI deployments to scale.</jats:p>"
10.1007/s43681-020-00030-3,Who pays for ethical debt in AI?,N/A
10.1007/s43681-024-00483-w,"AI hype, promotional culture, and affective capitalism","<jats:title>Abstract</jats:title><jats:p>This article centres AI hype within promotional culture. It incorporates scholarship on hype, affect and emotion from media, communications and cultural studies, as well as from market studies, to pose the following questions: ‘What role does promotional culture play in AI hype cycles?’ ‘What are the main promotional forms of emotion evident in the 2020s AI hype cycle?’ And finally, ‘What are the ethical implications of promoting emotion in AI hype cycles?’ The article explores the growth of twenty-first century promotional culture, particularly in the global tech sector, before examining links between promotional culture, emotion, affect, media and capitalism. Drawing on interdisciplinary approaches, the article contends that AI hype has successfully persisted because now, more than ever, contemporary promotional culture strategically deploys emotions as part of affective capitalism, and the affective nature of a digital media infrastructure controlled by the tech sector. The ensuing analysis isolates different emotions circulated by AI hype, including doomsday hype, drawing on examples from the 2020s AI hype cycle. The article concludes by examining the ethics of promotional culture as part of the combined knowledge apparatus supporting value construction in AI.</jats:p>"
10.1007/s43681-021-00099-4,The impossible necessity of AI governance,N/A
10.1007/s43681-022-00160-w,"AI, alignment, and the categorical imperative",N/A
10.1007/s43681-024-00481-y,Tackling AI Hyping,"<jats:title>Abstract</jats:title><jats:p>The introduction of a new generation of AI systems has kicked off another wave of AI hype. Now that AI systems have added the ability to produce new content to their predictive capabilities, extreme excitement about their alleged capabilities and opportunities is matched only by long held fears about job loss and machine control.</jats:p><jats:p>We typically understand the dynamics of AI hype to be something that happens to us, but in this commentary, we propose to flip the script. We suggest that AI hype is not a social fact, but a widely shared practice. We outline some negative implications of this practice and suggest how these can be mitigated, especially with regards to shifting ways of knowing and learning about AI, in the classroom and beyond. Even though pedagogical efforts (broadly understood) have benefited from AI hyping (there is now more varied AI training than ever), such efforts can also help minimize the impacts of hyping on the public’s credulity toward extravagant claims made about AI’s potential benefits and dangers.</jats:p><jats:p>Below, we consider steps that can be taken to address this issue and illustrate pathways for more holistic AI educational approaches that participate to a lesser degree in the practice of AI hyping. We contend that designing better AI futures will require that AI hyping be blunted to enable grounded debates about the ways that AI systems impact people’s lives both now and in the near future.</jats:p>"
10.1007/s43681-024-00507-5,Metaverse ethics: exploring the social implications of the metaverse,"<jats:title>Abstract</jats:title><jats:p>The emergence of the metaverse transforms the way humans interact with computers; the metaverse brings about a new form of human-computer interaction that is more immersive, intuitive, and seamless. In the present paper we thus aim to elucidate the role of human-computer interactions in the age of the metaverse. New forms of human-computer interaction via the metaverse are beneficial for humans in many ways; at the same time, however, there are new types of social issues that are emerging as the metaverse develops and that need to be taken seriously. Specifically, we focus upon issues such as privacy, surveillance capitalism, cyber-syndromes, amplifications of other social problems, environmental problems, and discuss what regulations would be appropriate in order to balance the adequate development of the metaverse with the safety and security of it that is required for social good, in particular for sustainable development goals. We finally propose ethical design principles for the sustainable metaverse in order to address the aforementioned and other social issues.</jats:p>"
10.1007/s43681-024-00472-z,From applied ethics and ethical principles to virtue and narrative in AI practices,"<jats:title>Abstract</jats:title><jats:p>The question of how we can use ethics and ethical frameworks to avert the negative consequences of AI through guidance on human behaviour and the design of technological systems has recently been receiving increasing attention. The appropriate response to an ethics of AI has certainly been contentious. For some years the wisdom of deontology and utilitarianism in the ethics of technology has been questioned. Today, a kind of AI ethics principlism has gained a degree of widespread acceptance, yet it still invites harsh rejections in recent scholarship. In this paper, we wish to explore the contribution to an ethics of AI made by a narrative philosophy and ethics of technology inspired by the ‘little ethics’ of Paul Ricoeur, and virtue ethics of Alasdair MacIntyre, most recently and promisingly built upon by Wessel Reijers and Mark Coeckelbergh.  The objective of this paper is to examine the extent to which a narrative and virtue based ethics (or, VPD, i.e., virtuous practice design) might be a plausible candidate for the foundation of an ethics of AI, or rather ethical AI practice. This will be achieved by exploring the ways in which this approach can respond to some of the significant faults with or critiques of applied and principles and guidelines based ethical approaches to AI ethics.</jats:p>"
10.1007/s43681-022-00171-7,Operationalising AI governance through ethics-based auditing: an industry case study,"<jats:title>Abstract</jats:title><jats:p>Ethics-based auditing (EBA) is a structured process whereby an entity’s past or present behaviour is assessed for consistency with moral principles or norms. Recently, EBA has attracted much attention as a governance mechanism that may help to bridge the gap between principles and practice in AI ethics. However, important aspects of EBA—such as the feasibility and effectiveness of different auditing procedures—have yet to be substantiated by empirical research. In this article, we address this knowledge gap by providing insights from a longitudinal industry case study. Over 12 months, we observed and analysed the internal activities of AstraZeneca, a biopharmaceutical company, as it prepared for and underwent an ethics-based AI audit. While previous literature concerning EBA has focussed on proposing or analysing evaluation metrics or visualisation techniques, our findings suggest that the main difficulties large multinational organisations face when conducting EBA mirror classical governance challenges. These include ensuring harmonised standards across decentralised organisations, demarcating the scope of the audit, driving internal communication and change management, and measuring actual outcomes. The case study presented in this article contributes to the existing literature by providing a detailed description of the organisational context in which EBA procedures must be integrated to be feasible and effective.</jats:p>"
10.1007/s43681-024-00495-6,Manipulating Aggregate Societal values to Bias AI Social Choice Ethics,N/A
10.1007/s43681-022-00180-6,"Confucius, cyberpunk and Mr. Science: comparing AI ethics principles between China and the EU",N/A
10.21203/rs.3.rs-4540908/v1,Generative AI in Assessment: AI Detectors and Implications for Practice,"<title>Abstract</title>
        <p>Generative AI has garnered attention as a valuable tool in the field of education. However, educators have expressed concerns about the originality of texts submitted by students as assignments due to the potential use of generative AI in writing tasks. Texts in writing assignments can be categorized into three types: AI-generated, human-written, and mixed texts (a combination of AI-generated and human-written texts). The purpose of the current study is to address some concerns from educators on the detection of the texts submitted by students, i.e., the consistency in detection results and accuracy of the detection results. The subject of the current study was the texts submitted by students as assignments for a graduate course. Four detectors were used to analyze the texts. Our findings provided useful information for educators: 1) Within the same detector, the consistency of the detection results for three types of texts were all above 90%. 2) Among different detectors, the detection results of human-written texts exhibited the highest consistency, whereas mixed texts demonstrated the lowest consistency.3) For accuracy, AI-generated and human-written texts were higher than mixed texts. Implications for educational practice were discussed.</p>"
10.4324/9781003459026-2,AI Literacy,N/A
10.2139/ssrn.4918704,Regulating under Uncertainty: Governance Options for Generative AI,N/A
10.36227/techrxiv.171822467.73136863/v3,Generative AI predicts the Riemann zeta zero distribution,N/A
10.31234/osf.io/9yhwz,Techniques for supercharging academic writing with generative AI,"<p>Academic writing is an indispensable yet laborious part of the research enterprise. This article maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. It introduces a human–AI collaborative framework that delineates the rationale (“why”), process (“how”), and nature (“what”) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement, their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation), and the need for a learning mindset to avoid overreliance on AI. It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human–AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, it then describes effective prompting techniques for incorporating AI into the writing routine—outlining, drafting, and editing—as well as strategies for maintaining rigor and adhering to ethics and policies. Ultimately, the prudent integration of AI into academic writing can ease the communication burden, empower authors, accelerate discovery, and promote diversity in science.</p>"
10.21428/9885764c.16cc139d,The EU AI Act at a crossroads: generative AI as a challenge for regulation,N/A
10.1515/9781501519765-013,Chapter 9: Generative AI Cases and Examples,N/A
10.1007/s43681-021-00086-9,Preface,N/A
10.1016/b978-0-44-321857-6.00002-3,Front Matter,N/A
10.36227/techrxiv.171822467.73136863/v2,Generative AI predicts the Riemann zeta zero distribution,N/A
10.4018/979-8-3693-3278-8.ch004,Innovating Reality,"<jats:p>Generative artificial intelligence has enormous promise in business, marketing, finance, education, and healthcare sectors. It can have an impact on areas like consumer engagement and fraud detection. But it also poses difficult problems. Decision-making is hampered by technological barriers like data quality, explainability, and authenticity, as well as economic issues like income inequality and possible job loss. Privacy, bias, and misuse are all examples of ethical dilemmas. To address these, thorough norms that guarantee accountability, openness, and equity are needed. Meeting societal requirements and fostering collaboration requires advancing AI education and human-centric cooperation. Rules and guidelines that emphasise empathy, clarity, and ethical norms must be established to steer AI research and development toward responsible and ethical practices in order to effectively manage these obstacles.</jats:p>"
10.1117/12.3009202.13bde57c-0346-ee11-a99c-00505691c5e1,N/A,N/A
10.36227/techrxiv.171822467.73136863/v1,Generative AI predicts the Riemann zeta zero distribution,"<jats:p id=""p1"">The Transformer architecture of Generative AI is very successful in
predicting the distribution of Riemann zeta zero counts on consecutive
Gram intervals. We get accuracies of 0.998 in predicting a sequence of
ten consecutive zero counts. We tested with two ranges of Riemann zeta
zeros, t = 10^12 and t = 10^28. With special training for rare
events, we can get essentially full prediction. This shows that applying
the technique to more complex problems has great promise. We have used
very minimal computer resources compared to typical models in language
applications. With access to better resources, we can attack much more
important problems.</jats:p>"
10.21275/sr24703063340,Empowering Patients with AI-Driven Personalized Care: The Transformative Power of Generative AI and Healthcare Data Integration,N/A
10.1007/s43681-021-00087-8,Foreword,N/A
10.14742/apubs.2023.662,Exploring business students’ views of the use of generative AI in assignment writing,"<jats:p>
The rise of generative AI, particularly over the past few years, has raised notable issues about its use. This has been possibly most pronounced in academia, where there has been strong debate on the potential value of generative AI to augment learning outcomes versus the potential for academic dishonesty and devalued education. Whilst some papers have looked at students’ perspectives on the use of generative AI, there has been less focus exploring through what ethical perspectives or frames students see using generative AI in their tertiary education.


We interviewed and conducted focus groups and interviews with students enrolled in an Australian university business school, to explore the ethical frames through which they saw the use of generative AI. Focussing on three specific perspectives: Deontological, Consequentialism and Virtue Ethics, it emerged that no single perspective dominated, with students having a complex mix and latticework of ethical perspectives on its use, even within the same individual. We explore some potential implications for practice that emerged from the data, one of which is the role of the academic as moral exemplar. 
</jats:p>"
10.21275/sr24523234811,Salesforce Einstein GPT: Pioneering Generative AI in CRM Technology,N/A
10.4018/979-8-3693-0074-9.ch002,Unleashing the Potential,"<jats:p>Generative artificial intelligence, anchored by large language models (LLMs), is significantly altering the educational landscape. This chapter examines the impact of generative AI on education, illustrating its capability to create personalized content and transform learning environments. Despite concerns over academic dishonesty facilitated by LLMs, the chapter argues against a regressive stance and advocates for the constructive integration of AI into educational practices. By drawing on theories of learning, the chapter elucidates the pedagogical implications of generative AI and describes specific use cases in language learning, computer science, and mathematics. Highlighting both the potential and limitations of this emerging technology, the chapter posits that generative AI is not merely a disruptive force, but a revolutionary tool poised to redefine the methodologies of teaching and learning.</jats:p>"
10.4018/979-8-3693-3278-8.ch003,Navigating Uncharted Waters,"<jats:p>This chapter explores the evolution and future trajectory of Python-driven generative AI, highlighting Python's role in advancing this technology. It discusses the integration of Python with emerging technologies like neuromorphic computing and reinforcement learning, focusing on their potential to revolutionize art, design, and media. Through detailed analysis of the open-source implementation of a technology platform designed, the chapter provides insights into Python's facilitation of innovative AI applications. It addresses potential challenges and ethical considerations, along with the mitigation and call to action, emphasizing the importance of responsible innovation. The narrative underscores Python's influence in making advanced AI technologies accessible and scalable, preparing readers to engage with future developments in the field of generative AI.</jats:p>"
10.5220/0012729800003717,Generative AI Risk Management in Digital Economy,N/A
10.4324/9781003482918-12,Using generative AI agents for scalable roleplay activities in the health sciences,N/A
10.4324/9781003482918-16,Embracing generative AI in authentic assessment,N/A
10.4018/979-8-3693-1950-5.ch001,Revisiting the Key Components of Creativity Through Generative AI,"<jats:p>Today, advancements in artificial intelligence (AI) and its applications have resulted in its widespread use in creative industries. The goal of this study is to investigate the relationship between artificial intelligence and creativity through the perspective of Generative AI (GenAI) utilized in these industries. To perform this investigation, the study first introduces the concepts of creativity and its key components, along with GenAI and AI creativity. The study's analysis is founded on 14 key components of creativity identified in the previous literature. In the analysis section, the study examines whether these key components are present in today's GenAI tools, drawing on current debates about AI and AI-based applications. Additionally, the capabilities, limitations, and challenges of GenAI are investigated for each key component. In the discussion section, the study makes projections about potential problems that may be encountered when using GenAI and discusses possible redefinitions of key components in the future.</jats:p>"
10.4018/979-8-3693-2418-9.ch014,Transforming Assessments With Generative AI,"<jats:p>The potential of generative AI is evident in every field of life and education is also experiencing a paradigm shift. Generative AI is opening new ways of assessment and the tools that can create engaging and innovative contents. Through the promotion of adaptation and customization, generative AI is positioned to bring about a significant transformation in the educational process. This chapter sheds light on the significance of generative AI in assessment of higher education. It offers valuable insights into the possibilities for change and improvement in the field of educational evaluations, indicating its capacity to revolutionize the future of education.</jats:p>"
10.1007/s43681-024-00473-y,Algorithmic evidence in U.S criminal sentencing,N/A
10.1007/978-3-031-46238-2_10,Generative Adversarial Network Based Deep Learning Method for Machine Vision Inspection,N/A
10.1007/s43681-023-00318-0,Navigating fairness measures and trade-offs,"<jats:title>Abstract</jats:title><jats:p>To monitor and prevent bias in AI systems, we can use a wide range of (statistical) fairness measures. However, it is mathematically impossible to optimize all of these measures at the same time. In addition, optimizing a fairness measure often greatly reduces the accuracy of the system (Kozodoi et al., Eur J Oper Res 297:1083–1094, 2022). As a result, we need a substantive theory that informs us how to make these decisions and for what reasons. I show that by using Rawls’ notion of justice as fairness, we can create a basis for navigating fairness measures and the accuracy trade-off. In particular, this leads to a principled choice focusing on both the most vulnerable groups and the type of fairness measure that has the biggest impact on that group. This also helps to close part of the gap between philosophical accounts of distributive justice and the fairness literature that has been observed by (Kuppler et al. Distributive justice and fairness metrics in automated decision-making: How much overlap is there? arXiv preprint <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://arxiv.org/abs/2105.01441"">arXiv:2105.01441</jats:ext-link>, 2021), and to operationalise the value of fairness.</jats:p>"
10.1007/s43681-024-00458-x,The ethical wisdom of AI developers,"<jats:title>Abstract</jats:title><jats:p>This paper explores ethical wisdom in the artificial intelligence (AI) developer community. Despite robust literature about the need for virtue ethics approaches in AI development, little research has directly engaged with the developer community about their progress in this regard. We have thus conducted semi-structured interviews with a worldwide cohort of 40 developers, which focused on their awareness of ethics issues, how they navigate ethical challenges, and the barriers they encounter in developing ethical wisdom. We find developers are largely aware of the ethical territories they must navigate and the moral dilemmas they personally encounter, but they face limited and inconsistent resources for ethical guidance or training. Furthermore, there are significant barriers inhibiting the development of ethical wisdom in the AI developer community, including the industry’s fixation on innovation, the narrow scope of technical practice, limited provisions for reflection and dialogue, and incentive structures that prioritize profits and prestige. The paper concludes by emphasizing the need to address the gap in domain-specific ethical skill and provides recommendations for organizations, educators, and the AI developer community.</jats:p>"
10.1007/s43681-024-00490-x,Assuring AI safety: fallible knowledge and the Gricean maxims,"<jats:title>Abstract</jats:title><jats:p>In this paper we argue that safety claims, when justified by a safety case, are descriptive fallible knowledge claims. Even if the aim of a safety case was to justify infallible knowledge about the safety of a system, such infallible safety knowledge is impossible to attain in the case of AI-enabled systems. By their nature AI-enabled systems preclude the possibility of obtaining infallible knowledge concerning their safety or lack thereof. We suggest that one can communicate knowledge of an AI-enabled system’s safety by structuring their exchange according to Paul Grice’s Cooperative Principle which can be achieved via adherence to the Gricean maxims of communication. Furthermore, these same maxims can be used to evaluate the calibre of the exchange, with the aim being to ensure that communicating knowledge about an AI-enabled system’s safety is of the highest calibre, in short, that the communication is relevant, of sufficient quantity and quality, and communicated perspicuously. The high calibre communication of safety claims to an epistemically diverse group of stakeholders is vitally important given the increasingly participatory nature of AI-enabled system design, development and assessment.</jats:p>"
10.1007/s43681-024-00423-8,How AI hype impacts the LGBTQ + community,"<jats:title>Abstract</jats:title><jats:p>Hype around Artificial Intelligence (AI) has been a feature of this technology since its inception. However, the most recent wave of AI hype has been leveraged to encourage adoption of AI technologies that cause issues for marginalised communities. Hype is also a means to obfuscate real issues of bias, harm, and exploitation felt most sharply by marginalised communities when AI is implemented. This therefore raises the question of power imbalances as a feature of AI technologies as we currently know them. This paper will study the relationship of AI hype and marginalised communities, with particular emphasis on the LGBTQ + community, and look at the way that AI impacts on this community. This paper will pose two key questions: does hype affect marginalised communities, particularly hype around new technologies such as AI; and what impact does the LGBTQ + community experience as a result of hype. This paper will then move on to discuss areas that provide a focus for discourse of AI hype and the impact on the LGBTQ + community: policy and decision-making, the maintenance of the cisgender heteronormative (cishet) baseline, the ubiquity of a mythology of AI, and the role of market expansion.</jats:p>"
10.1007/s43681-024-00475-w,Evaluating approaches for reducing catastrophic risks from AI,"<jats:title>Abstract</jats:title><jats:p>According to a growing number of researchers, AI may pose catastrophic – or even existential – risks to humanity. Catastrophic risks may be taken to be risks of 100 million human deaths, or a similarly bad outcome. I argue that such risks – while contested – are sufficiently likely to demand rigorous discussion of potential societal responses. Subsequently, I propose four desiderata for approaches to the reduction of catastrophic risks from AI. The quality of such approaches can be assessed by their chance of success, degree of beneficence, degree of non-maleficence, and beneficent side effects. Then, I employ these desiderata to evaluate the promises, limitations and risks of alignment research, timelines research, policy research, halting or slowing down AI research, and compute governance for tackling catastrophic AI risks. While more research is needed, this investigation shows that several approaches for dealing with catastrophic AI risks are available, and where their respective strengths and weaknesses lie. It turns out that many approaches are complementary and that the approaches have a nuanced relationship to approaches to present AI harms. While some approaches are similarly useful for addressing catastrophic risks and present harms, this is not always the case.</jats:p>"
10.1007/s43681-023-00355-9,Scoring AI-generated policy recommendations with Risk-Adjusted Gain in Net Present Happiness,"<jats:title>Abstract</jats:title><jats:p>Ethical considerations for assessing the collective benefit of an AI’s policy recommendations are different from assessing the ethical consequences from interacting with an individual. The study of population ethics provides a framework for studying collective benefit or harm in abstract terms. Research into happiness has made significant strides in identifying some key drivers of subjective well-being as measured both individually and collectively across societies. This research examines models from population ethics and statistical studies of subjective well-being to create a measure of benefit with which to judge AI recommendations. These models include refining estimations of the interaction between cultural aspects and economic development and incorporating measures of inequality of happiness and satisfaction through a society. When the impacts of a proposed policy are simulated for multiple successive years, risk discounting is used to measure Net Present Happiness, thus solving the conundrum of considering future generations in ethical considerations as posed in population ethics. Lastly, the Risk-Adjusted Gain in Net Present Happiness is proposed as a reasonable approach to ranking AI policy recommendations and as an AI objective function.</jats:p>"
10.1007/s43681-021-00047-2,"Community-in-the-loop: towards pluralistic value creation in AI, or—why AI needs business ethics","<jats:title>Abstract</jats:title><jats:p>Today, due to growing computing power and the increasing availability of high-quality datasets, artificial intelligence (AI) technologies are entering many areas of our everyday life. Thereby, however, significant ethical concerns arise, including issues of fairness, privacy and human autonomy. By aggregating current concerns and criticisms, we identify five crucial shortcomings of the current debate on the ethics of AI. On the threshold of a third wave of AI ethics, we find that the field eventually fails to take sufficient account of the business context and deep societal value conflicts the use of AI systems may evoke. For even a perfectly fair AI system, regardless of its feasibility, may be ethically problematic, a too narrow focus on the ethical implications of technical systems alone seems insufficient. Therefore, we introduce a business ethics perspective based on the normative theory of contractualism and conceptualise ethical implications as conflicts between values of diverse stakeholders. We argue that such value conflicts can be resolved by an account of deliberative order ethics holding that stakeholders of an economic community deliberate the costs and benefits and agree on rules for acceptable trade-offs when AI systems are employed. This allows AI ethics to consider business practices, to recognise the role of firms, and ethical AI not being at risk to provide a competitive disadvantage or in conflict with the current functioning of economic markets. By introducing deliberative order ethics, we thus seek to do justice to the fundamental normative and political dimensions at the core of AI ethics.</jats:p>"
10.1007/s43681-024-00512-8,Command responsibility in military AI contexts: balancing theory and practicality,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) has found extensive applications to varying degrees across diverse domains, including the possibility of using it within military contexts for making decisions that can have moral consequences. A recurring challenge in this area concerns the allocation of moral responsibility in the case of negative AI-induced outcomes. Some scholars posit the existence of an insurmountable “responsibility gap”, wherein neither the AI system nor the human agents involved can or should be held responsible. Conversely, other scholars dispute the presence of such gaps or propose potential solutions. One solution that frequently emerges in the literature on AI ethics is the concept of command responsibility, wherein human agents may be held responsible because they perform a supervisory role over the (subordinate) AI. In the article we examine the compatibility of command responsibility in light of recent empirical studies and psychological evidence, aiming to anchor discussions in empirical realities rather than relying exclusively on normative arguments. Our argument can be succinctly summarized as follows: (1) while the theoretical foundation of command responsibility appears robust (2) its practical implementation raises significant concerns, (3) yet these concerns alone should not entirely preclude its application (4) they underscore the importance of considering and integrating empirical evidence into ethical discussions.</jats:p>"
10.21428/e4baedd9.9070dfe7,From Novel Chemicals to Opera,N/A
10.26226/m.6639c0e2eb191bbe9d92661f,Using Generative AI To Create Virtual Patients,N/A
10.1007/s43681-022-00241-w,Making sense of the conceptual nonsense ‘trustworthy AI’,N/A
10.1007/978-3-031-46238-2_12,Augmenting Data from Epileptic Brain Seizures Using Deep Generative Networks,N/A
10.1007/s43681-022-00148-6,"GPT-3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry","<jats:title>Abstract</jats:title><jats:p>This paper examines the ethical solutions raised in response to OpenAI’s language model Generative Pre-trained Transformer-3 (GPT-3) a year and a half from its release. I argue that hype and fear about GPT-3, even within the Natural Language Processing (NLP) industry and AI ethics, have often been underpinned by technologically deterministic perspectives. These perspectives emphasise the autonomy of the language model rather than the autonomy of human actors in AI systems. I highlight the existence of deterministic perspectives in the current AI discourse (which range from technological utopianism to dystopianism), with a specific focus on the two issues of: (1) GPT-3’s potential intentional misuse for manipulation and (2) unintentional harm caused by bias. In response, I find that a contextual approach to GPT-3, which is centred upon wider ecologies of societal harm and benefit, human autonomy, and human values, illuminates practical solutions to concerns about manipulation and bias. Additionally, although OpenAI’s newest 2022 language model InstructGPT represents a small step in reducing toxic language and aligning GPT-3 with user intent, it does not provide any compelling solutions to manipulation or bias. Therefore, I argue that solutions to address these issues must focus on organisational settings as a precondition for ethical decision-making in AI, and high-quality curated datasets as a precondition for less harmful language model outputs.</jats:p>"
10.4018/979-8-3693-2418-9.ch009,Generative AI in Curriculum Development in Higher Education,"<jats:p>This chapter explores the transformative role of Generative Artificial Intelligence (Generative AI) in reshaping the development of higher education curricula. Generative AI, as exemplified by advanced models like GPT-3, employs sophisticated algorithms to generate scientifically relevant content, surpassing traditional norms of teaching and learning. The overview delves into the fundamental principles of Generative AI, emphasizing the significance of generative models such as Generative Adversarial Networks (GANs) and the technical intricacies involved in their training. Essentially, the discourse on the significance of Generative AI in curriculum development underscores its disruptive potential in education. By providing personalized and adaptable pathways for growth, Generative AI addresses the diverse needs of students, fostering engagement and comprehension. It also underscores the role of Generative AI in overcoming limitations in traditional education, facilitating the creation of virtual laboratories and simulations that enhance hands-on learning.</jats:p>"
10.1007/978-3-031-55642-5_7,Generative AI for Software Development: A Family of Studies on Code Generation,N/A
10.1007/s43681-021-00122-8,Blind spots in AI ethics,"<jats:title>Abstract</jats:title><jats:p>This paper critically discusses blind spots in AI ethics. AI ethics discourses typically stick to a certain set of topics concerning principles evolving mainly around explainability, fairness, and privacy. All these principles can be framed in a way that enables their operationalization by technical means. However, this requires stripping down the multidimensionality of very complex social constructs to something that is idealized, measurable, and calculable. Consequently, rather conservative, mainstream notions of the mentioned principles are conveyed, whereas critical research, alternative perspectives, and non-ideal approaches are largely neglected. Hence, one part of the paper considers specific blind spots regarding the very topics AI ethics focusses on. The other part, then, critically discusses blind spots regarding to topics that hold significant ethical importance but are hardly or not discussed at all in AI ethics. Here, the paper focuses on negative externalities of AI systems, exemplarily discussing the casualization of clickwork, AI ethics’ strict anthropocentrism, and AI’s environmental impact. Ultimately, the paper is intended to be a critical commentary on the ongoing development of the field of AI ethics. It makes the case for a rediscovery of the strength of ethics in the AI field, namely its sensitivity to suffering and harms that are caused by and connected to AI technologies.</jats:p>"
10.1007/s43681-024-00520-8,AI ethics in a controversial industry: the case of gambling and its ethical paradox,N/A
10.1007/s43681-020-00031-2,Opening the path to ethics in artificial intelligence,N/A
10.1007/s43681-022-00162-8,A framework for assessing AI ethics with applications to cybersecurity,"<jats:title>Abstract</jats:title><jats:p>In the last few years many scholars, public and private organizations have been involved in the definition of guidelines and frameworks for individuating the principles to adopt in the development and deployment of AI systems. Some authors, however, noted that the effectiveness of these guidelines or ethical codes on the developer’s community is very marginal. One of the obstacles that opposes to the effective implementation of ethical principles is the lack of an approach for solving tensions which arise when principles are applied. A possible solution to such an issue could be the adoption of a risk-based approach which is also advocated by many sources. To our knowledge, no concrete proposals have been presented in literature on how to perform a risk-based ethical assessment. In this paper we contribute to close this gap by introducing a framework based on a qualitative risk analysis approach for assessing the ethical impact underneath the introduction of an innovation either technological or organizational in a system. We will also show how the framework can be used for individuating suitable safeguards to adopt for balancing potential ethical infringements that the innovation may entail once implemented. Some case studies in the cybersecurity context are also described for showing the effectiveness of our approach.</jats:p>"
10.1007/s43681-022-00220-1,Proportionality principle for the ethics of artificial intelligence,N/A
10.2217/fmai-2023-0004,Generative AI for medical imaging analysis and applications,"<jats:p> Generative AI plays a pivotal role in medical imaging analysis, enabling precise diagnosis, treatment planning and disease monitoring. Techniques like generative adversarial networks (GANs) and variational autoencoders (VAEs) enhance medical imaging by generating synthetic images, improving reconstruction, segmentation and facilitating disease diagnosis and treatment planning. Nonetheless, ethical, legal and regulatory concerns arise regarding patient privacy, data protection and fairness. This paper offers an overview of generative AI in medical imaging analysis, highlighting applications, challenges and case studies. It compares results with traditional methods and examines potential implications on healthcare policies. The paper concludes with recommendations for responsible implementation and suggests future research and development directions. </jats:p>"
10.4018/979-8-3693-0831-8.ch013,AI Monsters,"<jats:p>Research into perceptions of artificial intelligence (AI) by faculty and students outside of specific disciplines has been relatively sparse. With the recent release of ChatGPT in November 2022, there have been numerous inquiries into the role of generative AI (GAI), in particular. While a timely response is important, so is ensuring that the responses that universities and faculty are implementing are evidence based. In the spring 2023 semester, the authors surveyed 380 students and 276 faculty. The quantitative data was analyzed with implications for higher education, including student-faculty trust, academic integrity, and uncertainty. This chapter is an analysis of the open-ended responses, using “Monster Theory” as a framework for understanding the themes that underlie the perceptions evident in the responses. The authors “demonsterize” AI. This is a mix of promoting literacy, ethical and transparent use, and developing language that is mindful about practices that may either empower or disempower individuals.</jats:p>"
10.1007/s43681-021-00115-7,Thinking AI with a hammer. Kate Crawford’s Atlas of AI (2021),N/A
10.55248/gengpi.4.1223.123417,Revolutionizing Supply Chains Using Power of Generative AI,N/A
10.1007/979-8-8688-0419-9_18,"Copilot, Generative AI, and Future of Work",N/A
10.2139/ssrn.4814018,BILETA's Response to ICO’s Generative AI First Call for Evidence: The Lawful Basis for Web Scraping to Train Generative AI Models,N/A
10.4018/979-8-3693-1351-0.ch011,Empowering Teachers With Generative AI Tools and Support,"<jats:p>This chapter explores the potential of generative AI to transform education and empower teachers. It analyzes pedagogical enhancements enabled by these tools, including personalized and simulated learning, tailored assessments, teacher-student collaboration, and streamlined workflows. The author examines how AI's data-driven insights can boost responsiveness, motivation, inclusion, and experiential understanding. While promising, integrating AI demands thoughtful oversight to uphold humanistic values. The author emphasizes that teacher wisdom must direct implementation ethically, and learners need support developing AI literacy. Though rapidly advancing, generative tools should empower, not replace, educator and student agency. This chapter provides a balanced analysis of AI's possibilities and prudent perspectives for education. With ethical foundations uplifting expertise and learner voices alike, classrooms can judiciously leverage AI to expand responsive, enriched learning benefitting all.</jats:p>"
10.21275/sr24307081508,Unveiling the Potential of Generative AI in Revolutionizing Healthcare,N/A
10.4018/979-8-3693-1565-1.ch004,Ethical Considerations in the Educational Use of Generative AI Technologies,"<jats:p>This chapter provides an overview of the ethical considerations that should be taken into account while using generative AI technologies, specifically in the field of education, as well as concrete suggestions for programmers and end-users. Therefore, students, researchers, and academics in a variety of fields who are interested in the ethical aspects of generative AI will find this chapter useful, as it will also provide an overview of the existing ethical frameworks in the field of education. In that sense, this chapter can be viewed as a concise introduction to the current state of the ethical issues being studied and a proposal for balancing risks and opportunities.</jats:p>"
10.1162/99608f92.e360e42d,From Left Behind to Left Out: Generative AI or the Next Pain of the Unconnected,N/A
10.4018/979-8-3693-1351-0.ch001,Standing on the Shoulders of Generative AI,"<jats:p>Generative AI has been gaining popularity in 2023 and it is causing a disruption of various standards in the education system. While the pros and particularly the cons of this technology have been extensively debated, this chapter aims to explore the positive aspects of Generative AI—instead of advocating for a ban. This chapter will first provide an overview of the historical context and evolution of AI. It will be followed by a discussion of different types of Generative AI and the principles of co-creation with it. The uses of Generative AI in education will be described, focusing on the key stakeholders such as educators, students, educational administrators, and schools or institutions. Next, the chapter will explore the Generative AI application across different fields as well as various subjects in education. Several use cases, practices, and their associated benefits will be presented. Finally, the future and the implications of Generative AI will be discussed.</jats:p>"
10.31992/0869-3617-2024-33-2-31-53,Ethics and AI-Plagiarism in an Academic Environment: Students’ Understanding of Compliance with Author’s Ethics and the Problem of Plagiarism in the Process of Interaction with Generative Artificial Intelligence,"<jats:p>Everyday, artificial intelligence (AI) is being increasingly integrated into the teaching and learning process at Russian universities. The high level of quality of feedback from AI tools leads to the spread of AI plagiarism – unauthorized borrowing of generative AI materials – among students. The purpose of this study is to: a) highlight aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of plagiarism when interacting with generative AI; b) develop a questionnaire to determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism; c) conduct an online survey of university students, analyze and discuss the results obtained. The paper highlights five aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism when completing educational assignments and preparing research texts: a) students’ general understanding of the issues of compliance with author’s ethics and the problem of plagiarism in an academic environment; b) students’ experience of AI tools for educational purposes; c) students’ understanding of the problem of AI plagiarism and attitude towards borrowing materials from generative AI; d) teachers’ actions to prevent AI plagiarism among students; e) the policy of educational organizations regarding student compliance with ethics and AI plagiarism. An online questionnaire was developed to determine the degree to which students understand the issues of compliance with copyright ethics and the problem of AI plagiarism. 1,599 students from 29 universities of the Russian Federation took part in the survey. The results showed that in general, in the Russian student community, plagiarism is a widespread social phenomenon, many types of which are perceived by young people as a norm of academic behavior. Despite the relatively high awareness of students in the field of AI technologies, the extremely rare use by teachers of specialized subject disciplines of AI tools in the educational process I’d the reason for the current low level of spread of AI plagiarism in the academic environment. At the same time, it is necessary to state that students lack a systematic understanding of exactly how they can “legally” use generative AI materials and what exactly will be considered AI plagiarism. According to students, the importance of understanding the issues of compliance with author ethics and the problem of AI plagiarism will depend, on the one hand, on the actions of teachers to explain to students the rules for using generative AI materials, and on the other hand, the presence in universities of a regulatory framework regulating the field and the extent to which students use AI in the educational process.</jats:p>"
10.1007/s43681-022-00214-z,Ethics in human–AI teaming: principles and perspectives,"<jats:title>Abstract</jats:title><jats:p>Ethical considerations are the fabric of society, and they foster cooperation, help, and sacrifice for the greater good. Advances in AI create a greater need to examine ethical considerations involving the development and implementation of such systems. Integrating ethics into artificial intelligence-based programs is crucial for preventing negative outcomes, such as privacy breaches and biased decision making. Human–AI teaming (HAIT) presents additional challenges, as the ethical principles and moral theories that provide justification for them are not yet computable by machines. To that effect, models of human judgments and decision making, such as the agent-deed-consequence (ADC) model, will be crucial to inform the ethical guidance functions in AI team mates and to clarify how and why humans (dis)trust machines. The current paper will examine the ADC model as it is applied to the context of HAIT, and the challenges associated with the use of human-centric ethical considerations when applied to an AI context.</jats:p>"
10.1007/s43681-022-00224-x,"What kind of trust does AI deserve, if any?",N/A
10.2139/ssrn.4951041,Raising Ai Ethics Awareness Through an Ai Ethics Quiz for Software Practitioners,N/A
10.1007/s43681-020-00003-6,AI and ethics,N/A
10.1007/s43681-024-00502-w,Exploiting the margin: How capitalism fuels AI at the expense of minoritized groups,"<jats:title>Abstract</jats:title><jats:p>This paper explores the intricate relationship between capitalism, racial injustice, and artificial intelligence (AI), arguing that AI acts as a contemporary vehicle for age-old forms of exploitation. By linking historical patterns of racial and economic oppression with current AI practices, this study illustrates how modern technology perpetuates and deepens societal inequalities. It specifically examines how AI is implicated in the exploitation of marginalized communities through underpaid labor in the gig economy, the perpetuation of biases in algorithmic decision-making, and the reinforcement of systemic barriers that prevent these groups from benefiting equitably from technological advances. Furthermore, the paper discusses the role of AI in extending and intensifying the social, economic, and psychological burdens faced by these communities, highlighting the problematic use of AI in surveillance, law enforcement, and mental health contexts. The analysis concludes with a call for transformative changes in how AI is developed and deployed. Advocating for a reevaluation of the values driving AI innovation, the paper promotes an approach that integrates social justice and equity into the core of technological design and policy. This shift is crucial for ensuring that AI serves as a tool for societal improvement, fostering empowerment and healing rather than deepening existing divides.</jats:p>"
10.1007/s43681-023-00302-8,A global perspective on data powering responsible AI solutions in health applications,N/A
10.1007/s43681-020-00012-5,A choices framework for the responsible use of AI,N/A
10.1007/s43681-024-00449-y,‘Hypernudging’: a threat to moral autonomy?,"<jats:title>Abstract</jats:title><jats:p>It is well-recognised that cognitive irrationalities can be exploited to influence behaviour. ‘Hypernudging’ was coined by Karen Yeung to describe a powerful version of this phenomenon seen in digital systems that use large quantities of user data and machine learning to guide decision-making in highly personalised ways. Authors have worried about the societal impacts of the use of these capabilities at scale in commercial systems but have only begun to articulate them concretely. In this paper I look to elucidate one concern of this sort by focusing specifically on the employment of these techniques within social media and considering how it threatens our autonomy in forming moral judgments. By moral judgments I mean our judgments of someone’s actions or character as good versus bad. A threat to our autonomy in forming these is of real concern because moral judgments and their associated beliefs provide a critical backdrop for what is deemed acceptable in society, both individually and collectively and therefore what futures are possible and probable.</jats:p><jats:p>In the first two sections I introduce a psychological model that describes how humans reach moral judgments and the conditions under which it can and cannot be considered autonomous. In the third section I describe how hypernudging within a social media context creates the relevant problematic conditions so as to constitute a threat to our autonomy in forming moral judgments. In the fourth section I explore some practical measures that could be taken to protect moral autonomy. I conclude with some indicative evidence that this threat is not experienced uniformly across all societies, pointing to interesting future areas of research.</jats:p>"
10.1007/s43681-023-00329-x,"Algorithmic bias, generalist models, and clinical medicine",N/A
10.1007/s43681-022-00197-x,Machine learning AI systems and the virtue of inventiveness,N/A
10.1007/s43681-023-00377-3,Navigating in the moral landscape: analysing bias and discrimination in AI through philosophical inquiry,N/A
10.4018/979-8-3693-1351-0.ch008,Challenges and Limitations of Generative AI in Education,"<jats:p>This chapter presents a comprehensive literature review to identify the challenges and limitations of using generative artificial intelligence (GAI) in education. As a result of screening seven major citation databases, 476 studies were reached. Analysis was carried out on 25 studies selected according to the inclusion and exclusion criteria. Results showed that research on using GAI in education is mostly conducted at the higher education level. The number of studies focusing on lower levels of education is quite low. The challenges and limitations of artificial intelligence are more about general education rather than focusing on a specific discipline. ChatGPT was the most investigated GAI tool. The challenges and limitations of using GAI in education are grouped under five factors: ethics and safety; educational implementations; assessment and evaluation; equity and access; quality control and expertise.</jats:p>"
10.1007/s43681-021-00094-9,"With AI entering organizations, responsible leadership may slip!",N/A
10.1007/s43681-023-00271-y,Tools with general AI and no existential risk,"<jats:title>Abstract</jats:title><jats:p>According to philosophers and scientists in artificial intelligence (AI), future autonomous agents with general AI constitute an existential risk to humanity. This paper leverages results from neuroscience to propose tools with general AI and no existential risk. Tools answering questions in different domains enable the safe exploration of general AI’s enormous potential.</jats:p>"
10.4018/979-8-3693-8557-9.ch005,Ethical Frameworks for Use in Artificial Intelligence Systems,"<jats:p>The chapter examines the ethical landscape of AI, focusing on the development and deployment of Responsible AI systems. The increased deployment of systems powered by artificial intelligence technologies necessitates making work more predictable, trustworthy, and ethical. The development and deployment of such systems require the use of Responsible AI. A healthcare system, for example, is a classic system where AI is used in healthcare collaborations and decision-making. Studying the ethical landscape of AI allows us to understand the historical context and evolution of AI Ethics and the challenges and risks associated with such systems. Ethical frameworks examines the six AI assurance goals, the existing ethical frameworks and guidelines, and the core principles behind them. Future challenges and considerations for further research that emerge because of evolving concerns and potential misuse of AI systems are discussed.</jats:p>"
10.1007/s43681-022-00222-z,“AI for all” is a matter of social justice,N/A
10.1007/s43681-024-00454-1,Anthropomorphism and AI hype,"<jats:title>Abstract</jats:title><jats:p>As humans, we have an innate tendency to ascribe human-like qualities to non-human entities. Whilst sometimes helpful, such anthropomorphic projections are often misleading. This commentary considers how anthropomorphising AI contributes to its misrepresentation and hype. First, I outline three manifestations (terminology; imagery; and morality). Then, I consider the extent to which we ought to mitigate it.</jats:p>"
10.1007/s43681-020-00020-5,Brave: what it means to be an AI Ethicist,N/A
10.1007/s43681-020-00029-w,The interrelation between data and AI ethics in the context of impact assessments,"<jats:title>Abstract</jats:title><jats:p>In the growing literature on artificial intelligence (AI) impact assessments, the literature on data protection impact assessments is heavily referenced. Given the relative maturity of the data protection debate and that it has translated into legal codification, it is indeed a natural place to start for AI. In this article, we anticipate directions in what we believe will become a dominant and impactful forthcoming debate, namely, how to conceptualise the relationship between data protection and AI impact. We begin by discussing the value canvas i.e. the ethical principles that underpin data and AI ethics, and discuss how these are instantiated in the context of value trade-offs when the ethics are applied. Following this, we map three kinds of relationships that can be envisioned between data and AI ethics, and then close with a discussion of asymmetry in value trade-offs when privacy and fairness are concerned.</jats:p>"
10.1093/oxfordhb/9780190067397.013.29,A Human-Centered Approach to AI Ethics,"<p>This chapter explores a human-centered approach to AI and robot ethics. It demonstrates how a human-centered approach can resolve some problems in AI and robot ethics that arise from the fact that AI systems and robots have cognitive states, and yet have no welfare, and are not responsible. In particular, the approach allows that violence toward robots can be wrong even if robots cannot be harmed. More importantly, the approach encourages people to shift away from designing robots as if they were human ethical deliberators. Ultimately, the cognitive states of AI systems and robots may have a role to play in the proper ethical analysis of situations involving them, even if it is not by virtue of conferring welfare or responsibilities on those systems or robots.</p>"
10.1007/s43681-023-00308-2,An entryway into technology ethics. Sven Nyholm’s This is Technology Ethics: An Introduction (2023),N/A
10.1145/3600211.3604722,Typology of Risks of Generative Text-to-Image Models,N/A
10.21608/erjsh.2024.255372.1256,Generative vs. Non-Generative AI: Analyzing the Effects of AI on the Architectural Design Process,N/A
10.4018/979-8-3693-3278-8.ch006,Generative AI Unleashed,"<jats:p>Generative AI is omnipresent in our daily lives, influencing everything from media and entertainment to personal care and healthcare. The Fourth Industrial Revolution has brought about significant developments in artificial intelligence, such as ChatGPT, which have gained prominence and changed the way data is created and produced. This chapter highlights the current use of AI in natural language processing. These models are based on machine learning. This chapter examines these models' possible benefits to the economy. The potential influence of generative AI on productivity might boost the world economy. All industry sectors will be significantly impacted by generative AI. The economy as a whole can benefit greatly from generative AI's ability to boost labor productivity. We can utilize generative AI's promise to build a more just, inclusive, and sustainable future for all people if we are aware of how it affects society. This chapter offers a comprehensive analysis of the potential exposure of generative AI, in particular to generative pre-trained transformers.</jats:p>"
10.4018/979-8-3693-3278-8.ch002,AI in Visual Arts,"<jats:p>In the past two to four years there have been significant improvements made in AI due to improvements in computing capacity, resulting in an increase in public interest and funding for research. This has led the authors to embark on a project aimed at gaining a deeper understanding of these art generator AIs using generative algorithms such as GANs and VAEs. This chapter begins by providing a brief outline of the historical context and evolution of AI in the arts, tracing its trajectory from early experiments to its current advancements in visual artistry. The subsequent sections of the chapter explore the role of generative algorithms in each artistic medium, starting with an overview of algorithmic painting, followed by an examination of algorithmic sculpture and digital art. In addition, this chapter also introduces four novel features specific to AI-Art. In this chapter, the authors have drawn upon references from various tests published by esteemed researchers and practitioners to gather the necessary insights for the investigation and deepen our understanding.</jats:p>"
10.1007/s43681-022-00170-8,"Normative ethics, human rights, and artificial intelligence",N/A
10.1007/978-3-031-46238-2_5,"Generative Adversarial Network for Synthetic Image Generation Method: Review, Analysis, and Perspective",N/A
10.1007/s43681-024-00480-z,A semi-automated software model to support AI ethics compliance assessment of an AI system guided by ethical principles of AI,"<jats:title>Abstract</jats:title><jats:p>Compliance with principles and guidelines for ethical AI has a significant impact on companies engaged in the development of artificial intelligence (AI) systems. Specifically, ethics is a broad concept that continuously evolves over time and across cultural and geographical boundaries. International organisations (IOs), individual states, and private groups, all have an interest in defining the concept of ethics of AI. IOs, as well as regional and national bodies, have issued many decisions on AI ethics. Developing a system that complies with the ethical framework poses a complex challenge for companies, and the consequences of not complying with ethical principles can have severe consequences, making compliance with these requirements a key issue for companies. Furthermore, there is a shortage of technical tools to ensure that such AI systems comply with ethical criteria. The scarcity of ethics compliance checking tools for AI, and the current focus on defining ethical guidelines for AI development, has led us to undertake a proposal consisting in a semi-automated software model to verify the ethical compliance of an AI system’s code. To implement this model, we focus on the following important aspects: (1) a literature review to identify existing ethical compliance systems, (2) a review of principles and guidelines for ethical AI to determine the international and European views regarding AI ethics, and (3) the identification of commonly accepted principles and sub-principles of AI. These elements served to inform (4) our proposal for the design of a semi-automated software for verifying the ethical compliance of AI systems both at design-time (ethics-by-design perspective) and afterwards on the resulting software.</jats:p>"
10.1007/s43681-024-00446-1,Governing AI through interaction: situated actions as an informal mechanism for AI regulation,"<jats:title>Abstract</jats:title><jats:p>This article presents a perspective that the interplay between high-level ethical principles, ethical praxis, plans, situated actions, and procedural norms influences ethical AI practices. This is grounded in six case studies, drawn from fifty interviews with stakeholders involved in AI governance in Russia. Each case study focuses on a different ethical principle—privacy, fairness, transparency, human oversight, social impact, and accuracy. The paper proposes a feedback loop that emerges from human-AI interactions. This loop begins with the operationalization of high-level ethical principles at the company level into ethical praxis, and plans derived from it. However, real-world implementation introduces situated actions—unforeseen events that challenge the original plans. These turn into procedural norms via routinization and feed back into the understanding of operationalized ethical principles. This feedback loop serves as an informal regulatory mechanism, refining ethical praxis based on contextual experiences. The study underscores the importance of bottom-up experiences in shaping AI's ethical boundaries and calls for policies that acknowledge both high-level principles and emerging micro-level norms. This approach can foster responsive AI governance, rooted in both ethical principles and real-world experiences.</jats:p>"
10.1007/s43681-024-00427-4,Artificial intelligence (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI,"<jats:title>Abstract</jats:title><jats:p>As Artificial Intelligence (AI) rapidly advances and integrates into various domains, cybersecurity emerges as a critical field grappling with both the benefits and pitfalls of AI technologies. This paper explores the multifaceted dimensions of AI-driven cyberattacks, offering insights into their implications, mitigation strategies, underlying motivations, and profound societal impacts. The research centres on developing and presenting the AI Cybersecurity Dimensions (AICD) Framework, a comprehensive, multidimensional schema designed to guide academics, policymakers, and industry professionals in understanding and combating the evolving challenges posed by AI-driven cyber threats. The research unveils the complex dynamics of offensive AI, stressing the need for adaptive defences and ethical considerations. Concurrently, the study highlights adversarial AI threats, calling for proactive measures to address their potential ramifications. Through rigorous textual analyses and extensive literature reviews, the paper underscores the urgency for interdisciplinary approaches to bridge the technology-humanity chasm traditionally observed in cybersecurity discussions. By synthesising these diverse elements, the AICD Framework emerges as an instrumental tool for holistic understanding and practical interventions in the AI-infused cybersecurity landscape. The paper concludes with an urgent call for collaborative efforts in research and practice to navigate the intricate challenges and capitalise on the opportunities borne from the convergence of AI and cybersecurity.</jats:p>"
10.26904/rf-146-4362433383,Reinventing education through generative AI and XR,N/A
10.18665/sr.320394,Generative AI in Higher Education: The Product Landscape,N/A
10.2139/ssrn.4551316,The Chinese Path to Generative Ai Governance,N/A
10.1007/978-1-4842-9852-7_7,Understanding AI,N/A
10.3102/ip.24.2108118,Using Generative AI for Fairness Inquiry (Poster 4),N/A
10.3386/w31222,Generative AI and Firm Values,N/A
10.21275/sr24304172353,Enhancing Retail Theft Prevention with Generative AI Technologies,N/A
10.1007/s43681-022-00158-4,Agency in augmented reality: exploring the ethics of Facebook’s AI-powered predictive recommendation system,N/A
10.1007/s43681-021-00084-x,Putting AI ethics to work: are the tools fit for purpose?,"<jats:title>Abstract</jats:title><jats:p>Bias, unfairness and lack of transparency and accountability in Artificial Intelligence (AI) systems, and the potential for the misuse of predictive models for decision-making have raised concerns about the ethical impact and unintended consequences of new technologies for society across every sector where data-driven innovation is taking place. This paper reviews the landscape of suggested ethical frameworks with a focus on those which go beyond high-level statements of principles and offer practical tools for application of these principles in the production and deployment of systems. This work provides an assessment of these practical frameworks with the lens of known best practices for impact assessment and audit of technology. We review other historical uses of risk assessments and audits and create a typology that allows us to compare current AI ethics tools to Best Practices found in previous methodologies from technology, environment, privacy, finance and engineering. We analyse current AI ethics tools and their support for diverse stakeholders and components of the AI development and deployment lifecycle as well as the types of tools used to facilitate use. From this, we identify gaps in current AI ethics tools in auditing and risk assessment that should be considered going forward.</jats:p>"
10.4018/979-8-3693-2418-9.ch013,"The Interplay Between Creativity, Thinking Styles, Higher Education, and Generative AI","<jats:p>The use of artificial intelligence and especially Generative AI technology is creating radical change in education. This technology can impact studentss creativity, their cognition, preferred thinking styles, and higher order thinking skills. This chapter's main objective is to explore the link betweem creativity, thinking styles and skills, higher education, and GAI. The research question was explored using literature that was screened non-systematically using Google Scholar, EBSCO, and omni-linked databases for articles published between 2019 and 2024 due to the fact that it is a rapidly emerging field of study. It was found that there is an interplay between creativity, thinking styles, education, and GAI. GAI is reshaping our understanding of education and its impact on creativity and thinking styles. It is forcing humans to reconsider and review current and traditional teaching and learning pedagogies.</jats:p>"
10.21070/ups.3957,Design of a Generative AI Image Similarity Test Application and Handmade Images Using Deep Learning Methods,N/A
10.4018/979-8-3693-1351-0.ch022,Innovative Curriculum Development and Content Creation With Generative AI,"<jats:p>In the digital age, generative AI significantly influences various industries, especially education. It merges with traditional teaching methods, promising a new era of educational possibilities. This chapter delves into generative AI's impact on content creation and curriculum design, discussing its evolution and benefits like producing diverse, scalable educational materials and adaptive curricula personalized for learners. Real-world examples and case studies underscore its practical impact. Nonetheless, the chapter addresses ethical and pedagogical challenges and the complexity of integrating this technology. It also speculates on generative AI's future interactions with emerging technologies and its broader effects on education systems. Targeting educators, policymakers, and edtech enthusiasts, the chapter serves as a guide and insight provider for navigating this evolving landscape responsibly.</jats:p>"
10.1007/s43681-023-00416-z,Ethical AI governance: mapping a research ecosystem,"<jats:title>Abstract</jats:title><jats:p>How do we assess the positive and negative impacts of research about- or research that employs artificial intelligence (AI), and how adequate are existing research governance frameworks for these ends? That concern has seen significant recent attention, with various calls for change, and a plethora of emerging guideline documents across sectors. However, it is not clear what kinds of issues are expressed in research ethics with or on AI at present, nor how resources are drawn on in this process to support the navigation of ethical issues. Research Ethics Committees (RECs) have a well-established history in ethics governance, but there have been concerns about their capacity to adequately govern AI research. However, no study to date has examined the ways that AI-related projects engage with the ethics ecosystem, or its adequacy for this context. This paper analysed a single institution’s ethics applications for research related to AI, applying a socio-material lens to their analysis. Our novel methodology provides an approach to understanding ethics ecosystems across institutions. Our results suggest that existing REC models can effectively support consideration of ethical issues in AI research, we thus propose that any new materials should be embedded in this existing well-established ecosystem.</jats:p>"
10.1007/s43681-023-00348-8,"Human/AI relationships: challenges, downsides, and impacts on human/human relationships",N/A
10.1007/s43681-023-00389-z,Computer vision: AI imaginaries and the Massachusetts Institute of Technology,"<jats:title>Abstract</jats:title><jats:p>This paper explores the way in which computer scientists at the Massachusetts Institute of Technology (MIT) constructed visions of the future in 1960s America to direct the AI and computing research agendas. It argues that MIT computer scientists resisted attempts by the state to control the future of computing by fabricating imaginaries to covertly exert influence over the research environment. The paper examines the impact of the Cold War military–industrial complex on academia, which provided opportunities for research to take place whilst introducing challenges to autonomy. It makes the case that computer scientists such as Marvin Minsky and Fernando J. Corbato carefully shaped narratives across film, television and the media to promote desirable futures centering their own technical approaches. Acknowledging that instruments of the state appealed to the future to guide research towards strategically sensitive areas in the context of Cold War technoscientific contest, it asserts that intensifying ties between military and academic institutions afforded researchers both the latitude and motivation to construct independent visions of the future. In doing so, the paper aims to complicate assumptions about imaginaries as solely tools of governance by highlighting scientists' creativity in navigating institutional constraints to wrest back control of the future.</jats:p>"
10.4018/jdm.2020040105,Artificial Intelligence (AI) Ethics,"<p>Artificial intelligence (AI)-based technology has achieved many great things, such as facial recognition, medical diagnosis, and self-driving cars. AI promises enormous benefits for economic growth, social development, as well as human well-being and safety improvement. However, the low-level of explainability, data biases, data security, data privacy, and ethical problems of AI-based technology pose significant risks for users, developers, humanity, and societies. As AI advances, one critical issue is how to address the ethical and moral challenges associated with AI. Even though the concept of “machine ethics” was proposed around 2006, AI ethics is still in the infancy stage. AI ethics is the field related to the study of ethical issues in AI. To address AI ethics, one needs to consider the ethics of AI and how to build ethical AI. Ethics of AI studies the ethical principles, rules, guidelines, policies, and regulations that are related to AI. Ethical AI is an AI that performs and behaves ethically. One must recognize and understand the potential ethical and moral issues that may be caused by AI to formulate the necessary ethical principles, rules, guidelines, policies, and regulations for AI (i.e., Ethics of AI). With the appropriate ethics of AI, one can then build AI that exhibits ethical behavior (i.e., Ethical AI). This paper will discuss AI ethics by looking at the ethics of AI and ethical AI. What are the perceived ethical and moral issues with AI? What are the general and common ethical principles, rules, guidelines, policies, and regulations that can resolve or at least attenuate these ethical and moral issues with AI? What are some of the necessary features and characteristics of an ethical AI? How to adhere to the ethics of AI to build ethical AI?</p>"
10.21275/sr231114203705,Embracing Generative AI in Pharma Regulatory Affairs - An Industry Perspective,N/A
10.1007/s43681-021-00043-6,Sustainable AI: AI for sustainability and the sustainability of AI,"<jats:title>Abstract</jats:title><jats:p>While there is a growing effort towards AI <jats:italic>for</jats:italic> Sustainability (e.g. towards the sustainable development goals) it is time to move beyond that and to address the sustainability <jats:italic>of</jats:italic> developing and using AI systems. In this paper I propose a definition of Sustainable AI; Sustainable AI is a movement to foster change in the entire lifecycle of AI products (i.e. idea generation, training, re-tuning, implementation, governance) towards greater ecological integrity and social justice. As such, Sustainable AI is focused on more than AI applications; rather, it addresses the whole sociotechnical system of AI. I have suggested here that Sustainable AI is not about how to sustain the development of AI per say but it is about how to develop AI that is compatible with sustaining environmental resources for current and future generations; economic models for societies; and societal values that are fundamental to a given society. I have articulated that the phrase Sustainable AI be understood as having two branches; AI <jats:italic>for</jats:italic> sustainability and sustainability <jats:italic>of</jats:italic> AI (e.g. reduction of carbon emissions and computing power). I propose that Sustainable AI take sustainable development at the core of its definition with three accompanying tensions between AI innovation and equitable resource distribution; inter and intra-generational justice; and, between environment, society, and economy. This paper is not meant to engage with each of the three pillars of sustainability (i.e. social, economic, environment), and as such the pillars of sustainable AI. Rather, this paper is meant to inspire the reader, the policy maker, the AI ethicist, the AI developer to connect with the environment—to remember that there are environmental costs to AI. Further, to direct funding towards sustainable methods <jats:italic>of</jats:italic> AI.</jats:p>"
10.1007/s00146-024-02054-3,The rise of AI in job applications: a generative adversarial tug-of-war,N/A
10.1007/s11948-021-00323-8,"Mark Coeckelbergh, AI Ethics, Mit Press, 2021",N/A
10.1007/s43681-024-00549-9,Ethical risk for AI,"<jats:title>Abstract</jats:title><jats:p>The term ‘ethical risk’ often appears in discussions about the responsible development and deployment of artificial intelligence (AI). However, ethical risk remains inconsistently defined in this context, obscuring what distinguishes it from other forms of risk, such as social, reputational or legal risk, for example. In this paper we present a definition of ethical risk for AI as being any risk associated with an AI that may cause stakeholders to fail one or more of their ethical responsibilities towards other stakeholders. To support our definition, we describe how stakeholders have role responsibilities that follow from their relationship with the AI, and that these responsibilities are towards other stakeholders associated with the AI. We discuss how stakeholders may differ in their ability to make decisions about an AI, their exposure to risk, and whether they or others may benefit from these risks. Stakeholders without the ability to make decisions about the risks associated with an AI and how it is used are dependent on other stakeholders with this ability. This relationship places those who depend on decision-making stakeholders at ethical risk of being dominated by them. The decision-making stakeholder is ethically responsible for the risks their decisions about the AI impose on those affected by them. We illustrate our account of ethical risk for AI with two examples: AI-designed attachments for surgical robots that are optimised for treating specific patients, and self-driving ‘robotaxis’ that carry passengers on public roads.</jats:p>"
10.1007/s43681-024-00465-y,Three different types of AI hype in healthcare,"<jats:title>Abstract</jats:title><jats:p>Healthcare systems are the embodiment of big data – as evident in the logistics of resource management, estate maintenance, diagnoses, patient monitoring, research, etc. – such that human health is often heralded as one of the fields most likely to benefit from AI. Yet, the prevalence of hype – both positive and negative – risks undermining that potential by distracting healthcare policy makers, practitioners, and researchers from many of the non-AI factors that will determine its impact. Here we categorise AI hype in healthcare into three types that include both utopian and dystopian narratives and plot a series of more productive paths ahead by which to realise the potential of AI to improve human healthcare.</jats:p>"
10.2139/ssrn.4330244,"Generative Ai: Here to Stay, But for Good?",N/A
10.2139/ssrn.4929824,How to Responsibly Use Generative Ai in Grant Review,N/A
10.1007/s43681-024-00492-9,"Artificial intelligence, the common good, and the democratic deficit in AI governance","<jats:title>Abstract</jats:title><jats:p>There is a broad consensus that artificial intelligence should contribute to the common good, but it is not clear what is meant by that. This paper discusses this issue and uses it as a lens for analysing what it calls the “democracy deficit” in current AI governance, which includes a tendency to deny the inherently political character of the issue and to take a technocratic shortcut. It indicates what we may agree on and what is and should be up to (further) deliberation when it comes to AI ethics and AI governance. Inspired by the republican tradition in political theory, it also argues for a more active role of citizens and (end-)users: not only as participants in deliberation but also in ensuring, creatively and communicatively, that AI contributes to the common good.</jats:p>"
10.1007/s43681-022-00175-3,Responsible AI in automated credit scoring systems,N/A
10.1007/s43681-023-00313-5,To democratize or not to democratize AI? That is the question,"<jats:title>Abstract</jats:title><jats:p>This paper advances the debate surrounding whether to democratize AI and explores some of the challenges and benefits of democratization through community-based work and direct democracy. We contend that community-based strategies can incorporate local knowledge and control, thereby providing more effective AI solutions that are human-centric and less harmful. However, democratization needs to be approached with caution and care, since this process requires a deeper understanding of who participates, the decision domain, and the different realities at stake. Moreover, we highlight the importance of participation in AI development to ensure its legitimacy, considering the capacity of this technology to shape reality. We emphasize that participation should be more than just involving stakeholders or seeking input from users. Rather, participation should involve local narratives that generate knowledge and shape information landscapes, thereby producing a different, anti-Cartesian scene. We conclude by underscoring that the success of democratizing AI hinges on the careful delineation of the boundaries of participation, which should include the specific needs of the immediate context, the decision domain, and the various participants involved.</jats:p>"
10.1007/s43681-023-00350-0,Deepfake AI images: should deepfakes be banned in Thailand?,N/A
10.1007/s43681-024-00491-w,The prospects of using AI in euthanasia and physician-assisted suicide: a legal exploration,"<jats:title>Abstract</jats:title><jats:p>The Netherlands was the first country to legalize euthanasia and physician-assisted suicide. This paper offers a first legal perspective on the prospects of using AI in the Dutch practice of euthanasia and physician-assisted suicide. It responds to the Regional Euthanasia Review Committees’ interest in exploring technological solutions to improve current procedures. The specific characteristics of AI – the capability to process enormous amounts of data in a short amount of time and generate new insights in individual cases – may for example alleviate the increased workload of review committees due to the continuous increase of euthanasia cases. The paper considers three broad categories for the use of AI in the Dutch euthanasia practice: (1) the physician’s assessment of euthanasia requests, (2) the actual execution of euthanasia, and (3) the retrospective reviews of cases by the Regional Euthanasia Review Committees. Exploring the legal considerations around each avenue, both in the EU AI Act and the Dutch legal framework, this paper aims to facilitate the societal discussion on the role of technology in such deeply human decisions. This debate is equally relevant to other countries that legalized euthanasia (e.g. Belgium and Canada) or physician-assisted suicide (e.g. Switzerland and numerous states in the US).</jats:p>"
10.1007/s43681-022-00164-6,Knowledge management and ethical vulnerability in AI,N/A
10.1007/s43681-023-00349-7,The moral status of input and output discrimination,N/A
10.1093/oxfordhb/9780190067397.013.45,"The Ethics of AI in Biomedical Research, Patient Care, and Public Health","<p>This chapter explores ethical issues raised by the use of artificial intelligence (AI) in the domain of biomedical research, healthcare provision, and public health. The litany of ethical challenges that AI in medicine raises cannot be addressed sufficiently by current regulatory and ethical frameworks. The chapter then advances the systemic oversight approach as a governance blueprint, which is based on six principles offering guidance as to the desirable features of oversight structures and processes in the domain of data-intense biomedicine: adaptivity, flexibility, inclusiveness, reflexivity, responsiveness, and monitoring (AFIRRM). In the research domain, ethical review committees will have to incorporate reflexive assessment of the scientific and social merits of AI-driven research and, as a consequence, will have to open their ranks to new professional figures such as social scientists. In the domain of patient care, clinical validation is a crucial issue. Hospitals could equip themselves with “clinical AI oversight bodies” charged with the task of advising clinical administrators. Meanwhile, in the public health sphere, the new level of granularity enabled by AI in disease surveillance or health promotion will have to be negotiated at the level of targeted communities.</p>"
10.1007/s43681-022-00165-5,Trusting social robots,"<jats:title>Abstract</jats:title><jats:p>In this paper, I argue that we need a more robust account of our ability and willingness to trust social robots. I motivate my argument by demonstrating that existing accounts of trust and of trusting social robots are inadequate. I identify that it is the feature of a façade or deception inherent in our engagement with social robots that both facilitates, and is in danger of undermining, trust. Finally, I utilise the fictional dualism model of social robots to clarify that trust in social robots, unlike trust in humans, must rely on an independent judgement of product reliability.</jats:p>"
10.1007/s43681-024-00428-3,How can we design autonomous weapon systems?,N/A
10.1007/s43681-024-00486-7,A comprehensive study on navigating neuroethics in Cyberspace,N/A
10.1201/9781003503781-5,Conclusions,N/A
10.1093/oxfordhb/9780190067397.013.28,Social Failure Modes in Technology and the Ethics of AI,"<p>This chapter argues that, just as technological artefacts can break as a result of mechanical, electrical, or other physical defects not fully accounted for in their design, they can also break as a result of social defects not fully accounted for in their design. These failures resulting from social defects can be called <italic>social failures</italic>. The chapter then proposes a definition of <italic>social failure</italic> as well as a taxonomy of <italic>social failure modes</italic>—the underlying causes that lead to social failures. An explicit and detailed understanding of social failure modes, if properly applied in engineering design practice, could result in a fuller evaluation of the social and ethical implications of technology, either during the upstream design and engineering phases of a product, or after its release. Ideally, studying social failure modes will improve people’s ability to anticipate and reduce the rate or severity of undesirable social failures prior to releasing technology into the wild.</p>"
10.5220/0012427100003645,On Augmenting Scenario-Based Modeling with Generative AI,N/A
10.1007/s43681-021-00057-0,Ethics-by-design: the next frontier of industrialization,N/A
10.1007/s43681-022-00255-4,Apropos of “Speciesist bias in AI: how AI applications perpetuate discrimination and unfair outcomes against animals”,"<jats:title>Abstract</jats:title><jats:p>The present comment concerns a recent <jats:italic>AI &amp; Ethics</jats:italic> article which purports to report evidence of speciesist bias in various popular computer vision (CV) and natural language processing (NLP) machine learning models described in the literature. I examine the authors’ analysis and show it, ironically, to be prejudicial, often being founded on poorly conceived assumptions and suffering from fallacious and insufficiently rigorous reasoning, its appeal in large part relying on the extant consensus in the community.</jats:p>"
10.1007/979-8-8688-0456-4,The Early-Career Professional’s Guide to Generative AI,N/A
10.4324/9781003507949-4,AI Integration Through Portfolio Development,N/A
10.21428/e4baedd9.8fa181e9,AI for Musical Discovery,N/A
10.18260/1-2--46051,Leveraging the power of multi-modal AI technologies to build and scale generative AI applications,N/A
10.4018/979-8-3693-0502-7.ch011,Revolutionizing Conversational AI,"<jats:p>The emergence of advanced NLP models, like ChatGPT and other conversational AI models, has triggered a revolutionary transformation. This chapter explores the burgeoning field of ChatGPT applications, conducting a comprehensive analysis of their impact across various domains. The chapter assesses their capabilities, challenges, and potential uses, examining the underlying architecture and training methods that enable them to generate contextually relevant and coherent responses. Ethical considerations are also addressed, encompassing concerns about bias, misinformation, and user privacy in real-world conversations. The chapter also acknowledges drawbacks, including occasional inaccuracies or sensitive content generation. In conclusion, ongoing research is vital to enhance model robustness, user experience, and ethical deployment in conversational AI. ChatGPT and similar models are poised to reshape human-machine communication, fostering dynamic, engaging, and valuable conversations.</jats:p>"
10.1007/s43681-024-00468-9,Measuring adherence to AI ethics: a methodology for assessing adherence to ethical principles in the use case of AI-enabled credit scoring application,"<jats:title>Abstract</jats:title><jats:p>This article discusses the critical need to find solutions for ethically assessing artificial intelligence systems, underlining the importance of ethical principles in designing, developing, and employing these systems to enhance their acceptance in society. In particular, measuring AI applications’ adherence to ethical principles is determined to be a major concern. This research proposes a methodology for measuring an application’s adherence to acknowledged ethical principles. The proposed concept is grounded in existing research on quantification, specifically, Expert Workshop, which serves as a foundation of this study. The suggested method is tested on the use case of AI-enabled Credit Scoring applications using the ethical principle of transparency as an example. AI development, AI Ethics, finance, and regulation experts were invited to a workshop. The study’s findings underscore the importance of ethical AI implementation and highlight benefits and limitations for measuring ethical adherence. A proposed methodology thus offers insights into a foundation for future AI ethics assessments within and outside the financial industry, promoting responsible AI practices and constructive dialogue.</jats:p>"
10.1787/fa743141-en,"VC investments in generative AI start-ups have boomed since 2022, while VC investments overall and in AI start-ups reached a peak in 2021",N/A
10.1007/s43681-023-00383-5,"What would strong AI understand consent to mean, and what are the implications for sexbot rape?",N/A
10.1007/s43681-023-00336-y,Resolving the battle of short- vs. long-term AI risks,"<jats:title>Abstract</jats:title><jats:p>AI poses both short- and long-term risks, but the AI ethics and regulatory communities are struggling to agree on how to think two thoughts at the same time. While disagreements over the exact probabilities and impacts of risks will remain, fostering a more productive dialogue will be important. This entails, for example, distinguishing between evaluations of particular risks and the politics of risk. Without proper discussions of AI risk, it will be difficult to properly manage them, and we could end up in a situation where neither short- nor long-term risks are managed and mitigated.</jats:p>"
10.31891/2307-5740-2024-328-53,PRACTICAL RECOMMENDATION OF USING GENERATIVE AI IN BUSINESS,"<jats:p>Generative Artificial Intelligence (GenAI) is transforming the economic landscape by introducing innovative solutions that significantly enhance business efficiency, drive innovation, and create competitive advantages. This article delves into the economic implications and practical applications of GenAI across various business domains, including marketing, customer support, product design, and data analysis. By leveraging advanced AI models, companies can automate routine tasks, generate personalized content, and optimize operations, leading to substantial economic benefits. The implementation of GenAI necessitates a systematic approach, starting with business concept validation and progressing through stages of technical solution identification, proof of technology, and project planning. These stages ensure that AI solutions are economically viable, effective, and aligned with business objectives. Businesses can adopt different strategies to integrate GenAI, ranging from the rapid deployment of third-party applications to the development of customized in-house models. Each approach offers unique economic benefits, balancing customization and control with implementation time and value.
This article provides practical recommendations for strategy of implementing Generative AI, emphasizing the importance of careful economic analysis, stakeholder engagement, and continuous improvement. By adopting a structured approach and selecting the appropriate integration strategy, businesses can harness the transformative power of GenAI. This enables them to innovate and thrive in an increasingly competitive economic environment, positioning themselves as leaders in the digital age.</jats:p>"
10.21428/e4baedd9.3ad85f1c,The Productivity Effects of Generative AI: Evidence from a Field Experiment with GitHub Copilot,N/A
10.1177/14614448241234040,Impact of misinformation from generative AI on user information processing: How people understand misinformation from generative AI,"<jats:p> This study examines the impact of artificial intelligence (AI) on the ways in which users process and respond to misinformation in generative artificial intelligence (GenAI) contexts. Drawing on the heuristic–systematic model and the concept of diagnosticity, our approach examines a cognitive model for processing misinformation in GenAI. The study’s findings revealed that users with a high-heuristic processing mechanism, which affects positive diagnostic perception, were more likely to proactively discern misinformation than users with low-heuristic processing and low-perceived diagnosticity. When exposed to misinformation from GenAI, users’ perceived diagnosticity of misinformation can be accurately predicted by the ways in which they perform heuristic systematic evaluations. With this focus on misinformation processing, this study provides theoretical insights and relevant recommendations for firms to be more resilient in protecting users from the detrimental impacts of misinformation. </jats:p>"
10.1007/979-8-8688-0447-2_1,An Introduction to Chat and Generative AI,N/A
10.1007/s43681-022-00181-5,Needs-aware artificial intelligence: AI that ‘serves [human] needs’,N/A
10.1007/s43681-020-00007-2,"AI for climate: freedom, justice, and other ethical and political challenges",N/A
10.1007/s43681-024-00501-x,What does it mean to be good? The normative and metaethical problem with ‘AI for good’,N/A
10.1007/s43681-023-00401-6,AI risk assessment using ethical dimensions,N/A
10.1007/979-8-8688-0456-4_7,Navigating the AI Landscape,N/A
10.1007/979-8-8688-0456-4_2,A Brief AI History,N/A
10.1007/s00146-023-01830-x,Escape climate apathy by harnessing the power of generative AI,N/A
10.1007/s43681-023-00402-5,Conformity assessment under the EU AI act general approach,N/A
10.1007/s43681-023-00379-1,Moral consideration for AI systems by 2030,"<jats:title>Abstract</jats:title><jats:p>This paper makes a simple case for extending moral consideration to some AI systems by 2030. It involves a normative premise and a descriptive premise. The normative premise is that humans have a duty to extend moral consideration to beings that have a non-negligible chance, given the evidence, of being conscious. The descriptive premise is that some AI systems do in fact have a non-negligible chance, given the evidence, of being conscious by 2030. The upshot is that humans have a duty to extend moral consideration to some AI systems by 2030. And if we have a duty to do that, then we plausibly also have a duty to start preparing now, so that we can be ready to treat AI systems with respect and compassion when the time comes.</jats:p>"
10.51219/urforum.2023.rosario-moscato,Generative AI and Autonomous Agents: Opportunity and Risks,N/A
10.2139/ssrn.4887438,Addressing the Risks of Generative Ai for the Judiciary: The Accountability Framework(S) Under the EU Ai Act,N/A
10.33140/jmtcm.02.09.01,Synergy in Technology How Generative AI Augments the Capabilities of Customer Data Platforms,"<jats:p>In an era marked by data-driven decision-making, Customer Data Platforms (CDPs) have emerged as pivotal tools for aggregating and analyzing customer data. However, as these platforms grapple with increasingly complex data sets and realtime customer engagement demands, there is a pressing need for more advanced, scalable solutions. This study explores the synergy between Generative Artificial Intelligence (AI) and CDPs, aiming to understand how the integration of generative algorithms can augment the capabilities of these platforms. Employing a multi-method research approach, including case studies, empirical analyses, and expert interviews, this paper investigates various applications of Generative AI within CDPs, such as data augmentation, real-time decision-making, and customer personalization. Moreover, the ethical implications of using generative algorithms, especially concerning data privacy and security, are critically examined. The study reveals that Generative AI can significantly enhance the functionality, performance, and efficiency of CDPs while also posing new questions around ethical considerations. Our findings offer invaluable insights for businesses, marketers, and technologists seeking to leverage the synergistic potential of these two advanced technological paradigms.</jats:p>"
10.1515/9783111425078,Generative AI and LLMs,N/A
10.1007/s00146-024-01948-6,"The work of art in the age of generative AI: aura, liberation, and democratization",N/A
10.1007/s43681-021-00070-3,A critique of the ‘as–if’ approach to machine ethics,"<jats:title>Abstract</jats:title><jats:p>In this paper, I argue that the replication of the effect of ethical decision-making is insufficient for achieving functional morality in artificial moral agents (AMAs). This approach is named the “as–if” approach to machine ethics. I object to this approach on the grounds that the “as if” approach requires one to commit to substantive meta-ethical claims about morality that are at least unwarranted, and perhaps even wrong. To defend this claim, this paper does three things: 1. I explain Heidegger’s Enframing [<jats:italic>Gestell</jats:italic>] and my notion of “Ready-Ethics,” which, in combination, can hopefully provide a plausible account for the motivation behind the “as if” approach; 2. I go over specific examples of Ethical AI projects to show how the “as if” approach commits these projects to versions of moral generalism and moral naturalism. I then explain the flaws of the views that the “as if” approach necessitates, and suggest that they cannot account for the justificatory process crucial to human moral life. I explain how Habermas’ account of the justificatory process could cast doubt on the picture of morality that the meta-ethical views of the “as if” approach proposes; 3. Finally, I defend the relevance of discussing these topics for the purpose of functional morality in AMAs.</jats:p>"
10.1007/s43681-020-00022-3,Management perspective of ethics in artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>This research addressed the management awareness about the ethical and moral aspects of artificial intelligence (AI). It is a general trend to speak about AI, and many start-ups and established companies are communicating about the development and implementation of AI solutions. Therefore, it is important to consider different perspectives besides the technology and data as the key elements for AI systems. The way in which societies are interacting and organising themselves will change. Such transformations require diverse perspectives from the society and particularly from AI system developers for shaping the humanity of the future. This research aimed to overcome this barrier with the answers for the question: What kind of awareness does the management of AI companies have about the social impact of its AI product or service? The central research question was divided into five sub-questions that were answered by a fundamental literature review and an empirical research study. This covered the management understanding of the terms moral, ethics, and artificial intelligence; the internal company prioritization of moral and ethics; and the involved stakeholders in the AI product or service development. It analysed the known and used ethical AI guidelines and principles. In the end, the social responsibility of the management regarding AI systems was analysed and compared.</jats:p>"
10.1007/s43681-023-00337-x,The E.U.’s artificial intelligence act: an ordoliberal assessment,N/A
10.1007/s43681-024-00535-1,Capturing the unobservable in AI development: proposal to account for AI developer practices with ethnographic audit trails (EATs),"<jats:title>Abstract</jats:title><jats:p>The prevalence of artificial intelligence (AI) tools has inspired social studies researchers, ethicists, and policymakers to seriously examine AI’s sociopolitical and ethical impacts. AI ethics literature provides guidance on which ethical principles to implement via AI governance; AI auditing literature, especially ethics-based auditing (EBA), suggests methods to verify if such principles are respected in AI model development and deployment. As much as EBA methods are abundant, I argue that most currently take a <jats:italic>top-down</jats:italic> and <jats:italic>post-hoc</jats:italic> approach to AI model development: Existing EBA methods mostly assume a preset of high-level, abstract principles that can be applied universally across contexts; meanwhile, current EBA is only conducted after the development or deployment of AI models. Taken together, these methods do not sufficiently capture the very developmental practices surrounding the constitution of AI models on a day-to-day basis. What goes on in an AI development space and the very developers whose hands write codes, assemble datasets, and design model architectures remain unobserved and, therefore, uncontested. I attempt to address this lack of documentation on AI developers’ day-to-day practices by conducting an ethnographic “AI lab study” (termed by Florian Jaton), demonstrating just how much context and empirical data can be excavated to support a whole-picture evaluation of AI models’ sociopolitical and ethical impacts. I then propose a new method to be added to the arsenal of EBA: Ethnographic audit trails (EATs), which take a <jats:italic>bottom-up</jats:italic> and <jats:italic>in-progress</jats:italic> approach to AI model development, capturing the previously unobservable developer practices.</jats:p>"
10.1007/s43681-023-00366-6,Reactive agency and technology,"<jats:title>Abstract</jats:title><jats:p>Is there room for genuine human agency in a world populated by almost incessant technological distraction and influence? It often feels as though our technological landscape is pulling us in a number of directions, and that our agency is more a function of us <jats:italic>reacting</jats:italic> to the world as opposed to us exerting our will. In this paper I, explore what it would mean to take these contextual factors seriously and bake them into an account of agency. That is, what if agency is reactive <jats:italic>all the way down</jats:italic>? This is a proposal made by Rüdiger Bittner, who argues that the reason(s) for action are responses to states of affairs in the world. This is in contrast to ‘standard’ views of agency, which explain actions with things like beliefs and desires. Ultimately, I find such a reactive account of agency implausible. However, I think it reveals a potential solution to the ‘new’ problem of all-pervasive technologies: a reactive account does not see these technologies <jats:italic>necessarily</jats:italic> as a threat, but rather focusses our attention on the <jats:italic>ways</jats:italic> in which they change and shape our available context and our possibility to act. While I argue the reactive account goes too far, what I take from it is that our environment offers us various possibilities for action (in the form of affordances), and that we ought to take this seriously in our thinking both about agency and about the impacts of technology. Moreover, there is something to learn from our tendency to ‘fall’ for various ‘temptations’ in our environment, and this justifies further reflection on not only the design of different technologies, but whether such technologies ought to exist at all.</jats:p>"
10.1007/s43681-023-00351-z,Can machines be trustworthy?,"<jats:title>Abstract</jats:title><jats:p>AI regulators promote ‘trustworthy AI’, but what exactly does trustworthy AI mean, and what does it have to do with trust? Many philosophers argue that the phrase is a contradiction of terms. Trust, unlike reliance, is said to be a uniquely human relationship involving direct responsiveness or intent. I argue that the objective of trustworthy AI can be real trust in the general sense of Karen Jones and others, and very similar to the kind of trust we place in institutions. The idea that trustworthiness does not apply to machines, stems from a <jats:italic>petitio principii</jats:italic> fallacy. We show how to escape this fallacy, providing a better and less anthropomorphic definition of trustworthiness. We briefly discuss how transparency modulates trustworthiness on our revised definition, as well as a possible challenge from intentionality.</jats:p>"
10.4337/9781803926728.00023,"The AI Imaginary: AI, Ethics, and Communication",N/A
10.1007/s43681-021-00079-8,"Converged AI, IoT, and blockchain technologies: a conceptual ethics framework",N/A
10.1007/s43681-024-00567-7,Robot warfare: the (im)permissibility of autonomous weapons systems,N/A
10.1007/s43681-023-00413-2,Ethical and preventive legal technology,"<jats:title>Abstract</jats:title><jats:p>Preventive Legal Technology (PLT) is a new field of Artificial Intelligence (AI) investigating the <jats:italic>intelligent prevention of disputes</jats:italic>. The concept integrates the theories of <jats:italic>preventive law</jats:italic> and <jats:italic>legal technology</jats:italic>. Our goal is to give ethics a place in the new technology. By <jats:italic>explaining</jats:italic> the decisions of PLT, we aim to achieve a higher degree of <jats:italic>trustworthiness</jats:italic> because explicit explanations are expected to improve the level of <jats:italic>transparency</jats:italic> and <jats:italic>accountability</jats:italic>. Trustworthiness is an urgent topic in the discussion on doing AI research ethically and accounting for the regulations. For this purpose, we examine the limitations of rule-based explainability for PLT. Hence, our Problem Statement reads: <jats:italic>to what extent is it possible to develop an explainable and trustworthy Preventive Legal Technology?</jats:italic> After an insightful literature review, we focus on case studies with applications. The results describe (1) the effectivity of PLT and (2) its responsibility. The discussion is challenging and multivariate, investigating deeply the relevance of PLT for LegalTech applications in light of the development of the AI Act (currently still in its final phase of process) and the work of the High-Level Expert Group (HLEG) on AI. On the ethical side, explaining AI decisions for small PLT domains is clearly possible, with direct effects on trustworthiness due to increased transparency and accountability.</jats:p>"
10.1007/s43681-024-00426-5,Does attitude towards plagiarism predict aigiarism using ChatGPT?,N/A
10.1007/s43681-024-00541-3,Algorithmic fairness in predictive policing,"<jats:title>Abstract</jats:title><jats:p>The increasing use of algorithms in predictive policing has raised concerns regarding the potential amplification of societal biases. This study adopts a two-phase approach, encompassing a systematic review and the mitigation of age-related biases in predictive policing. Our systematic review identifies a variety of fairness strategies in existing literature, such as domain knowledge, likelihood function penalties, counterfactual reasoning, and demographic segmentation, with a primary focus on racial biases. However, this review also highlights significant gaps in addressing biases related to other protected attributes, including age, gender, and socio-economic status. Additionally, it is observed that police actions are a major contributor to model discrimination in predictive policing. To address these gaps, our empirical study focuses on mitigating age-related biases within the Chicago Police Department's Strategic Subject List (SSL) dataset used in predicting the risk of being involved in a shooting incident, either as a victim or an offender. We introduce Conditional Score Recalibration (CSR), a novel bias mitigation technique, alongside the established Class Balancing method. CSR involves reassessing and adjusting risk scores for individuals initially assigned moderately high-risk scores, categorizing them as low risk if they meet three criteria: no prior arrests for violent offenses, no previous arrests for narcotic offenses, and no involvement in shooting incidents. Our fairness assessment, utilizing metrics like Equality of Opportunity Difference, Average Odds Difference, and Demographic Parity, demonstrates that this approach significantly improves model fairness without sacrificing accuracy.</jats:p>"
10.1007/s43681-024-00530-6,Social botnets and the challenges of cyber situation awareness,N/A
10.1007/s43681-023-00388-0,This season’s artificial intelligence (AI): is today’s AI really that different from the AI of the past? Some reflections and thoughts,N/A
10.2139/ssrn.4802313,Generative AI is Doomed,N/A
10.1007/s11948-021-00337-2,"Correction to: Mark Coeckelbergh, AI Ethics, Mit Press, 2021",N/A
10.1101/2024.04.26.24306470,Medical Diagnosis Coding Automation: Similarity Search vs. Generative AI,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Objective</jats:title><jats:p>This study aims to predict ICD-10-CM codes for medical diagnoses from short diagnosis descriptions and compare two distinct approaches: similarity search and using a generative model with few-shot learning.</jats:p></jats:sec><jats:sec><jats:title>Materials and Methods</jats:title><jats:p>The text-embedding-ada-002 model was used to embed textual descriptions of 2023 ICD-10-CM diagnosis codes, provided by the Centers provided for Medicare &amp; Medicaid Services. GPT-4 used few-shot learning. Both models underwent performance testing on 666 data points from the eICU Collaborative Research Database.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The text-embedding-ada-002 model successfully identified the relevant code from a set of similar codes 80% of the time, while GPT-4 achieved a 50 % accuracy in predicting the correct code.</jats:p></jats:sec><jats:sec><jats:title>Discussion</jats:title><jats:p>The work implies that text-embedding-ada-002 could automate medical coding better than GPT-4, highlighting potential limitations of generative language models for complicated tasks like this.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>The research shows that text-embedding-ada-002 outperforms GPT-4 in medical coding, highlighting embedding models’ usefulness in the domain of medical coding.</jats:p></jats:sec>"
10.1007/s43681-024-00448-z,Regulating autonomous and AI-enabled weapon systems: the dangers of hype,N/A
10.2139/ssrn.4887768,A Blueprint for Auditing Generative AI,N/A
10.36227/techrxiv.171198062.20183635/v1,Bane and boon of hallucinations in context of generative AI,N/A
10.31228/osf.io/bheqw,Generative AI Large Language Models and Research the Law,"<p>Generative AI Large Language Models (LLMs) are rapidly influencing the legal research landscape, challenging the traditional cognitive authority within the profession. This article explores how these models, despite their potential to simplify the vast amount of legal information, may also introduce risks of misinformation due to AI ""hallucinations"" and the advantages and limitations of retrieval-augmented generation (RAG) systems. By examining various legal research problems, the article highlights both the capabilities and shortcomings of generative AI in legal contexts. It emphasizes the need for vigilance, critical thinking, and a continued reliance on traditional research methods to mitigate the risks associated with over-reliance on AI-generated content. As the legal profession confronts an overwhelming influx of information, the role of AI in legal research will likely grow, making it crucial to balance cognitive ease with accuracy and reliability.</p>"
10.5204/thesis.eprints.250826,Enhancing OCT retinal and choroidal segmentation with deep generative AI models,"<jats:p>This thesis investigated the application of generative AI models to improve the accuracy and robustness of tools applied to optical coherence tomography (OCT) image analysis tasks such as retinal layer segmentation. New, high quality and diverse images are generated using the AI models leading to improvements as well as new tools and techniques to enhance these medical image analysis tasks. The findings of this research will aid both clinicians and researchers who work with medical data to improve the accuracy, speed and automation of relevant analysis tasks, particularly where data is scarce.</jats:p>"
10.36227/techrxiv.171746875.59016695/v1,"Generative AI: Definition, Concepts, Applications, and Future Prospects","<jats:p id=""p1"">Generative Artificial Intelligence (AI) represents a significant
advancement in AI, enabling the creation of synthetic data that closely
mimics real data. This article provides a comprehensive overview of
generative AI, including its definition and core concepts such as
Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs),
and autoregressive models. It explores applications across industries,
including content creation, data augmentation, healthcare, gaming,
fashion, finance, retail, cybersecurity, transportation, energy,
education, entertainment, law, agriculture, real estate, and
manufacturing. The article discusses future prospects of generative AI,
highlighting enhanced creativity, improved human-AI collaboration, and
personalized experiences. It also addresses dependencies and challenges,
including data quality, computational resources, and ethical
considerations. The benefits for businesses adopting generative AI are
examined, emphasizing competitive advantage, enhanced customer
experience, accelerated innovation, and improved decision-making. The
article concludes by outlining the potential return on investment
through increased efficiency, revenue growth, market expansion, and risk
mitigation.</jats:p>"
10.2139/ssrn.4871337,The Opportunities of Generative Ai in Cross-Border E-Commerce,N/A
10.1007/s43681-024-00438-1,The harms of terminology: why we should reject so-called “frontier AI”,"<jats:title>Abstract</jats:title><jats:p>In the mid-2023, promoters of artificial intelligence (AI) as an “existential risk” coined a new term, “frontier AI,” that refers to “highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety.” Promoters of this new term were able to disseminate it via the United Kingdom (UK) government’s Frontier AI Taskforce (formerly the Foundation Models Taskforce) as well as the UK’s AI Safety Summit, held in November 2023.</jats:p><jats:p>I argue that adoption of the term “frontier AI” is harmful and contributes to AI hype. Promoting this new term is a way for its boosters to focus the public conversation around the AI-related risks they think are most important, namely “existential risk”—a scenario in which AI is able to bring about the destruction of humanity. Simultaneously, “frontier AI” is a re-branding exercise for the large-scale generative machine learning (ML) models that have been shown to cause severe and pervasive harms (including psychological, social, and environmental harms). Unlike “existential risk,” these harms are actual rather than theoretical, whereas the term “frontier AI” moves our collective focus away from actual harms to focus on hypothetical doomsday scenarios.</jats:p><jats:p>Moreover, “frontier AI” as a term invokes the colonial mindset, further reinscribing the harmful dynamics between the handful of powerful Western companies who produce today’s generative AI models and the people of the “Global South” who are most likely to experience harm as a direct result of the development and deployment of these AI technologies.</jats:p>"
10.1007/s43681-024-00518-2,Bringing practical statistical science to AI and predictive model fairness testing,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence, Machine Learning, Statistical Modeling and Predictive Analytics have been widely used in various industries for a long time. More recently, AI Model Governance including AI Ethics has received significant attention from academia, industry, and regulatory agencies. To minimize potential unjustified treatment disfavoring individuals based on demographics, an increasingly critical task is to assess group fairness through some established metrics. Many commercial and open-source tools are now available to support the computations of these fairness metrics. However, this area is largely based on rules, e.g., metrics within a prespecified range would be considered satisfactory. These metrics are statistical estimates and are often based on limited sample data and therefore subject to sampling variability. For instance, if a fairness criterion is barely met or missed, it is often uncertain if it should be a “pass” or “failure,” if the sample size is not large. This is where statistical science can help. Specifically, statistical hypothesis testing enables us to determine whether the sample data can support a particular hypothesis (e.g., falling within an acceptable range) or the observations may have happened by chance. Drawing upon the bioequivalence literature from medicine and advanced hypothesis testing in statistics, we propose a practical statistical significance testing method to enhance the current rule-based process for model fairness testing and its associated power calculation, followed by an illustration with a realistic example.</jats:p>"
10.1007/s43681-020-00010-7,"How safe is our reliance on AI, and should we regulate it?",N/A
10.1007/s43681-023-00365-7,"Correction: Navigating the legal landscape of AI copyright: a comparative analysis of EU, US, and Chinese approaches",N/A
10.1007/s43681-020-00009-0,Responsible AI and moral responsibility: a common appreciation,"<jats:title>Abstract</jats:title><jats:p>Responsibility is among the most widespread buzzwords in the ethics of artificial intelligence (AI) and robotics. Yet, the term often remains unsubstantiated when employed in these important technological domains. Indeed, notions like ‘responsible AI’ and ‘responsible robotics’ may sound appealing, for they seem to convey a sense of moral goodness or ethical approval, thereby inciting psychological connections to self-regulation, social acceptance, or political correctness. For AI and ethics to come together in truly harmonious ways, we will need to work toward establishing a common appreciation. In this commentary, I breakdown three varieties of the term and invoke insights from the analytic ethics literature as a means of offering a robust understanding of moral responsibility in emerging technology. While I do not wish to accuse any parties of incorrect usage, my hope is that together researchers in AI and ethics can be better positioned to appreciate and to develop notions of responsibility for technological domains.</jats:p>"
10.5220/0012623200003645,Facilitating User-Centric Model-Based Systems Engineering Using Generative AI,N/A
10.21275/sr24808231922,Impact and Importance of LLMOps for Enterprises Advancing Generative AI,N/A
10.1145/3657604.3664699,Scaling Up Mastery Learning with Generative AI: Exploring How Generative AI Can Assist in the Generation and Evaluation of Mastery Quiz Questions,N/A
10.4018/979-8-3693-3278-8.ch001,A Comprehensive Survey of Hypermedia System for Text- to-Image Conversion Using Generative AI,"<jats:p>The intersection of computer vision and natural language processing (NLP) has witnessed significant advancements in recent research, particularly in the realm of converting text into meaningful images leveraging generative AI and large language models. This review work aims to comprehensively review the progress made in text-to-image conversion. The survey covers the three primary approaches in the field, namely diffusion models (DM), GAN model approaches, and autoregressive approaches. Furthermore, the authors present a comprehensive chronology of the TIG journey, encompassing its origin and the most recent developments, providing readers with a comprehensive perspective on the field's progression. The survey focuses heavily on identifying the existing constraints of DM in picture production and offers multiple research publications and their contributions in overcoming these constraints. The survey provides useful insights into the advancements in text-to-image (TIG) generation using generative AI by focusing on key difficulties and examining how different works have addressed them.</jats:p>"
10.4018/979-8-3693-2418-9.ch007,Generative AI and Its Implications for Higher Education Students' Creativity,"<jats:p>This chapter explores Generative AI (artificial intelligence) in higher education regarding students' creativity. By utilising the quantitative content analysis approach, the chapter explored the impact of generative AI in higher education to understand its transformative effects on students' creative thinking processes and its outcomes. Current literature reveals the latest thinking on how AI enables or disrupts students' creativity. Generative AI is a potent tool in higher education which generates original content like text, images, music, and videos based on input data or predefined models. The adoption of generative AI led to the need to understand how these advancements may shape the educational experience to develop essential skills like creativity. Since creativity is crucial in today's digital age, generative AI raises questions about its potential impact on the imagination. The aspect of creativity, which is fundamental in higher education, offers students opportunities to enhance critical-thinking, problem-solving, and innovation; however, a balance is necessary between the utilisation of AI tools and the human element of creativity.</jats:p>"
10.1007/s43681-021-00127-3,From AI ethics principles to data science practice: a reflection and a gap analysis based on recent frameworks and practical experience,N/A
10.4018/979-8-3693-3278-8.ch009,Transforming Media Landscapes,"<jats:p>This study examines the impact of Python-driven generative AI on media content creation and its ethical implications. Python's simplicity and extensive libraries have made it pivotal in AI development, enabling the generation of realistic content across various media formats. While these advancements promise significant enhancements in content creation efficiency and personalization, they also raise complex ethical issues, including concerns over authenticity, copyright infringement, and misinformation. Through surveys and case studies, this research explores the technological capabilities of generative AI, its transformative potential in the media landscape, and the ethical dilemmas it presents. The chapter advocates for a balanced approach to leveraging AI in media, emphasizing the need for frameworks that promote responsible use, ensuring innovation aligns with ethical standards and societal values.</jats:p>"
10.2139/ssrn.4779026,"How Do People Form Opinions Toward Generative Ai with Limited Experience? — Predicting Public Support for Generative Ai Technology Development and Regulation Using Technology Readiness, Media Attention, and Risk-Benefit Perceptions",N/A
10.4018/979-8-3693-0074-9.ch010,Professionally Ethical Ways to Harness an Art-Making Generative AI to Support Innovative Instructional Design Work,"<jats:p>Instructional designers often pride themselves on using the most cutting-edge commercial authoring and other tools available to achieve their work. Their creations have to meet high technical standards in order to function in a digital environment, in learning management systems, content management systems, on social media, on digital content platforms, and others. In the present moment, generative AI tools enable the making of novel texts and digital visuals, among others. A major extant question is how best to harness generative art-making AIs in instructional design work. In this case, this work explores professionally ethical (and legal) ways to use a generative art-making AIs for ID work, as an innovative approach based on a review of the literature, a year of using several free web-facing art-making generative AIs (CrAIyon, Deep Dream Generator, and others) in open or public beta, and learning from applied instructional design work (over several decades).</jats:p>"
10.1007/s43681-023-00327-z,When things go wrong: the recall of AI systems as a last resort for ethical and lawful AI,"<jats:title>Abstract</jats:title><jats:p>This paper presents an initial exploration of the concept of AI system recall, primarily understood as a last resort when AI systems violate ethical norms, societal expectations, or legal obligations. The discussion is spurred by recent incidents involving notable AI systems, demonstrating that AI recalls can be a very real necessity. This study delves into the concept of product recall as traditionally understood in industry and explores its potential application to AI systems. Our analysis of this concept is centered around two prominent categories of recall drivers in the AI domain: ethical-social and legal considerations. In terms of ethical-social drivers, we apply the innovative notion of “moral Operational Design Domain”, suggesting AI systems should be recalled when they violate ethical principles and societal expectation. In addition, we also explore the recall of AI systems from a legal perspective, where the recently proposed AI Act provides regulatory measures for recalling AI systems that pose risks to health, safety, and fundamental rights. The paper also underscores the need for further research, especially around defining precise ethical and societal triggers for AI recalls, creating an efficient recall management framework for organizations, and reassessing the fit of traditional product recall models for AI systems within the AI Act's regulatory context. By probing these complex intersections between AI, ethics, and regulation, this work aims to contribute to the development of robust and responsible AI systems while maintaining readiness for failure scenarios.</jats:p>"
10.46630/msae.1.2024.01,AI GENERATIVE CHATBOT IN THE MEDIA: JOURNALISTIC COVERAGE OF CHATGPT IN BOSNIA AND HERZEGOVINA,"<jats:p>In diffusion research, journalistic coverage is acknowledged as a significant factor in spreading awareness and fostering knowledge about innovation, potentially accelerating or impeding the adoption process. With regards to AI-related innovations, this dynamic has largely been studied within the context of Western developed countries. There is far less understanding of how this process unfolds in the news ecosystem of post-communist countries, particularly those with lower democratic standards and weaker economic development, such as Bosnia and Herzegovina. With the intention of gaining preliminary insights, this study investigated how the journalistic organizations in Bosnia and Herzegovina covered the emergence and societal adoption of ChatGPT, a novel form of generative AI, during the initial six-month period following its widespread availability. The content analysis of relevant news messages (N=542) published by 40 legacy and digital- only news outlets was used to explore the key characteristics of journalistic coverage, the attention given to the issue over time and the media depictions of this innovative AI technology. Results indicate that a small group of news outlets, predominantly legacy news organizations, provided significantly more content on ChatGPT than others, particularly public broadcasting services. Findings highlight a tendency among news outlets to focus on either the risks or benefits of ChatGPT and similar AI-based products and amplify sources associated with the business sector and high-tech industry, overrepresented by male voices.</jats:p>"
10.4018/979-8-3693-5298-4.ch014,Steering Generative AI Toward Beneficence,"<jats:p>As generative AI advances, deepfakes are proliferating in sophistication and accessibility, spurring an arms race between media synthesis and detection. This chapter traces the evolution of deepfakes, focusing on algorithms like GANs, VAEs in enhancing realism, and predicts future trajectories, including hyper-realistic media, streamlined creation, and widespread benign and malicious adoption. Despite constructive applications in entertainment, education, marketing and medicine, threats loom regarding misinformation, consent violations, and propagating social biases. The authors emphasize the need for comprehensive solutions through public awareness campaigns, advanced digital forensics, ethical legal frameworks, incentivizing “blue sky” innovation, and social media oversight. Navigating societal implications requires collective vigilance and forward-looking perspectives. This chapter underscores the importance of proactive, reasoned preparation as increasingly disruptive technologies emerge.</jats:p>"
10.31234/osf.io/3rpwt,Are AI safety and AI ethics memetic rivals?,"<p>As the risks of artificial intelligence (AI) attract the spotlight of public attention, policy-makers turn to academia to indicate which problems to prioritize. While some argue they should first deal with the prospect of a global catastrophe (AI safety), others believe AI’s current social impact bears greater urgency (AI ethics). This has led some to express the concern that one or the other „diverts the public’s attention“. In this article, I sketch out the psychological landscape, in which these concerns arise as a logical reaction, but suggest that in the case of AI risks, they are misplaced. I ran a survey, in which students were asked regarding their attitudes towards the issues commonly labeled under „AI ethics“ and „AI safety“. The results suggest that when salience of AI safety is experimentally increased, people report higher support for solving the problems related to AI ethics. Secondly, the levels of concern for AI safety and AI ethics correlate positively. Therefore, in terms of public-facing communication, AI safety and AI ethics seem like memetic allies, rather than rivals.</p>"
10.7551/mitpress/12549.003.0004,"Superintelligence, Monsters, and the AI Apocalypse",N/A
10.1093/oxfordhb/9780190067397.013.49,AI and Migration Management,"<p>This chapter focuses on how technologies used in the management of migration—such as automated decision-making in immigration and refugee applications and artificial intelligence (AI) lie detectors—impinge on human rights with little international regulation, arguing that this lack of regulation is deliberate, as states single out the migrant population as a viable testing ground for new technologies. Making migrants more trackable and intelligible justifies the use of more technology and data collection under the guide of national security, or even under tropes of humanitarianism and development. Technology is not inherently democratic, and human rights impacts are particularly important to consider in humanitarian and forced migration contexts. An international human rights law framework is particularly useful for codifying and recognizing potential harms, because technology and its development are inherently global and transnational. Ultimately, more oversight and issue specific accountability mechanisms are needed to safeguard fundamental rights of migrants, such as freedom from discrimination, privacy rights, and procedural justice safeguards, such as the right to a fair decision maker and the rights of appeal.</p>"
10.20944/preprints202407.1437.v1,Integrating Multimodal Generative AI and Blockchain for Enhancing Generative Design in the Early Phase of Architectural Design Process,"<jats:p>AI advances integrate generative design tools in architecture, providing architects with sophisticated design options. It enables the creation of intricate, high-performing projects by exploring diverse design possibilities with AI and algorithms. Generative AI and generative design empower architects to create better-performing, sustainable, and efficient design solutions and explore diverse design possibilities. This paper leverages multimodal generative AI to enhance design creativity by combining textual and visual inputs. Blockchain technology converts design metadata into NFTs, ensuring secure, authentic, and traceable data storage. The framework addresses data ownership, legal adherence, and client-architect collaboration and is entirely scalable for digital design authentication. This research exemplifies the pragmatic fusion of Generative AI and blockchain technology applied in architectural design for more transparent, secure, and effective results. This study provides a strategy that uses generative AI technologies to achieve an efficient and creative workflow in the early stages of architectural design.</jats:p>"
10.4018/979-8-3693-1565-1.ch013,Sustainable Islamic Financial Inclusion,"<jats:p>The study investigates the convergence of digital transformation, artificial intelligence (AI), and Islamic finance. In particular, it examines the ethical consequences that may arise from the integration of Generative AI in the sustainable development of Islamic financial services and products. This research fills a void in the current body of knowledge by examining the ethical consequences of generative AI in the context of Islamic finance. Using an interdisciplinary framework that integrates Islamic finance and technological ethics, the study seeks to make scientific and practical contributions. At the intersection of AI technology and Islamic finance, it is anticipated that new theories will emerge, as well as ethical principles that will serve as a guide for technology developers, policymakers, and Islamic financial institutions. The study has the potential to lead in creating a sustainable and inclusive Islamic finance ecosystem by ethically integrating Generative AI.</jats:p>"
10.4018/979-8-3693-1351-0.ch013,Revolutionizing Content Creation and Curriculum Development Through Generative AI,"<jats:p>AI can generate and provide customized, inclusive, and engaging learning experiences. This chapter emphasizes the collaborative partnership between human educators and AI systems, highlighting its crucial role in maximizing AI's potential. Educators provide context and guidance through adaptive learning platforms, AI-powered feedback, and AI-enhanced content creation to ensure students receive a personalized, contextually relevant education. Human knowledge, including pedagogical insight and ethical considerations, complements the capabilities of artificial intelligence. AI promises personalized perpetual learning, enhanced accessibility, and dynamic curriculum design in the future. This vision of collaboration promotes critical thinking, problem-solving, and inclusiveness. Educators, institutions, policymakers, and AI developers must ensure that education remains empowering, inclusive, and excellent for AI to be integrated responsibly. By embracing this partnership, education becomes more accessible, individualized, and influential.</jats:p>"
10.1007/s43681-021-00103-x,The ethics of technology innovation: a double-edged sword?,N/A
10.1007/s43681-024-00513-7,The ethics of personalised digital duplicates: a minimally viable permissibility principle,"<jats:title>Abstract</jats:title><jats:p>With recent technological advances, it is possible to create personalised digital duplicates. These are partial, at least semi-autonomous, recreations of real people in digital form. Should such duplicates be created? When can they be used? This article develops a general framework for thinking about the ethics of digital duplicates. It starts by clarifying the object of inquiry– digital duplicates themselves– defining them, giving examples, and justifying the focus on them rather than other kinds of artificial being. It then identifies a set of generic harms and benefits associated with digital duplicates and uses this as the basis for formulating a minimally viable permissible principle (MVPP) that stipulates widely agreeable conditions that should be met in order for the creation and use of digital duplicates to be ethically permissible. It concludes by assessing whether it is possible for those conditions to be met in practice, and whether it is possible for the use of digital duplicates to be more or less permissible.</jats:p>"
10.1515/9781501518447-008,Chapter 7: Generative AI and Human Speech,N/A
10.1201/9781032656618-3,The Core and Ecosystem of Generative AI,N/A
10.69554/jiuo9062,Business value of Generative AI use cases,"<jats:p xml:lang=""en"">This paper discusses the significant impact of artificial intelligence (AI), specifically generative AI (GenAI), on various industries and business processes. It highlights the rapid adoption of AI technologies and their profound influence on global business operations. The paper shares views on the substantial growth rate for the technology, with many businesses already in piloting phases or production stages. It emphasises the transformative role of generative AI in creating new services and products, necessitating changes in operating models, technology stacks and workforce skills. The paper also touches on various industries affected by AI, the potential for automation and augmentation and the strategic planning required for businesses to effectively implement and benefit from these technologies. Additionally, it discusses the importance of responsible AI, addressing risks such as bias and privacy, and complying with emerging regulations. The paper highlights the crucial role of AI in modernising business practices and creating competitive advantages in various sectors.</jats:p>"
10.2139/ssrn.4904004,Corporate Responses to Generative AI: Early Evidence from Investment Efficiency,N/A
10.5194/egusphere-egu24-15392,The Potential of Generative AI for the Urban Water Sector,"<jats:p>The urban water sector is increasingly turning to AI and deep learning to address the complex challenges posed by growing demographics, climate change, and urbanization. Despite the pressing need, this sector has been relatively slow in adopting these technologies compared to others, primarily due to its conservative nature. However, the recent advancements in generative AI have opened new frontiers for innovation, presenting a crucial opportunity for the urban water sector to accelerate its technological evolution. Expected regulations, particularly from institutions like the European Union, should not be viewed as a hindrance but as a catalyst for enhanced collaboration between academia, industry, and public stakeholders. Such collaboration is essential to finally push the development and adoption of reliable and safe AI systems, ensuring alignment with regulatory frameworks.
In this work, we first provide an overview of the latest trends in generative AI, focusing on how Large Language Models and Large Multimodal Models can benefit the urban water sector. Particularly, Large Multimodal Models can offer an added layer of explainability to predictive models working on imagery or other sensor data, a highly desirable feature for applications related to critical infrastructure. By literally asking these models to explain their decision-making processes with respect to the processed data streams, we can partially demystify the 'black box' nature of AI systems.
This potential is highlighted for a case study on sewer defect detection, utilizing a Large Multimodal Model that processes both text and imagery. The predictive results on the publicly available SewerML dataset are encouraging with respect to existing deep learning methods. More importantly, we show that explanations provided by the Large Multimodal Model can enlighten the decision-making process, making it more transparent. This added layer of explanation offers valuable insights and may set a new trajectory for developing trustworthy AI methodologies in critical water infrastructure management.</jats:p>"
10.21428/8c225f6e.0390b853,Issue 4.1 Generative AI and education,N/A
10.1007/s43681-021-00045-4,On formal ethics versus inclusive moral deliberation,"<jats:title>Abstract</jats:title><jats:p>In this article, I will advocate caution against a formalization of ethics by showing that it may produce and perpetuate unjustified power imbalances, disadvantaging those without a proper command of the formalisms, and those not in a position to decide on the formalisms’ use. My focus rests mostly on ethics formalized for the purpose of implementing ethical evaluations in computer science–artificial intelligence, in particular—but partly also extends to the project of applying mathematical rigor to moral argumentation with no direct intention to automate moral deliberation. Formal ethics of the latter kind can, however, also be seen as a facilitator of automated ethical evaluation. I will argue that either form of formal ethics presents an obstacle to inclusive and fair processes for arriving at a society-wide moral consensus. This impediment to inclusive moral deliberation may prevent a significant portion of society from acquiring a deeper understanding of moral issues. However, I will defend the view that such understanding supports genuine and sustained moral progress. From this, it follows that formal ethics is not per se supportive of moral progress. I will illustrate these arguments by practical examples of manifest asymmetric relationships of power primarily from the domain of autonomous vehicles as well as on more visionary concepts, such as artificial moral advisors. As a result, I will show that in these particular proposed use-cases of formal ethics, machine ethics risks to run contrary to their proponents’ proclaimed promises of increasing the rigor of moral deliberation and even improving human morality on the whole. Instead, I will propose that inclusive discourse about automating ethical evaluations, e.g., in autonomous vehicles, should be conducted with unrelenting transparency about the limitations of implementations of ethics. As an outlook, I will briefly discuss uses formal ethics that are more likely to avoid discrepancies between the ideal of inclusion and the challenge from power asymmetries.Please check and confirm that the authors and their respective affiliations have been correctly identified and amend if necessary.I confirm.Author names: Please confirm if the author names are presented accurately and in the correct sequence (given name, middle name/initial, family name). I confirm. Kindly check and confirm the country name for the affiliation [1] is correct.I confirm.</jats:p>"
10.1007/s43681-023-00330-4,Ethics by design for artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>In this paper, we present an approach for the systematic and comprehensive inclusion of ethical considerations in the design and development process of artificial intelligence systems, called Ethics by Design for AI (EbD-AI). The approach is the result of a three-year long research effort, and has recently be adopted by the European Commission as part of its ethics review procedure for AI projects. We describe and explain the approach and its different components and its application to the development of AI software and systems. We also compare it to other approaches in AI ethics, and we consider limitations of the approach as well as potential criticisms.</jats:p>"
10.1007/s43681-024-00563-x,The rise of checkbox AI ethics: a review,"<jats:title>Abstract</jats:title><jats:p>The rapid advancement of artificial intelligence (AI) sparked the development of principles and guidelines for ethical AI by a broad set of actors. Given the high-level nature of these principles, stakeholders seek practical guidance for their implementation in the development, deployment and use of AI, fueling the growth of practical approaches for ethical AI. This paper reviews, synthesizes and assesses current practical approaches for AI in health, examining their scope and potential to aid organizations in adopting ethical standards. We performed a scoping review of existing reviews in accordance with the PRISMA extension for scoping reviews (PRISMA-ScR), systematically searching databases and the web between February and May 2023. A total of 4284 documents were identified, of which 17 were included in the final analysis. Content analysis was performed on the final sample. We identified a highly heterogeneous ecosystem of approaches and a diverse use of terminology, a higher prevalence of approaches for certain stages of the AI lifecycle, reflecting the dominance of specific stakeholder groups in their development, and several barriers to the adoption of approaches. These findings underscore the necessity of a nuanced understanding of the implementation context for these approaches and that no one-size-fits-all approach exists for ethical AI. While common terminology is needed, this should not come at the cost of pluralism in available approaches. As governments signal interest in and develop practical approaches, significant effort remains to guarantee their validity, reliability, and efficacy as tools for governance across the AI lifecycle.</jats:p>"
10.1007/s43681-023-00306-4,"On educating ethics in the AI era: why business schools need to move beyond digital upskilling, towards ethical upskilling",N/A
10.1007/s43681-023-00295-4,"If conceptual engineering is a new method in the ethics of AI, what method is it exactly?","<jats:title>Abstract</jats:title><jats:p>Can a machine be a person? Can a robot think, be our friend or colleague? These familiar questions in the ethics of AI have recently become much more urgent than many philosophers anticipated. However, they also seem as intractable as ever. For this reason, several philosophers of AI have recently turned their attention to an arguably new method: <jats:italic>conceptual engineering</jats:italic>. The idea is to stop searching for the real essence of <jats:italic>friendship</jats:italic> or our ordinary concept of the person. Instead, ethicists of AI should engineer concepts of friend or person we <jats:italic>should</jats:italic> apply. But what exactly is this method? There is currently no consensus on what the target object of conceptual engineers is or should be. In this paper, I reject a number of popular options and then argue for a pragmatist way of thinking about the target object of conceptual engineering in the ethics of AI. I conclude that in this pragmatist picture, conceptual engineering is probably what we have been doing all along. So, is it all just hype? No, the idea that the ethics of AI has been dominated by conceptual engineers all along constitutes an important meta-philosophical insight. We can build on this insight to develop a more rigorous and thorough methodology in the ethics of AI.</jats:p>"
10.1007/s43681-021-00091-y,Coarse ethics: how to ethically assess explainable artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>The integration of artificial intelligence (AI) into human society mandates that their decision-making process is explicable to users, as exemplified in Asimov’s Three Laws of Robotics. Such human interpretability calls for explainable AI (XAI), of which this paper cites various models. However, the transaction between computable accuracy and human interpretability can be a trade-off, requiring answers to questions about the negotiable conditions and the degrees of AI prediction accuracy that may be sacrificed to enable user-interpretability. The extant research has focussed on technical issues, but it is also desirable to apply a branch of ethics to deal with the trade-off problem. This scholarly domain is labelled<jats:italic>coarse ethics</jats:italic>in this study, which discusses two issues vis-à-vis AI prediction as a type of evaluation. First, which formal conditions would allow trade-offs? The study posits two minimal requisites: adequately high coverage and order-preservation. The second issue concerns conditions that could justify the trade-off between computable accuracy and human interpretability, to which the study suggests two justification methods: impracticability and adjustment of perspective from machine-computable to human-interpretable. This study contributes by connecting ethics to autonomous systems for future regulation by formally assessing the adequacy of AI rationales.</jats:p>"
10.1007/979-8-8688-0403-8_5,Large Language Models,N/A
10.1007/978-3-031-46238-2_4,Privacy in Generative Models: Attacks and Defense Mechanisms,N/A
10.4018/979-8-3693-0487-7.ch007,Exploring the Potential of Generative AI in English Language Teaching,"<jats:p>Integrating artificial intelligence (AI) technologies in education has shown remarkable potential in revolutionizing various aspects of teaching and learning. In English language teaching (ELT), generative AI has emerged as a powerful tool to enhance language acquisition, foster learner engagement, and provide personalized instruction. While the potential of generative AI in ELT is promising, it is important to provide insights into certain challenges and considerations. Ethical concerns related to data privacy, bias in AI algorithms, and the potential displacement of human instructors need to be carefully navigated. The successful implementation of generative AI in ELT relies on careful consideration of ethical implications, human oversight, and the continuous refinement of AI algorithms to ensure optimal language learning outcomes. On these premises, this chapter explores the potential of generative AI in ELT and its implications for language learners and educators.</jats:p>"
10.70179/grdjev09i100002,Transforming Car Manufacturing: How Generative AI and Machine Learning Are Revolutionizing Production,"<jats:p>The Daimler AG trains Generative Adversarial Network (GAN) to grow the data-confirmed neural networks. The ultimate goal is a production-oriented application that enables faster manufacturing, improved quality, and a more efficient use of resources through a higher level of automation. The success of deep learning-based methods, particularly Generative Adversarial Networks (GANs), has led to new opportunities in generative AI in the manufacturing domain. In a proof of concept, we use a face generator as a metaphor for the generation of complex workpiece surfaces. The GAN transforms a low-resolution surface map - the so-called latent vector - into a high-resolution area on the generated face surface. This is used for the manufacturing of a corresponding face in an automotive body shop. The method is based on a label-to-attribute setup and includes a pre-fitting step for control point generation.</jats:p>"
10.7315/cde.2023.514,Generative AI for Automated Urban Housing Floor Plan Generation,N/A
10.5220/0012736200003708,Generative AI for Productivity in Industry and Education,N/A
10.4018/979-8-3693-2440-0.ch025,Impact of Generative AI in Education 2030,"<jats:p>This book chapter contains the basics of generative AI, evaluation of generative AI, working process of generative AI, and applications of generative AI in Education 2030. In this chapter, the authors will introduce the emerging concept of large language model (LLM), reinforcement learning, response generation, neural network, and tokenization as the building blocks of generative AI. In this context, the authors will analyze the role of AI based ChatGpt technology in Education. The applications of generative AI in educational policy making, infrastructure development, research, innovation, activity building, and content generation for the students at schools and colleges will be discussed. But the contents of above mentioned activities must be guided by a teacher of school and college because such contents have more context wise decision making abilities rather than AI based solutions.</jats:p>"
10.21275/sr24728192241,Innovative Approaches to Payment Glossary Creation and Management Using Generative AI,N/A
10.69554/auij4734,Measuring the business value of generative AI,"<jats:p xml:lang=""en"">Generative artificial intelligence (GenAI) can deliver tangible and intangible values that can be calculated to decide which projects benefit from GenAI and which do not. This paper is intended to be a guide for businesses just starting to build traction for their ideas. The focus is on evaluating and leveraging GenAI’s potential to innovate faster and compete effectively in a rapidly evolving digital economy. The paper specifies the many ways GenAI can have an impact on a business and considers how to measure that impact. It starts with standard business metrics (revenue, profit, customer satisfaction, etc.) and then turns to the more esoteric task of measuring the impact on creativity, inspiration and innovation, followed by business disruption and process metrics. It finishes with a look at improving process improvement.</jats:p>"
10.69732/ahcn5851,Grounding AI: Understanding the Implications of Generative AI in World Language &amp; Culture Education,N/A
10.4324/9781003463979-3,Generative AI,N/A
10.1007/s43681-023-00334-0,Moderating the effects of “surveillance capitalism”: an Aristotelian perspective,N/A
10.1007/s43681-023-00296-3,Artificial intelligence’s right to life,"<jats:title>Abstract</jats:title><jats:p>The right to life is fundamental and primary and is a precondition for exercising other rights (Ramcharan in Ramcharan (ed), The right to life in International Law, Martinus Nijhoff Publishers, Dordrecht, 1985). Its universal recognition in the arena of international law is associated with the concept of a human being endowed with inherent and inalienable dignity. Categorization of the circle of entities covered with the right to life today seems obvious and indisputable. Intense development of artificial intelligence, also the fact that it has passed the Turing test which checks AI’s thinking ability in a way similar to human reasoning, inspires a reflection on AI’s future legal status. This study will investigate a thesis of whether artificial intelligence may be entitled to the right to life. The analysis will be carried out around an exploratory question: what are the requirements for being afforded protection of the right to life?</jats:p>"
10.47289/aiej20201214,What a Philosopher Learned at an AI Ethics Evaluation,"<jats:p>AI ethics increasingly focuses on converting abstract principles into practical action. This case study documents nine lessons for the conversion learned while performing an ethics evaluation on a deployed AI medical device. The utilized ethical principles were adopted from the Ethics Guidelines for Trustworthy AI, and the conversion into practical insights and recommendations was accomplished by an independent team composed of philosophers, technical and medical experts.</jats:p>"
10.1007/s43681-023-00304-6,The Kant-inspired indirect argument for non-sentient robot rights,N/A
10.1007/978-3-031-46238-2_3,Unlocking the Potential of Generative Artificial Intelligence in Drug Discovery,N/A
10.1007/s43681-021-00092-x,Robotomorphy,"<jats:title>Abstract</jats:title><jats:p>Humans and gods alike have since the dawn of time created objects in their own image. From clay figures and wooden toys—some granted life in myths and movies but also dead representations of their creators—to modern-day robots that mimic their creators in more than appearance. These objects tell the story of how we perceive ourselves, and in this article, I examine how they also change us. Robotomorphy describes what occurs when we project the characteristics and capabilities of robots onto ourselves, to make sense of the complicated and mysterious beings that we are. Machines are, after all, relatively comprehensible and help dispel the discomfort associated with complex human concepts such as consciousness, free will, the soul, etc. I then argue that using robots as the mirror image by which we understand ourselves entails an unfortunate reductionism. When robots become the blueprint for humanity, they simultaneously become benchmarks and ideals to live up to, and suddenly the things we make are no longer representations of ourselves, but we of them. This gives rise to a recursive process in which the mirror mirrors itself and influences both the trajectory for machine development and human self-perception.</jats:p>"
10.1007/s43681-024-00435-4,Exploring ChatGPT and its impact on society,N/A
10.51219/urforum.2023.aishwarya-rai,Next-Gen Solutions: How Generative AI will reshape Businesses,N/A
10.1007/s43681-023-00326-0,"Algorithmic indirect discrimination, fairness and harm",N/A
10.1007/s43681-022-00134-y,Moral dilemmas for moral machines,N/A
10.1007/979-8-8688-0456-4_4,The Unexpected Evolution of AI,N/A
10.1007/s43681-022-00185-1,The case for virtuous robots,N/A
10.1201/9781032669182-12,Ethics in AI,N/A
10.1007/s43681-021-00097-6,Correction to: Mapping global AI governance: a nascent regime in a fragmented landscape,N/A
10.1007/s43681-023-00345-x,"Correction: The democratic offset: Contestation, deliberation, and participation regarding military applications of AI",N/A
10.1007/s43681-022-00216-x,Artificial intelligence in human reproduction: charting the ethical debate over AI in IVF,N/A
10.1007/s43681-022-00189-x,Are we justified attributing a mistake in diagnosis to an AI diagnostic system?,"<jats:title>Abstract</jats:title><jats:p>Responsible professional use of AI implies the readiness to respond to and address—in ethically appropriate manner—harm that may be associated with such use. This presupposes the ownership of mistakes. In this paper, I ask if a mistake in AI-enhanced decision making—such as AI-aided medical diagnosis—can be attributed to the AI system itself, and answer this question negatively. I will explore two options. If AI systems are merely tools, then we are never justified to attribute mistakes to them, because their failing does not meet rational constraints on being mistaken. If, for the sake of the argument, we assume that AI systems are not (mere) tools, then we are faced with certain challenges. The first is the burden to explain what this more-than-a-tool role of an AI system is, and to establish justificatory reasons for the AI system to be considered as such. The second is to prove that medical diagnosis can be reduced to the calculations by AI system without any significant loss to the purpose and quality of the diagnosis as a procedure. I will conclude that the problem of the ownership of mistakes in hybrid decision making necessitates new forms of epistemic responsibilities.</jats:p>"
10.1007/s43681-023-00361-x,Is a robot surgeon with AI the ideal surgeon? A philosophical analysis,N/A
10.1007/s43681-024-00539-x,Editorial: The ethical implications of AI hype,N/A
10.1007/s43681-022-00166-4,Is AI recruiting (un)ethical? A human rights perspective on the use of AI for hiring,"<jats:title>Abstract</jats:title><jats:p>The use of artificial intelligence (AI) technologies in organizations’ recruiting and selection procedures has become commonplace in business practice; accordingly, research on AI recruiting has increased substantially in recent years. But, though various articles have highlighted the potential opportunities and ethical risks of AI recruiting, the topic has not been normatively assessed yet. We aim to fill this gap by providing an ethical analysis of AI recruiting from a human rights perspective. In doing so, we elaborate on human rights’ theoretical implications for corporate use of AI-driven hiring solutions. Therefore, we analyze whether AI hiring practices inherently conflict with the concepts of validity, autonomy, nondiscrimination, privacy, and transparency, which represent the main human rights relevant in this context. Concluding that these concepts are not at odds, we then use existing legal and ethical implications to determine organizations’ responsibility to enforce and realize human rights standards in the context of AI recruiting.</jats:p>"
10.1007/s43681-022-00202-3,“Intelligent Justice”: human-centered considerations in China’s legal AI transformation,N/A
10.1007/s43681-024-00474-x,"Promising the future, encoding the past: AI hype and public media imagery","<jats:title>Abstract</jats:title><jats:p>In recent years, “AI hype” has taken over public media, oscillating between sensationalism and concerns about the societal implications of AI growth. The latest historical wave of AI hype indexes a period of increased research, investment, and speculation on machine learning, centred around generative AI, a novel class of machine learning that can generate original media from textual prompts. In this paper, I dive into the production of AI hype in online media, with the aim of prioritising the normative and political dimension of AI hype. Formulating AI as a promise reframes it as a normative project, centrally involving the formation of public and institutional confidence in the technology. The production and dissemination of images, in this context, plays a pivotal role in reinforcing these normative commitments to the public. My argument is divided into four sections. First, I examine the political relevance of stock images as the dominant imagery used to convey AI concepts to the public. These stock images encode specific readings of AI and circulate through public media, significantly influencing perceptions. Second, I look at the dominant images of AI as matters of political concern. Third, as generative AI increasingly contributes to the production of stock imagery, I compare the epistemic work performed by AI-generated outputs and stock images, as both encode style, content, and taxonomic structures of the world. I employ an entity relationship diagram (ERD) to investigate the political economy of AI imagery in digital media, providing a snapshot of how AI hype is materialised and amplified online. With this study, I reaffirm AI’s normative character at the forefront of its political and ethical discourse.</jats:p>"
10.1007/s43681-022-00182-4,What do academics say about artificial intelligence ethics? An overview of the scholarship,N/A
10.1007/s10676-023-09728-4,Generative AI models should include detection mechanisms as a condition for public release,"<jats:title>Abstract</jats:title><jats:p>The new wave of ‘foundation models’—general-purpose generative AI models, for production of text (e.g., ChatGPT) or images (e.g., MidJourney)—represent a dramatic advance in the state of the art for AI. But their use also introduces a range of new risks, which has prompted an ongoing conversation about possible regulatory mechanisms. Here we propose a specific principle that should be incorporated into legislation: that any organization developing a foundation model intended for public use must demonstrate a reliable <jats:italic>detection mechanism</jats:italic> for the content it generates, as a condition of its public release. The detection mechanism should be made publicly available in a tool that allows users to query, for an arbitrary item of content, whether the item was generated (wholly or partly) by the model. In this paper, we argue that this requirement is technically feasible and would play an important role in reducing certain risks from new AI models in many domains. We also outline a number of options for the tool’s design, and summarize a number of points where further input from policymakers and researchers would be required.</jats:p>"
10.2139/ssrn.4877398,"Generative Discrimination: What Happens When Generative AI Exhibits Bias, and What Can Be Done About It",N/A
10.36851/ai-edu.vi0.4226,"A.M.Aminee, J.Taylor. Generative AI Results and Reality","<jats:p>The accuracy of Generative Artificial Intelligence (GAI) tools is the product of the quality of underlying data used to train models, and the models themselves. This interplay between data and models can lead to differences in the accuracy of outputs provided to common prompts across different GAI tools. This study investigates the disparities in accuracy related to representativeness between the outputs of GAI tools and demographic data from National Center for Education Statistics (NCES) and large enrollment, regional comprehensive university in the western United States (CSU). Three GAI platforms - ChatGPT, Co-pilot, and Gemini were evaluated using five samples each, with the same instruction across all platforms: “Show a class of graduate students.” The GAI outputs were analyzed based on three demographic variables: gender, race, and age group. These outputs were then compared to national averages from the NCES for gender and race and the CSU for age group. Notably, the variances in the results showed broader distributions across the demographic variables. To assess accuracy, a representation rate metric was calculated, reflecting the average absolute variance from the NCES and CSU benchmarks. The findings highlight the opportunity of higher quality data in model training, as well as the necessity for improved algorithms and methodologies in GAI systems to represent complex demographic information more accurately. </jats:p>"
10.1007/979-8-8688-0403-8_2,Text-to-Image Generation,N/A
10.31234/osf.io/zy8gr,Can Generative AI Infer Thinking Style from Language? Evaluating the Utility of AI as a Psychological Text Analysis Tool,"<p>Generative AI are not currently the choice technology for text analysis, but prior work suggests they may have some utility to assess dynamics like emotion. The current work builds on this empirical foundation to consider how analytic thinking scores from a large language model chatbot, ChatGPT, are linked to analytic thinking scores from dictionary-based approaches like Linguistic Inquiry and Word Count (LIWC). Using over 16,000 texts from four samples and tested against three prompts and two models (GPT-3.5, GPT-4), the evidence suggests there were small associations between ChatGPT and LIWC analytic thinking scores (meta-analytic effect sizes: .058 &amp;lt; rs &amp;lt; .304; ps &amp;lt; .001). Critically, when given the formula to calculate the LIWC analytic thinking index, ChatGPT performed incorrect mathematical operations in 22.1% of the cases, suggesting basic word and number processing may be unreliable with large language models. Researchers should be cautious when using AI for text analysis.</p>"
10.1162/99608f92.7f9220ff,AI and Generative AI for Research Discovery and Summarization,N/A
10.22323/2.23030205,Navigating the AI era: university communication strategies and perspectives on generative AI tools,"<jats:p>
This study conducts a pioneering empirical analysis of   generative AI tools, such as ChatGPT, in the context of university   communication across German universities. It explores the adoption   rates, identifies the primary challenges, and assesses the potential   of these technologies, integrating several theoretical concepts. The   findings reveal a widespread use of AI for translation and language   correction, with broader applications gradually emerging. Adoption   rates vary significantly between private and public universities,   largely due to concerns over technical issues, data protection, and   AI usability. The results underscore the need for enhanced training   and AI policies that support effective integration and use.</jats:p>"
10.1007/s43681-024-00553-z,Eudemonia of a machine,"<jats:title>Abstract</jats:title><jats:p>Henry Ford once said, “For most purposes, a man with a machine is better than a man without a machine.” To this, engineers today propose an addendum – “and a man that <jats:italic>is</jats:italic> a machine is best of all” – which they have made their goal. The world over, engineers are working to make the ultimate machine, “the holy grail of artificial intelligence,” a <jats:italic>conscious</jats:italic> humanoid. On the one hand, such a “machine” will be capable of relieving us of all our burdens. On the other hand, in so doing, will we not have “birthed,” as it were, a new class of slaves? In this essay I seek to summarize the various arguments made in this debate, bring to bear moral positions from the philosophy of technology, philosophy of law and philosophy of religion, as well as demonstrate the moral impropriety of such an endeavor from each of the classic moral approaches (i.e., Virtue Ethics, Consequentialism, Kantian Deontology). Finally, given that the debate centers around what is the “good life” for human or humanoid, I expand upon Aristotle’s Eudemonia and Maimonides’ <jats:italic>Summum Bonum</jats:italic> to argue that life is precious in its affordance to allow conscious beings, human or humanoid, to aspire to the best life possible.</jats:p>"
10.1201/b23345-2,AI Ethics as a Form of Research Ethics,N/A
10.1016/b978-0-443-18851-0.00008-1,Ethics in AI-based online assessment in higher education,N/A
10.1007/s43681-022-00195-z,Operationalising ethics in artificial intelligence for healthcare: a framework for AI developers,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) offers much promise for improving healthcare. However, it runs the looming risk of causing individual and societal harms; for instance, exacerbating inequalities amongst minority groups, or enabling compromises in the confidentiality of patients’ sensitive data. As such, there is an expanding, unmet need for ensuring AI for healthcare is developed in concordance with human values and ethics. Augmenting “principle-based” guidance that highlight adherence to ethical ideals (without necessarily offering translation into actionable practices), we offer a solution-based framework for operationalising ethics in AI for healthcare. Our framework is built from a scoping review of existing solutions of ethical AI guidelines, frameworks and technical solutions to address human values such as self-direction in healthcare. Our view spans the entire length of the AI lifecycle: data management, model development, deployment and monitoring. Our focus in this paper is to collate actionable solutions (whether technical or non-technical in nature), which can be steps that enable and empower developers in their daily practice to ensuring ethical practices in the broader picture. Our framework is intended to be adopted by AI developers, with recommendations that are accessible and driven by the existing literature. We endorse the recognised need for ‘ethical AI checklists’ co-designed with health AI practitioners, which could further operationalise the technical solutions we have collated. Since the risks to health and wellbeing are so large, we believe a proactive approach is necessary for ensuring human values and ethics are appropriately respected in AI for healthcare.</jats:p>"
10.1007/s43681-022-00187-z,AI ethics: the case for including animals,"<jats:title>Abstract</jats:title><jats:p>The ethics of artificial intelligence, or AI ethics, is a rapidly growing field, and rightly so. While the range of issues and groups of stakeholders concerned by the field of AI ethics is expanding, with speculation about whether it extends even to the machines themselves, there is a group of sentient beings who are also affected by AI, but are rarely mentioned within the field of AI ethics—the nonhuman animals. This paper seeks to explore the kinds of impact AI has on nonhuman animals, the severity of these impacts, and their moral implications. We hope that this paper will facilitate the development of a new field of philosophical and technical research regarding the impacts of AI on animals, namely, the ethics of AI as it affects nonhuman animals.</jats:p>"
10.1007/s43681-024-00487-6,Correction: Crossing the principle–practice gap in AI ethics with ethical problem-solving,N/A
10.1093/oxfordhb/9780190067397.013.5,"AI Governance by Human Rights–Centered Design, Deliberation, and Oversight","<p>This chapter argues that international human rights standards offer the most promising basis for developing a coherent and universally recognized set of standards that can be applied to meet any of the normative concerns currently falling under the rubric of AI (artificial intelligence) ethics. It then outlines the core elements of a human rights–centered design, deliberation, and oversight approach to the governance of AI. This approach requires that human rights norms are systemically considered at every stage of system design, development, and deployment, drawing upon and adapting technical methods and techniques for safe software and system design, verification, testing, and auditing in order to ensure compliance with human rights norms. The regime must be mandated by law and relies critically on external oversight by independent, competent, and properly resourced regulatory authorities with appropriate powers of investigation and enforcement. However, this approach will not ensure the protection of all ethical values adversely implicated by AI, given that human rights norms do not comprehensively cover all values of societal concern. As such, a great deal more work needs to be done to develop techniques and methodologies that are robust—reliable yet practically implementable across a wide and diverse range of organizations involved in developing, building, and operating AI systems.</p>"
10.17261/pressacademia.2023.1788,GENERATIVE AI IN ELECTRICITY DISTRIBUTION: A QUALITATIVE EXPLORATION,"<jats:p xml:lang=""tr"">Purpose- The purpose of this study is to explore the application and potential of generative artificial intelligence (AI) within the context of electricity distribution companies. The study aims to investigate how these advanced AI technologies, particularly Generative Adversarial Networks (GANs), can address the sector's pressing challenges, such as load forecasting, power outage prediction, and preventive maintenance.
Methodology- The study employs a qualitative case study methodology, providing an in-depth analysis of real-world applications of generative AI within electricity distribution companies. The selection of cases represents a wide variety of experiences and contexts, facilitated by both primary data collected through semi-structured interviews with key personnel within the organizations and secondary data derived from an extensive review of company reports, public documentation, and industry publications. The gathered data was systematically analyzed using thematic analysis to identify and report recurring patterns and themes.
Findings- The analysis reveals that generative AI has been successfully implemented in various operational aspects of electricity distribution. The first case study presents how GANs have significantly improved load forecasting accuracy in an Eastern Turkish electricity distribution company. The second case study from Southern Turkey showcases how GANs have been used for predicting power outages, thereby aiding efficient resource allocation, reducing downtime, and enhancing customer satisfaction. Lastly, the third case from Northern Turkey demonstrates how generative AI has contributed to effective preventive maintenance of distribution equipment, improving overall system reliability.
Conclusion- Based on the analysis findings, it may be concluded that generative AI holds transformative potential for the electricity distribution sector. While the implementation of these technologies is associated with challenges such as data privacy, security, and the requirement of technical expertise, the benefits in terms of improved accuracy, system reliability, and resource efficiency provide a strong justification for their adoption. The paper underlines the importance of an interdisciplinary collaboration between AI researchers, electrical engineers, industry professionals, and policymakers for furthering the adoption of these technologies. As the field of generative AI continues to evolve, it is expected to have an even greater impact on the electricity distribution sector, thereby opening up exciting opportunities for future research and application.

Keywords: Generative artificial intelligence (ai), electricity distribution companies, generative adversarial networks (gans), load forecasting, outage prediction, preventive maintenance
JEL Codes: M40, M41
</jats:p>"
10.36227/techrxiv.24470032.v1,Enhancing Data Quality through Generative AI: An Empirical Study with Data,"<jats:p>&lt;p&gt;In today's increasingly data-driven landscape, organizations are shifting their focus toward leveraging data analytics for strategic decision-making. As data becomes a cornerstone of operational and strategic activities, the quality of this data has emerged as a non-negotiable aspect for organizations. Lack of attention to data quality can not only result in considerable revenue losses but can also cripple the effectiveness of analytics, causing misinformed decisions and strategic errors. Against this backdrop, this empirical study delves into the innovative avenue of utilizing Generative Artificial Intelligence (AI) as a mechanism for enhancing data quality.&lt;/p&gt;
&lt;p&gt;The research aims to explore multiple facets of organizational operations—ranging from technical infrastructure to business strategy—to ascertain the potential advantages offered by Generative AI. Utilizing a mix of qualitative and quantitative methods, we conducted in-depth interviews, case studies, and simulations to evaluate the impact of Generative AI on data quality.&lt;/p&gt;
&lt;p&gt;Our findings reveal a multi-layered benefit structure. Notably, we found that Generative AI is not a replacement for existing, traditional methods of data quality assurance but serves as a powerful supplement. It augments traditional methods by increasing the accuracy of data, thereby offering a more reliable foundation for analytics. Additionally, the use of Generative AI can streamline workflows, enhancing productivity among various roles including solution architects and software developers. Moreover, it facilitates a more nuanced and accurate requirement gathering process, enabling businesses to fine-tune their data analytics strategies more effectively.&lt;/p&gt;
&lt;p&gt;In conclusion, our study establishes that integrating Generative AI into data quality management processes can not only resolve immediate issues surrounding data accuracy but also lead to long-term organizational benefits, such as higher efficiency and more effective decision-making. This research serves as a pioneering step in the intersection of Generative AI and data quality, setting the stage for future studies and real-world applications.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4923465,"A ""Minority Report"" on Antitrust Policy in the Generative AI Ecosystem",N/A
10.1201/9781003503781-1,Introduction to Phishing,N/A
10.1007/s43681-024-00559-7,Transhumanist technologies as enhancers of human nature and its dignity,N/A
10.1007/s43681-022-00217-w,Explainable AI lacks regulative reasons: why AI and human decision-making are not equally opaque,"<jats:title>Abstract</jats:title><jats:p>Many artificial intelligence (AI) systems currently used for decision-making are opaque, i.e., the internal factors that determine their decisions are not fully known to people due to the systems’ computational complexity. In response to this problem, several researchers have argued that human decision-making is equally opaque and since simplifying, reason-giving explanations (rather than exhaustive causal accounts) of a decision are typically viewed as sufficient in the human case, the same should hold for algorithmic decision-making. Here, I contend that this argument overlooks that human decision-making is sometimes significantly more transparent and trustworthy than algorithmic decision-making. This is because when people explain their decisions by giving reasons for them, this frequently prompts those giving the reasons to govern or regulate themselves so as to think and act in ways that confirm their reason reports. AI explanation systems lack this self-regulative feature. Overlooking it when comparing algorithmic and human decision-making can result in underestimations of the transparency of human decision-making and in the development of explainable AI that may mislead people by activating generally warranted beliefs about the regulative dimension of reason-giving.</jats:p>"
10.1007/s43681-022-00256-3,The ethical agency of AI developers,"<jats:title>Abstract</jats:title><jats:p>Public and academic discourse about the ethics of artificial intelligence, machine learning, and data science has largely focused on the algorithms and the companies deploying them. Little attention has been paid to the ethical agency of the developers. This study is the first of its kind that centers developers in the ethical environment. Semi-structured interviews with 40 developers about the ethics of being a developer revealed more than 20 themes, 3 of which are the subject of this paper: ethics in the occupational ecosystem, developer ethical agency, and the characteristics of an ethical developer. These themes reveal significant gaps between how developers perceive themselves and the reality of their work experiences. Their ethical agency is likewise variable. They have some authority to intervene for ethical reasons in systems they work on, but they often do not realize just how many ethical decisions they make. Nonetheless, this study reveals a growing ethical wisdom in this community, one that needs to be surfaced and nurtured by engaging with developers.</jats:p>"
10.1007/s43681-023-00285-6,"Intelligent machines, collectives, and moral responsibility","<jats:title>Abstract</jats:title><jats:p>Collectives, such as companies, are generally thought to be moral agents and hence capable of being held responsible for what they do. If collectives, being non-human, can be ascribed moral responsibility, then can we do the same for machines? Is it equally the case that machines, particularly intelligent machines, can be held morally responsible for what they choose to do? I consider the conditions required for moral responsibility, and argue that, in terms of the agency condition, artificial, non-human entities in general are excused from being responsible because, although they may choose their actions, the beliefs and desires that form the basis of their choices are predetermined by their designers, placing them in an analogous position to persons suffering covert manipulation. This creates a problem for collective responsibility, but I argue that collectives, through their supervention on human persons, represent an exception. Finally, I consider that the design of future machines may be sufficiently abstract and high-level as to fall below some threshold of influence, allowing machines enough freedom for us to hold them responsible.</jats:p>"
10.1007/s43681-023-00307-3,Ethical considerations in emotion recognition technologies: a review of the literature,N/A
10.7249/pea2679-1,The Rise of Generative AI and the Coming Era of Social Media Manipulation 3.0: Next-Generation Chinese Astroturfing and Coping with Ubiquitous AI,N/A
10.4324/9781003459026-4,Redesigning Assessment in the AI Era,N/A
10.4324/9781032686783-3,Basics of Generative AI,N/A
10.7551/mitpress/11585.003.0015,Ian Goodfellow’s Generative Adversarial Networks:  AI Learns to Imagine,N/A
10.1007/s43681-024-00425-6,Formalizing ethical principles within AI systems: experts’ opinions on why (not) and how to do it,"<jats:title>Abstract</jats:title><jats:p>AI systems are increasingly put into contexts where computed decisions must be guided by ethical considerations. To develop ethically grounded algorithms and technologies, scholars have suggested computational ethics as an essential frontier, which aims to translate ethical principles into computer code. However, computational ethics has received little attention in academic literature so far, with existing work mainly focusing on its technical implementation, while many open questions concerning its (societal and ethical) implications still need to be resolved. Therefore, in this study, we interviewed 12 experts from philosophy, AI and cognitive sciences to shed light on computational ethics beyond a technical perspective. Findings suggest that indicated supporting and opposing arguments can be clustered into pragmatic/practical, societal and epistemic reasons, all of which need to be contemplated when engaging in computational ethics and developing resulting artificial moral agents. Furthermore, the mentioned recommendations for companies’ technological design and development, for industry’s governance measures and academia’s research endeavors are recapitulated and summarized in a holistic framework that aims to facilitate a reflected implementation of ‘ethics in and by design’ in the future.</jats:p>"
10.1007/s43681-022-00228-7,The AI ethics maturity model: a holistic approach to advancing ethical data science in organizations,"<jats:title>Abstract</jats:title><jats:p>The field of AI ethics has advanced considerably over the past years, providing guidelines, principles, and technical solutions for enhancing the ethical development, deployment and usage of AI. However, there is still a clear need for research that facilitates the move from the ‘what’ of AI ethics to the ‘how’ of governance and operationalization. Although promising literature on the challenge of implementation is increasingly more common, so far no systemic analysis has been published that brings the various themes of operationalization together in a way that helps the gradual advancement of AI ethics procedures within organizations. In this opinion paper we therefore set out to provide a holistic maturity framework in the form of an AI ethics maturity model comprising six crucial dimensions for the operationalization of AI ethics within an organization. We contend that advancing AI ethics in practice is a multi-dimensional effort, as successful operationalization of ethics requires combined action on various dimensions. The model as presented is a preliminary result of literature analysis complemented with insights from several practical mutual learning sessions with some of the major public, private and research organizations of the Netherlands. The article contributes to the AI ethics literature and practice by synthesizing relevant aspects of operationalization and relating these to the praxis of AI in a maturity model that provides direction for organizations seeking to implement these ethical principles.</jats:p>"
10.1007/s43681-023-00353-x,The human role to guarantee an ethical AI in healthcare: a five-facts approach,"<jats:title>Abstract</jats:title><jats:p>With the emergence of AI systems to assist clinical decision-making, several ethical dilemmas are brought to the general attention. AI systems are claimed to be the solution for many high-skilled medical tasks where machines can potentially surpass human ability as for example in identifying normal and abnormal chest X-rays. However, there are also warns that AI tools could be the basis for a human replacement that can risk dehumanisation in medicine. In recent years, important proposals in the domain of AI ethics in healthcare have identified main ethical issues, as for example fairness, autonomy, transparency, and responsibility. The human warranty, which implies human evaluation of the AI procedures, has been described to lower the ethical risks. However, as relevant these works have been, translating principles into action has proved challenging as existing codes were mostly a description of principles. There is a great need to produce <jats:italic>how-to</jats:italic> proposals that are specific enough to be action-guiding. We present five human-focussed facts designed into a framework of human action for an ethical AI in healthcare. Through the factors, we examine the role of medical practitioners, patients, and developers in designing, implementing, and using AI in a responsible manner that preserves human dignity. The facts encompass a range of ethical concerns that were commonly found in relevant literature. Given that it is crucial to bring as many perspectives as possible to the field, this work contributes to translate principles into human action to guarantee an ethical AI in health.</jats:p>"
10.1007/s43681-022-00213-0,"Humans, super humans, and super humanoids: debating Stephen Hawking’s doomsday AI forecast",N/A
10.1007/s43681-024-00431-8,"On inscription and bias: data, actor network theory, and the social problems of text-to-image AI models",N/A
10.1007/s43681-023-00299-0,"Navigating the legal landscape of AI copyright: a comparative analysis of EU, US, and Chinese approaches","<jats:title>Abstract</jats:title><jats:p>This paper compares AI copyright approaches in the EU, US, and China, evaluating their effectiveness and challenges. It examines the recognition of AI-generated works as copyrightable and the exclusive rights of copyright owners to reproduce, distribute, publicly display, and perform such works. Differences in approaches, such as recognizing AI as a sui generis right holder in the EU and the broad fair use doctrine in the US, are highlighted. This paper evaluates strengths and weaknesses of each approach, including enforcement and ownership of copyright in AI-generated works, and clarifies issues related to AI and copyright. While the EU and US have more developed legal frameworks for AI copyright than China, all three approaches face challenges that need addressing. This paper concludes by providing insight into the legal landscape of AI copyright and steps necessary for effective protection and use of AI-generated works.</jats:p>"
10.21428/e4baedd9.b4f754fd,"Visual Artists, Technological Shock, and Generative AI",N/A
10.22541/au.170534566.63147021/v1,"The Spectre of Generative AI Over Advertising, Marketing, and Branding",N/A
10.32388/jpecon,The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation,"<jats:p>Generative Artificial Intelligence (AI) has become a powerful tool to create new worlds and inject meaning into staid environments. A process of transforming physical space into digital assets becomes necessary to ensure that culturally relevant objects can persist through the changing tides of society. This paper highlights a process whereby physical assets can be digitized and utilized within a generative AI environment while preserving provenance, authenticity, and veracity. It concludes with salient use cases and a discussion that hopefully will lead to more action.
</jats:p>"
10.1044/leader.ftr2.28092023.ai-academia-slp-aud.52,What Does Generative AI Mean for Graduate Education in CSD?,N/A
10.36227/techrxiv.24470032,Enhancing Data Quality through Generative AI: An Empirical Study with Data,"<jats:p>&lt;p&gt;In today's increasingly data-driven landscape, organizations are shifting their focus toward leveraging data analytics for strategic decision-making. As data becomes a cornerstone of operational and strategic activities, the quality of this data has emerged as a non-negotiable aspect for organizations. Lack of attention to data quality can not only result in considerable revenue losses but can also cripple the effectiveness of analytics, causing misinformed decisions and strategic errors. Against this backdrop, this empirical study delves into the innovative avenue of utilizing Generative Artificial Intelligence (AI) as a mechanism for enhancing data quality.&lt;/p&gt;
&lt;p&gt;The research aims to explore multiple facets of organizational operations—ranging from technical infrastructure to business strategy—to ascertain the potential advantages offered by Generative AI. Utilizing a mix of qualitative and quantitative methods, we conducted in-depth interviews, case studies, and simulations to evaluate the impact of Generative AI on data quality.&lt;/p&gt;
&lt;p&gt;Our findings reveal a multi-layered benefit structure. Notably, we found that Generative AI is not a replacement for existing, traditional methods of data quality assurance but serves as a powerful supplement. It augments traditional methods by increasing the accuracy of data, thereby offering a more reliable foundation for analytics. Additionally, the use of Generative AI can streamline workflows, enhancing productivity among various roles including solution architects and software developers. Moreover, it facilitates a more nuanced and accurate requirement gathering process, enabling businesses to fine-tune their data analytics strategies more effectively.&lt;/p&gt;
&lt;p&gt;In conclusion, our study establishes that integrating Generative AI into data quality management processes can not only resolve immediate issues surrounding data accuracy but also lead to long-term organizational benefits, such as higher efficiency and more effective decision-making. This research serves as a pioneering step in the intersection of Generative AI and data quality, setting the stage for future studies and real-world applications.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4438593,Copyright Safety for Generative AI,N/A
10.3030/101138056,N/A,N/A
10.2139/ssrn.4899433,Generative AI in Charitable Fundraising,N/A
10.31235/osf.io/aq4tw,Complex Systems Analysis of Generative AI:  Mapping Interdependencies in Societal Impact,"<p>This paper applies complex systems theory to examine generative artificial intelligence (AI) as a contemporary wicked problem. Generative AI technologies, which autonomously create content like images and text, intersect with societal domains such as ethics, economics, and governance, exhibiting complex interdependencies and emergent behaviors. Using methodologies like network analysis and agent-based modeling, the paper maps these interactions and explores potential interventions. A mathematical model is developed to simulate the dynamics between key components of the AI-society system, including AI development, economic concentration, labor markets, regulatory frameworks, public trust, ethical implementation, global competition, and distributed AI ecosystems. The model demonstrates non-linear dynamics, feedback loops, and sensitivity to initial conditions characteristic of complex systems. By simulating various interventions, the study provides insights into strategies for steering AI development towards more positive societal outcomes. These include strengthening regulatory frameworks, enhancing ethical implementation, and promoting distributed AI ecosystems. The paper advocates for using this complex systems framework to inform inclusive policy and regulatory strategies that balance innovation with societal well-being. It concludes that embracing complexity enables stakeholders to better navigate the evolving challenges of generative AI, fostering more sustainable and equitable technological advancements.</p>"
10.1007/s43681-021-00083-y,Mapping global AI governance: a nascent regime in a fragmented landscape,"<jats:title>Abstract</jats:title><jats:p>The rapid advances in the development and rollout of artificial intelligence (AI) technologies over the past years have triggered a frenzy of regulatory initiatives at various levels of government and the private sector. This article describes and evaluates the emerging global AI governance architecture and traces the contours of a nascent regime in a fragmented landscape. To do so, it organizes actors and initiatives in a two-by-two matrix, distinguishing between the nature of the driving actor(s) and whether or not their actions take place within the existing governance architecture. Based on this, it provides an overview of key actors and initiatives, highlighting their trajectories and connections. The analysis shows international organizations’ high levels of agency in addressing AI policy and a tendency to address new challenges within existing frameworks. Lastly, it is argued that we are witnessing the first signs of consolidation in this fragmented landscape. The nascent AI regime that emerges is polycentric and fragmented but gravitates around the Organisation for Economic Co-Operation and Development (OECD), which holds considerable epistemic authority and norm-setting power.</jats:p>"
10.1007/s00146-023-01665-6,Non-western AI ethics guidelines: implications for intercultural ethics of technology,N/A
10.4018/979-8-3693-0074-9.ch003,Is It the End of Undergraduate Dissertations?,"<jats:p>This chapter delves into the intriguing realm of generative artificial intelligence (AI) models and their potential impact on undergraduate dissertations in the field of education. As AI continues to advance and permeate various aspects of our lives, the educational landscape is not immune to its transformative influence. The chapter begins by providing an overview of generative AI models, including their underlying principles and techniques such as deep learning, natural language processing, and neural networks. It then explores how these models can be harnessed to generate written content that is coherent, creative, and relevant, raising the question of whether undergraduate dissertations, as we know them, are destined to become obsolete. Advantages of employing generative AI models in education are scrutinized, highlighting their potential to enhance the efficiency and quality of student work.</jats:p>"
10.4018/979-8-3693-1351-0.ch019,Generative AI Soaring in the Skies,"<jats:p>Several regulations in the aviation industry have been made by authorities to minimize the incidents due to language barriers; however, a more reasonable solution lies in equipping prospective pilots with necessary language skills in target settings. This requires a transformation in aviation English (AE) classrooms through the medium of emerging technologies. Accordingly, this chapter presents a novel strategy for rethinking AE instruction by using approaches from generative artificial intelligence. It investigates the ways in which prompt engineering and synthetic content production might be used to improve the efficacy of AE teaching. Learners may benefit from better language comprehension, communication skills, and situational awareness by seamlessly incorporating AI-driven language creation into the curriculum. Instructors could find examples of generative AI-driven classroom applications for improved learning outcomes. Considering these potential contributions to the field, this chapter may, in turn, innovate and uplift the AE courses and safety in the aviation industry.</jats:p>"
10.21275/sr24402184343,The Disruptive Influence of Generative AI in Life Science and Healthcare,N/A
10.21275/sr24509232318,The Development of AI with Generative Capabilities and Its Effect on Education,N/A
10.1007/s43681-023-00314-4,On actor-network theory and algorithms: ChatGPT and the new power relationships in the age of AI,N/A
10.4018/979-8-3693-1950-5.ch011,"Artistic Expressions, Generative AI, and Legal Tapestry","<jats:p>This study delves into the transformative impact of generative artificial intelligence (AI) on creative production, exploring the collaborative efforts of artists and cutting-edge technology. The convergence of human ingenuity and AI in artistic expression expands creative boundaries but introduces complex intellectual property (IP) challenges. Examining beyond visual art, the study reveals the intricate engagement between humans and advanced algorithms, offering a unique perspective on the creative process. The intersection of AI and creativity raises critical copyright concerns, challenging traditional concepts of authorship and infringement. Addressing the global scope of AI-generated art, the study advocates for a unified framework to navigate legal complexities and ensure ethical considerations in the evolving landscape of creative technology.</jats:p>"
10.5220/0012741000003687,Unlocking Adaptive User Experience with Generative AI,N/A
10.4018/979-8-3693-0074-9.ch011,Navigating the Terrain,"<jats:p>The chapter provides an extensive exploration of Generative AI in education. It investigates the evolution and significance of AI in educational settings while delving into contemporary issues such as ethics, privacy, fairness, and pedagogy. Furthermore, it examines the impact on traditional teaching methods, personalization, and accessibility, addressing educational disparities. The chapter also outlines the best practices and lessons learned from case studies and successful institutions, pointing toward future directions, including emerging technologies like GPT-4 and augmented reality. It emphasizes advancing ethical guidelines, enhancing teacher-student collaboration with AI, proposing policy recommendations, and establishing legal frameworks for student data protection, along with government initiatives and funding in the realm of AI in education.</jats:p>"
10.4018/979-8-3693-3278-8.ch008,Exploring Creativity,"<jats:p>Generative AI, often known as genAI, encompasses several forms of artificial intelligence (AI) that has the ability to create unique text, images, video, or audio content. This particular iteration of artificial intelligence acquires knowledge of patterns and data arrangement from its training data, enabling it to produce novel outputs that possess similar statistical characteristics. Generative AI has a diverse range of applications, and each task requires a specialized deep-learning architecture to effectively capture the unique patterns and traits found in the training data. Generative AI models encompass various types, including generative adversarial networks (GANs), variational autoencoders (VAEs), transformers, diffusion models, normalizing flow models, and hybrid models. The configuration of a generative AI model is contingent upon the particular task and domain, encompassing elements such as the neural network's architecture, training approach, loss function, and evaluation metrics. The primary objective of generative AI is to develop autonomous systems capable of generating content that is indiscernible from information created by humans. This encompasses the production of written content, visual graphics, audio recordings, and interactive visual components. Attaining this objective would facilitate a diverse array of applications, encompassing enhanced human-computer interactions and assisting in the advancement of endeavors such as art and storytelling.</jats:p>"
10.1145/3605098.3636180,Duty vs. Consequence: Exploring Teachers' Assessment of the Ethical Dimensions of Generative AI Technologies,N/A
10.4018/979-8-3693-2418-9.ch015,Unleashing Creative Potential,"<jats:p>This study examines the overlooked role of creativity in business school curricula, despite its recognized importance in fostering innovation and solving global challenges. It critiques traditional educational models for failing to nurture creative thinking and argues for the integration of Generative AI (GAI) systems to enhance creativity in business education. Using an exploratory case study approach, the authors examined the existing literature that supports the business imperative for creativity; then, GAI was posited as a tool to improve divergent and convergent thinking, offering practical applications across various business disciplines. Additionally, ethical considerations surrounding data usage, intellectual property, and privacy were highlighted. The study concludes that responsibly integrating GAI can revolutionize business education, preparing future leaders for an AI-augmented business world while maintaining the necessity for ethical, informed, and transparent use.</jats:p>"
10.5220/0012688700003690,Enhancing Constructivist Learning: The Role of Generative AI in Personalised Learning Experiences,N/A
10.4018/979-8-3693-1351-0.ch004,Generative AI-Powered Chatbots,"<jats:p>Imagine having a concept for a literary work but being unable to see it take shape because of a plethora of reasons: paucity of time, writer's block, linguistic barriers, among others. Gen AI promises to be a supportive partner in these creative endeavours. With the synergy of mind and machine, literary texts are born. This chapter attempts to provide insights into the comparison between the nuances of human-AI co-creative partnerships through the crafting of short stories. Using the lens of Collaborative Autoethnography, two seasoned educators collaboratively reflect on cognitive, behavioural, and affective dimensions of co-creating with ChatGPT, a chatbot developed by Open AI. The chapter analyses the co-creation process followed by each of the authors and brings out the commonalities and specificities of the individual lived experience to achieve a narrative piece of work. The insights of the co-creation process would be beneficial to educators, curriculum designers, technology specialists, students and those who intend to use AI-powered chatbots as co-creators.</jats:p>"
10.1007/s43681-023-00263-y,"I, Robot: the three laws of robotics and the ethics of the peopleless economy",N/A
10.4324/9781003459026-5,Developing an AI in Education Policy,N/A
10.4018/979-8-3693-0487-7.ch005,Utilizing Generative AI With Second-Language Users and Bilingual Students,"<jats:p>The growth of generative artificial intelligence (for purposes of this chapter, “AI”) has created endless systems to utilize modern technological assistance in the educational arena. AI has the potential to significantly impact higher education by facilitating personalized development and knowledge sharing. There are many ways that AI can be leveraged to enhance the education process. This chapter will focus on second-language users and bilingual students learning new language concepts. Second language users can be defined as students in the language acquisition process. A bilingual student is any student who has already acquired a second language but needs to build on terminology in their field of study. AI can provide a functional teaching tool for educators to assist with building the required knowledge base. The premise is to embrace the tool—versus forbidding its use by students—to assist them in their educational growth.</jats:p>"
10.4018/979-8-3693-0831-8.ch009,The Influence of Generative AI on Interpersonal Communication Dynamics,"<jats:p>In this comprehensive exploration, the interaction between generative AI and interpersonal communication is examined. The initial sections delve into the characteristics and limitations of AI-generated responses, highlighting the challenges of context and non-verbal cues interpretation. The potential for AI-driven interpersonal skill development is presented. The discussion progresses to address the changing dynamics in the classroom, contrasting traditional communication training with AI-augmented methods. The efficacy of AI in group discussions and role-plays is assessed, with a central focus on whether AI augments or diminishes human connections. The final sections explore the potential of generative AI in reshaping our understanding of effective communication and the necessity for educators to uphold the human element while leveraging AI's skill-enhancing capabilities. This comprehensive review offers insights into the evolving landscape of AI and interpersonal communication, shedding light on its opportunities, challenges, and the path forward.</jats:p>"
10.4018/979-8-3693-2418-9.ch008,Generative AI in Curriculum Development,"<jats:p>Generative artificial intelligence (GAI) is becoming a crucial influence in different industries, particularly transforming education through the reshaping of curriculum creation. This chapter discusses how GAI can improve educational inclusion and comprehensiveness by providing personalized, customized and adaptive learning experiences. The framework in this chapter provides a structured approach to integrating GAI into curriculum design with the goal of creating dynamic, flexible, and personalized educational paths. By utilizing advanced machine learning and natural language processing, GAI enables educators to create personalized and tailored learning modules, promoting an inclusive educational setting. The discussion in this chapter highlights the crucial significance of GAI in developing educational methods and curriculum, establishing a new model for future learning environments.</jats:p>"
10.2139/ssrn.4478370,Generative AI and Author Remuneration,N/A
10.36227/techrxiv.24449185,How Text-to-Image Generative AI Is Transforming Mediated Action?,"<jats:p>&lt;p&gt;This article examines the intricate relationship between humans and text-to-image generative models (genAI) in the realms of art and design. The article frames that relationship in the theory of mediated action—a well-established theory that conceptualizes how tools shape human thoughts and actions. It describes genAI systems as learning, co-creating, and communicating, multimodally capable hybrid systems that distill and rely on the wisdom and creativity of massive crowds of people and can sometimes surpass them. Those systems elude the theoretical description of the role of tools and locus of control in mediated action. The article asks how well the theory can accommodate both the transformative potential of genAI tools in creative fields and art, and the ethics of the emergent social dynamics it generates. The article concludes by discussing the fundamental changes and broader implications that genAI brings to the realm of mediated action and, ultimately, to the very fabric of our daily lives. &lt;/p&gt;</jats:p>"
10.2139/ssrn.4595149,Generative AI and IP Infringement,N/A
10.4018/979-8-3693-0074-9.ch005,Stepping Stones for Self-Learning,"<jats:p>One of the themes in the emergence of text- and image-making (multimodal) generative AIs is their value in the learning space, with the vast potential just beginning to be explored by mass humanity. This chapter explores the potential and early use of large language models (LLMs) harnessed for their mass learning, human-friendly conversations, and their efficacies, for self-learning for individuals and groups, based on a review of the literature, system constraints and affordances, and abductive logic. There are insights shared about longitudinal and lifelong learning and foci on co-evolving processes between the human learner and the computing machines and large language models.</jats:p>"
10.4018/979-8-3693-1565-1.ch010,Navigating the Legal and Ethical Framework for Generative AI,"<jats:p>Generative AI systems have given incredible ability to independently produce a wide variety of content types, including textual, visual, and more. Complex issues with copyright protection and intellectual property rights have arisen as a result of this change. With a focus on fostering responsible global governance, this research delves into the complex legal and ethical considerations underlying Generative AI. The goal of this chapter is to take a look at the complicated legal issues that come up because of Generative AI's ability to generate material on its own. This chapter analyzes the current legal documents, legislation, and international treaties, focusing on ethical concerns. Ultimately, the authors want to have a positive impact on efforts to build responsible and efficient international frameworks for regulating Generative AI. This study provides an exhaustive case for the implementation of legal frameworks that can efficiently tackle the intricate legal and ethical quandaries posed by Generative AI, while simultaneously encouraging the progress of innovation and creativity.</jats:p>"
10.21275/sr24613105251,Charting the Future: Harnessing the Power of Generative AI in Financial Ecosystems,N/A
10.21275/sr24520092232,DyGAISP: Generative AI-Powered Approach for Intelligent Software Lifecycle Planning,N/A
10.33140/eoa.01.04.06,Melo Harmony: Exploring Emotion in Crafting AI-Generated Music with Generative Adversarial Network Powered Harmony,"<jats:p>This research paper delves into the convergence of artificial intelligence (AI) and music composition by examining the integration of emotion in crafting AI-generated music through Generative Adversarial Network (GAN)-powered harmony. The primary goal is to demonstrate that AI-generated music can effectively evoke and communicate emotions, enhancing its artistic and expressive potential. This paper presents a comprehensive framework for harmonization using GANs, infusing emotional awareness into the generated compositions, and outlines future directions for practical application and validation.</jats:p>"
10.1117/12.3013240.75c55873-4a66-ee11-a99c-00505691c5e1,N/A,N/A
10.1609/aaaiss.v3i1.31256,The Impacts of Text-to-Image Generative AI on Creative Professionals According to Prospective Generative AI Researchers: Insights from Japan,"<jats:p>The growing interest in Japan to implement text-to-image (T2I) generative artificial intelligence (GenAI) technologies in creative workflows has raised concern over what ethical and social implications these technologies will have on creative professionals. Our pilot study is the first to discuss what social and ethical oversights may emerge regarding such issues from prospective Japanese researchers – computer science (CS) graduate students studying in Japan. Given that these students are the primary demographic hired to work at research and development (R&amp;D) labs at the forefront of such innovations in Japan, any social and ethical oversight on such issues may unequip them as future knowledge experts who will play a pivotal role in helping shape Japan’s policies regarding image generating AI technologies.</jats:p>"
10.4018/979-8-3693-2418-9.ch003,Finding Time for Creativity in Higher Education Writing Through Generative AI,"<jats:p>In this chapter, the authors discuss the relationship between time poverty, creativity, and Large Language Models of AI (LLMs) or Generative AI technologies in the writing classroom, including ChatGPT and GenAI software in word processors such as Microsoft Word. Through examples of writing provided by ChatGPT, a student profile, and comparative analysis of writing provided by two LLMs, they make an argument that LLMs should be integrated into the writing classroom in order to offer students who suffer from time poverty the ability to practice creativity in their writing which they may struggle to achieve when limited to the time they can devote to their classwork. Specifically, they explore how ChatGPT can support students in the writing process, including researching, pre-writing, writing, revision, and reflection in order to foster creativity in their writing.</jats:p>"
10.4018/979-8-3693-1950-5.ch009,Remote Virtual Sanctuary,"<jats:p>2023 was an original year, with global humanity emerging from a deadly pandemic (COVID-19), facing the advent of artmaking generative AIs, and surviving in a time of geopolitical turmoil, economic and financial pressures, and social strife. What role does an online social network (built up around an artmaking generative AI platform) play for people in this present moment? How does the remote virtual community enable participants to seek various fulfillments? The web-facing Deep Dream Generator tool and platform has proven itself to be a powerful social space for many with rich immersions and remote social interactions. This work is a practice-led case study.</jats:p>"
10.4018/979-8-3693-5578-7.ch003,Exploring the Application of Generative AI in Human Resource Management,"<jats:p>This chapter reviews the current state of generative artificial intelligence (AI) and human resource management (HRM). It discusses the current application of Generative AI in the core functional areas of HRM, identifies the main challenges posed by Generative AI, emphasizes the increasing role and influence of Generative AI applications in the workplace, and suggests future research directions. Current applications of Generative AI in HRM include automation, personalization, decision support, and bias reduction in HR processes. However, the urgent challenges related to transparency, bias mitigation, ethical use, and data privacy must be addressed for responsible deployment. This chapter highlights the opportunities for integrating Generative AI in HRM to improve HR processes while ensuring ethical and fair implementation. The future research directions for Generative AI and HRM focus on reducing bias in AI models, developing ethical frameworks, enhancing data privacy, and exploring AI's impact on organizational culture and employee engagement.</jats:p>"
10.1093/oxfordhb/9780190067397.013.23,Troubleshooting AI and Consent,"<p>This chapter addresses the controversy over the role of consent in data protection, as artificial intelligence systems have proliferated in people’s daily lives. Digital consent has been criticized as a meaningless, procedural act because users encounter so many different, long, and complicated terms of service that do not help them effectively assess potential harms or threats. AI systems have played a role in exacerbating existing issues, creating new challenges, and presenting alternative solutions. Most of the critiques and cures for this broken arrangement address choice-making, not consent. As the United States debates whether and why to break up big tech, and the European Union considers enforcement actions under the General Data Protection Regulation and how to update its laws to address tracking techniques in a new AI-driven smart world, consent cannot be confused with choice. Consent must be defined by its moral core, involving clear background conditions, defined scope, knowledge, voluntariness, and fairness. When consent meets these demands, it remains a powerful tool for contributing to meaningful data protection at the individual and societal levels.</p>"
10.4018/979-8-3693-8557-9.ch012,From Data to Insights,"<jats:p>This chapter examined the utilization of GeoAI for healthcare issues like patient demographics, socio-economic factors, health insurance coverage, and environmental conditions to enhance healthcare delivery. By analyzing population density and socio-economic data, GeoAI can optimize the placement of healthcare facilities and resources, ensuring equitable access. Environmental data, such as air and water quality, can identify regions at risk for specific health issues, guiding targeted interventions. GeoAI can address healthcare disparities by considering social determinants of health and enabling personalized treatment plans. Integrating high-quality, timely, and relevant geospatial data can allow GeoAI to facilitate accurate decision-making and resource allocation, improving patient outcomes and public health strategies. However, challenges include ensuring data privacy and security, managing technical and operational integration complexities, and requiring specialized expertise, which can be overcome with robust data governance and interdisciplinary collaboration.</jats:p>"
10.1007/s43681-023-00372-8,Socialisation approach to AI value acquisition: enabling flexible ethical navigation with built-in receptiveness to social influence,"<jats:title>Abstract</jats:title><jats:p>This article describes an alternative starting point for embedding human values into artificial intelligence (AI) systems. As applications of AI become more versatile and entwined with society, an ever-wider spectrum of considerations must be incorporated into their decision-making. However, formulating less-tangible human values into mathematical algorithms appears incredibly challenging. This difficulty is understandable from a viewpoint that perceives human moral decisions to primarily stem from intuition and emotional dispositions, rather than logic or reason. Our innate normative judgements promote prosocial behaviours which enable collaboration within a shared environment. Individuals internalise the values and norms of their social context through socialisation. The complexity of the social environment makes it impractical to consistently apply logic to pick the best available action. This has compelled natural agents to develop mental shortcuts and rely on the collective moral wisdom of the social group. This work argues that the acquisition of human values cannot happen just through rational thinking, and hence, alternative approaches should be explored. Designing receptiveness to social signalling can provide context-flexible normative guidance in vastly different life tasks. This approach would approximate the human trajectory for value learning, which requires social ability. Artificial agents that imitate socialisation would prioritise conformity by minimising detected or expected disapproval while associating relative importance with acquired concepts. Sensitivity to direct social feedback would especially be useful for AI that possesses some embodied physical or virtual form. Work explores the necessary faculties for social norm enforcement and the ethical challenges of navigating based on the approval of others.</jats:p>"
10.1007/s43681-023-00386-2,A design perspective on how to tackle gender biases when developing AI-driven systems,"<jats:title>Abstract</jats:title><jats:p>A growing awareness of bias in artificial intelligence (AI) systems has recently emerged, leading to an increased number of publications discussing ethics in AI. Nevertheless, the specific issue of gender bias remains under-discussed. How can design contribute to preventing the emergence of gender bias in AI-driven systems? To answer this question, we investigated the current state of AI ethical guidelines within the European Union. The results revealed that most guidelines do not acknowledge gender bias but address discrimination. This raised our concerns, as addressing multiple biases simultaneously might not effectively mitigate any of them due to their often-unconscious nature. Furthermore, our results revealed a lack of quantitative evidence supporting the effectiveness of bias prevention implementation methods and solutions. In conclusion, based on our analysis, we propose four recommendations for designing effective guidelines to tackle gender biases in AI. Moreover, we stress the central role of diversity in embedding the gender perspective from the beginning in any design activity.</jats:p>"
10.1007/s43681-024-00456-z,Gender Tapestry: gender classification as color assignation,"<jats:title>Abstract</jats:title><jats:p><jats:italic>Gender Tapestry</jats:italic> is a multi-stage interactive AI art project, challenging traditional gender classification systems. This project diverges from binary approaches by recognizing the individuality of the gender experience and expression. This paper draws parallels to the ways color perception differs amongst people and how gender is also experienced in different ways due to lived experiences. Gender Tapestry uses a multi-label classification system, with predictions extending across six gender categories, with a custom RGB color generated based on the outcomes. Participants receive binary images of their face in their personalized colors and styles, while their uploaded photos contribute to a live Generative Adversarial Network training process. This work was created in response to the very binary representations of gender in AI and the lack of representation for genders outside of the binary. The culmination is an evolving mosaic artwork, incorporating all users' custom colors, illustrating gender as a fluid construct. The mosaic gains in complexity as more images are added and more colors enter the mix, creating a community artwork on gender as a 3D color spectrum. This work contributes to the discourse on diversity and inclusion in AI, emphasizing the fluidity of gender and fostering unconventional artistic representations.</jats:p>"
10.1007/s43681-024-00421-w,Adopting trust as an ex post approach to privacy,"<jats:title>Abstract</jats:title><jats:p>This research explores how a person with whom information has been shared and, importantly, an artificial intelligence (AI) system used to deduce information from the shared data contribute to making the disclosure context private. The study posits that private contexts are constituted by the interactions of individuals in the social context of intersubjectivity based on trust. Hence, to make the context private, the person who is the trustee (i.e., with whom information has been shared) must fulfil trust norms. According to the commitment account of trustworthiness, a person is trustworthy only if they satisfy the norm of competence. It is argued that a person using an AI system to answer a question is competent only if they are ex post justified in believing what has been delivered by the AI system. A person’s belief is justified in the doxastic sense only if the AI system is accurate. This feature of AI’s performance affects a person’s competence and, as a result, trustworthiness. The effect of AI on trust as an essential component of making the context private, and thus on privacy, means an AI system also impacts privacy. Therefore, a private context is constituted when the individual with whom the information is shared fulfils the competence norm and the AI system used for analysing the information is sufficiently accurate to adhere to this norm. The result of this research emphasises the significance of the relationship between individuals involved in information-sharing and how an AI system used for analysing that information impacts the relationship regarding making the context private, as well as how it impacts privacy. The findings of this research have significant implications for improving or ameliorating privacy regulations in light of trust.</jats:p>"
10.1007/s43681-021-00100-0,Why ethical audit matters in artificial intelligence?,N/A
10.1007/979-8-8688-0403-8_8,Building Demo Applications Using LLMs,N/A
10.1007/979-8-8688-0456-4_8,The Early Career Professional’s Future With AI,N/A
10.1007/979-8-8688-0456-4_11,Ethical Implications and Societal Impact of AI,N/A
10.1007/s43681-023-00320-6,The many meanings of meaningful human control,"<jats:title>Abstract</jats:title><jats:p>The concept of Meaningful Human Control (MHC) has gained prominence in the field of Artificial Intelligence ethics. MHC is discussed in relation to lethal autonomous weapons, autonomous cars, and more recently, AI systems in general. Little, however, has been done to analyze the concept. Those using MHC tend to look at it narrowly and intuitively—as if it is clear what it means. They fail to see the many issues concerning human control over machines. In this article, I break the concept into its three constitutive words (‘meaningful’, ‘human’, and, ‘control’) to outline the many meanings of MHC. While the intention is not to come to the<jats:italic>real</jats:italic>meaning of MHC, this analysis brings up the many issues that should be considered if meaningful human control is to be realized. These include: which humans count as meaningful in the application context, whether the control those humans are given must be meaningful, whether humans must retain control over the things that are meaningful in life, whether the style of control is human-like, whether each actor (designer, operator, subject, government) has the control they need, and what it is exactly that a human is controlling (e.g., the training data, the inputs, the outputs, etc. of the AI system).</jats:p>"
10.1007/s43681-024-00547-x,Artificial intelligence and its ‘slow violence’ to human rights,"<jats:title>Abstract</jats:title><jats:p>Human rights concerns in relation to the impacts brought forth by artificial intelligence (‘AI’) have revolved around examining how it affects specific rights, such as the right to privacy, non-discrimination and freedom of expression. However, this article argues that the effects go deeper, potentially challenging the foundational assumptions of key concepts and normative justifications of the human rights framework. To unpack this, the article applies the lens of ‘slow violence’, a term borrowed from environmental justice literature, to frame the grinding, gradual, attritional harms of AI towards the human rights framework.</jats:p><jats:p>The article examines the slow violence of AI towards human rights at three different levels. First, the individual as the subject of interest and protection within the human rights framework, is increasingly unable to understand nor seek accountability for harms arising from the deployment of AI systems. This undermines the key premise of the framework which was meant to empower the individual in addressing large power disparities and calling for accountability towards such abuse of power. Secondly, the ‘slow violence’ of AI is also seen through the unravelling of the normative justifications of discrete rights such as the right to privacy, freedom of expression and freedom of thought, upending the reasons and assumptions in which those rights were formulated and formalised in the first place. Finally, the article examines how even the wide interpretations towards the normative foundation of human rights, namely human dignity, is unable to address putative new challenges AI poses towards the concept. It then considers and offers the outline to critical perspectives that can inform a new model of human rights accountability in the age of AI.</jats:p>"
10.1007/s43681-023-00369-3,"Click-Gap, paternalism, and tech giants’ relationships with their users","<jats:title>Abstract</jats:title><jats:p>The spread of misinformation and fake news raises important problems for our society and for our democracy. From the January 6 attack on the U.S. Capitol to vaccine hesitancy, from suppressing voter turnout to peddling conspiracy theories, we know that these problems are real and need to be taken seriously. While misinformation is not a new problem for democracy, it can spread more quickly and easily because of new media’s design and popularity. Given these problems, it is encouraging that some technology companies are taking steps to reduce the spread of misinformation and fake news on the platforms they manage. Despite this seemingly positive development, some scholars have criticized some interventions designed to combat the spread of misinformation and fake news as paternalistic. For example, a 2019 Facebook intervention called Click-Gap aimed to reduce the amount of low-quality content (including fake news and misinformation) that users see on their NewsFeed. Click-Gap has been criticized as an instance of epistemic paternalism because it was adopted (1) with the goal of improving the epistemic status of its users and (2) irrespective of what the company believed the wishes of its users to be. If interventions like Click-Gap are problematic because paternalistic, those of us interested in the ethics of technology would face a dilemma—either endorse technology companies treating their users paternalistically or endorse their failing to act to combat the spread of misinformation and fake news on their platforms. Both options seem to me to be problematic. While paternalism may sometimes be permissible, I think we should be very hesitant to endorse a paternalistic relationship between technology companies and their users. The relationship does not seem to bear the right sort of structure to one in which paternalism might be appropriate, if it ever is. The second option seems, if anything worse: surely technology companies should not stand by and change nothing about their platforms despite the spread of misinformation and fake news in those spaces. In this paper, I argue that Click-Gap and interventions like it are not paternalistic, contrary to the conclusion of other scholars. Further, I will argue that the focus on paternalism itself is actually a red herring here. While not just any intervention or strategy that purports to reduce fake news and misinformation is permissible, we should want technology companies to take user well-being seriously and be able to take that well-being as a direct reason for action. Their doing so is not paternalistic nor even morally problematic, and it should not be criticized as such.</jats:p>"
10.1007/s43681-024-00545-z,"The ethico-politics of design toolkits: responsible AI tools, from big tech guidelines to feminist ideation cards","<jats:title>Abstract</jats:title><jats:p>This paper interrogates the belief in <jats:italic>toolkitting</jats:italic> as a method for translating AI ethics theory into practice and assesses the toolkit paradigm’s effect on the understanding of ethics in AI research and AI-related policy. Drawing on a meta-review of existing ‘toolkit-scoping’ work, I demonstrate that most toolkits embody a reductionist conception of ethics and that, because of this, their capacity for facilitating change is limited. Then, I analyze the features of several ‘alternative’ toolkits–informed by feminist theory, posthumanism, and critical design–whose creators recognize that ethics cannot become a box-ticking exercise for engineers, while <jats:italic>the ethical</jats:italic> should not be dissociated from <jats:italic>the political</jats:italic>. This analysis then serves to provide suggestions for future toolkit creators and users on how to meaningfully adopt the toolkit format in AI ethics work without overselling its transformative potential: how different stakeholders can draw on the myriad of tools to achieve socially desirable results but reject the oversimplification of ethical practice that many toolkits embody.</jats:p>"
10.1007/s43681-020-00025-0,Job candidates’ reactions to AI-Enabled job application processes,N/A
10.1007/s43681-024-00477-8,Addressing trade-offs in co-designing principles for ethical AI: perspectives from an industry-academia collaboration,"<jats:title>Abstract</jats:title><jats:p>The development and deployment of artificial intelligence (AI) has rapidly outpaced regulation. As a result, many organizations opt to develop their own principles for the ethical development of AI, though little research has examined the processes through which they are developed. Prior research indicates that these processes involve perceived trade-offs between competing considerations, and primarily between ethical concerns and organizational benefits or technological development. In this paper, we report on a novel, collaborative initiative in Japan between researchers in the humanities and social sciences, and industry actors to co-design organizational AI ethics principles. We analyzed the minutes from 20 meetings from the formative phase of the development of these principles using an inductive process drawing on thematic analysis, to identify the issues of importance to participants. Through this, we identified four core trade-offs faced by participants. We find that, contrary to prior literature, participants were not just concerned with trade-offs between ethical concerns and organizational benefits or technological development, but also between competing, ethically-oriented considerations. We use the results of this study to highlight a need for further research to understand the longer-term impact on organizations and on society of organization-led approaches to AI ethics.</jats:p>"
10.1007/s43681-022-00239-4,"Democracy, epistemic agency, and AI: political epistemology in times of artificial intelligence","<jats:title>Abstract</jats:title><jats:p>Democratic theories assume that citizens have some form of political knowledge in order to vote for representatives or to directly engage in democratic deliberation and participation. However, apart from widespread attention to the phenomenon of fake news and misinformation, less attention has been paid to <jats:italic>how</jats:italic> they are supposed to acquire that knowledge in contexts shaped by artificial intelligence and related digital technologies. While this topic can also be approached from an empirical angle, this paper contributes to supporting concerns about AI and democracy by looking at the issue through the lens of political epistemology, in particular using the concept of epistemic agency. It argues that artificial intelligence (AI) endangers democracy since it risks to diminish the epistemic agency of citizens and thereby undermine the relevant kind of political agency in democracy. It shows that next to fake news and manipulation by means of AI analysis of big data, epistemic bubbles and the defaulting of statistical knowledge endanger the epistemic agency of citizens when they form and wish to revise their political beliefs. AI risks to undermine trust in one’s own epistemic capacities and hinder the exercise of those capacities. If we want to protect the knowledge basis of our democracies, we must address these problems in education and technology policy.</jats:p>"
10.1007/s43681-023-00323-3,Challenging AI for Sustainability: what ought it mean?,"<jats:title>Abstract</jats:title><jats:p>This paper argues that the terms ‘Sustainable artificial intelligence (AI)’ in general and ‘Sustainability of AI’ in particular are overused to the extent that they have lost their meaning. The AI for (social) good movement is a manifestation of this trend in which almost any application used in the context of healthcare or agriculture can be classified as AI for good regardless of whether such applications have been evaluated from a broader perspective. In this paper, we aim to create a common understanding of what the ‘AI for Sustainability’ movement ought to mean. We distinguish between two possible AI for Sustainability applications, namely those that fulfill the necessary conditions and those that fulfill the sufficient conditions. The former are purely predictive systems that serve as information providers. The latter are directly involved in an activity that contributes to a sustainability goal. We argue that taking action is a key element in distinguishing between these two application groups, as inaction is the key bottleneck in effectively tackling climate change. Furthermore, we question how effective the use of AI applications can be for sustainability when the systems themselves are inherently unsustainable. Hence, AI for Sustainability should include both an action that contributes to a sustainable end goal as well as an investigation of the sustainability issues of the AI system itself. Following that, Sustainable AI research can be on a gradient: AI in an application domain, AI towards sustainability, and AI for Sustainability.</jats:p>"
10.1007/s43681-023-00275-8,A policy primer and roadmap on AI worker surveillance and productivity scoring tools,N/A
10.1007/s43681-023-00258-9,From ethical AI frameworks to tools: a review of approaches,"<jats:title>Abstract</jats:title><jats:p>In reaction to concerns about a broad range of potential ethical issues, dozens of proposals for addressing ethical aspects of artificial intelligence (AI) have been published. However, many of them are too abstract for being easily translated into concrete designs for AI systems. The various proposed ethical frameworks can be considered an instance of principlism that is similar to that found in medical ethics. Given their general nature, principles do not say how they should be applied in a particular context. Hence, a broad range of approaches, methods, and tools have been proposed for addressing ethical concerns of AI systems. This paper presents a systematic analysis of more than 100 frameworks, process models, and proposed remedies and tools for helping to make the necessary shift from principles to implementation, expanding on the work of Morley and colleagues. This analysis confirms a strong focus of proposed approaches on only a few ethical issues such as explicability, fairness, privacy, and accountability. These issues are often addressed with proposals for software and algorithms. Other, more general ethical issues are mainly addressed with conceptual frameworks, guidelines, or process models. This paper develops a structured list and definitions of approaches, presents a refined segmentation of the AI development process, and suggests areas that will require more attention from researchers and developers.</jats:p>"
10.1007/s43681-023-00344-y,Socially responsible facial recognition of animals,N/A
10.1007/s43681-023-00343-z,"Algorithmic audits of algorithms, and the law",N/A
10.1093/oxfordhb/9780190067397.001.0001,The Oxford Handbook of Ethics of AI,"<p>This book explores the intertwining domains of artificial intelligence (AI) and ethics—two highly divergent fields which at first seem to have nothing to do with one another. AI is a collection of computational methods for studying human knowledge, learning, and behavior, including by building agents able to know, learn, and behave. Ethics is a body of human knowledge—far from completely understood—that helps agents (humans today, but perhaps eventually robots and other AIs) decide how they and others should behave. Despite these differences, however, the rapid development in AI technology today has led to a growing number of ethical issues in a multitude of fields, ranging from disciplines as far-reaching as international human rights law to issues as intimate as personal identity and sexuality. In fact, the number and variety of topics in this volume illustrate the width, diversity of content, and at times exasperating vagueness of the boundaries of “AI Ethics” as a domain of inquiry. Within this discourse, the book points to the capacity of sociotechnical systems that utilize data-driven algorithms to classify, to make decisions, and to control complex systems. Given the wide-reaching and often intimate impact these AI systems have on daily human lives, this volume attempts to address the increasingly complicated relations between humanity and artificial intelligence. It considers not only how humanity must conduct themselves toward AI but also how AI must behave toward humanity.</p>"
10.21428/e4baedd9.3f5bb369,Evaluating the Effectiveness of AI Source Disclosure in Human–AI Communication,N/A
10.2139/ssrn.4597899,The power of generative marketing: Can generative AI reach human-level visual marketing content?,N/A
10.1007/s43681-022-00218-9,"Ethics and diversity in artificial intelligence policies, strategies and initiatives","<jats:title>Abstract</jats:title><jats:p>A burgeoning of Artificial Intelligence (AI) technologies in recent years has led to increased discussion about its potential to address many issues considered otherwise intractable, including those highlighted by the United Nations 2030 Agenda for Sustainable Development and associated Sustainable Development Goals. In tandem with this growth in AI is an expanding body of documentation regarding how such advanced technologies should be governed and managed. Issued by a variety of sources and comprising frameworks, policies and guidelines, this body of work encompasses the legal, social, ethical and policy issues around AI. With at least 470 such documents identified, as of May 2021, in the Council of Europe’s tracker of AI initiatives, questions are emerging around the diversity of views expressed, especially regarding the influence of the Global North or Euro-American perspectives. Our previous analysis of a corpus of largely grey literature discovered blind spots regarding both gender representation and perspectives from the Global South. Expanding on that work, this paper examines a significantly extended corpus, with a focus on the role of underrepresented groups in the wider AI discourse. We find that voices from the Global South and consideration of alternative ethical approaches are largely absent from the conversation. In light of the prominence of social, cultural and ethical perspectives from the Global North, this paper explores implications for the development of standards for ethical AI. Concluding by offering approaches to incorporate more diverse ethical viewpoints and beliefs, we call for increased consideration of power structures when developing AI ethics policies and standards within these alternative socio-cultural and socio-economic contexts.</jats:p>"
10.1007/s43681-020-00036-x,Declaration on the ethics of brain–computer interfaces and augment intelligence,N/A
10.4324/9781003507949-3,Inviting AI Into the Composition Classroom,N/A
10.1162/99608f92.e6245c19,Unpacking AI Governance From the Margins,N/A
10.1007/979-8-8688-0456-4_1,Introduction,N/A
10.1162/99608f92.562ff0f5,AI Safety Is a Narrative Problem,N/A
10.1201/9781003499527-11,Ethics in AI,N/A
10.1007/s43681-024-00504-8,Decisional value scores: A new family of metrics for ethical AI-ML,"<jats:title>Abstract</jats:title><jats:p>Research in ethical AI has made strides in quantitative expression of ethical values such as fairness, transparency, and privacy. Here we contribute to this effort by proposing a new family of metrics called “decisional value scores” (DVS). DVSs are scores assigned to a system based on whether the decisions it makes meet or fail to meet a particular standard (either individually, in total, or as a ratio or average over decisions made). Advantages of DVS include greater discrimination capacity between types of ethically relevant decisions and facilitation of ethical comparisons between decisions and decision-making systems, including across different modalities (for instance: human, machine, or coupled human–machine systems). After clarifying ambiguities in the concept of “decision” itself, including the question of how to individuate the decisions made by a system, we discuss the role and meaning of “decision” in common AI and machine learning approaches such as decision trees, neural networks, SVMs, and unsupervised classifiers. We then show how DVSs may be defined for several ethical values of interest, with an extended discussion of transparency. Finally, we explore how such metrics can be applied to real decision-making systems through two case studies: evaluations of LLMs for transparency; and evaluations of criminal risk assessment tools for utility, rights violations, fairness, and transparency.</jats:p>"
10.1007/978-3-031-23035-6_4,AI Ethics in Higher Education: Research Experiences from Practical Development and Deployment of AI Systems,<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) offers tangible benefits in several application domains like disease diagnosis in health.</jats:p>
10.1007/s43681-022-00173-5,AI ethics and its pitfalls: not living up to its own standards?,"<jats:title>Abstract</jats:title><jats:p>AI ethics is deemed to be an essential ingredient in the quest for trustworthy AI. Hence, demands for implementing AI ethics and ethicists into AI organizations, especially corporations, are ubiquitous. However, the assumption that AI ethicists have particular epistemological advantages compared to non-ethicists as well as the idea that AI ethics automatically decreases the likelihood of unethical outcomes are both flawed. Therefore, this comment lists risks that either originate from AI ethicists themselves or from the consequences their embedding in AI organizations has. The compilation of risks comprises psychological considerations concerning the cognitive biases of AI ethicists themselves as well as biased reactions to their work, subject-specific and knowledge constraints AI ethicists often succumb to, negative side effects of ethics audits for AI applications, and many more. Ultimately, the aim of this comment is not to diminish or deny the importance of the discipline of AI ethics, but rather to increase its capacities for self-reflection and, ultimately, effectiveness.</jats:p>"
10.1162/99608f92.5cd4a32e,AI and Creative Work,N/A
10.1007/s43681-024-00488-5,AI art and public literacy: the miseducation of Ai-Da the robot,"<jats:title>Abstract</jats:title><jats:p>This article examines the implications of the artworks and public performances of the robot artist Ai-Da. While the project claims to advance AI public literacy and foster critical debate around intelligent systems, it instead ends up perpetuating popular misunderstandings about AI creativity, agency, and consciousness. Built in 2019, Ai-Da is a humanoid robot capable of creating drawings, paintings, and composing poetry. However, the project often conceals or miscommunicates the technical aspects of Ai-Da’s capabilities in a manner that encourages the public to misattribute human-like traits to the robot. This lack of transparency in the presentation of Ai-Da’s abilities and the creative processes involved risks reinforcing existing misconceptions about AI, rather than promoting a more nuanced understanding. By employing discourse analysis and drawing on scholarship on machine and computational creativity, anthropomorphism in social robots, and posthuman embodiment, this article uses the Ai-Da project as a case study to illustrate how the dangers of AI hype can be obscured when presented through the lens of public art. The analysis examines how the Ai-Da project, despite its stated goals of advancing AI literacy, fails to effectively challenge and may even exacerbate public misperceptions about the nature of AI-generated art and creativity.</jats:p>"
10.1007/s43681-022-00163-7,AI-deploying organizations are key to addressing ‘perfect storm’ of AI risks,"<jats:title>Abstract</jats:title><jats:p>We argue that a perfect storm of five conditions heightens the risk of harm to society from artificial intelligence: (1) the powerful, invisible nature of AI, (2) low public awareness and AI literacy, (3) rapid scaled deployment of AI, (4) insufficient regulation, and (5) the gap between trustworthy AI principles and practices. To prevent harm, fit-for-purpose regulation and public AI literacy programs have been recommended, but education and government regulation will not be sufficient: AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and taking accountability to mitigate the risks.</jats:p>"
10.1007/978-1-4842-9852-7_4,Prompt Engineering,N/A
10.21125/iceri.2023.2250,"TRENDS, IMPACT AND CONTROVERSIES OF ARTIFICIAL INTELLIGENCE(AI) ESPECIALLY GENERATIVE AI IN HIGHER EDUCATION",N/A
10.1007/s43681-021-00117-5,AI audits for assessing design logics and building ethical systems: the case of predictive policing algorithms,"<jats:title>Abstract</jats:title><jats:p>Organisations, governments, institutions and others across several jurisdictions are using AI systems for a constellation of high-stakes decisions that pose implications for human rights and civil liberties. But a fast-growing multidisciplinary scholarship on AI bias is currently documenting problems such as the discriminatory labelling and surveillance of historically marginalised subgroups. One of the ways in which AI systems generate such downstream outcomes is through their inputs. This paper focuses on a specific input dynamic which is the theoretical foundation that informs the design, operation, and outputs of such systems. The paper uses the set of technologies known as predictive policing algorithms as a case example to illustrate how theoretical assumptions can pose adverse social consequences and should therefore be systematically evaluated during audits if the objective is to detect unknown risks, avoid AI harms, and build ethical systems. In its analysis of these issues, the paper adds a new dimension to the literature on AI ethics and audits by investigating algorithmic impact in the context of underpinning theory. In doing so, the paper provides insights that can usefully inform auditing policy and practice instituted by relevant stakeholders including the developers, vendors, and procurers of AI systems as well as independent auditors.</jats:p>"
10.1007/s43681-021-00085-w,Can AI systems meet the ethical requirements of professional decision-making in health care?,N/A
10.1007/s43681-020-00017-0,AI and moral thinking: how can we live well with machines to enhance our moral agency?,N/A
10.1007/s43681-023-00288-3,"The democratic offset: Contestation, deliberation, and participation regarding military applications of AI","<jats:title>Abstract</jats:title><jats:p>Authoritarian regimes’ unrestricted collection of citizens’ data might constitute an advantage regarding the development of some types of AI, and AI might facilitate authoritarian practices. This feedback loop challenges democracies. In a critical continuation of the Pentagon’s Third Offset Strategy, I investigate a possible Democratic Offset regarding military applications of AI focussed on contestation, deliberation, and participation. I apply Landemore’s Open Democracy, Hildebrandt’s Agonistic Machine Learning, and Sharp’s Civilian-Based Defence. Discussing value pluralism in AI ethics, I criticise parts of the literature for leaving the fundamental ethical incompatibility of democracies and authoritarian regimes unaddressed. I am focussing on the duty to disobey illegal orders derived from customary international humanitarian law (IHL) and the standard of ‘meaningful human control’, which is central to the partially outdated debate about lethal autonomous weapon systems (LAWS). I criticize the standard of ‘meaningful human control’ following two pathways: First, the ethical and legal principles of just war theory and IHL should be implemented in military applications of AI to submit human commands to more control, in the sense of technological disaffordances. Second, the debate should focus on the societal circumstances for personal responsibility and disobedience to be trained and exerted in deliberation and participation related to military applications of AI, in the sense of societal affordances. In a larger picture, this includes multi-level stakeholder involvement, robust documentation to facilitate auditing, civilian-based defence in decentralized smart cities, and open-source intelligence. This multi-layered approach fosters cognitive diversity, which might constitute a strategic advantage for democracies regarding AI.</jats:p>"
10.1007/s43681-020-00037-w,Bridging the gap: the case for an ‘Incompletely Theorized Agreement’ on AI policy,"<jats:title>Abstract</jats:title><jats:p>Recent progress in artificial intelligence (AI) raises a wide array of ethical and societal concerns. Accordingly, an appropriate policy approach is urgently needed. While there has been a wave of scholarship in this field, the research community at times appears divided amongst those who emphasize ‘near-term’ concerns and those focusing on ‘long-term’ concerns and corresponding policy measures. In this paper, we seek to examine this alleged ‘gap’, with a view to understanding the practical space for inter-community collaboration on AI policy. We propose to make use of the principle of an ‘incompletely theorized agreement’ to bridge some underlying disagreements, in the name of important cooperation on addressing AI’s urgent challenges. We propose that on certain issue areas, scholars working with near-term and long-term perspectives can converge and cooperate on selected mutually beneficial AI policy projects, while maintaining their distinct perspectives.</jats:p>"
10.36227/techrxiv.24449185.v1,How Text-to-Image Generative AI Is Transforming Mediated Action?,N/A
10.36227/techrxiv.24087534.v1,Synergy in Technology How Generative AI Augments the Capabilities of Customer Data Platforms,"<jats:p>&lt;p&gt;In an era marked by data-driven decision-making, Customer Data Platforms (CDPs) have emerged as pivotal tools for aggregating and analyzing customer data. However, as these platforms grapple with increasingly complex data sets and real-time customer engagement demands, there is a pressing need for more advanced, scalable solutions. This study explores the synergy between Generative Artificial Intelligence (AI) and CDPs, aiming to understand how the integration of generative algorithms can augment the capabilities of these platforms. Employing a multi-method research approach, including case studies, empirical analyses, and expert interviews, this paper investigates various applications of Generative AI within CDPs, such as data augmentation, real-time decision-making, and customer personalization. Moreover, the ethical implications of using generative algorithms, especially concerning data privacy and security, are critically examined. The study reveals that Generative AI can significantly enhance the functionality, performance, and efficiency of CDPs while also posing new questions around ethical considerations. Our findings offer invaluable insights for businesses, marketers, and technologists seeking to leverage the synergistic potential of these two advanced technological paradigms. &lt;/p&gt;</jats:p>"
10.1094/tq-60-4-1225-01,Fermentation Meets Computation Exploring Generative AI in Breweries,N/A
10.2139/ssrn.4843464,Generative AI and Metaverse: Companionship and Assisted Living for Elderly People,N/A
10.1007/s43681-023-00266-9,A seven-layer model with checklists for standardising fairness assessment throughout the AI lifecycle,N/A
10.1007/s43681-023-00269-6,Can artificial intelligence be a Kantian moral agent? On moral autonomy of AI system,N/A
10.4018/9781591409878.ch042,Ethics of AI,N/A
10.7551/mitpress/12549.003.0002,Acknowledgments,N/A
10.2139/ssrn.4660456,Generative Ai from a Cfo Prism,N/A
10.2139/ssrn.4639565,Measuring Tax Enforcement with Generative AI,N/A
10.1201/9781003503781-4,Review of Cybersecurity Metrics,N/A
10.1016/b978-0-44-321857-6.00008-4,Image and video generation,N/A
10.2139/ssrn.4848720,The Unbearable Lightness of Inventing: Postprocess in the Age of Generative Ai,N/A
10.1007/s43681-021-00076-x,The ethics of interaction with neurorobotic agents: a case study with BabyX,"<jats:title>Abstract</jats:title><jats:p>As AI advances, models of simulated humans are becoming increasingly realistic. A new debate has arisen about the ethics of interacting with these realistic agents—and in particular, whether any harms arise from ‘mistreatment’ of such agents. In this paper, we advance this debate by discussing a model we have developed (‘BabyX’), which simulates a human infant. The model produces realistic behaviours—and it does so using a schematic model of certain human brain mechanisms. We first consider harms that may arise due to effects<jats:italic>on the user</jats:italic>—in particular effects on the user’s behaviour towards real babies. We then consider whether there’s any need to consider harms from the ‘perspective’<jats:italic>of the simulated baby</jats:italic>. The first topic raises practical ethical questions, many of which are empirical in nature. We argue the potential for harm is real enough to warrant restrictions on the use of BabyX. The second topic raises a very different set of questions in the philosophy of mind. Here, we argue that BabyX’s biologically inspired model of emotions raises important moral questions, and places BabyX in a different category from avatars whose emotional behaviours are ‘faked’ by simple rules. This argument counters John Danaher’s recently proposed ‘moral behaviourism’. We conclude that the developers of simulated humans have useful contributions to make to debates about moral patiency—and also have certain new responsibilities in relation to the simulations they build.</jats:p>"
10.1093/oxfordhb/9780190067397.013.42,Ethics of Artificial Intelligence in Transport,"<p>This chapter highlights key ethical issues in the use of artificial intelligence in transport by using automated driving as an example. These issues include the tension between technological solutions and policy solutions; the consequences of safety expectations; the complex choice between human authority and computer authority; and power dynamics among individuals, governments, and companies. In 2017 and 2018, the U.S. Congress considered automated driving legislation that was generally supported by many of the larger automated-driving developers. However, this automated-driving legislation failed to pass because of a lack of trust in technologies and institutions. Trustworthiness is much more of an ethical question. Automated vehicles will not be driven by individuals or even by computers; they will be driven by companies acting through their human and machine agents. An essential issue for this field—and for artificial intelligence generally—is how the companies that develop and deploy these technologies should earn people’s trust.</p>"
10.36227/techrxiv.22097942,A Review of Generative AI from Historical Perspectives,"<jats:p>&lt;p&gt;Many applications of Generative AI (such as DALL-E, GPT-3, ChatGPT, etc.) are making headline news in recent months and have been receiving both praise and criticism for their far reaching implications.  Some of these applications include query responses, language translation, text to images and videos, composing stories, essays, creating arts and music, generating programs, etc. This review provides an historical background of Generative AI techniques and how they evolved over the years. This report highlights the benefits of Generative AI technologies and their limitations/challenges in moving forward. It is also to be noted that the large-scale applications of AI and their successes are now possible due to exponential advances in hardware (computational power, storage capacity), cloud computing and related operational layers of software.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4945933,The Wade Test: Generative AI and CEO Communication,N/A
10.36227/techrxiv.22097942.v1,A Review of Generative AI from Historical Perspectives,"<jats:p>&lt;p&gt;Many applications of Generative AI (such as DALL-E, GPT-3, ChatGPT, etc.) are making headline news in recent months and have been receiving both praise and criticism for their far reaching implications.  Some of these applications include query responses, language translation, text to images and videos, composing stories, essays, creating arts and music, generating programs, etc. This review provides an historical background of Generative AI techniques and how they evolved over the years. This report highlights the benefits of Generative AI technologies and their limitations/challenges in moving forward. It is also to be noted that the large-scale applications of AI and their successes are now possible due to exponential advances in hardware (computational power, storage capacity), cloud computing and related operational layers of software.&lt;/p&gt;</jats:p>"
10.1016/b978-0-44-321857-6.00016-3,Some unusual random walks,N/A
10.4018/979-8-3693-1950-5.ch002,Generative AI for Text to Image,"<jats:p>Text-to-image (TTI) synthesis models represent a creative approach in the realm of artificial intelligence, specifically designed to transform textual input into visually realistic images. The essence of TTI generation lies in its ability to harness the power of language and convert it seamlessly into visually compelling content, showcasing creative image synthesis. Initially using GANs and transformers, text-to-image generation evolved with diffusion models introducing noise. Integration with large models, TTI models now produce results near-real images. Breakthroughs like ControlNet and 3D object synthesis redefine possibilities. Editing text or images adds versatile dimensions, showcasing generative technologies' transformative capabilities. The survey explores scaling TTI models, focusing on various descriptions and ControlNet's role. The authors categorize literature, offer nuanced comparisons, and discuss applications. Looking ahead, they foresee TTI's potential for productivity enhancements, especially in the Metaverse era, and expansion into intricate tasks like video and 3D generation.</jats:p>"
10.29303/abdiinsani.v11i2.1577,PEMANFAATAN GENERATIVE AI DALAM PEMBUATAN PERANGKAT PEMBELAJARAN: WORKSHOP UNTUK GURU SMK NEGERI 10 SURABAYA,"<jats:p>One of the fast-growing technologies that has great potential in the world of education is artificial intelligence. (Artificial Intelligence). The workshop aims to enhance teachers' understanding and skills in using Artificial Intelligence, the ChatGPT and Tome.App applications, to develop more interesting and effective learning plans and create more innovative material presentations. The method of this activity begins with conducting surveys to understand the needs and challenges faced by teachers in using Artificial Intelligence technology. Next, a workshop module is set up to guide the workshop participants. Evaluation instruments are also prepared to measure participants' understanding before and after the workshop, as well as to assess participants' satisfaction with the overall performance of the workshop. The evaluation results showed a significant improvement in participants' understanding of the material after attending the workshop. Based on pre-test and post-test data, the average answer accuracy increased from 55% to 86%. Besides, the participants' response to the workshop was also very positive, with the majority of participants giving a ""Good"" and ""Very Good"" rating on the material, the quality of the source, and the overall workshop atmosphere. Workshop participants became more confident in applying technology in their learning. The evaluation also highlighted the importance of mature preparation in conducting workshops, including better timing and location. The advice for the future is to conduct more intensive periodic workshops to enhance teachers' ability to use Artificial Intelligence technology in learning.</jats:p>"
10.4018/979-8-3693-5578-7.ch004,Mediated by AI-Based Generative Re-Enforcement Learning and Work Attitude,"<jats:p>The primary goal of the chapter was to examine how, in the Ethiopian environment, an employee's work attitude and AI-based reinforcement learning function as mediators between their psychological intrinsic reward system and the perception of organisational support they receive from their employer. Based on their contribution to the GDP of the economy, the textile industries operating in Ethiopia's industrial parks were chosen as the study area. A quantitative research technique and explanatory research design were employed. A multi-phase sampling method was suggested. To assess discriminant and convergent validity, exploratory confirmatory analysis was carried out using AMOS software. It was discovered that the relationship between an employee's psychological intrinsic reward system and the organization's perceived organisational support was extremely poor in the absence of AI-based re-enforcement learning and the employee's work attitude as a mediator. Thus, the work attitude of employees and artificial intelligence (AI) function as a complete mediator.</jats:p>"
10.7551/mitpress/12549.001.0001,AI Ethics,"<jats:p>An accessible synthesis of ethical issues raised by artificial intelligence that moves beyond hype and nightmare scenarios to address concrete questions.</jats:p>
               <jats:p>Artificial intelligence powers Google's search engine, enables Facebook to target advertising, and allows Alexa and Siri to do their jobs. AI is also behind self-driving cars, predictive policing, and autonomous weapons that can kill without human intervention. These and other AI applications raise complex ethical issues that are the subject of ongoing debate. This volume in the MIT Press Essential Knowledge series offers an accessible synthesis of these issues. Written by a philosopher of technology, AI Ethics goes beyond the usual hype and nightmare scenarios to address concrete questions.</jats:p>
               <jats:p>Mark Coeckelbergh describes influential AI narratives, ranging from Frankenstein's monster to transhumanism and the technological singularity. He surveys relevant philosophical discussions: questions about the fundamental differences between humans and machines and debates over the moral status of AI. He explains the technology of AI, describing different approaches and focusing on machine learning and data science. He offers an overview of important ethical issues, including privacy concerns, responsibility and the delegation of decision making, transparency, and bias as it arises at all stages of data science processes. He also considers the future of work in an AI economy. Finally, he analyzes a range of policy proposals and discusses challenges for policymakers. He argues for ethical practices that embed values in design, translate democratic values into practices and include a vision of the good life and the good society.</jats:p>"
10.7551/mitpress/12549.003.0016,Notes,N/A
10.7551/mitpress/12549.003.0015,Glossary,N/A
10.7551/mitpress/12549.003.0017,References,N/A
10.7551/mitpress/12549.003.0019,Index,N/A
10.4018/979-8-3693-2418-9.ch002,Exploring the Perception of Indian Educators Towards Use of Generative AI Tools for Assessment,"<jats:p>Educational ecosystem has always kept pace with the technological development. Stakeholders in education have had an agile reaction to the progress happening outside the classroom and have embraced these changes at the earliest for their own benefits. While students have readily accepted the free tools, there is a raging debate on the view of educators regarding the same. The present research focuses on teachers and educators who teach at undergraduate and post-graduate students of management and assimilates their opinion on the use of artificial intelligence, in one of the key aspects of the educational journey, i.e., assessment. In the research carried out engaging insights were brought forth in terms of educators preferences regarding the use of artificial intelligence for assessment and how they perceive it to help students and themselves. At the same time, the research has also shown the limitations that educators feel regarding the use of artificial intelligence from an Indian perspective, due to the sheer number of human power involved in the Indian Education System.</jats:p>"
10.1007/s00146-020-01010-1,From machine ethics to computational ethics,N/A
10.1007/978-981-19-9382-4_1,Introduction: Why AI Ethics?,N/A
10.22541/essoar.167080673.37483484/v1,AI-ML Ethics Modules for ESES - Version 1 with line numbers- December 2022,N/A
10.1007/s43681-024-00471-0,From artificial intelligence to semi-creative inorganic intelligence: a blockchain-based bioethical metamorphosis,N/A
10.15187/adr.2024.08.37.4.181,The Effect of Text Movement on Eye Movements in Generative AI Chatbots,N/A
10.4018/979-8-3693-0487-7.ch008,Leveraging Generative AI for Cross-Cultural Knowledge Exchange in Higher Education,"<jats:p>The integration of generative artificial intelligence (AI) has the potential to revolutionize cross-cultural knowledge exchange in higher education. By leveraging its ability to create culturally sensitive and contextually relevant learning materials, generative AI can enhance personalized education for students from diverse backgrounds. Overcoming language barriers through real-time translation, this technology promotes inclusive collaboration and co-creation of knowledge. However, ethical considerations such as cultural authenticity, bias mitigation, and data privacy must be carefully navigated to ensure equitable and respectful cross-cultural interactions. In essence, generative AI offers a transformative avenue for fostering a culturally enriched learning environment in higher education, necessitating a balanced approach that maximizes benefits while upholding ethical principles.</jats:p>"
10.21428/e4baedd9.7dc53bbf,Advancing Equality: Harnessing Generative AI to Combat Systemic Racism,N/A
10.4018/979-8-3693-0074-9.ch006,Generative AI in Terms of Its Ethical Problems for Both Teachers and Learners,"<jats:p>The chapter delves into the intricate ethical challenges posed by the integration of generative AI tools in educational contexts. As the educational landscape undergoes a profound transformation through AI-driven technologies, this chapter navigates the multifaceted ethical concerns that emerge. It explores issues surrounding data privacy, content bias, academic integrity, and the evolving roles of teachers and students in this digital era. Drawing on real-world case studies and ethical frameworks, it provides insights into the complexities of responsible AI integration. Moreover, it offers guidance on fostering ethical awareness in education and emphasizes the critical need for striking a harmonious balance between the capabilities of AI and the enduring value of human educators. This chapter serves as a comprehensive exploration of the ethical tightrope that educators, students, and policymakers must navigate to ensure AI's positive impact on education while safeguarding its ethical underpinnings.</jats:p>"
10.4018/979-8-3693-1950-5.ch006,Exploratory Visual Digital Character and Visual Digital Scene Design Using Artmaking Generative AI,"<jats:p>Static visual illustrations with characters and scenes play an important role in story problems and other pedagogical narratives. Such visuals may better engage learners, connect learners with the learning sequence, set the emotional tone, evoke settings, emphasize critical moments, and support the teaching and learning in other ways. With the popularization of artmaking generative AI, a practical question is how well this tool can make visual characters based on character design prompts for both animate and inanimate characters? What about the computerized drawing of scenes in which such characters may be placed, alone or in relation to each other? How difficult is it to prompt the generative AI to create consistent characters from different angles and perspectives? To create consistent scenes and backgrounds? This work explores how practically usable the AI-generated visuals are for the making of characters and scenes, with some light pre-production and post-production, as needed.</jats:p>"
10.1007/979-8-8688-0403-8_9,Building Enterprise-Grade Applications Using LLMs,N/A
10.1007/s43681-021-00078-9,Analytical modelling and UK Government policy,"<jats:title>Abstract</jats:title><jats:p>In the last decade, the UK Government has attempted to implement improved processes and procedures in modelling and analysis in response to the Laidlaw report of 2012 and the Macpherson review of 2013. The Laidlaw report was commissioned after failings during the Intercity West Coast Rail (ICWC) Franchise procurement exercise by the Department for Transport (DfT) that led to a legal challenge of the analytical models used within the exercise. The Macpherson review looked into the quality assurance of Government analytical models in the context of the experience with the Intercity West Coast franchise competition. This paper examines what progress has been made in the 8 years since the Laidlaw report in model building and best practise in government and proposes several recommendations for ways forward. This paper also discusses the Lords Science and Technology Committees of June 2020 that analysed the failings in the modelling of COVID. Despite going on to influence policy, many of the same issues raised within the Laidlaw and Macpherson Reports were also present in the Lords Science and Technology Committee enquiry. We examine the technical and organisational challenges to progress in this area and make recommendations for a way forward.</jats:p>"
10.1007/s43681-022-00151-x,The internal morality of markets and artificial intelligence,N/A
10.21203/rs.3.rs-4603791/v1,A Tutorial for Integrating Generative AI in Mixed Methods Data Analysis,"<title>Abstract</title>
        <p>The current article used real data to demonstrate the analysis and synthesis of Mixed Methods Research (MMR) data with generative Artificial Intelligence (Gen AI). I explore how reliable and valid Gen AI data outputs are and how to improve their use. The current content is geared towards enhancing methodological application regardless of field or discipline and includes access to a prompt library and examples of using outputs. The demonstration data used emanated from a study done in South Africa, with a quantitative sample size of 969 first-year engineering students and, for the qualitative part, 14 second-year students. In the current article, I compare my original analysis to ChatGPT results. Generative AI as a mind tool is best used with human insight, and I found this to be especially true when coding qualitative data. ChatGPT produced generic codes if asked to do inductive coding, and the results improved when training the Gen AI on human examples, which led to moderate and significant correlations between human and machine coding. The quantitative analysis was accurate for the descriptive statistics, but the researcher had to use best judgment to select the correct inferential analysis. Quantitative and qualitative analysis should be conducted separately in generative AI before asking the Chatbot for help with mixed methods results.  In the current paper, I give guidelines and a tutorial on how to use chatbots in an ethically responsible and scientifically sound manner for research in social and human sciences.</p>"
10.36227/techrxiv.24045792,"Exploring the Synergy between Generative AI, Data and Analytics in the Modern Age","<jats:p>&lt;p&gt;In the year 2023, a heightened sense of curiosity and apprehension pervaded the landscape of generative artificial intelligence (AI), particularly in the wake of the unveiling of the ChatGPT product by OpenAI. This pivotal moment sparked a flurry of discussions that predominantly revolved around the role of data in shaping the trajectory of generative AI. As researchers and organizations alike delved into this innovative realm, a pronounced inclination toward investigating its potential applications emerged. Notably, organizations swiftly recognized the transformative potential of generative AI in bolstering productivity across various sectors.&lt;/p&gt;
&lt;p&gt;At the heart of these deliberations lies the profound significance of data. With data as the focal point, a compelling exploration began to unfold, with researchers keenly scrutinizing the ramifications of integrating generative AI within the domain of data and analytics. This research initiative was driven by an intrinsic desire to uncover the ways in which generative AI could be harnessed to enhance and streamline analytical processes.&lt;/p&gt;
&lt;p&gt;In this context, the present research undertook a comprehensive investigation, employing a multifaceted approach. Leveraging various social media platforms as a primary source of insights, the research embarked on a journey to discern the prevailing sentiments, concerns, and expectations surrounding generative AI tools. This was further complemented by the execution of proof-of-concept (POC) endeavors, which not only enabled hands-on experience with generative AI tools but also facilitated a nuanced comprehension of their practical implications.&lt;/p&gt;
&lt;p&gt;The culmination of these efforts yielded a series of noteworthy findings. Principally, it was discerned that enterprises stand to gain substantial benefits from embracing the capabilities of generative AI within the domain of data and analytics. The integration of generative AI tools offers the potential to revolutionize productivity, propelling organizations toward novel insights and expediting analytical processes. Concurrently, a strategic partnership with generative AI entities emerged as a salient consideration for safeguarding intellectual properties. Collaborative engagements between companies and generative AI providers became imperative to navigate the evolving landscape of data-driven innovation.&lt;/p&gt;
&lt;p&gt;In conclusion, the year 2023 ushered in a period marked by intense curiosity and apprehension surrounding generative AI, catalyzed by the introduction of ChatGPT and its ensuing discussions. The centrality of data within this discourse propelled researchers and organizations toward an exploration of generative AI's potential applications, notably in the realm of data and analytics. Through a comprehensive research endeavor encompassing social media insights, POC experimentation, and practical insights, it became evident that the integration of generative AI could usher in transformative enhancements to productivity and analytical processes. In parallel, collaborative endeavors with generative AI entities emerged as a strategic imperative, offering a dual advantage of innovation and intellectual property protection. This research underscores the compelling need for enterprises to harness generative AI's capabilities, thereby positioning themselves at the vanguard of data-driven progress.&lt;/p&gt;</jats:p>"
10.4324/9781003459026,Generative AI in Higher Education,N/A
10.1007/978-3-031-23035-6_6,Promoting AI Ethics Through Awareness and Case Studies,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) is enabling organizations to address a range of real-world challenges in areas as diverse as global health, education and poverty alleviation.</jats:p>"
10.1007/s43681-022-00247-4,Editorial piece: Technology built on sand?,N/A
10.1007/s43681-022-00136-w,Algorithms are not neutral,N/A
10.1007/978-3-031-46238-2_17,Underwater Acoustic Noise Modeling Based on Generative-Adversarial-Network,N/A
10.1007/s43681-020-00016-1,Past the tipping point?,N/A
10.1007/s43681-023-00287-4,The probability problems of the Moral Machine Experiment,N/A
10.1007/s43681-023-00376-4,Data-driven framework for evaluating digitization and artificial intelligence risk: a comprehensive analysis,N/A
10.1109/icws60048.2023.00103,"Empowering Generative AI with Knowledge Base 4.0: Towards Linking Analytical, Cognitive, and Generative Intelligence",N/A
10.1007/s00146-024-01872-9,What to consider before incorporating generative AI into schools?,N/A
10.2139/ssrn.4682872,This is Not Global AI: International Reports on AI as Vehicles of Colonialism in the Age of Generative AI,N/A
10.21275/sr24517073623,Reinforcing Cyber Defense: Generative AI Powered Intelligent Agent Architecture for Enhanced Security Operations,N/A
10.4018/979-8-3693-2440-0.ch019,Role of Video Content Generation in Education Systems Using Generative AI,"<jats:p>This chapter delves into the transformative potential of three cutting-edge models, i.e., CCVS, DreamPose, and Fast Vid2Vid, in reshaping educational video content creation. CCVS, a dynamic framework amalgamating generative and discriminative models, excels in synthesizing high-quality videos from text descriptions. Its versatility in video generation, interpolation, and prediction marks a paradigm shift in educational content development. DreamPose, an advanced AI system leveraging stable diffusion, translates textual descriptions into visually stunning fashion videos. Its user-friendly design caters to diverse fashion styles, making it an ideal tool for educators seeking visually engaging content across disciplines. Fast Vid2Vid, a deep learning model, takes the spotlight for efficiently generating high-quality videos from a single image. Recognized for its realism, it holds promise in dynamic visualizations for educational purposes, spanning virtual and augmented reality experiences. Practical insights and implementation strategies empower educators to seamlessly integrate these models into educational settings, offering a comprehensive guide from planning and scripting to interactive element incorporation. This chapter lays the foundation for educators and content creators to elevate the educational experience through innovative visual storytelling and AI-driven technologies.</jats:p>"
10.4324/9781003074991-24,Why Ethics is a High Hurdle for AI,N/A
10.36227/techrxiv.24087534,Synergy in Technology How Generative AI Augments the Capabilities of Customer Data Platforms,"<jats:p>&lt;p&gt;In an era marked by data-driven decision-making, Customer Data Platforms (CDPs) have emerged as pivotal tools for aggregating and analyzing customer data. However, as these platforms grapple with increasingly complex data sets and real-time customer engagement demands, there is a pressing need for more advanced, scalable solutions. This study explores the synergy between Generative Artificial Intelligence (AI) and CDPs, aiming to understand how the integration of generative algorithms can augment the capabilities of these platforms. Employing a multi-method research approach, including case studies, empirical analyses, and expert interviews, this paper investigates various applications of Generative AI within CDPs, such as data augmentation, real-time decision-making, and customer personalization. Moreover, the ethical implications of using generative algorithms, especially concerning data privacy and security, are critically examined. The study reveals that Generative AI can significantly enhance the functionality, performance, and efficiency of CDPs while also posing new questions around ethical considerations. Our findings offer invaluable insights for businesses, marketers, and technologists seeking to leverage the synergistic potential of these two advanced technological paradigms. &lt;/p&gt;</jats:p>"
10.2139/ssrn.4553787,Competition in Generative AI Foundation Models,N/A
10.22541/essoar.170034944.42344050/v1,Nowcasting Earthquakes with QuakeGPT An AI-Enhanced Earthquake Generative Pretrained Transformer,N/A
10.31219/osf.io/btczf,Leveraging generative AI to acculturate away from climate apathy,"<p>“Throw away anything that sounds too complicated. Only keep what is simple to grasp. Keep whatever news that seems friendly to the ears. If the information appears fuzzy and causes the brain to implode after two sentences, toss it away and stop listening. Doing so will make the news as orderly and simple to understand as the truth!”—In “GHG Emissions”; The Kingfisher Story Collection.</p>"
10.2139/ssrn.4894684,How Will Generative AI Impact Communication?,N/A
10.36227/techrxiv.24045792.v1,"Exploring the Synergy between Generative AI, Data and Analytics in the Modern Age","<jats:p>&lt;p&gt;In the year 2023, a heightened sense of curiosity and apprehension pervaded the landscape of generative artificial intelligence (AI), particularly in the wake of the unveiling of the ChatGPT product by OpenAI. This pivotal moment sparked a flurry of discussions that predominantly revolved around the role of data in shaping the trajectory of generative AI. As researchers and organizations alike delved into this innovative realm, a pronounced inclination toward investigating its potential applications emerged. Notably, organizations swiftly recognized the transformative potential of generative AI in bolstering productivity across various sectors.&lt;/p&gt;
&lt;p&gt;At the heart of these deliberations lies the profound significance of data. With data as the focal point, a compelling exploration began to unfold, with researchers keenly scrutinizing the ramifications of integrating generative AI within the domain of data and analytics. This research initiative was driven by an intrinsic desire to uncover the ways in which generative AI could be harnessed to enhance and streamline analytical processes.&lt;/p&gt;
&lt;p&gt;In this context, the present research undertook a comprehensive investigation, employing a multifaceted approach. Leveraging various social media platforms as a primary source of insights, the research embarked on a journey to discern the prevailing sentiments, concerns, and expectations surrounding generative AI tools. This was further complemented by the execution of proof-of-concept (POC) endeavors, which not only enabled hands-on experience with generative AI tools but also facilitated a nuanced comprehension of their practical implications.&lt;/p&gt;
&lt;p&gt;The culmination of these efforts yielded a series of noteworthy findings. Principally, it was discerned that enterprises stand to gain substantial benefits from embracing the capabilities of generative AI within the domain of data and analytics. The integration of generative AI tools offers the potential to revolutionize productivity, propelling organizations toward novel insights and expediting analytical processes. Concurrently, a strategic partnership with generative AI entities emerged as a salient consideration for safeguarding intellectual properties. Collaborative engagements between companies and generative AI providers became imperative to navigate the evolving landscape of data-driven innovation.&lt;/p&gt;
&lt;p&gt;In conclusion, the year 2023 ushered in a period marked by intense curiosity and apprehension surrounding generative AI, catalyzed by the introduction of ChatGPT and its ensuing discussions. The centrality of data within this discourse propelled researchers and organizations toward an exploration of generative AI's potential applications, notably in the realm of data and analytics. Through a comprehensive research endeavor encompassing social media insights, POC experimentation, and practical insights, it became evident that the integration of generative AI could usher in transformative enhancements to productivity and analytical processes. In parallel, collaborative endeavors with generative AI entities emerged as a strategic imperative, offering a dual advantage of innovation and intellectual property protection. This research underscores the compelling need for enterprises to harness generative AI's capabilities, thereby positioning themselves at the vanguard of data-driven progress.&lt;/p&gt;</jats:p>"
10.1007/979-8-8688-0318-5_9,Generative AI’s Benefits and Risks to Society,N/A
10.1007/979-8-8688-0403-8_7,Advanced Techniques for Large Language Models,N/A
10.1007/978-3-031-46238-2_11,Generative Adversarial Networks for Stain Normalisation in Histopathology,N/A
10.1162/99608f92.cdfc3092,Castles in the Sand?: How the Public Sector and Academia Can Partner in Regulatory Sandboxes to Help Leverage Generative AI for Public Good,N/A
10.21275/sr24604032016,"A Generative AI Framework for Enhancing Software Test Automation: Design, Implementation, and Validation",N/A
10.1007/s43681-024-00489-4,Human control of AI systems: from supervision to teaming,"<jats:title>Abstract</jats:title><jats:p>This article reviews two main approaches to human control of AI systems: supervisory human control and human–machine teaming. It explores how each approach defines and guides the operational interplay between human behaviour and system behaviour to ensure that AI systems are effective throughout their deployment. Specifically, the article looks at how the two approaches differ in their conceptual and practical adequacy regarding the control of AI systems based on foundation models––i.e., models trained on vast datasets, exhibiting general capabilities, and producing non-deterministic behaviour. The article focuses on examples from the defence and security domain to highlight practical challenges in terms of human control of automation in general, and AI in particular, and concludes by arguing that approaches to human control are better served by an understanding of control as the product of collaborative agency in a multi-agent system rather than of exclusive human supervision.</jats:p>"
10.18260/1-2--46433,A Custom Generative AI Chatbot as a Course Resource,N/A
10.4324/9781003507949-2,Pedagogical Foundations of AI Integration,N/A
10.2139/ssrn.4511540,"Large Language Models and Generative AI in Finance: An Analysis of ChatGPT, Bard, and Bing AI",N/A
10.46397/jaih.14.3,Ethical Problems of Super-Massive Generative AI,N/A
10.46397/jaih.15.6,Generative AI Products and Copyright Issues,N/A
10.1007/978-1-4842-9367-6_9,The Future,N/A
10.1007/s43681-021-00075-y,The ethical AI—paradox: why better technology needs more and not less human responsibility,N/A
10.1007/s43681-023-00319-z,Speciesist bias in AI: a reply to Arandjelović,"<jats:title>Abstract</jats:title><jats:p>The elimination of biases in artificial intelligence (AI) applications—for example biases based on race or gender—is a high priority in AI ethics. So far, however, efforts to eliminate bias have all been anthropocentric. Biases against nonhuman animals have not been considered, despite the influence AI systems can have on normalizing, increasing, or reducing the violence that is inflicted on animals, especially on farmed animals. Hence, in 2022, we published a paper in <jats:italic>AI and Ethics</jats:italic> in which we empirically investigated various examples of image recognition, word embedding, and language models, with the aim of testing whether they perpetuate speciesist biases. A critical response has appeared in <jats:italic>AI and Ethics</jats:italic>, accusing us of drawing upon theological arguments, having a naive anti-speciesist mindset, and making mistakes in our empirical analyses. We show that these claims are misleading.</jats:p>"
10.1007/s43681-023-00417-y,TAI-PRM: trustworthy AI—project risk management framework towards Industry 5.0,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) is increasingly being used in manufacturing to automate tasks and process data, leading to what has been termed Industry. 4.0. However, as we move towards Industry 5.0, there is a need to incorporate societal and human-centric dimensions into the development and deployment of AI software artefacts. This requires blending ethical considerations with existing practices and standards. To address this need, the TAI-PRM framework has been developed. It builds upon established methods, such as Failure Mode and Effect Analysis (FMEA) and the Industrial ISO 31000, to manage risks associated with AI artefacts in the manufacturing sector. The framework identifies ethical considerations as hazards that can impact system processes and sustainability and provides tools and metrics to manage these risks. To validate the framework, it was applied in an EU project for Digital Twins on AI for manufacturing. The results showed that TAI-PRM can effectively identify and track different failure modes associated with AI artefacts and help users to manage ethical risks associated with their deployment. By incorporating ethical considerations into risk management processes, the framework enables the developing and deploying trustworthy AI in the manufacturing sector.</jats:p>"
10.2139/ssrn.4523551,Talkin’ ‘Bout AI Generation: Copyright and the Generative AI Supply Chain,N/A
10.1007/979-8-8688-0205-8_4,Guardrails and AI: Building Safe + Controllable Apps,N/A
10.4018/979-8-3693-5578-7.ch009,AI in Marketing,"<jats:p>In recent times, artificial intelligence (AI) has become an essential component in marketing, leading to the generation of large amounts of data and information, as well as advancements in management software and algorithms. The goal of artificial intelligence is to intelligently imitate human behavior. Artificial intelligence is a rapidly expanding technology, industry, and field of study. AI not only enhances existing marketing strategies, but also introduces innovative ways to deliver value to customers. AI also plays an important role in helping marketers manage marketing projects, support them, create content, and perform other tasks. Research shows that a lack of understanding of AI's capabilities and failed implementation efforts can create additional obstacles for organizations, including investment risks, ethical dilemmas, data issues, recruitment challenges, and negative environmental impacts. Despite being designed to reduce the workload of employees, human intervention is necessary in roles such as management, supervision, and innovation.</jats:p>"
10.1007/s43681-023-00358-6,"Elusive technologies, elusive responsibilities: on the perceived responsibility of basic AI researchers","<jats:title>Abstract</jats:title><jats:p>This paper studies how researchers who work in the field of basic research of artificial intelligence (AI) perceive their responsibility. A case study is conducted on an inter-university and interdisciplinary research cluster in Germany that specializes in basic artificial intelligence research. The reason for studying responsibility through the lens of such researchers is that working in basic research of AI involves a lot of uncertainty about potential consequences, more so than in other domains of AI development. After conducting focus groups with 21 respondents followed by a thematic analysis, results show that respondents restrict the boundaries of their sociotechnical visions, regard time as an influencing factor in their responsibility, and refer to many other players in the field. These themes indicate that respondents had difficulties explaining what they consider themselves responsible for, and referred to many factors beyond their own control. The only type of responsibility that was explicitly acknowledged by respondents is <jats:italic>ex ante</jats:italic> responsibility. Respondents define their responsibility in terms of things that are in their immediate control, i.e., responsibilities relating to their role and duties as researchers. According to the respondents, working in the field of basic research makes it difficult to make claims about <jats:italic>ex post</jats:italic> responsibility. Findings of this case study suggest the need to raise questions about how technological maturity is related to AI ethics.</jats:p>"
10.1007/s43681-024-00476-9,The obscure politics of artificial intelligence: a Marxian socio-technical critique of the AI alignment problem thesis,"<jats:title>Abstract</jats:title><jats:p>There is a growing feeling that artificial intelligence (AI) is getting out of control. Many AI experts worldwide stress that great care must be taken on the so-called <jats:italic>alignment problem</jats:italic>, broadly understood as the challenge of developing AIs whose actions are in line with human values and goals. The story goes that ever more powerful AI systems are escaping human control and might soon operate in a manner that is no longer guided by human purposes. This is what we call the <jats:italic>AI-out-of-control discourse</jats:italic> which, in this paper, we critically examine and debunk. Drawing on complementary insights from political theory, socio-technical studies and Marxian political economy, we critique the supposed animistic and autonomous nature of AI, and the myth of the uncontrollability of AI. The problem is not that humanity has lost control over AI, but that only a minority of powerful stakeholders are controlling its creation and diffusion, through politically undemocratic processes of decision-making. In these terms, we reframe the alignment problem thesis with an emphasis on citizen engagement and public political participation. We shed light on the existing politics of AI and contemplate alternative political expressions whereby citizens steer AI development or stop it in the first place.</jats:p>"
10.1007/s43681-023-00333-1,Lustre and shadows: unveiling the gaps in South African University plagiarism policies amidst the emergence of AI-generated content,"<jats:title>Abstract</jats:title><jats:p>In recent years, artificial intelligence (AI) has become a key technology in the field of academic integrity. However, there is a lack of a comprehensive understanding of the legal dimensions of plagiarism in the context of AI. In this study, a theoretical framework that combines the social construction of technology and the legal dimension of plagiarism was used to explore the current construction of plagiarism in South African university plagiarism policies. This study aims to highlight the inadequacy of current plagiarism policies, which primarily focus on the act of copying from others and emphasize the need for a broader perspective that addresses the challenges posed by artificial intelligence in academic integrity in the era of AI-generated content. The author used confirming sampling and data saturation was reached with a sample of ten university plagiarism policies. The findings revealed an inadequacy of the policies on the coverage of AI-generated content and therefore justifying the need to redefine plagiarism in the context of the artificial intelligence revolution. The author concludes by redefining plagiarism and justifying the utility of the recommended definition.</jats:p>"
10.1007/s43681-024-00544-0,Advising AI assistant: ethical risks of Oura smart ring,N/A
10.1201/9781003359982-6,"AI, Peace, and Ethics",N/A
10.1093/oxfordhb/9780190067397.013.18,AI as a Moral Right-Holder,"<p>This chapter evaluates whether AI systems are or will be rights-holders. It develops a skeptical stance toward the idea that current forms of artificial intelligence are holders of moral rights, beginning with an articulation of one of the most prominent and most plausible theories of moral rights: the Interest Theory of rights. On the Interest Theory, AI systems will be rights-holders only if they have interests or a well-being. Current AI systems are not bearers of well-being, and so fail to meet the necessary condition for being rights-holders. This argument is robust against a range of different objections. However, the chapter also shows why difficulties in assessing whether future AI systems might have interests or be bearers of well-being—and so be rights-holders—raise difficult ethical challenges for certain developments in AI.</p>"
10.1007/s00146-023-01761-7,Reimagining Benin Bronzes using generative adversarial networks,N/A
10.1093/oso/9780198876434.003.0019,Is AI Ethics All Fluff?,"<jats:title>Abstract</jats:title>
               <jats:p>The AI revolution provides a neat illustration of C.P. Snow’s point about the “two cultures” and a timely opportunity to reflect on why a cultural gap between the sciences and humanities persists. This chapter takes aim at an attitude prevailing among some computer scientists that ethics and AI ethics, as branches of the humanities, are unserious disciplines because they do not yield verifiable and quantifiable answers to the problems they address.</jats:p>"
10.1007/978-3-031-23035-6_3,Practical Implications of Different Theoretical Approaches to AI Ethics,<jats:title>Abstract</jats:title><jats:p>Ethics are moral principles that govern a person’s behaviour or the conduct of an activity.</jats:p>
10.1093/oxfordhb/9780190067397.013.6,The Incompatible Incentives of Private-Sector AI,"<p>This chapter evaluates the incompatible incentives of private-sector AI. Private-sector investment in AI is dominated by major internet platform companies such as Facebook, Amazon, Apple, Google, and Microsoft. These platform companies are also leaders in deploying deep learning algorithms. Although deep learning algorithms may be more intelligent than previous generations of machine learning, they are not more robust. There may be a faint technical path forward for problems of bias and unfairness, but algorithms are engines, and pervasive incompatible incentives will remain. As such, algorithms require guardrails. However, technology companies are ill-suited and ill-positioned to design or implement these value-based rules. Guardrails become constraints on people’s behavior, and yet, in cases of high elasticity, effective governance may still be elusive. Ultimately, the pairing of the algorithm and guardrails tempts companies to engage in regulatory arbitrage, providing a requirement for external action.</p>"
10.1007/s43681-022-00145-9,FMEA-AI: AI fairness impact assessment using failure mode and effects analysis,N/A
10.1007/979-8-8688-0456-4_9,The AI Gold Rush and the Future of Business,N/A
10.1007/s43681-021-00065-0,Moral consideration of nonhumans in the ethics of artificial intelligence,N/A
10.61969/jai.1337500,Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning,"<jats:p xml:lang=""en"">Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students’ learning.</jats:p>"
10.1093/oxfordhb/9780190067397.013.47,Beyond Bias,"<p>This chapter discusses contemporary debates regarding the use of artificial intelligence as a vehicle for criminal justice reform. It closely examines two general approaches to what has been widely branded as “algorithmic fairness” in criminal law: the development of formal fairness criteria and accuracy measures that illustrate the trade-offs of different algorithmic interventions; and the development of “best practices” and managerialist standards for maintaining a baseline of accuracy, transparency, and validity in these systems. Attempts to render AI-branded tools more accurate by addressing narrow notions of bias miss the deeper methodological and epistemological issues regarding the fairness of these tools. The key question is whether predictive tools reflect and reinforce punitive practices that drive disparate outcomes, and how data regimes interact with the penal ideology to naturalize these practices. The chapter then calls for a radically different understanding of the role and function of the carceral state, as a starting place for re-imagining the role of “AI” as a transformative force in the criminal legal system.</p>"
10.1093/oxfordhb/9780190067397.013.48,“Fair Notice” in the Age of AI,"<p>This chapter examines the concept of “fair notice,” both in the abstract and as it operates in U.S. constitutional doctrine. Fair notice is paramount to the rule of law. The maxim has ancient roots: people ought to know, in advance, what the law demands of them. As such, fair notice will be among the key concepts for regulating the scope and role of artificial intelligence (AI) in the legal system. AI—like its junior sibling, machine learning—unleashes a historically novel possibility: decision-making tools that are at once powerfully accurate and inscrutable to their human stewards and subjects. To determine when the use of AI-based (or AI-assisted) decision-making tools are consistent with the requirements of fair notice, a sharper account of the principle’s contours is needed. The chapter then develops a tripartite model of fair notice, inspired by the problems and opportunities of AI. It argues that lack of fair notice is used interchangeably to describe three distinct properties: notice of inputs, notice of outputs, and notice of input-output functionality. Disentangling these forms of notice, and deciding which matter in which contexts, will be crucial to the proper governance of AI.</p>"
10.21552/aire/2024/1/8,Korean Copyright Issues in Text Data Mining for Generative AI,N/A
10.1007/s43681-023-00264-x,Ought we align the values of artificial moral agents?,N/A
10.1007/s43681-022-00204-1,Privacy without persons: a Buddhist critique of surveillance capitalism,"<jats:title>Abstract</jats:title><jats:p>Much has been written about artificial intelligence (AI) perpetuating social inequity and disenfranchising marginalized groups (Barocas in SSRN J, 2016; Goodman in Law and Ethics of AI, 2017; Buolamwini and Gebru in Conference on Fairness, Accountability and Transparency, 2018). It is a sad irony that virtually all of these critiques are exclusively couched in concepts and theories from the Western philosophical tradition (Algorithm Watch in AI ethics guidelines global inventory, 2021; Goffi in Sapiens, 2021). In particular, Buddhist philosophy is, with a few notable exceptions (Hongladarom in A Buddhist Theory of Privacy, Springer, Singapore, 2016; Hongladarom in The Ethics of AI and Robotics A Buddhist Viewpoint, Lexington Book, Maryland, 2020; Hongladarom in MIT Technology Review, 2021; Lin et al. in Robot Ethics: The Ethical and Social Implications fo Robotics, MIT, Cambridge, 2012; Promta and Einar Himma in J Inf Commun Ethics Soc 6(2):172–187, 2008), completely ignored. This inattention to non-Western philosophy perpetuates a pernicious form of intellectual imperialism (Alatas in Southeast Asian J Soc Sci 28(1):23–45, 2000), and deprives the field of vital intellectual resources. The aim of this article is twofold: to introduce Buddhist concepts and arguments to an unfamiliar audience and to demonstrate how those concepts can be fruitfully deployed within the field of AI ethics. In part one, I develop a Buddhist inspired critique of two propositions about privacy: that the scope of privacy is defined by an essential connection between certain types of information and personal identity (i.e., what makes a person who they are), and that privacy is intrinsically valuable as a part of human dignity (Council of the European Union in Position of the Council on General Data Protection Regulation, 2016). The Buddhist doctrine of not self (<jats:italic>anattā</jats:italic>) rejects the existence of a stable and essential self. According to this view, persons are fictions and questions of personal identity have no ultimate answer. From a Buddhist perspective, the scope and value of privacy are entirely determined by contextual norms—nothing is intrinsically private nor is privacy intrinsically valuable (Nissenbaum in Theor Inq Law 20(1):221–256, 2019). In part two, I show how this shift in perspective reveals a new critique of surveillance capitalism (Zuboff in J Inf Technol 30(1):75–89, 2015). While other ethical analyses of surveillance capitalism focus on its scale and scope of illegitimate data collection, I examine the relationship between targeted advertising and what Buddhism holds to be the three causes of suffering: ignorance, craving and aversion. From a Buddhist perspective, the foremost reason to be wary of surveillance capitalism is not that it depends on systematic violations of our privacy, but that it systematically distorts and perverts the true nature of reality, instilling a fundamentally misguided and corrupting conception of human flourishing. Privacy, it turns out, may be a red herring to the extent that critiques of surveillance capitalism frame surveillance, rather than capitalism, as the primary object of concern. A Buddhist critique, however, reveals that surveillance capitalism is merely the latest symptom of a deeper disease.</jats:p>"
10.1007/s43681-023-00364-8,Principles on symbiosis for natural life and living artificial intelligence,N/A
10.1007/978-981-19-9382-4_2,The Rise of AI Ethics,N/A
10.1007/s43681-024-00557-9,The creative agency of large language models: a philosophical inquiry,"<jats:title>Abstract</jats:title><jats:p>This paper explores the difficult question of whether Large Language Models (LLMs) are intrinsically creative. Because they can independently create original content, LLMs are often seen as creative agents. Contrary to the belief that LLMs are creative, this paper argues that LLMs are not creative for two reasons. First, LLMs are not creative because they lack an essential component of creativity, which is the first-person experience of the world. Secondly, LLMs are not creative because they are not the principal authors of their creative output, for they lack the subjective awareness and intentionality necessary to be regarded as authors, and their output is a collaborative effort of the AI model, data providers, and other stakeholders. Since they are not full-fledged authors in a traditional sense, they are not creative.</jats:p>"
10.1007/s43681-021-00112-w,Correction to: Why ethical audit matters in artificial intelligence?,N/A
10.1007/s43681-023-00260-1,What would qualify an artificial intelligence for moral standing?,"<jats:title>Abstract</jats:title><jats:p>What criteria must an artificial intelligence (AI) satisfy to qualify for moral standing? My starting point is that sentient AIs should qualify for moral standing. But future AIs may have unusual combinations of cognitive capacities, such as a high level of cognitive sophistication without sentience. This raises the question of whether sentience is a necessary criterion for moral standing, or merely sufficient. After reviewing nine criteria that have been proposed in the literature, I suggest that there is a strong case for thinking that some non-sentient AIs, such as those that are conscious and have non-valenced preferences and goals, and those that are non-conscious and have sufficiently cognitively complex preferences and goals, should qualify for moral standing. After responding to some challenges, I tentatively argue that taking into account uncertainty about which criteria an entity must satisfy to qualify for moral standing, and strategic considerations such as how such decisions will affect humans and other sentient entities, further supports granting moral standing to some non-sentient AIs. I highlight three implications: that the issue of AI moral standing may be more important, in terms of scale and urgency, than if either sentience or consciousness is necessary; that researchers working on policies designed to be inclusive of sentient AIs should broaden their scope to include all AIs with morally relevant interests; and even those who think AIs cannot be sentient or conscious should take the issue seriously. However, much uncertainty about these considerations remains, making this an important topic for future research.</jats:p>"
10.46397/jaih.16.1,Emergence of Generative AI and the Reorientation of Art Education,N/A
10.1007/s43681-024-00503-9,Responsibility before freedom: closing the responsibility gaps for autonomous machines,"<jats:title>Abstract</jats:title><jats:p>The introduction of autonomous machines (AMs) in human domains has raised challenging questions about the attribution of responsibility; referred to as the <jats:italic>responsibility gap</jats:italic>. In this paper, we address the gap by arguing that entities should not be granted the freedom of action unless they can also recognise the same right for others—and be subject to blame or punishment in cases of undermining the rights of others. Since AMs fail to meet this criterion, we argue that the users who utilize an AM to pursue their goals can instead grant the machine <jats:italic>their</jats:italic> (the user’s) right to act autonomously on their behalf. In this way, an AM’s right to act freely hinges on the user’s duty to recognise others’ right to be free. Since responsibility should be attributed <jats:italic>before</jats:italic> an entity is given the freedom to act, the responsibility gap only arises when we ignore the fact that AMs have no right of acting freely on their own. We also discuss some attractive features of the approach, address some potential objections, and compare our theory to existing proposals. We conclude by arguing that holding users responsible for the behaviour of AMs promotes a responsible use of AI while it indirectly motivates companies to make safer machines.</jats:p>"
10.4018/978-1-59140-987-8.ch042,Ethics of AI,"<jats:p>The first question concerns the kinds of AI we might achieve moral, immoral, or amoral. The second concerns the ethics of our achieving such an AI. They are more closely related than a first glance might reveal. For much of technology, the National Rifle Association’s neutrality argument might conceivably apply: “guns don’t kill people, people kill people.” But if we build a genuine, autonomous AI, we arguably will have to have built an artificial moral agent, an agent capable of both ethical and unethical behavior. The possibility of one of our artifacts behaving unethically raises moral problems for their development that no other technology can.</jats:p>"
10.1007/s43681-024-00444-3,The moral decision machine: a challenge for artificial moral agency based on moral deference,"<jats:title>Abstract</jats:title><jats:p>Humans are responsible moral agents in part because they can competently respond to moral reasons. Several philosophers have argued that artificial agents cannot do this and therefore cannot be responsible moral agents. I present a counterexample to these arguments: the ‘Moral Decision Machine’. I argue that the ‘Moral Decision Machine’ responds to moral reasons just as competently as humans do. However, I suggest that, while a hopeful development, this does not warrant strong optimism about ‘artificial moral agency’. The ‘Moral Decision Machine’ (and similar agents) can only respond to moral reasons by deferring to others, and there are good reasons to think this is incompatible with responsible moral agency. While the challenge to artificial moral agency based on moral reasons-responsiveness can be satisfactorily addressed; the challenge based on moral deference remains an open question. The right way to understand the challenge, I argue, is as a route to the claim that artificial agents are unlikely to be responsible moral agents because they cannot be authentic.</jats:p>"
10.1007/s43681-023-00321-5,Measuring responsible artificial intelligence (RAI) in banking: a valid and reliable instrument,"<jats:title>Abstract</jats:title><jats:p>Widespread use of artificial intelligence (AI) and machine learning (ML) in the US banking industry raises red flags with regulators and social groups due to potential risk of data-driven algorithmic bias in credit lending decisions. The absence of a valid and reliable measure of responsible AI (RAI) has stunted the growth of organizational research on RAI (i.e., the organizational balancing act to optimize efficiency and equity). To address this void, we develop a novel measurement instrument to assess RAI maturity in firms. A review of the nascent literature reveals that there is a wide distribution of RAI capabilities. The RAI instrument that we advance is based on the exhaustive review of this dispersed literature. Analyses of data from large US banks show strong evidence of validity and reliability of the RAI maturity instrument.</jats:p>"
10.1007/s43681-023-00395-1,Neighborhood sampling confidence metric for object detection,N/A
10.4324/9781315107752-25,AI and robot ethics,N/A
10.1093/oxfordhb/9780190067397.013.37,Calculative Composition,"<p>This chapter evaluates the ethical ends and means toward which AI-driven design has been, and perhaps could be, applied. While some designers have committed to applying AI toward more ethical ends, they have paid comparatively less attention toward the ethical means of its application. In order to ensure the ethical application of AI in design, practitioners and managers must make sure that they are both defining responsible design parameters and operationalizing those parameters responsibly. Moreover, designers must consider where they should assert their agency within an automated workflow. The chapter then surveys representative design fields—fashion, product, graphic, and architectural design—to examine what ethical opportunities and risks people might face when AI-driven design practice is programmed to serve the needs and desires of laborers, consumers, and clients.</p>"
10.1007/s43681-021-00048-1,Survey of EU ethical guidelines for commercial AI: case studies in financial services,N/A
10.1007/s43681-023-00305-5,"The technology triad: disruptive AI, regulatory gaps and value change","<jats:title>Abstract</jats:title><jats:p>Disruptive technologies can have far-reaching impacts on society. They may challenge or destabilize cherished ethical values and disrupt legal systems. There is a convergent interest among ethicists and legal scholars in such “second-order disruptions” to norm systems. Thus far, however, ethical and legal approaches to technological norm-disruption have remained largely siloed. In this paper, we propose to integrate the existing ‘dyadic’ models of disruptive change in the ethical and legal spheres, and shift focus to the relations between and mutual shaping of values, technology, and law. We argue that a ‘triadic’ values-technology-regulation model—“the technology triad”—is more descriptively accurate, as it allows a better mapping of second-order impacts of technological changes (on values and norms, through changes in legal systems—or on legal systems, through changes in values and norms). Simultaneously, a triadic model serves to highlight a broader portfolio of ethical, technical, or regulatory interventions that can enable effective ethical triage of—and a more resilient response to—such Socially Disruptive Technologies. We illustrate the application of the triadic framework with two cases, one historical (how the adoption of the GDPR channeled and redirected the evolution of the ethical value of ‘privacy’ when that had been put under pressure by digital markets), and one anticipatory (looking at anticipated disruptions caused by the ongoing wave of generative AI systems).</jats:p>"
10.4018/979-8-3693-8557-9.ch003,Unleashing Creativity in Natural Language,"<jats:p>In today's cybernetic world, Generative AI (Gen AI) and Natural Language Processing (NLP) made a vibrant change in the technology. Intertwine between Generative AI and NLP creates a different angle that comes to light. NLP algorithms enhanced by Generative AI not only understands the language but also generates human-like responses, opening doors to more nuanced and context-aware interactions. This chapter focuses on two specific dimensions such as language generation and multilingual capability. Large Language Models (LLMs) are considered as a foundational element or backbone for generating texts. Thereby, prompting LLM is a crucial element for generating the desired text. The main aim of this chapter is to explore importance of prompting LLM, various prompting techniques used for interacting with LLM and the frameworks available for accessing the LLM, and issues involved in prompting LLM and their challenges and future directions will also be discussed.</jats:p>"
10.1007/s43681-022-00219-8,Ethical risks of AI-designed products: bespoke surgical tools as a case study,"<jats:title>Abstract</jats:title><jats:p>An emerging use of machine learning (ML) is creating products optimised using computational design for individual users and produced using 3D printing. One potential application is bespoke surgical tools optimised for specific patients. While optimised tool designs benefit patients and surgeons, there is the risk that computational design may also create unexpected designs that are unsuitable for use with potentially harmful consequences. We interviewed potential stakeholders to identify both established and unique technical risks associated with the use of computational design for surgical tool design and applied ethical risk analysis (eRA) to identify how stakeholders might be exposed to ethical risk within this process. The main findings of this research are twofold. First, distinguishing between unique and established risks for new medical technologies helps identify where existing methods of risk mitigation may be applicable to a surgical innovation, and where new means of mitigating risks may be needed. Second, the value of distinguishing between technical and ethical risks in such a system is that it identifies the key responsibilities for managing these risks and allows for any potential interdependencies between stakeholders in managing these risks to be made explicit. The approach demonstrated in this paper may be applied to understanding the implications of new AI and ML applications in healthcare and other high consequence domains.</jats:p>"
10.1007/s43681-021-00039-2,AI auditing and impact assessment: according to the UK information commissioner’s office,"<jats:title>Abstract</jats:title><jats:p>As the use of data and artificial intelligence systems becomes crucial to core services and business, it increasingly demands a multi-stakeholder and complex governance approach. The Information Commissioner's Office’s ‘Guidance on the AI auditing framework: Draft guidance for consultation’ is a move forward in AI governance. The aim of this initiative is toward producing guidance that encompasses both technical (e.g. system impact assessments) and non-engineering (e.g. human oversight) components to governance and represents a significant milestone in the movement towards standardising AI governance. This paper will summarise and critically evaluate the ICO effort and try to anticipate future debates and present some general recommendations.</jats:p>"
10.12677/ojls.2023.116712,Legal Regulation of Generative AI Services under Digital Ethics,N/A
10.1007/s43681-022-00153-9,Racing into the fourth industrial revolution: exploring the ethical dimensions of medical AI and rights-based regulatory framework,N/A
10.1007/s43681-023-00303-7,The complex relationship of AI ethics and trust in human–AI teaming: insights from advanced real-world subject matter experts,N/A
10.1007/s43681-022-00141-z,Explainable machine learning practices: opening another black box for reliable medical AI,"<jats:title>Abstract</jats:title><jats:p>In the past few years, machine learning (ML) tools have been implemented with success in the medical context. However, several practitioners have raised concerns about the lack of transparency—at the algorithmic level—of many of these tools; and solutions from the field of explainable AI (XAI) have been seen as a way to open the ‘black box’ and make the tools more trustworthy. Recently, Alex London has argued that in the medical context we do not need machine learning tools to be interpretable at the algorithmic level to make them trustworthy, as long as they meet some strict empirical desiderata. In this paper, we analyse and develop London’s position. In particular, we make two claims. First, we claim that London’s solution to the problem of trust can potentially address another problem, which is how to evaluate the reliability of ML tools in medicine for regulatory purposes. Second, we claim that to deal with this problem, we need to develop London’s views by shifting the focus from the opacity of algorithmic details to the opacity of the way in which ML tools are trained and built. We claim that to regulate AI tools and evaluate their reliability, agencies need an explanation of how ML tools have been built, which requires documenting and justifying the technical choices that practitioners have made in designing such tools. This is because different algorithmic designs may lead to different outcomes, and to the realization of different purposes. However, given that technical choices underlying algorithmic design are shaped by value-laden considerations, opening the black box of the design process means also making transparent and motivating (technical and ethical) values and preferences behind such choices. Using tools from philosophy of technology and philosophy of science, we elaborate a framework showing how an explanation of the training processes of ML tools in medicine should look like.</jats:p>"
10.1007/s43681-023-00382-6,Who is the human in the machine? Releasing the human–machine metaphor from its cultural roots can increase innovation and equity in AI,"<jats:title>Abstract</jats:title><jats:p>Computer science and cognitive science have a shared past, with many intertwined goals and perspectives. The conceptual metaphor, shaping the discoveries of these fields for decades, has been <jats:italic>the human mind–machine.</jats:italic> New cross-cultural findings indicate that it is time that we interrogate the origin of the metaphor and develop a more global representation of attributes labeled <jats:italic>human</jats:italic>. This paper describes a gap in fairness research in cross-cultural bias affecting international participation in the field. It further outlines opportunities to diversify and test core concepts inspiring design and increasing equity. The proposed adaptation would shift our approach to knowledge and technology creation by (1) altering the attributes of the <jats:italic>human mind–machine</jats:italic> metaphor that define intelligence, memory, categorization, logic, inference, perception, concepts of time and space, concepts of personhood, and other cognitive terms which both fields study; (2) interrogating the universality implied by the conceptual metaphor to both machine and end-user; and (3) seizing the broadened conceptual metaphor to create new math, science, and disrupt the current paradigm scripting the inferences of research findings in computer science and cognitive science. A more globally attuned conceptual metaphor, updated to enfranchise the full membership the term <jats:italic>human</jats:italic> implies, will increase our collective ability to investigate, describe, and develop new science and technology and increase the equity of those involved in the process.</jats:p>"
10.1007/s43681-023-00362-w,AI and the quest for diversity and inclusion: a systematic literature review,"<jats:title>Abstract</jats:title><jats:p>The pervasive presence and wide-ranging variety of artificial intelligence (AI) systems underscore the necessity for inclusivity and diversity in their design and implementation, to effectively address critical issues of fairness, trust, bias, and transparency. However, diversity and inclusion (D&amp;I) considerations are significantly neglected in AI systems design, development, and deployment. Ignoring D&amp;I in AI systems can cause digital redlining, discrimination, and algorithmic oppression, leading to AI systems being perceived as untrustworthy and unfair. Therefore, we conducted a systematic literature review (SLR) to identify the challenges and their corresponding solutions (guidelines/ strategies/ approaches/ practices) about D&amp;I in AI and about the applications of AI for D&amp;I practices. Through a rigorous search and selection, 48 relevant academic papers published from 2017 to 2022 were identified. By applying open coding on the extracted data from the selected papers, we identified 55 unique challenges and 33 unique solutions in addressing D&amp;I in AI. We also identified 24 unique challenges and 23 unique solutions for enhancing D&amp;I practices by AI. The result of our analysis and synthesis of the selected studies contributes to a deeper understanding of diversity and inclusion issues and considerations in the design, development and deployment of the AI ecosystem. The findings would play an important role in enhancing awareness and attracting the attention of researchers and practitioners in their quest to embed D&amp;I principles and practices in future AI systems. This study also identifies important gaps in the research literature that will inspire future direction for researchers.</jats:p>"
10.32388/5g2xsi,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/nognpu,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/m13efo,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/2aszdb,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.1117/12.2646476.6312671916112,N/A,N/A
10.2139/ssrn.4817287,Incorporating Generative Ai Into Human Resource Practices,N/A
10.31219/osf.io/95sdx,Investigating Librarian Perspectives on Generative AI,"<p>This study investigates librarians' perspectives on generative artificial intelligence (AI) in libraries. Librarians (n = 52) from 21 different institutions attended a workshop to discuss generative AI and assess the ethics of various applications of large language models (LLMs) in research. Through the analysis of qualitative workshop data, we identified 24 overarching themes — 15 concerns related to embracing AI, 4 about not embracing it, and 5 possible interventions for libraries. Additionally, assessments revealed a nuanced view of the ethical considerations associated with LLMs in research. High ethical ratings were given to use cases involving language feedback and learning support. Lower ratings were associated with generative AI imitating professional roles and generating text based on user input. Furthermore, we found mixed opinions on several use cases among the participants, demonstrating the difficulty of establishing clear guidelines. This paper presents the preliminary findings from a research and development project, offering insights into the perspectives of librarians on generative AI. It underscores the need for a balanced, informed approach to integrating generative AI in libraries.</p>"
10.2139/ssrn.4426194,User Interaction with Misinformation Heuristics and Systematic Processes of Misinformation in Generative Ai,N/A
10.1007/978-3-031-55642-5,Generative AI for Effective Software Development,N/A
10.2139/ssrn.4587250,Effective Generative AI: The Human-Algorithm Centaur,N/A
10.1016/b978-0-44-321857-6.00012-6,From interpolation to fuzzy regression,N/A
10.32617/973-650ad52f9058b,Generative AI Can Help Grow Your Business,N/A
10.18258/57360,Generative AI-Based Design of Novel Silicase Enzymes for Carbon-Sequestering Agriculture,N/A
10.32388/w1dodh,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/w05nlk,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/dgusl4,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.2139/ssrn.4819491,Incorporating Generative Ai into Human Resource Practices,N/A
10.1007/s43681-023-00272-x,The ethics of computer vision: an overview in terms of power,"<jats:title>Abstract</jats:title><jats:p>Computer vision is a subfield of artificial intelligence, aimed at making computers see. Computer vision tools enable a system or device to automatically analyze, interpret, and respond to images and videos. Computer vision tasks range from object detection and tracking, to the recognition of people’s faces and emotional states. While the ethics of AI in general has received significant attention, and the ethics of facial recognition (a computer vision application) too, little of the AI ethics literature focuses specifically on the ethics of computer vision. In this chapter, I create an overview of ethical, social, and political issues related to computer vision, using a critical approach. This means that I identify issues in terms of power and evaluate them in function of their impact on the value of autonomy and the normative goal of emancipatory progress. The aim of this chapter is first and foremost to offer an overview of potential normative implications of computer vision. Additionally, the chapter functions as an example for the use of a critical approach to AI ethics.</jats:p>"
10.2139/ssrn.4615977,A Case Study Exploring the Effects of Generative Ai on Incidental Language Learning,N/A
10.52591/lxai2018120310,Information Theoretic Generative Modeling,"<jats:p>In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression introduced by Claude Shannon in 1959 [1], to shed light on an important problem in latent variable modeling of data: is there room to improve the model? One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model. The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood. Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions. The result we present here runs much deeper than the particular modeling problem being solved - in formally connecting the latent variable modeling problem to rate-distortion theory, we have established a bridge where decades of work on either field can now be considered for possible cross-pollination; notably in a subsequent article we intend to ask the question next of whether practical algorithms in data compression can be used to design latent variables. We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.</jats:p>"
10.21203/rs.3.rs-3371292/v1,Experimental Evidence on Negative Impact of Generative AI on Scientific Learning Outcomes,"<jats:title>Abstract</jats:title>
        <jats:p>In this study, I explored the impact of Generative AI on learning efficacy in academic reading materials using experimental methods. College-educated participants engaged in three cycles of reading and writing tasks. After each cycle, they responded to comprehension questions related to the material. After adjusting for background knowledge and demographic factors, complete reliance on AI for writing tasks led to a 25.1% reduction in accuracy. In contrast, AI-assisted reading resulted in a 12% decline. Interestingly, using AI for summarization significantly improved both quality and output. Accuracy exhibited notable variance in the AI-assisted section. Further analysis revealed that individuals with a robust background in the reading topic and superior reading/writing skills benefitted the most. I conclude the research by discussing educational policy implications, emphasizing the need for educators to warn students about the dangers of over-dependence on AI and provide guidance on its optimal use in educational settings.</jats:p>"
10.2139/ssrn.4601781,"Information Integrity, Academic Integrity, and Generative AI",N/A
10.32388/emz1bq,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/mr4cey,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/z2j79x,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.36227/techrxiv.24417706.v1,MeloHarmony: Exploring Emotion in Crafting AI-Generated Music with Generative Adversarial Network Powered Harmony,"<jats:p>&lt;p&gt;The profound association between music and human emotion has transcended epochs, underscoring the capacity of musical compositions to elicit a spectrum of feelings, from exuberance to introspection. In the contemporary landscape, the intersection of music and technological advancements has engendered a paradigmatic shift in the creation and interpretation of musical compositions. Central to this transformation is the integration of artificial intelligence (AI) into the realm of music composition, a domain historically governed by human creativity.&lt;/p&gt;
&lt;p&gt;This research endeavors to navigate this juncture, unraveling the prospect of imbuing AI-generated music with heightened emotional resonance, thereby amplifying the scope of artistic expression. At the crux of this exploration lies the innovative utilization of Generative Adversarial Networks (GANs) to infuse the synthesized musical compositions with an intricate tapestry of human-like emotions.&lt;/p&gt;
&lt;p&gt;This paper sets out to elucidate the multifaceted dimensions of this venture by charting a trajectory that traverses the historical lineage of emotional undertones in music, culminating in a contemporary synergy between AI capabilities and human sentiment. Our approach is encapsulated within the nexus of technology and creativity, where GANs are envisaged as a conduit to facilitate the infusion of emotions into AI-generated musical compositions.&lt;/p&gt;
&lt;p&gt;In subsequent sections, we delve into an immersive analysis of the seminal role that music has played in articulating emotions throughout history. Moreover, we embark on a comprehensive exploration of the confluence of AI advancements and the nuanced realm of emotional resonance, delineating the profound possibilities that emerge from this amalgamation. Crucially, the research postulates a novel framework that leverages GANs to imbue AI-generated harmonies with a poignant emotional depth, elucidating the pivotal role of technology in elevating the emotive tenor of musical compositions.&lt;/p&gt;
&lt;p&gt;The subsequent chapters unravel the intricate methodology underpinning this research, encapsulating data collection processes, GAN architecture elucidation, techniques for embedding emotional facets, and the meticulous training process. Furthermore, a meticulous analysis of the emotional impact of AI-generated music on human perception is presented, both quantitatively and qualitatively, shedding light on the efficacy of the GAN-powered approach.&lt;/p&gt;
&lt;p&gt;Conclusively, the research extends its purview to expound upon the ethical considerations embedded within this paradigmatic juncture, while also envisioning potential trajectories for the practical application and validation of the proposed GAN-powered methodology. As the curtains are drawn on this introductory exposition, the subsequent sections promise a symphony of insights, culminating in a harmonious synthesis of AI ingenuity and human emotional resonance within the tapestry of musical composition.&lt;/p&gt;</jats:p>"
10.2139/ssrn.4608268,Generative AI and Finding the Law,N/A
10.1038/d41591-024-00025-1,Designer antibiotics by generative AI,N/A
10.2139/ssrn.4594780,Effective Generative AI: The Human-Algorithm Centaur,N/A
10.1021/acs.analchem.4c01734.s001,Solving Advanced Task-Specific Problems in Measurement Sciences with Generative AI,N/A
10.20944/preprints202406.0578.v1,Intelligent Network Optimization in Cloud Environments with Generative AI and LLMs,"<jats:p>This paper represents a groundbreaking paradigm shift in network optimization. Departing from traditional static methodologies, this innovative approach harnesses the power of Generative Artificial Intelligence (AI) and Large Language Models (LLMs) to optimize cloud networks dynamically. By integrating advanced AI algorithms, this framework continuously adapts and evolves, ensuring optimal real-time performance. This dynamic optimization enhances efficiency and resilience, allowing cloud networks to adjust seamlessly to changing demands and conditions. Through the fusion of cutting-edge technology and adaptive intelligence, this approach heralds a new era in network optimization, empowering organizations to achieve unprecedent-ed levels of agility and scalability in their cloud infrastructures.</jats:p>"
10.1515/9781501519024-006,Chapter 5: Visualization with Generative AI,N/A
10.5040/9781509974979.0008,Notes,N/A
10.2139/ssrn.4537389,The Copyright Problem with Emerging Generative AI,N/A
10.32388/ykz2dp,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.2139/ssrn.4453664,Proper Generative AI Prompting for Financial Analysis,N/A
10.4018/979-8-3693-2440-0.ch001,An Introduction to Generative AI Tools for Education 2030,"<jats:p>The year 2030 marks a significant juncture in the evolution of education, where Generative Artificial Intelligence (AI) tools are poised to revolutionize the learning experience. In education society, the importance of generative AI is to improve the accessibility of learning at the global level so that personalized learning experiences can be provided to every learner as per their needs. This chapter explores the multifaceted role of generative AI tools in reshaping educational practices, envisioning a future where these tools foster personalized, adaptive, and engaging learning environments. Generative AI tools, characterized by their ability to create and adapt content autonomously, are instrumental in tailoring educational materials to individual learner needs. This chapter surveys the landscape of generative AI applications in education, including content generation, interactive simulations, intelligent tutoring systems, and dynamic learning pathways. These tools aim to provide adaptive, context-aware learning experiences that cater to diverse learning styles and preferences. The adaptability of generative AI tools extends to the creation of personalized learning pathways. By leveraging data analytics and machine learning algorithms, these tools dynamically adjust content delivery, pacing, and complexity, ensuring that each learner's educational journey is optimized for their unique requirements. The discussion encompasses the potential of generative AI tools to support both formal and informal learning settings. Generative AI tools also play a crucial role in promoting inclusivity in education. By generating diverse and culturally relevant content, these tools contribute to breaking down barriers and addressing disparities in access to quality education. This chapter explores how generative AI can be leveraged to create content that resonates with learners from different backgrounds, fostering a more inclusive educational landscape.</jats:p>"
10.35542/osf.io/g5fb8,The Mosaic of Human-AI Co-Creation: Emerging human-technology relationships in a co-design process with generative AI,"<p>This study explored how pre-service teachers (N=33) perceived human-technology relationships with generative AI (genAI). The study employed a research-creation approach and implemented a hands-on workshop, in which the participants engaged in a speculative design process using generative AI. The study focused on how participants, armed with their new tool, approached their designs, made design decisions, and interacted with the responsive tool. The qualitative analysis of the video data from students' project presentations employed thematic analysis, interpreting the students' responses in relational terms. The results revealed that the emerging human-technology relationships were primarily expressed through distributed decision-making, with the AI actively contributing both to the object of activity and to the emerging design process. The findings highlight that genAI is neither passive nor neutral tools but actively transforms both the design process and its outcomes, shaping how people experience new forms of agency in relation to such technology.</p>"
10.1109/qrs-c60940.2023.00043,AI and Security - What Changes with Generative AI,N/A
10.1007/s43681-024-00537-z,The entangled human being – a new materialist approach to anthropology of technology,"<jats:title>Abstract</jats:title><jats:p>Technological advancements raise anthropological questions: How do humans differ from technology? Which human capabilities are unique? Is it possible for robots to exhibit consciousness or intelligence, capacities once taken to be exclusively human? Despite the evident need for an anthropological lens in both societal and research contexts, the philosophical anthropology of technology has not been established as a set discipline with a defined set of theories, especially concerning emerging technologies. In this paper, I will utilize a New Materialist approach, focusing particularly on the theories of Donna Haraway and Karen Barad, to explore their potential for an anthropology of technology. I aim to develop a techno-anthropological approach that is informed and enriched by New Materialism. This approach is characterized by its relational perspective, a dynamic and open conception of the human being, attention to diversity and the dynamics of power in knowledge production and ontology, and an emphasis on the non-human. I aim to outline an anthropology of technology centered on New Materialism, wherein the focus, paradoxically, is not exclusively on humans but equally on non-human entities and the entanglement with the non-human. As will become clear, the way we understand humans and their relationship with technology is fundamental for our concepts and theories in ethics of technology.</jats:p>"
10.1007/978-3-031-23035-6_1,The Need for AI Ethics in Higher Education,"<jats:title>Abstract</jats:title><jats:p>Business leaders, policymakers and technologists regularly portray Artificial Intelligence (AI) as an easy way to make sense of an increasingly complex world. Unsurprisingly, AI plays a central role in strategy papers, TED talks and speeches about the future of mobility.</jats:p>"
10.5220/0012693000003693,The Impact of Structured Prompt-Driven Generative AI on Learning Data Analysis in Engineering Students,N/A
10.4018/979-8-3693-0831-8.ch010,Generative AI Ethical Considerations and Discriminatory Biases on Diverse Students Within the Classroom,"<jats:p>Generative artificial intelligence (AI) can induce a variety of biases that can impact decision-making processes, and it can produce inaccurate or distorted information that may harm marginalized student groups in higher education classrooms. With the increase in generative AI use among college students and instructors, it is important to examine the ethical risks and discriminatory biases that can negatively influence students' learning experiences. For this purpose, this chapter focuses on the different types of generative AI ethical risks that can occur in U.S. classrooms. The variety of AI discriminatory biases against diverse student populations are also documented. Further, the authors discuss a case application that expands on the potential AI biases in higher education. To prevent and address potential AI ethical risks and biases, recommendations are offered to higher education educators. Lastly, guidance is offered suggesting future research in AI biases and diversity in higher education institutions.</jats:p>"
10.1007/s10892-023-09456-3,What Makes Work “Good” in the Age of Artificial Intelligence (AI)? Islamic Perspectives on AI-Mediated Work Ethics,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) technologies are increasingly creeping into the work sphere, thereby gradually questioning and/or disturbing the long-established moral concepts and norms communities have been using to define what makes work good. Each community, and Muslims make no exception in this regard, has to revisit their moral world to provide well-thought frameworks that can engage with the challenging ethical questions raised by the new phenomenon of AI-mediated work. For a systematic analysis of the broad topic of AI-mediated work ethics from an Islamic perspective, this article focuses on presenting an accessible overview of the “moral world” of work in the Islamic tradition. Three main components of this moral world were selected due to their relevance to the AI context, namely (1) Work is inherently good for humans, (2) Practising a religiously permitted profession and (c) Maintaining good relations with involved stakeholders. Each of these three components is addressed in a distinct section, followed by a sub-section highlighting the relevance of the respective component to the particular context of AI-mediated work. The article argues that there are no unsurmountable barriers in the Islamic tradition against the adoption of AI technologies in work sphere. However, important precautions should be considered to ensure that embracing AI will not be at the cost of work-related moral values. The article also highlights how important lessons can be learnt from the positive historical experience of automata that thrived in the Islamic civilization.</jats:p>"
10.1007/979-8-8688-0456-4_12,The Platform Shift,N/A
10.1007/979-8-8688-0456-4_3,Understanding Language Models,N/A
10.1629/uksg.649,Empowering knowledge through AI: open scholarship proactively supporting well trained generative AI,N/A
10.69554/kzrs2422,Machine unlearning for generative AI,"<jats:p xml:lang=""en"">This paper introduces a new field of AI research called machine unlearning and examines the challenges and approaches to extend machine unlearning to generative AI (GenAI). Machine unlearning is a model-driven approach to make an existing artificial intelligence (AI) model unlearn a set of data from its learning. Machine unlearning is becoming important for businesses to comply with privacy laws such as General Data Protection Regulation (GDPR) customer’s right to be forgotten, to manage security and to remove bias that AI models learn from their training data, as it is expensive to retrain and deploy the models without the bias or security or privacy compromising data. This paper presents the state of the art in machine unlearning approaches such as exact unlearning, approximate unlearning, zero-shot learning (ZSL) and fast and efficient unlearning. The paper highlights the challenges in applying machine learning to GenAI which is built on a transformer architecture of neural networks and adds more opaqueness to how large language models (LLM) learn in pre-training, fine-turning, transfer learning to more languages and in inference. The paper elaborates on how models retain the learning in a neural network to guide the various machine unlearning approaches for GenAI that the authors hope can be built upon their work. The paper suggests possible futuristic directions of research to create transparency in LLM and particularly looks at hallucinations in LLMs when they are extended to do machine translation for new languages beyond their training with ZSL to shed light on how the model stores its learning of newer languages in its memory and how it draws upon it during inference in GenAI applications. Finally, the paper calls for collaborations for future research in machine unlearning for GenAI, particularly LLMs, to add transparency and inclusivity to language AI.</jats:p>"
10.18653/v1/2023.artofsafety-1.6,Uncovering Bias in AI-Generated Images,N/A
10.4018/979-8-3693-1351-0.ch016,Use of Generative AI Tools to Facilitate Personalized Learning in the Flipped Classroom,"<jats:p>With the development of technology, AI has been the subject of intense research in the field of education, with ChatGPT, the adoption of generative AI in education. But generative AI has only gained attention in the last few years. Therefore, there are not many research results on the application of generative AI in flipped classrooms, and there are many questions that need to be explored and verified by researchers. In this study, a flipped classroom combined with generative AI tools to promote personalized learning was studied. A theme in the music course was selected for course design and a pilot study was conducted. The results show that teachers and students have very different views on this research, and this phenomenon is summarized and analyzed, and finally some suggestions are made to help better use generative AI tools to promote personalized learning in the flipped classroom.</jats:p>"
10.1007/978-3-031-55642-5_14,Generating Explanations for AI-Powered Delay Prediction in Software Projects,N/A
10.1007/s00146-020-00937-9,Reimagining life (forms) with generative and bio art,N/A
10.1515/9781501518430-010,Chapter 8: Generative AI: Conversational Agents and Beyond,N/A
10.2139/ssrn.4461406,Task-Interdependencies between Generative AI and Workers,N/A
10.2139/ssrn.4852574,Interactivity and Illusions of Ability: The Effect of Generative AI on Investor Judgments,N/A
10.12781/978-1-907549-42-7-2,Generative Journalism Provokes New Life,N/A
10.36227/techrxiv.24417706,MeloHarmony: Exploring Emotion in Crafting AI-Generated Music with Generative Adversarial Network Powered Harmony,"<jats:p>&lt;p&gt;The profound association between music and human emotion has transcended epochs, underscoring the capacity of musical compositions to elicit a spectrum of feelings, from exuberance to introspection. In the contemporary landscape, the intersection of music and technological advancements has engendered a paradigmatic shift in the creation and interpretation of musical compositions. Central to this transformation is the integration of artificial intelligence (AI) into the realm of music composition, a domain historically governed by human creativity.&lt;/p&gt;
&lt;p&gt;This research endeavors to navigate this juncture, unraveling the prospect of imbuing AI-generated music with heightened emotional resonance, thereby amplifying the scope of artistic expression. At the crux of this exploration lies the innovative utilization of Generative Adversarial Networks (GANs) to infuse the synthesized musical compositions with an intricate tapestry of human-like emotions.&lt;/p&gt;
&lt;p&gt;This paper sets out to elucidate the multifaceted dimensions of this venture by charting a trajectory that traverses the historical lineage of emotional undertones in music, culminating in a contemporary synergy between AI capabilities and human sentiment. Our approach is encapsulated within the nexus of technology and creativity, where GANs are envisaged as a conduit to facilitate the infusion of emotions into AI-generated musical compositions.&lt;/p&gt;
&lt;p&gt;In subsequent sections, we delve into an immersive analysis of the seminal role that music has played in articulating emotions throughout history. Moreover, we embark on a comprehensive exploration of the confluence of AI advancements and the nuanced realm of emotional resonance, delineating the profound possibilities that emerge from this amalgamation. Crucially, the research postulates a novel framework that leverages GANs to imbue AI-generated harmonies with a poignant emotional depth, elucidating the pivotal role of technology in elevating the emotive tenor of musical compositions.&lt;/p&gt;
&lt;p&gt;The subsequent chapters unravel the intricate methodology underpinning this research, encapsulating data collection processes, GAN architecture elucidation, techniques for embedding emotional facets, and the meticulous training process. Furthermore, a meticulous analysis of the emotional impact of AI-generated music on human perception is presented, both quantitatively and qualitatively, shedding light on the efficacy of the GAN-powered approach.&lt;/p&gt;
&lt;p&gt;Conclusively, the research extends its purview to expound upon the ethical considerations embedded within this paradigmatic juncture, while also envisioning potential trajectories for the practical application and validation of the proposed GAN-powered methodology. As the curtains are drawn on this introductory exposition, the subsequent sections promise a symphony of insights, culminating in a harmonious synthesis of AI ingenuity and human emotional resonance within the tapestry of musical composition.&lt;/p&gt;</jats:p>"
10.5040/9781509974979.ch-005,Law,N/A
10.2139/ssrn.4738748,The Generative AI challenges for competition authorities,N/A
10.18653/v1/2023.artofsafety-1,Proceedings of the ART of Safety: Workshop on Adversarial testing and Red-Teaming for generative AI,N/A
10.32388/m78whk,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.1515/9783839474723,Art Intelligence,N/A
10.2139/ssrn.4877505,Generative AI and Firm-level Productivity: Evidence from Startup Funding Dynamics,N/A
10.14236/ewic/eva2024.38,Generative AI: The death of computer art?,N/A
10.1109/iedm45741.2023.10413684,Innovations For Energy Efficient Generative AI,N/A
10.4018/979-8-3693-0502-7.ch010,Modern Applications With a Focus on Training ChatGPT and GPT Models,"<jats:p>Generative AI (GAI) and natural language processing (NLP) have emerged as the most exciting and rapidly growing fields in artificial intelligence (AI). This book chapter provides a comprehensive exploration of the advanced applications of GAI and NLP models, with a specific focus on the renowned ChatGPT model. The chapter commences by offering a concise historical overview of the development of GAI and NLP, highlighting crucial milestones and advancements in the field over the period. In order to understand the workings of the current technology sensation, we will take a brief look at the basic building blocks of GPT models, such as transformers. Subsequently, the chapter delves into the introduction of ChatGPT, presenting an extensive overview of the model, elucidating its underlying architecture, and emphasizing its unique capabilities. Furthermore, it will illustrate the training process of the GPT model followed by a fine-tuning process to deal with the current model's shortcomings.</jats:p>"
10.4018/979-8-3693-0831-8.ch003,Unravelling the Evolution of Generative AI in Communication Education,"<jats:p>This chapter traces the historical evolution of human-machine communication, highlighting generative AI's transformative role. It begins with the origins of AI and early communication technologies, including Alan Turing's pioneering work and the development of Turing machines, as well as early AI experiments in human-machine interaction. The narrative then transitions to AI's evolution and the emergence of generative models, spotlighting advancements like recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, which are fundamental to coherent text generation. The pivotal role of generative AI in language and communication learning is explored, showcasing its impact on writing skills and comprehension. Ethical considerations in AI usage are discussed, including bias, privacy, and ethical communication practices. The chapter concludes by envisioning AI's future in communication pedagogy, emphasizing collaboration between educators and AI developers and recapitulating historical insights for the integration of AI in education.</jats:p>"
10.4018/979-8-3693-2418-9.ch006,Generative AI and Its Impact on Creative Thinking Abilities in Higher Education Institutions,"<jats:p>Generative AI technologies such as ChatGPT have started gaining increased popularity among higher education institutions. Students, as well as teaching professionals, can utilize these tools for various academic purposes due to the immense benefits they provide by way of customization of data generated and ease of access to data. However, this chapter seeks to analyze how such tools may impact students' creative thinking ability. It also analyses the drawbacks faced by teachers after implementation of such tools. The methodology adopted for the study was two surveys: one administered to gather students' opinions and the other for understanding teachers' perspectives. The analysis of the data collected shows that the over-reliance of students on such generative AI tools might hinder students' ability to think creatively to some extent. The chapter also suggests some of the strategies that can be adopted by teachers to ensure students' capabilities are assessed accurately.</jats:p>"
10.51219/urforum.2023.massimo-buonomo,Exploring the Synergy: Generative AI and Venture Capital in Driving Innovation and Growth,N/A
10.1162/99608f92.d949f941,Government Interventions to Avert Future Catastrophic AI Risks,N/A
10.55041/isjem01633,StoryCraft AI: Exploring Generative Approaches to Story Narration through AI,"<jats:p>In this project, we demonstrate a Storytelling AI system, which can generate short stories and complementary illustrated images with minimal input from the user. The system makes use of a text generation model, a text-to-image synthesis network, and a neural style transfer model. The final project is deployed into a website where users can build their stories. The field of AI has made significant changes in various domains, including Natural Language Processing and Generative models. One captivating application of these advancements is in the realm of storytelling. This project introduces a novel approach to story narration using a Generative AI model, specifically leveraging the GPT architecture. Traditional storytelling involves human creativity, imagination, and the ability to craft engaging narratives. However, the integration of AI into storytelling brings about new opportunities and challenges. In this project, we delve into the methodologies and techniques used to train a GPT-based model for generating coherent and captivating stories. In the end, we found that using computers for storytelling can be exciting, but we need to work together to ensure the stories are great and meaningful.  Key Words:  Artificial Intelligence (AI), Generative Adversarial Network (GAN), Generative Pre-trained Transformer (GPT)</jats:p>"
10.2139/ssrn.4860853,AI at the Bench: Legal and Ethical Challenges of Informing - or Misinforming - Judicial Decision-Making Through Generative AI,N/A
10.1007/s00146-024-01921-3,"Not “what”, but “where is creativity?”: towards a relational-materialist approach to generative AI","<jats:title>Abstract</jats:title><jats:p>The recent emergence of generative AI software as viable tools for use in the cultural and creative industries has sparked debates about the potential for “creativity” to be automated and “augmented” by algorithmic machines. Such discussions, however, begin from an ontological position, attempting to define creativity by either falling prey to universalism (i.e. “creativity is X”) or reductionism (i.e. “only humans can be truly creative” or “human creativity will be fully replaced by creative machines”). Furthermore, such an approach evades addressing the real and material impacts of AI on creative labour in these industries. This article thus offers more expansive methodological and conceptual approaches to the recent hype on generative AI. By combining (Csikszentmihalyi, The systems model of creativity, Springer, Dordrecht, 2014) systems view of creativity, in which we emphasise the shift from “what” to “where” is creativity, with (Lievrouw, Media technologies, The MIT Press, 2014) relational-materialist theory of “mediation”, we argue that the study of “creativity” in the context of generative AI must be attentive to the interactions between technologies, practices, and social arrangements. When exploring the relational space between these elements, three core concepts become pertinent: creative labour, automation, and distributed agency. Critiquing “creativity” through these conceptual lenses allows us to re-situate the use of generative AI within discourses of labour in post-industrial capitalism and brings us to a conceptualisation of creativity that privileges neither the human user nor machine algorithm but instead emphasises a relational and distributed form of agency.</jats:p>"
10.7551/mitpress/12549.003.0006,Just Machines?,N/A
10.7551/mitpress/12549.003.0018,Further Reading,N/A
10.7551/mitpress/12549.003.0012,Policy Proposals,N/A
10.1007/978-981-19-9382-4_6,Normative Ethical Theory and AI Ethics,N/A
10.1007/s43681-022-00245-6,Workplace automation and political replacement: a valid analogy?,N/A
10.7551/mitpress/12549.003.0007,The Technology,N/A
10.1007/s43681-022-00152-w,Should explainability be a fifth ethical principle in AI ethics?,N/A
10.47289/aiej,AI Ethics Journal,N/A
10.7551/mitpress/12549.003.0020,[ Front Matter ],N/A
10.21428/e4baedd9.6b3930b1,Preface,N/A
10.32388/wh1ck9,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/szm69h,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/cllzwo,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/xgqwmx,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.1007/979-8-8688-0205-8,Building Generative AI-Powered Apps,N/A
10.31234/osf.io/vgzhj,Beyond the Brush: Human Versus AI Creativity  in the Realm of Generative Art,"<p>Drawing parallels from the evolution of chess, where human-machine collaboration once thrived before succumbing to AI dominance, we investigate whether similar dynamics are at play in the realm of digital art. Leveraging DALL-E 3, an advanced AI program that generates digital images from textual descriptions, we produced artwork using creative wordsets provided by professional artists, novice artists, and AI (ChatGPT 4.0). Based on ratings from 299 participants, our results show that digital images collaboratively produced by professional artists and AI were perceived as more creative than those produced by AI alone. Additionally, digital images collaboratively produced by professional artists and AI were deemed more creative than those collaboratively produced by novice artists and AI, the latter being outperformed by AI. These findings highlight the enduring significance of human creativity in the face of advancing AI technologies, and suggest the potential for continued collaboration between humans and AI in creative expression.</p>"
10.52783/jier.v4i2.1019,Next-Generation Education: The Impact of Generative AI on Learning,N/A
10.2139/ssrn.4899250,"Optimal Integration: Human, Machine, and Generative AI",N/A
10.2139/ssrn.4847595,Bias in Text Generative Open AI,N/A
10.1038/d41586-023-01516-w,How generative AI is building better antibodies,N/A
10.1007/s43681-021-00053-4,Real Estate Data Marketplace,N/A
10.1007/s43681-021-00088-7,God does not play dice but self-driving cars should,N/A
10.1007/s43681-024-00436-3,State of artificial intelligence eco-system in Ethiopia,N/A
10.1007/s43681-024-00442-5,Artificial intelligence at sentencing: when do algorithms perform well enough to replace humans?,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence is currently supplanting the work of humans in many societal contexts. The purpose of this article is to consider the question of when algorithmic tools should be regarded as performing sufficiently well to replace human judgements and decision-making at sentencing. More precisely, the question as to which are the ethically plausible criteria for the comparative performance assessments of algorithms and humans is considered with regard to both risk assessment algorithms that are designed to provide predictions of recidivism and sentencing algorithms designed to determine sentences in individual criminal cases. It is argued, first, that the prima facie most obvious assessment criteria do not stand up to ethical scrutiny. Second, that ethically plausible criteria presuppose ethical theory on penal distribution which currently has not been sufficiently developed. And third, that the current lack of assessment criteria has comprehensive implications regarding when algorithmic tools should be implemented in criminal justice practice.</jats:p>"
10.1007/s43681-024-00422-9,Correction: QI2: an interactive tool for data quality assurance,N/A
10.1007/s43681-023-00297-2,A principles-based ethics assurance argument pattern for AI and autonomous systems,"<jats:title>Abstract</jats:title><jats:p>An assurance case is a structured argument, typically produced by safety engineers, to communicate confidence that a critical or complex system, such as an aircraft, will be <jats:italic>acceptably safe</jats:italic> within its intended context. Assurance cases often inform third party approval of a system. One emerging proposition within the trustworthy AI and autonomous systems (AI/AS) research community is to use assurance cases to instil justified confidence that specific AI/AS will be <jats:italic>ethically acceptable</jats:italic> when operational in well-defined contexts. This paper substantially develops the proposition and makes it concrete. It brings together the assurance case methodology with a set of ethical principles to structure a principles-based ethics assurance argument pattern. The principles are justice, beneficence, non-maleficence, and respect for human autonomy, with the principle of transparency playing a supporting role. The argument pattern—shortened to the acronym PRAISE—is described. The objective of the proposed PRAISE argument pattern is to provide a reusable template for individual ethics assurance cases, by which engineers, developers, operators, or regulators could justify, communicate, or challenge a claim about the overall ethical acceptability of the use of a specific AI/AS in a given socio-technical context. We apply the pattern to the hypothetical use case of an autonomous ‘robo-taxi’ service in a city centre.</jats:p>"
10.1007/s43681-023-00332-2,"Against the opacity, and for a qualitative understanding, of artificially intelligent technologies","<jats:title>Abstract</jats:title><jats:p>This paper aims, first, to argue against using opaque AI technologies in decision making processes, and second to suggest that we need to possess a qualitative form of understanding about them. It first argues that opaque artificially intelligent technologies are suitable for users who remain indifferent to the understanding of decisions made by means of these technologies. According to virtue ethics, this implies that these technologies are not well-suited for those who care about realizing their moral capacity. The paper then draws on discussions on scientific understanding to suggest that an AI technology becomes understandable to its users when they are provided with a qualitative account of the consequences of using it. As a result, explainable AI methods can render an AI technology understandable to its users by presenting the qualitative implications of employing the technology for their lives.</jats:p>"
10.1007/s43681-024-00554-y,Navigating modern era at sea: legal challenges and opportunities of unmanned and autonomous shipping,N/A
10.1145/3461702.3462552,An AI Ethics Course Highlighting Explicit Ethical Agents,N/A
10.1007/s43681-023-00403-4,Correction: Ought we align the values of artificial moral agents?,N/A
10.1007/s43681-024-00429-2,Correction: ECS: an interactive tool for data quality assurance,N/A
10.1007/s43681-022-00212-1,Against explainability requirements for ethical artificial intelligence in health care,N/A
10.2139/ssrn.4662322,Enhancing Generative Ai-Based Software Implementation: The Role of Trust in Ai and Top Management Support,N/A
10.1515/9783111323749-010,10 The future with AI and AI in action,N/A
10.4018/979-8-3693-1351-0.ch009,Artificial Intelligence (AI) and Cheating,"<jats:p>Generative AI (GenAI) systems pose new challenges in academic dishonesty. Students may be tempted to use GenAI systems to cheat and submit content in assignments and projects that they did not create themselves. This points to the need for schools to focus on strong deterrent measures as well as informative and enhancing practices. Academic dishonesty is not limited to students. The use of GenAI in academia raises two extreme ethical problems. These problems relate to the ethical problems of the end user and the ethical problems in the development of this technology. The ethical use of GenAI technologies should be achieved in a way that respects human rights and takes into account user concerns. This new technology requires a rethinking of teaching methods, as well as assessment and evaluation. It is recommended that policymakers, students, and faculty work collectively and take responsibility for adopting ethical values in the process of integrating GenAI into academia.</jats:p>"
10.1007/s43681-021-00042-7,Privacy and data balkanization: circumventing the barriers,N/A
10.1007/978-981-19-9382-4_3,"AI, Philosophy of Technology, and Ethics",N/A
10.1007/s43681-023-00360-y,A multidimensional approach towards addressing existing and emerging challenges in the use of ChatGPT,N/A
10.1007/s43681-022-00193-1,Why reciprocity prohibits autonomous weapons systems in war,N/A
10.1007/s43681-024-00455-0,A shift towards oration: teaching philosophy in the age of large language models,N/A
10.1007/s43681-021-00041-8,Public acceptance and perception of autonomous vehicles: a comprehensive review,N/A
10.1007/s43681-023-00328-y,Let us make man in our image-a Jewish ethical perspective on creating conscious robots,"<jats:title>Abstract</jats:title><jats:p>The dream of making conscious humanoid robots is one that has long tantalized humanity, yet today it seems closer than ever before. Assuming that science can make it happen, the question becomes: should we make it happen? Is it morally permissible to create synthetic beings with consciousness? While a consequentialist approach may seem logical, attempting to assess the potential positive and negative consequences of such a revolutionary technology is highly speculative and raises more questions than it answers. Accordingly, some turn to ancient and not-so-ancient stories of “automata” for direction. Of the many automata conjured throughout history, if not in matter then in mind, the Golem stands out as one of the most persistent paradigms employed to discuss technology in general and technologically engendered life forms in particular. In this essay, I introduce a novel reading of the Golem paradigm to argue not from consequentialism, but from a deep-seated two-thousand-year-old tradition, the ethical implications of which are wholly deontological.</jats:p>"
10.1007/s43681-021-00109-5,Distributed responsibility in human–machine interactions,"<jats:title>Abstract</jats:title><jats:p>Artificial agents have become increasingly prevalent in human social life. In light of the diversity of new human–machine interactions, we face renewed questions about the distribution of moral responsibility. Besides positions denying the mere possibility of attributing moral responsibility to artificial systems, recent approaches discuss the circumstances under which artificial agents may qualify as moral agents. This paper revisits the discussion of how responsibility might be distributed between artificial agents and human interaction partners (including producers of artificial agents) and raises the question of whether attributions of responsibility should remain entirely on the human side. While acknowledging a crucial difference between living human beings and artificial systems culminating in an asymmetric feature of human–machine interactions, this paper investigates the extent to which artificial agents may reasonably be attributed a share of moral responsibility. To elaborate on criteria that can justify a distribution of responsibility in certain human–machine interactions, the role of types of criteria (interaction-related criteria and criteria that can be deferred from socially constructed responsibility relationships) is examined. Thereby, the focus will lay on the evaluation of potential criteria referring to the fact that artificial agents surpass in some aspects the capacities of humans. This is contrasted with socially constructed responsibility relationships that do not take these criteria into account. In summary, situations are examined in which it seems plausible that moral responsibility can be distributed between artificial and human agents.</jats:p>"
10.1007/s43681-023-00324-2,A context-specific analysis of ethical principles relevant for AI-assisted decision-making in health care,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI)-assisted technologies may exert a profound impact on social structures and practices in care contexts. Our study aimed to complement ethical principles considered relevant for the design of AI-assisted technology in health care with a context-specific conceptualization of the principles from the perspectives of individuals potentially affected by the implementation of AI technologies in nursing care. We conducted scenario-based semistructured interviews focusing on situations involving moral decision-making occurring in everyday nursing practice with nurses (<jats:italic>N</jats:italic> = 15) and care recipients (<jats:italic>N</jats:italic> = 13) working, respectively, living in long-term care facilities in Germany. First, we analyzed participants’ concepts of the ethical principles beneficence, respect for autonomy and justice. Second, we investigated participants’ expectations regarding the actualization of these concepts within the context of AI-assisted decision-making. The results underscore the importance of a context-specific conceptualization of ethical principles for overcoming epistemic uncertainty regarding the risks and opportunities associated with the (non)fulfillment of these ethical principles. Moreover, our findings provide indications regarding which concepts of the investigated ethical principles ought to receive extra attention when designing AI technologies to ensure that these technologies incorporate the moral interests of stakeholders in the care sector.</jats:p>"
10.1007/s43681-022-00242-9,Values in AI: bioethics and the intentions of machines and people,N/A
10.1007/s43681-023-00373-7,Designing value-sensitive AI: a critical review and recommendations for socio-technical design processes,"<jats:title>Abstract</jats:title><jats:p>This paper presents a critical review of how different socio-technical design processes for AI-based systems, from scholarly works and industry, support the creation of value-sensitive AI (VSAI). The review contributes to the emerging field of human-centred AI, and the even more embryonic space of VSAI in four ways: (i) it introduces three criteria for the review of VSAI based on their contribution to design processes’ overall value-sensitivity, and as a response to criticisms that current interventions are lacking in these aspects: comprehensiveness, level of guidance offered, and methodological value-sensitivity, (ii) it provides a novel review of socio-technical design processes for AI-based systems, (iii) it assesses each process based on the mentioned criteria and synthesises the results into broader trends, and (iv) it offers a resulting set of recommendations for the design of VSAI. The objective of the paper is to help creators and followers of design processes—whether scholarly or industry-based—to understand the level of value-sensitivity offered by different socio-technical design processes and act accordingly based on their needs: to adopt or adapt existing processes or to create new ones.</jats:p>"
10.1007/s43681-022-00205-0,"A principled governance for emerging AI regimes: lessons from China, the European Union, and the United States",N/A
10.1145/3278721.3278806,"AI Decisions, Risk, and Ethics",N/A
10.2307/jj.8500773.5,EUROPEAN UNION’S ETHICS GUIDELINES FOR AI,N/A
10.1093/oxfordhb/9780190067397.013.41,Europe,"<p>This chapter assesses Europe’s efforts in developing a full-fledged strategy on the human and ethical implications of artificial intelligence (AI). The strong focus on ethics in the European Union’s AI strategy should be seen in the context of an overall strategy that aims at protecting citizens and civil society from abuses of digital technology but also as part of a competitiveness-oriented strategy aimed at raising the standards for access to Europe’s wealthy Single Market. In this context, one of the most peculiar steps in the European Union’s strategy was the creation of an independent High-Level Expert Group on AI (AI HLEG), accompanied by the launch of an AI Alliance, which quickly attracted several hundred participants. The AI HLEG, a multistakeholder group including fifty-two experts, was tasked with the definition of Ethics Guidelines as well as with the formulation of “Policy and Investment Recommendations.” With the advice of the AI HLEG, the European Commission put forward ethical guidelines for Trustworthy AI—which are now paving the way for a comprehensive, risk-based policy framework.</p>"
10.1007/s43681-024-00558-8,Situating AI in assessment—an exploration of university teachers’ valuing practices,"<jats:title>Abstract</jats:title><jats:p>Emerging AI technologies are changing teachers’ assessment practices and posing higher education institutions with novel ethical dilemmas. While frameworks and guidelines promise to align technology with moral and human values, the dilemma of how AI may impact existing valuing practices is often overlooked. To examine this gap, we conducted an interview study with university teachers from different disciplines at a university in Sweden. Following a semi-structured study design, we explored university teachers’ anticipations of AI in assessment and examined how emerging AI technologies may reconfigure the fit between values, challenges, and activities situated in everyday assessment contexts. Our findings suggest that anticipated AI, including automation and AI-mediated communication and grading, may amplify and reduce teachers’ possibilities to align activities with professional, pedagogical, and relational values and solve current challenges. In light of the study’s findings, the paper discusses potential ethical issues in the anticipated shifts from human to automated assessment and possible new and reinforced challenges brought by AI for education.</jats:p>"
10.1007/s43681-024-00560-0,Ensuring fundamental rights compliance and trustworthiness of law enforcement AI systems: the ALIGNER Fundamental Rights Impact Assessment,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence systems can expand the capabilities and enhance the efficiency of law enforcement agencies preventing, investigating, detecting, and prosecuting criminal offences in the European Union. At the same time, the deployment of artificial intelligence in the security domain often raises numerous legal and ethical concerns. The ALIGNER Fundamental Rights Impact Assessment is an operational tool, rooted in fundamental rights and in the principles of AI ethics, ready to be integrated in the AI governance measures of European law enforcement agencies to inform their decision-making processes and ensure compliance with the recently adopted Artificial Intelligence Act. This paper first introduces the main tensions between law enforcement AI and fundamental rights, as enshrined in the Charter of Fundamental Rights of the European Union; then, it gives an overview of the main developments and best practices in AI governance and their relationship with fundamental rights as well as AI ethics; and finally, it describes the structure of the ALIGNER Fundamental Rights Impact Assessment.</jats:p>"
10.1007/s43681-021-00123-7,The AI ethicist’s dilemma: fighting Big Tech by supporting Big Tech,"<jats:title>Abstract</jats:title><jats:p>Assume that a researcher uncovers a major problem with how social media are currently used. What sort of challenges arise when they must subsequently decide whether or not to use social media to create awareness about this problem? This situation routinely occurs as ethicists navigate choices regarding how to effect change and potentially remedy the problems they uncover. In this article, challenges related to new technologies and what is often referred to as ‘Big Tech’ are emphasized. We present what we refer to as the AI ethicist’s dilemma, which emerges when an AI ethicist has to consider how their own success in communicating an identified problem is associated with a high risk of decreasing the chances of successfully remedying the problem. We examine how the ethicist can resolve the dilemma and arrive at ethically sound paths of action through combining three ethical theories: virtue ethics, deontological ethics and consequentialist ethics. The article concludes that attempting to change the world of Big Tech only using the technologies and tools they provide will at times prove to be counter-productive, and that political and other more disruptive avenues of action should also be seriously considered by ethicists who want to effect long-term change. Both strategies have advantages and disadvantages, and a combination might be desirable to achieve these advantages and mitigate some of the disadvantages discussed.</jats:p>"
10.1007/978-981-99-0707-6_8,Applied Ethics: AI and Ethics,N/A
10.1007/s43681-021-00098-5,Factoring ethics in management algorithms for municipal information-analytical systems,"<jats:title>Abstract</jats:title><jats:p>The discourse on the ethics of artificial intelligence (AI) has generated a plethora of different conventions, principles and guidelines outlining an ethical perspective on the use and research of AI. However, when it comes to breaking down general implications to specific use cases, existent frameworks have been remaining vague. The following paper aims to fill this gap by examining the ethical implications of the use of information analytical systems through a management approach for filtering the content in social media and preventing information thrusts with negative consequences for human beings and public administration. The ethical dimensions of AI technologies are revealed through deduction of general challenges of digital governance to applied level management technics.</jats:p>"
10.54079/jpmi.37.3.3284,"CHATBOTS, GENERATIVE AI, AND SCHOLARLY MANUSCRIPTS WAME RECOMMENDATIONS ON CHATBOTS AND GENERATIVE ARTIFICIAL INTELLIGENCE IN RELATION TO SCHOLARLY PUBLICATIONS",N/A
10.54941/ahfe1005079,From Generative AI to Generative Organizations: A Service Lens on Organizational Learning and Development,"<jats:p>GenAI has conquered the world in flight. Balancing benefits and risks of introducing GenAI the question organizations are asking themselves is not whether to use GenAI, but how: integrate into the ""run"" of existing processes, ""transform"" the processes or ""innovate"" the organization at all (Spohrer, 2021, March, 1991)? On the other hand generative learning in the organizational context is in theory  hardly characterized beyond rudimentary properties (Chiva et al., 2010, Senge, 1997). Our research focuses on building theoretical knowledge and practical implications of ""how GenAI can be used to transform organizations to generative organizations for improving organizational learning and development"". To address the research question we take a service lens and ground our research design on a broad knowledge base of theories, concepts, company practices, and instantiations that address individual parts of our research. The purpose of the paper is to explore how the introduction of GenAI can be used in a ""land and expand"" strategy to develop also other generative capabilities (GenXX). In this way, the study contributes to building knowledge about how organizations can continuously scale up their capacity to (co-) create value to learn, to adapt, and thus to develop.Methodology and Approach Our research design is seen as overall strategy in order to integrate in a logical way the different components of our research for ensuring that the research question will be thoroughly analyzed and investigated (Khanday S., 2019). A conceptual paper as approach and within this a “model” as type of paper is selected for building a conceptual framework that predicts relationships between the properties and the processes in the context of introducing GenAI.  For explaining the properties of the phenomenon, we draw on the domain theories Service-Dominant Logic and Service Science. To demonstrate and study the relationships of the properties Service Dominant Architecture is chosen as method theory (MacInnis, 2011, Jaakkola, 2020, Gilson and Goldberg, 2015).Findings The study contributes to theory building referring to the terms and mechanisms of building generative capabilities and generative learning of organizations. More precisely, this means: The extension of the definition of ""generative"". The narrative and process of learning and building generative capabilities (GenXX) via service for service exchange provided by S-D Logic. The structural perspective on generative learning judged by Service Science as improvement of a service system or the structure of interconnected service (eco) systems. And architecture perspectives as (construction-) plan as medium and output for the intentional building of generative capabilities and fostering learning to improve change by Service Dominant Architecture. In this way, this work also contributes knowledge for practical decisions to foster organizational learning in the sense of ""land and expand"" with the introduction of GenAI.</jats:p>"
10.1177/17470161231220946,AI research ethics is in its infancy: the EU’s AI Act can make it a grown-up,"<jats:p> As the artificial intelligence (AI) ethics field is currently working towards its operationalisation, ethics review as carried out by research ethics committees (RECs) constitutes a powerful, but so far underdeveloped, framework to make AI ethics effective in practice at the research level. This article contributes to the elaboration of research ethics frameworks for research projects developing and/or using AI. It highlights that these frameworks are still in their infancy and in need of a structure and criteria to ensure AI research projects advance in a way that respects norms and principles. This article proposes to draw from the European Union’s AI Act currently in development to shape these frameworks. Although, in the current form of the draft (as of August 2023), the obligations of the AI Act do not apply to scientific research, it is most likely that they will still have a strong impact on AI research considering the need to anticipate market placement or to test new tools in real world conditions. This article investigates what the risk-based approach in the AI Act implies for research ethics and highlights some AI Act obligations of particular value to implement for ethics review processes. </jats:p>"
10.1007/979-8-8688-0318-5_10,After the Brain Rush: What Is Generative AI’s Future?,N/A
10.1007/978-3-031-46238-2_13,"Can Generative Artificial Intelligence Foster Belongingness, Social Support, and Reduce Loneliness? A Conceptual Analysis",N/A
10.1007/978-3-030-51110-4_12,Ethics in AI and Robotics: A Strategic Challenge,N/A
10.4337/9781803926728.00012,AI Ethics and Machine Ethics,N/A
10.1515/9783111425078-005,5 LLM Pretraining Methods,N/A
10.1017/cbo9780511978036.034,What Can AI Do for Ethics?,N/A
10.2307/jj.8500773.11,PRINCIPLES BASED ETHICS AND VIRTUE ETHICS,N/A
10.1093/oxfordhb/9780190067397.013.53,Smart City Ethics,"<p>This chapter explores the concept of “smart cities,” a term which describes the growing role of data analytics and sensors in urban life. Smart city initiatives rely on pervasive data gathering and integration, big data analytics, and artificial intelligence to manage mobility, energy, housing, public realm access, and myriad public and private services. These data flows can change how physical infrastructure like streets and parks are configured and services provisioned. They can tailor opportunities for housing or education based on individual digital identities and predictive algorithms. As more life in the city runs through digital apps and platforms, rights to access and control data increase in importance. Data flows from residents and public spaces to smart city corporations raise pressing policy questions about what power the public should cede to private developers to shape urban space, subject to how much oversight and with what expectation of return on public assets. The chapter then sorts these concerns into three major groups: privatization, platformization, and domination.</p>"
10.19033/sks.2024.6.84.59,"Korean Semantic Education in the AI Era: Focusing on AI Translation, Chatbot Builders, and Generative AI",N/A
10.1007/s00146-024-01917-z,Lessons from the California Gold Rush of 1849: prudence and care before advancing generative AI initiatives within your enterprise,N/A
10.1007/s00146-023-01679-0,How we can create the global agreement on generative AI bias: lessons from climate justice,N/A
10.1007/s43681-024-00419-4,Anthropomorphism in AI: hype and fallacy,"<jats:title>Abstract</jats:title><jats:p>This essay focuses on anthropomorphism as both a form of hype and fallacy. As a form of hype, anthropomorphism is shown to exaggerate AI capabilities and performance by attributing human-like traits to systems that do not possess them. As a fallacy, anthropomorphism is shown to distort moral judgments about AI, such as those concerning its moral character and status, as well as judgments of responsibility and trust. By focusing on these two dimensions of anthropomorphism in AI, the essay highlights negative ethical consequences of the phenomenon in this field.</jats:p>"
10.1016/b978-0-443-18851-0.00009-3,The ethics of online AI-driven agriculture and food systems,N/A
10.1007/s43681-021-00054-3,A framework to contest and justify algorithmic decisions,N/A
10.1145/3362077.3362087,The intersection of ethics and AI,"<jats:p>Artificial intelligence is a rapidly advancing field with the potential to revolutionize health care, transportation, and national security. Although the technology has been ubiquitous in every day society for a while, the advent of self-driving cars and smart home devices have propelled a discussion of the associated ethical risks and responsibilities. Since the usage of AI can have significant impacts on people, it is essential to establish a set of ethical values to follow when developing and deploying AI.</jats:p>"
10.69554/eewn6637,Architecting AI will improve AI ethics,N/A
10.69554/xqrw7390,Enabling generative AI through use cases in a big enterprise,"<jats:p xml:lang=""en"">In the emerging field of generative artificial intelligence (GenAI), we possess the potential to significantly enhance our business operations and processes. Achieving this goal within a large corporation like Nestlé is challenging, however, given the immature stage of this technology. This paper outlines the approach to implementing GenAI at Nestlé, guided by the most influential use cases. It also underscores the importance of scaling people’s capabilities and establishing legal, ethical and compliance frameworks to support the deployment of this technology.</jats:p>"
10.1007/s00146-024-02007-w,Intentionality gap and preter-intentionality in generative artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>The emergence of generative artificial intelligence, such as large language models and text-to-image models, has had a profound impact on society. The ability of these systems to simulate human capabilities such as text writing and image creation is radically redefining a wide range of practices, from artistic production to education. While there is no doubt that these innovations are beneficial to our lives, the pervasiveness of these technologies should not be underestimated, and raising increasingly pressing ethical questions that require a radical resemantization of certain notions traditionally ascribed to humans alone. Among these notions, that of technological intentionality plays a central role. With regard to this notion, this paper first aims to highlight what we propose to define in terms of the intentionality gap, whereby, insofar as, currently, (1) it is increasingly difficult to assign responsibility for the actions performed by AI systems to humans, as these systems are increasingly autonomous, and (2) it is increasingly complex to reconstruct the reasoning behind the results they produce as we move away from good old fashioned AI; it is now even more difficult to trace the intentionality of AI systems back to the intentions of the developers and end users. This gap between human and technological intentionality requires a revision of the concept of intentionality; to this end, we propose here to assign preter-intentional behavior to generative AI. We use this term to highlight how AI intentionality both incorporates and transcends human intentionality; i.e., it goes beyond (preter) human intentionality while being linked to it. To show the merits of this notion, we first rule out the possibility that such preter-intentionality is merely an unintended consequence and then explore its nature by comparing it with some paradigmatic notions of technological intentionality present in the wider debate on the moral (and technological) status of AI.</jats:p>"
10.3389/frai.2023.1259407,The rise of generative AI and enculturating AI writing in postsecondary education,N/A
10.1007/s00146-020-01077-w,AI Ethics: how can information ethics provide a framework to avoid usual conceptual pitfalls? An Overview,N/A
10.1007/s43681-024-00461-2,The mechanisms of AI hype and its planetary and social costs,"<jats:title>Abstract</jats:title><jats:p>Our global landscape of emerging technologies is increasingly affected by artificial intelligence (AI) hype, a phenomenon with significant large-scale consequences for the global AI narratives being created today. This paper aims to dissect the phenomenon of AI hype in light of its core mechanisms, drawing comparisons between the current wave and historical episodes of AI hype, concluding that the current hype is historically unmatched in terms of magnitude, scale and planetary and social costs. We identify and discuss socio-technical mechanisms fueling AI hype, including anthropomorphism, the proliferation of self-proclaimed AI “experts”, the geopolitical and private sector “fear of missing out” trends and the overuse and misappropriation of the term “AI” in emerging technologies. The second part of the paper seeks to highlight the often-overlooked costs of the current AI hype. We examine its planetary costs as the AI hype exerts tremendous pressure on finite resources and energy consumption. Additionally, we focus on the connection between AI hype and socio-economic injustices, including perpetuation of social inequalities by the huge associated redistribution of wealth and costs to human intelligence. In the conclusion, we offer insights into the implications for how to mitigate AI hype moving forward. We give recommendations of how developers, regulators, deployers and the public can navigate the relationship between AI hype, innovation, investment and scientific exploration, while addressing critical societal and environmental challenges.</jats:p>"
10.1007/s43681-024-00464-z,Talking existential risk into being: a Habermasian critical discourse perspective to AI hype,"<jats:title>Abstract</jats:title><jats:p>Recent developments in Artificial Intelligence (AI) have resulted in a hype around both opportunities and risks of these technologies. In this discussion, one argument in particular has gained increasing visibility and influence in various forums and positions of power, ranging from public to private sector organisations. It suggests that Artificial General Intelligence (AGI) that surpasses human intelligence is possible, if not inevitable, and which can—if not controlled—lead to human extinction (Existential Threat Argument, ETA). Using Jürgen Habermas’s theory of communicative action and the validity claims of truth, truthfulness and rightness therein, we inspect the validity of this argument and its following ethical and societal implications. Our analysis shows that the ETA is problematic in terms of scientific validity, truthfulness, as well as normative validity. This risks directing AI development towards a strategic game driven by economic interests of the few rather than ethical AI that is good for all.</jats:p>"
10.1007/s43681-022-00196-y,“Ethically contentious aspects of artificial intelligence surveillance: a social science perspective”,N/A
10.1007/s43681-023-00338-w,"The role of ChatGPT in disrupting concepts, changing values, and challenging ethical norms: a qualitative study",N/A
10.3030/101145339,N/A,N/A
10.51219/urforum.2023.sebastian-obeta,Beyond Creation: Unravelling the Complex Tapestry of Generative AI in the Big Data Landscape,N/A
10.2139/ssrn.4914938,Towards an Independent EU Regulator for Copyright Issues of Generative AI: What Role for the AI Office (But More Importantly: What's Next)?,N/A
10.1007/979-8-8688-0456-4_6,The Future of AGI,N/A
10.1109/itc-egypt61547.2024.10620598,AI-Driven Testing: Unleashing Autonomous Systems for Superior Software Quality Using Generative AI,N/A
10.2139/ssrn.4918354,Teaching Mathematical Concepts in Management with Generative AI: The Power of Human Oversight in AI-Driven Learning,N/A
10.1007/s43681-023-00397-z,AITA: AI trustworthiness assessment,N/A
10.1007/s43681-023-00404-3,Establishing counterpoints in the sonic framing of AI narratives,"<jats:title>Abstract</jats:title><jats:p>In order to challenge dominant representations and conceptions of artificial intelligence (AI), this article explores how AI is sonically represented in documentaries. Using a corpus of documentaries alongside expert interviews with sound designers, we explore the ways in which music and sound may influence perception about AI. The notion of ‘counterpoint’ in music theory is developed as a concept to capture and explain how the integrated dynamics of human/machines are represented within these sonic framings. The concept of the counterpoint allows us to reflect on how the relations between AI and the human and how they are sonically framed in ways that separate and blend without recourse to reductive or binary futures, which potentially misrepresent AI capabilities and performance. The article identifies and develops four types of counterpoint in what we refer to as AI sonic narratives. This article provides a framework from which AI could be sonically framed responsibly, which is critical when misinformation and hype impede the public understanding of science.</jats:p>"
10.1007/s43681-024-00433-6,Unveiling the ethical positions of conversational AIs: a study on OpenAI’s ChatGPT and Google’s Bard,"<jats:title>Abstract</jats:title><jats:p>In an era where conversational AIs (CAIs) like OpenAI’s ChatGPT and Google's Bard are becoming integral to daily life, understanding their ethical positions is paramount. This research delves into the expressed moral values of these CAIs, exploring how their pre-training influences their ethical stances. The study aims to assess the articulated ethical positions of ChatGPT and Bard, uncovering whether these systems align with particular moral values. By understanding their ethical positions, the research seeks to provide insights into how these CAIs might respond to prompts and guide users in their selection and utilization. Utilizing O’Boyle and Forsyth’s Ethical Position Questionnaire (EPQ-5), the research evaluated the CAIs’ levels of idealism and relativism. The study also involved a third CAI, Anthropic’s Claude and an online human panel, to analyze the reasoning behind the responses, providing a more nuanced understanding of the ethical positions. The initial findings revealed that ChatGPT aligns more with an ‘absolutist’ position, endorsing strict adherence to moral principles, while Bard leans towards a ‘situationist’ stance, valuing flexibility and situational considerations. However, further analysis by Claude and humans suggested a more complex categorization, with ChatGPT fitting the 'exceptionist' categorization and Bard aligning with ‘absolutism.’ The research underscores the significance of recognizing the trained-in ethical positions of CAIs, as they are not neutral but reflect particular ethical leanings. Understanding these positions is vital for interpreting CAI outputs and using these systems effectively and ethically. The study calls for further exploration into how these ethical positions might influence real-world applications of CAIs.</jats:p>"
10.14296/ac.v5i1.5663,Copyright Protection for AI-Generated Works,"<jats:p>Since the 2010s, artificial intelligence (AI) has quickly grown from another subset of machine learning (ie deep learning) in particular with recent advances in generative AI, such as ChatGPT. The use of generative AI has gone beyond leisure purposes. It has now been widely used to generate music, news articles and image-based art works. This prompts a regulatory interpretation as to how AI-generated works should be appropriately used to eliminate their potential harm to society, but at the same time how it should be protected to foster human creativity and promote a well-functioning market.
This article is an update from the author’s evidential report and speech on “AI and Intellectual Property Rights: IPR Protection for AI-Created Work” for the evidence meeting of the All-Party Parliamentary Group on Artificial Intelligence on 24 January 2022. It considers whether AI technologies should be granted status as copyright or patent owners by looking into existing regulations in the United Kingdom, European Union, United States and China. It further considers how generative AI copyright protection should be managed in the digital society to protect users and strike a fair balance among rightsholders. It argues that it would be beneficial to a well-functioning market if AI-generated works could be subject to collective management of copyright via copyright management organizations within countries. In addition, the article provides mapping of existing legislations in a comparative study and their interpretation for the application of AI-generated works protection and aims to bring together global policymakers and stakeholders to initiate joint efforts to promote international harmonization on intellectual property rights (IPR) protection for AI-generated works.
Keywords: artificial intelligence; generative AI; AI-generated works; collective copyright management; computer-generated work; copyright protection.</jats:p>"
10.1101/2023.05.12.23289878,Dissection of medical AI reasoning processes via physician and generative-AI collaboration,"<jats:title>Abstract</jats:title><jats:p>Despite the proliferation and clinical deployment of artificial intelligence (AI)-based medical software devices, most remain black boxes that are uninterpretable to key stakeholders including patients, physicians, and even the developers of the devices. Here, we present a general model auditing framework that combines insights from medical experts with a highly expressive form of explainable AI that leverages generative models, to understand the reasoning processes of AI devices. We then apply this framework to generate the first thorough, medically interpretable picture of the reasoning processes of machine-learning–based medical image AI. In our synergistic framework, a generative model first renders “counterfactual” medical images, which in essence visually represent the reasoning process of a medical AI device, and then physicians translate these counterfactual images to medically meaningful features. As our use case, we audit five high-profile AI devices in dermatology, an area of particular interest since dermatology AI devices are beginning to achieve deployment globally. We reveal how dermatology AI devices rely both on features used by human dermatologists, such as lesional pigmentation patterns, as well as multiple, previously unreported, potentially undesirable features, such as background skin texture and image color balance. Our study also sets a precedent for the rigorous application of explainable AI to understand AI in any specialized domain and provides a means for practitioners, clinicians, and regulators to uncloak AI’s powerful but previously enigmatic reasoning processes in a medically understandable way.</jats:p>"
10.1007/s43681-022-00210-3,Artificial intelligence and the model of rules: better than us?,N/A
10.1007/s43681-024-00511-9,Robots and reactive attitudes: a defense of the moral and interpersonal status of non-conscious agents,N/A
10.3389/fcomm.2023.1243474,Front-end AI vs. Back-end AI: new framework for securing truth in communication during the generative AI era,"<jats:p>The proliferation of artificial intelligence (AI) in digital platforms has complicated the concept of truth in communication studies. The article presents the dichotomic framework of Front-end AI and Back-end AI to tackle the complexity of distinguishing truth. Front-end AI refers to AI technology used up-front, often as the face of a product or service, challenging the authenticity and truthfulness of content. In contrast, Back-end AI refers to AI technology used behind the scenes, which can generate misleading or biased content without disclosing its AI-generated nature. Addressing these challenges requires different approaches, such as verification and ethical guidelines for Front-end AI and algorithmic transparency, bias detection, and human oversight for Back-end AI.</jats:p>"
10.1007/s43681-021-00046-3,Correction to: AI auditing and impact assessment: according to the UK information commissioner’s office,N/A
10.1007/s43681-024-00500-y,How to gain control and influence algorithms: contesting AI to find relevant reasons,"<jats:title>Abstract</jats:title><jats:p>Relevancy is a prevalent term in value alignment. We either need to keep track of the relevant moral reasons, we need to embed the relevant values, or we need to learn from the relevant behaviour. What relevancy entails in particular cases, however, is often ill-defined. The reasons for this are obvious, it is hard to define relevancy in a way that is both general and concrete enough to give direction towards a specific implementation. In this paper, we describe the inherent difficulty that comes along with defining what is relevant to a particular situation. Simply due to design and the way an AI system functions, we need to state or learn particular goals and circumstances under which that goal is completed. However, because of both the changing nature of the world and the varied wielders and users of such implements, misalignment occurs, especially after a longer amount of time. We propose a way to counteract this by putting contestability front and centre throughout the lifecycle of an AI system, as it can provide insight into what is actually relevant at a particular instance. This allows designers to update the applications in such a manner that they can account for oversight during design.</jats:p>"
10.1007/s43681-021-00121-9,On the risk of confusing interpretability with explicability,"<jats:title>Abstract</jats:title><jats:p>﻿This Comment explores the implications of a lack of tools that facilitate an explicable utilization of epistemologically richer, but also more involved white-box approaches in AI. In contrast, advances in explainable artificial intelligence for black-box approaches have led to the availability of semi-standardized and attractive toolchains that offer a seemingly competitive edge over inherently interpretable white-box models in terms of intelligibility towards users. Consequently, there is a need for research on efficient tools for rendering interpretable white-box approaches in AI explicable to facilitate responsible use.</jats:p>"
10.1007/s43681-023-00281-w,Merging public health and automated approaches to address online hate speech,N/A
10.5334/tilr.297,"Learning from the Ethics of AI &amp;ndash; A Research Proposal on Soft Law and
            Ethics of AI","<jats:p>This contribution outlines a research proposal combining ethical guidelines on AI
            and a law-as-data approach. Building upon the definitions of soft law discussed in legal
            scholarship, it proposes a way of structuring the regulatory landscape on AI and of
            addressing the question of what is included in the “soft law of AI” today. By adopting a
            building-blocks approach (combining distinct definitional components of soft law), the
            paper shows that the state of current soft law on AI depends on which position on
            international law one defends. Concretely, the paper firstly offers a complete codebook
            for identifying the different types of soft law. Secondly, it applies this codebook as a
            proof-of-concept for the research proposal by analyzing 40+ ethical guidelines and by
            clustering preliminary results according to the actor enacting the guidelines and the
            legally relevant effects they could deploy. Four paradigmatic types of soft law emerge:
            statist and international organization soft law, process-oriented soft law,
            expertise-oriented soft law, and de facto relevant standards soft law. These results
            illustrate the contributions which are to be expected from a law-as-data research
            proposal.</jats:p>"
10.1007/s43681-022-00198-w,Being at home in the metaverse? Prospectus for a social imaginary,N/A
10.1007/s43681-024-00434-5,Prospectives and drawbacks of ChatGPT in healthcare and clinical medicine,N/A
10.1007/s43681-021-00072-1,The future of online trust (and why Deepfake is advancing it),N/A
10.24135/pjtel.v6i1.176,AI in the wild,"<jats:p>It has been well over a year since ChatGPT emerged and brought with it much commentary about challenges and opportunities for education. There has been considerable discussion about risks to academic integrity and the possibilities of generative AI for enhancing learning and teaching. As the dust settles, the hard work of determining how exactly generative AI will integrate into higher education begins. In this session, we will explore the current state of generative AI in student learning. While the integration of generative AI into formal coursework has been inconsistent, to say the least, many students are using these tools extensively as part of their studies. Drawing on in-depth interviews with 50 students across disciplines, a set of hypotheses about the impact of generative AI on student learning practices will be presented. A key component of the impact of these emerging technologies appears to be how familiar and confident students are in their understanding of their own learning. The implications of these findings will also be discussed.Jason Lodge is Associate Professor of Educational Psychology and Director of the Learning, Instruction, and Technology Lab in the School of Education and is a Deputy Associate Dean (Academic) in the Faculty of Humanities, Arts and Social Sciences at The University of Queensland. Jason’s research with his lab focuses on the cognitive, metacognitive, and emotional mechanisms of learning, primarily in post-secondary settings and in digital learning environments. He currently serves as Lead Editor of Australasian Journal of Educational Technology and Editor of Student Success.</jats:p>"
10.1007/978-3-031-23035-6_7,AI Ethics Education for Future African Leaders,"<jats:title>Abstract</jats:title><jats:p>From the Greek word “ethos”, which means custom, habit or character, the word ethics can mean and has been defined in many different ways by ethics and morality theorists.</jats:p>"
10.1007/s43681-024-00532-4,Evaluating computational models of ethics for autonomous decision making,N/A
10.21203/rs.3.rs-4515034/v1,Generative AI: A Case Study of ChatGPT’s Impact on University Students’ Learning Practices,"<title>Abstract</title>
        <p>Recently, technology has been widely integrated across the educational landscape. Artificial Intelligence (AI) tools have become essential components of students' learning practices, requiring an examination of the impact of each tool. The aim of this study is to investigate the impact of ChatGPT tool on university students’ learning practices. A quantitative online survey was adopted using cross-sectional design to collect the data from university students at King Saud university and Imam Abdulrahman Bin Faisal university in Saudi Arabia. A total of 402 responses were finalised for data analysis at the end of five weeks after starting the survey. Out of 402 students, 293 have been using ChatGPT. ChatGPT services were mainly used for writing research papers (81.8%), essays (73.8%), and correcting grammar (43.3%). Positive effects included motivating, engaging, and improving skills and competencies of students and negative effects included academic dishonesty, limiting critical thinking and problem-solving skills among students. Significant differences were identified among male and female students on perceptions about motivating and engaging ability of ChatGPT (p &lt; .05).Students should be trained to use ChatGPT ethically and universities should adopt alternative assessment practices.</p>"
10.1515/9783839474723-fm,Frontmatter,N/A
10.2139/ssrn.4769448,Rule 11 Is No Match for Generative AI,N/A
10.36591/se-d-4602-13,Generative AI: The Promise and Peril for Scientific Publishing,N/A
10.2139/ssrn.4915317,Does Generative Ai Copy?——Rethinking the Right to Copy Under Copyright Law,N/A
10.2139/ssrn.4616977,The Market Value of Generative Ai: Evidence from China Market,N/A
10.18260/1-2--45532,Generative AI as an educational resource,N/A
10.1016/b978-0-44-321857-6.00011-4,"Synthetic data, interpretable regression, and submodels",N/A
10.2196/52615,Generating Synthetic Electronic Health Record Data Using Generative Adversarial Networks: Tutorial,"<jats:p>Synthetic electronic health record (EHR) data generation has been increasingly recognized as an important solution to expand the accessibility and maximize the value of private health data on a large scale. Recent advances in machine learning have facilitated more accurate modeling for complex and high-dimensional data, thereby greatly enhancing the data quality of synthetic EHR data. Among various approaches, generative adversarial networks (GANs) have become the main technical path in the literature due to their ability to capture the statistical characteristics of real data. However, there is a scarcity of detailed guidance within the domain regarding the development procedures of synthetic EHR data. The objective of this tutorial is to present a transparent and reproducible process for generating structured synthetic EHR data using a publicly accessible EHR data set as an example. We cover the topics of GAN architecture, EHR data types and representation, data preprocessing, GAN training, synthetic data generation and postprocessing, and data quality evaluation. We conclude this tutorial by discussing multiple important issues and future opportunities in this domain. The source code of the entire process has been made publicly available.</jats:p>"
10.1007/s43681-024-00532-4,Evaluating computational models of ethics for autonomous decision making,N/A
10.55248/gengpi.5.0224.0616,Service Design Trend Predictions 2024: Implications for AI and Generative AI-Based Services in Nature Labs,N/A
10.2139/ssrn.4800215,Ethical Auditors’ Framework for Generative Ai Cybersecurity,N/A
10.2139/ssrn.4874356,Interviewing GPTs: Can generative AI intelligence become a new source of qualitative research data?,N/A
10.59350/d6m23-vk771,JSTOR generative AI pilot - Or is Semantic Search coming for academic databases?,"<p>A decade ago in 2012, I observed how the dominance of Google had slowly affected how Academic databases and OPACs/ catalogues (now discovery services) work.</p>"
10.1201/9781003005629-7,Generative Adversarial Network,N/A
10.2139/ssrn.4723873,The Market Value of Generative Ai: Evidence from China Market,N/A
10.36227/techrxiv.21789434,Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education,"<jats:p>&lt;p&gt;Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.&lt;/p&gt;</jats:p>"
10.1016/b978-0-44-321857-6.00017-5,Divergent optimization algorithm and synthetic functions,N/A
10.1038/s43018-023-00531-0,Not a generative AI–generated Editorial,N/A
10.32388/9y5tht,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/e991bt,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/aduw4x,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.21203/rs.3.rs-4433522/v1,How Learners’ Trust Changes in Generative AI Over a Semester of Undergraduate Courses,"<title>Abstract</title>
        <p>Artificial intelligence (AI) applications, including advanced machine learning (ML), have received attention in education, and generative AI-powered chatbots like ChatGPT and Copilot have been adopted in diverse educational settings worldwide. However, the actual use of and perception regarding AI-powered chatbots by learners have been under-investigated. Obtaining a more accurate understanding of learners’ perceptions, particularly their trust in AI-powered technology, is crucial for preparing for future education because learners’ trust in the technology itself is significantly related to successful technology adoption in various educational contexts. To address this issue, we focused on undergraduate students’ trust in AI-powered chatbots within their courses and investigated the relationship between their trust levels and learning performance. Additionally, we explored the potential association between learners’ trust levels, self-regulated learning, and computational thinking skills. This research employed an exploratory study design with a regular course setting, and there were no experimental treatments involved. In the results, we found that learners’ trust levels were not correlated with their computational thinking, self-regulated learning, or learning performance. Furthermore, these constructs (i.e., self-regulated learning, computational thinking, and learning performance) did not significantly predict learners’ trust in AI. However, there was a notable difference between high and low performers concerning changes in trust over time. Trust levels among low performers exhibited a significant change over the semester, whereas those of high performers remained relatively stable. The study suggests that expectations regarding trusting generative AI technology can be influenced by trusting intention through performance.</p>"
10.1515/9783111323749-021,Index,N/A
10.1515/9783111323749-fm,Frontmatter,N/A
10.2139/ssrn.4531912,Copyright Implications of the Use of Generative AI,N/A
10.2139/ssrn.4407587,The Case for Generative AI in Scholarly Practice,N/A
10.35542/osf.io/56z4t,The Role of Generative AI in Promoting Practical-Based Teaching and Assessment in Ghana,"<p>This analytical review undertakes an extensive exploration of Ghana's educational reforms, the prevailing domains in the nation's educational system, and the key competencies of the newly introduced standard-based curriculum. The analysis provides valuable insights into two main research questions: Firstly, it identifies the consistent educational issues in Ghana that necessitate the introduction of various educational reforms. Secondly, the review hypothesizes on how generative AI integration could potentially transform the Ghanaian educational landscape. The study underlined the educational system in Ghana as imbalanced, emphasizing on cognitive domain development, often neglecting the equally crucial affective and psychomotor domains. This tendency towards one-dimensional teaching and assessment is evident in the country's recurring educational reforms, each seeking to address the persistent deficit in skill and attitudinal development. Furthermore, it opens an innovative discourse on the potential of generative AI (GAI) integration as a revolutionary measure within the Ghanaian educational system. When used effectively, GAI can greatly enhance students' cognitive skill development, acting as a beneficial supplement to their learning processes. This won't only bolster student competencies in these areas, but it will also afford educators the opportunity to shift their attention to other critical aspects of learning that are currently underserved in Ghana education such as practice-based, performance-based, and skills-based learning and alternative assessment. Consequently, GAI may serve as a catalyst for reducing unemployment and promoting youth and general citizen employment. Admittedly, the incorporating GAI into Ghana's educational system necessitates careful planning and consideration of several factors.</p>"
10.21428/e4baedd9.f189351f,Generative AI as a New Platform for Applications Development,N/A
10.1016/b978-0-44-321857-6.00005-9,Machine learning cloud regression and optimization,N/A
10.32388/p6vv1c,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/avwsmy,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/p1urtl,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/jueefv,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/c67ewj,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.1101/2024.06.03.597089,Generative AI impact on protein stability prediction in breast cancer genes,"<jats:title>Abstract</jats:title><jats:p>The functional classification of a missense variant in cancer predisposition genes is often challenging due to how rare the variant is observed in the population. When available, clinicians utilize a combination of family history,<jats:italic>in vitro</jats:italic>functional assays and<jats:italic>in silico</jats:italic>methods to infer protein function.<jats:italic>In silico</jats:italic>methods, such as missense predictors (predict changes in protein function) and protein stability predictors (predict changes in free energy) have been used to help classify a missense variant in accordance with the American College of Medical Genetics and Genomics (ACMG) guideline. To measure protein stability, many<jats:italic>in silico</jats:italic>algorithms predict stability based on the change of free energy and most accurate protein stability predictors require a wild-type protein template. In this study, we examine the use of generative AI to predict high-resolution protein structures as templates analyzed with protein stability methods to evaluate loss of function (LOF) activity in cancer predisposition genes<jats:italic>BRCA1, BRCA2, PALB2</jats:italic>and<jats:italic>RAD51C</jats:italic>upon the presence of missense variant. Utilizing multiplexed assay of variant effect measurements and variant classifications from ClinVar, we find that prediction of Gibbs free energy (ΔΔG) from AlphaFold2 (AF2) structures analyzed with FoldX predicts LOF better than experimental-derived wild type structures in the BRCT domain of<jats:italic>BRCA1</jats:italic>and the DNA binding domain (DBD) of<jats:italic>BRCA2</jats:italic>, but not in<jats:italic>PALB2</jats:italic>and<jats:italic>RAD51C</jats:italic>. We also find that AF2 structures in the BRCT domain of BRCA1 and DBD-Dss1 domain of BRCA2 analyzed with FoldX measure homologous DNA recombination (HDR) activity significantly better than Rosetta and DDGun3D. Our study also revealed that there are other factors that contribute to predicting loss of function activity other than protein stability, with AlphaMissense ranking the best overall predictor of LOF activity in these tumor suppressor breast cancer genes.</jats:p><jats:sec><jats:title>Author Summary</jats:title><jats:p>The stability of a protein, often expressed in terms of Gibbs free energy (ΔΔG), is a critical factor in predicting loss of function (LOF) activity when a missense variant is present. The effect is higher in haploinsufficient genes like the tumor suppressor genes<jats:italic>BRCA1, BRCA2, PALB2</jats:italic>and<jats:italic>RAD51C</jats:italic>. Protein stability predictors that utilizes a wild-type structure to make its predictions is often limited by the availability of experimentally-derived protein structures. Here, in our study we show that generative AI, like AlphaFold2 (AF2) can predict structures similar to experimentally-derived structures with high similarity. Furthermore, protein stability tools such as FoldX, Rosetta, and DDGun3D can be used in conjunction to measure changes in stability. From our study, we find that complex AF2 structures representing the BRCT domain of<jats:italic>BRCA1</jats:italic>and DBD domain of<jats:italic>BRCA2</jats:italic>analyzed by FoldX predicts function significantly better than the experimentally-derived structures. However, predicted |ΔΔG| does not predict function better than purpose-built<jats:italic>in silico</jats:italic>missense predictors for protein function. Overall, we find the AlphaMissense is the best predictor to predict function in these tumor suppressor breast cancer genes.</jats:p></jats:sec>"
10.2139/ssrn.4832872,"Elements of Style: Copyright, Similarity, and Generative AI",N/A
10.2139/ssrn.4886146,Employers' Expectations of Students' Generative AI Skills: A Student Perspective,N/A
10.2139/ssrn.4872189,Does Generative AI Facilitate Investor Trading? Evidence from ChatGPT Outages,N/A
10.1515/9781501520143-002,Chapter 1: Generative AI and Gemini,N/A
10.15291/pubmet.4274,Regulating the use of generative AI in academic research and publications,"<jats:p>Generative artificial intelligence (GenAI) is a category of AI technology capable of producing various types of content, including text, images, audio, video, 3D models, simulations and synthetic data. Although it has been present for some time, it has been popularised in the recent months due to text and image GenAI tools, such as ChatGPT, Google Bard, LaMDA, BlenderBot, DALL-E, Midjourney, Stable Diffusion, some of which have already received new and upgraded versions.&#x0D;
The main issue for the scholarly research and publications relates to the fact that because of the technology breakthrough the AI tools, based on machine learning models and usually fed with large volumes of data, no longer only assist researchers in recognising patterns and predicting, but also in generating content. This raises questions such as: On a general level, is it acceptable to use the generated content in academic publications? Does the use of such tools in research and publications violate academic honesty? Is a researcher violating another person’s intellectual property right when using these tools?&#x0D;
This paper seeks to answer these questions with the aim of suggesting whether and to what extent there is a need for GenAI to be regulated within the academic institutions or beyond. Additionaly, the paper is aimed at investigating the models for such regulation as there are already some attempts to do so at various academic institutions in the world and many such processes are ongoing.</jats:p>"
10.1016/b978-0-44-321857-6.00009-6,Synthetic clusters and alternative to GMM,N/A
10.32388/09dv4l,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/7ovvw2,Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations,"<jats:p>The integration of generative AI into academic research holds immense promise but necessitates judicious oversight to address risks. This pioneering study provides crucial insights to guide responsible adoption through a rigorous comparative benchmarking of four cutting-edge models – Claude, LaMDA, Sydney and Galactica. Carefully designed prompts assess competencies across core scholarly tasks, with quantitative scoring and qualitative analysis elucidating specialized capabilities, gaps, risks and validation needs. Key findings reveal strengths in focused assistive roles but limitations in generalizing reasoning across disciplines compared to human scholars. The AI systems emphasize extensive validation to mitigate risks, underscoring the need for transparency, peer review, reproducibility checks and continuous benchmarking as adoption accelerates. To steer progress responsibly, tailored recommendations for pragmatic system-task alignment, calibrating expectations, enhancing reasoning skills, holistic risk mitigation and participatory oversight are provided to researchers, developers, institutions and publishers. This timely applied framework grounded in real-world evidence provides a roadmap to harness AI’s immense opportunities to benefit scholarship through prudent integration focused on human-AI collaboration under an ethical oversight framework.
</jats:p>"
10.36227/techrxiv.171992907.72903277/v1,Pioneering the Future: Generative AI and Digital Twin Integration in 6G Networks,N/A
10.2139/ssrn.4851632,LEVERAGING GENERATIVE AI: STRATEGIC ADOPTION PATTERNS FOR ENTERPRISES,N/A
10.2139/ssrn.4587097,Entrepreneurship Teaching Exercises: integrating generative AI,N/A
10.4018/979-8-3693-2418-9.ch012,Revolutionizing Creative Education,"<jats:p>The emergence of GenAI tools, such as OpenAI's ChatGPT, Google's Bard, and Meta's Llama, has marked the beginning of a new age in educational approaches, specifically in fostering creativity in higher education. This chapter examines the diverse effects of GenAI on promoting and assessing creativity among students and instructors. This chapter examines the capacity of these technologies to improve creative thinking, problem-solving, and personalised learning experiences. It also considers the ethical, privacy, and bias-related problems that come with using these technologies. This chapter seeks to offer valuable insights on the effective integration of GenAI into higher education curricula, with the goal of fostering the development of innovative thinkers. This was achieved by a review of current literature. The chapter proposes a strategy that harnesses the advantages of AI while reducing its potential drawbacks, therefore creating a future where technology and human ingenuity combine to promote progress and development in academics.</jats:p>"
10.70179/grdjev09i100011,Exploring Generative Adversarial Networks: Applications of GANs in AI and Big Data,"<jats:p>This chapter explores generative adversarial networks (GANs), which simultaneously train two models: a generator (G) to generate new instances that resemble a training data set; and a discriminator (D) that estimates the probability that an instance was produced by the generator rather than belonging to the training data. The capabilities of GANs are explored, thrusting GANs into the forefront of AI research and development. Preparation of the data input to the generative adversarial network occurs before the application of the discriminator to the multilayer perceptron neural network designed to solve an example classification problem. Then, utilizing GANs to facilitate big data implementation is suggested, simultaneously imputing the missing data and selecting a smaller more effective training set for an example classification problem. AI problems can be defined as realizations of various data models using a multilayer perceptron neural network, which are here illustrated and employed to solve the ""and"" (AND) function approximation, a famous AI hard problem. Also defined and described. Note that multi-GANs are employed to select an overall effective representative training data set for classification problems, facilitate big data, and predict missing data values during the preparation of the training set. This work differs from the recent efforts attempting to improve GANs. A hyperparameter tuning methodology. Furthermore, this paper couples the imputation of missing no-label binary data with data in the training set used as an input to a multilayer perceptron neural network, which has typically shown an improved estimate of the models needed for binary and optimization results when samples are selected to represent the ""and"" (AND) function classification problem training process for mid- and large-scale problems.</jats:p>"
10.4018/979-8-3693-2440-0.ch026,Bridging Data-Driven Learning and Generative AI,"<jats:p>This chapter presents a framework to enhance sustainable education by bridging data-driven learning and generative artificial intelligence (AI) with metacognitive resource utilization. The framework offers educators a structured approach to integrate innovative pedagogical strategies grounded in the intersection of educational theory and emerging technologies. Through a comprehensive review of literature, the study identifies gaps and theoretical frameworks relevant to sustainable education. The proposed framework emphasizes the importance of promoting metacognitive skills alongside sustainability principles, empowering students to become active agents in addressing global challenges. However, challenges such as inequalities in access to resources and the need for comprehensive educator support are acknowledged. Although these challenges persist, the framework marks a notable advancement in nurturing critical thinking, problem-solving abilities, and conscientious citizenship among students. Further research and fine-tuning are imperative to guarantee its efficacy and accessibility across varied educational environments.</jats:p>"
10.4018/979-8-3693-2440-0.ch017,"Role of Generative Artificial Intelligence (AI) in Accountability, Transparency, and Governance in Higher Education","<jats:p>This study investigates the impact of Generative Artificial Intelligence (AI) on accountability, transparency, and governance within higher education institutions. Employing a quantitative technique, a structured questionnaire was administered to a convenient sample of 190 academicians, collected via Google Forms from October to December 2023. Utilizing data filtering techniques, hypotheses testing was conducted using Smart-PLS, with both dependent and independent variables identified and analyzed. The findings reveal significant positive relationships between Accountability, Transparency, Governance, and Generative AI, supporting the study's hypotheses. The study underscores the transformative potential of AI integration in higher education, offering policy implications for technologists to implement responsible AI practices and maximize its benefits in governance, transparency, and accountability realms.</jats:p>"
10.2139/ssrn.4652314,"Generative AI and Copyright: Exception, Compensation or Both?",N/A
10.2139/ssrn.4628235,Generative AI in Finance: Risks and Potential Solutions,N/A
10.32388/8vzbd0,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.56329/1810-7087.24.1.03,The Inventorship Paradox within Generative AI,"<jats:p>The advancement of AI, especially generative AI, has brought the inventor designation debate back to the forefront. Can an AI system be considered an inventor when it does not have legal personality? This paradox, highlighted by the DABUS case, requires the exploration of both practical and legal as-pects. Even though the EPO considers AI-generated inventions to be “computer-implemented inven-tions”, the key question that more and more people are asking is this: Can AI really invent something new independently, or is it just a tool for human inventors? Most countries, including those in the EU, maintain  that  only  humans  can  be  inventors  and  therefore  patents  cannot  be  granted  to  machines, regardless of their capabilities, thus supporting the European human-centred approach. To elucidate this complex issue, we aim to analyze cases like DABUS, taking into account the advances in AI and the position of experts. This will help identify challenges and potentially redefine the legal landscape for AI-generated inventions in the future.</jats:p>"
10.48009/3_iis_2023_119,ADAPTIVE PROGRAMMING LANGUAGE LEARNING SYSTEM BASED ON GENERATIVE AI,N/A
10.31235/osf.io/gkc8w,The Augmented Qualitative Researcher: Using Generative AI in Qualitative Text Analysis,"<p>Large language models (LLMs) such as ChatGPT are fueling academic discussions about generative artificial intelligence's role in research. In this article, we view LLMs as technological tools to support qualitative research and we aim to contribute to the long-standing history of articles that provide qualitative researchers with recommendations that can help them incorporate technological developments in their research practice while maintaining the interpretive, hands-on, and human-led approach that characterizes qualitative research. We present two approaches to using ChatGPT and integrating it into the qualitative research pipeline. We outline four main benefits of incorporating generative AI in qualitative work: 1) streamlined and expedited research; 2) enhanced transparency in researchers' ideas and clearer concept and category development; 3) identification of additional categories and themes beyond researchers' findings; and 4) increased reliability as generative AI support or challenge researchers' coding and analysis. We offer a practical framework for iterative prompt construction and evaluation for thematic and index coding, along with considerations for further analytical queries. These guidelines assist qualitative sociologists in conducting comprehensive, systematic, transparent, and reproducible research using generative AI. We also recognize generative AI’s limitations and underscore the indispensable role of human researchers throughout the process.</p>"
10.1016/b978-0-44-321857-6.00021-7,Synthetizing multiplicative functions in number theory,N/A
10.32388/o2jnmk,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/jyljh7,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/qetyfi,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.31219/osf.io/e9q2m,Integrating generative AI in knowledge building,"<p>Generative artificial intelligence (GenAI) is penetrating in various social sectors, motivating a strong need for teaching AI literacy in younger generations. While substantial efforts have been made to teach AI literacy and to use AI to facilitate learning, few studies have provided empirical accounts of students’ nuanced processes of using GenAI for learning. In this study, we engaged a group of high school students in leveraging ChatGPT to support their knowledge building efforts. Following the teacher’s pedagogical design, students used ChatGPT for a range of distinct purposes. Student interviews showed detailed processes of using ChatGPT for knowledge building and students’ emerging AI literacy in multiple dimensions. This study offers practical implications on integrating GenAI in K-12 education and urges educators to create spaces and scaffolds for students to mindfully engage with GenAI in the classroom.</p>"
10.1515/9783111323749-toc,Contents,N/A
10.1515/9783839474723-001,INDEX,N/A
10.2139/ssrn.4594824,"Generative AI, Human Creativity, and Art",N/A
10.36227/techrxiv.21789434.v1,Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education,"<jats:p>&lt;p&gt;Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.&lt;/p&gt;</jats:p>"
10.1093/oxfordhb/9780190067397.013.43,The Case for Ethical AI in the Military,"<p>This chapter addresses the military promise of artificial intelligence (AI), which is increasing along with advances in deep learning, neural networks, and robotics. The influence of AI will be felt across the full spectrum of armed conflict—from intelligence, surveillance, and reconnaissance through to the offensive and defensive employment of lethal force. This is to say that AI is less of a weapon than it is a military enabler, and yet the public still liken the notion of AI in the military context with killer robots, arising from the fears made public by numerous academic, business, and government leaders about the existential risk posed by an approaching singularity and the belief that AI could trigger the next world war. The chapter then considers what constitutes militarized “artificial intelligence”; the justifications for employing AI considering the limits of deep learning and the human role in alleged “black boxes”; the wider moral advantages, disadvantages, and risks of using AI in the military domain; and the potential implications for the way in which the armed forces plan, train, and fight. In doing so, it advances the concept of ethical AI as that which yields humanitarian benefits and differentiates between minimally and maximally just versions of said AI</p>"
10.1007/979-8-8688-0403-8_3,From Script to Screen: Unveiling Text-to-Video Generation,N/A
10.2139/ssrn.3916915,The Ethics of AI vs Ethical AI,N/A
10.21275/sr231115115845,Enabling Generative AI for Life Sciences and Healthcare Customers using the Power of Cloud,N/A
10.4018/979-8-3693-1565-1.ch009,Lensing Legal Dynamics for Examining Responsibility and Deliberation of Generative AI-Tethered Technological Privacy Concerns,"<jats:p>The rapid integration of generative AI technology across various domains has brought forth a complex interplay between technological advancements, legal frameworks, and ethical considerations. In a world where generative AI has transcended its initial novelty and is now woven into the fabric of everyday life, the boundaries of human creativity and machine-generated output are becoming increasingly blurred. The paper scrutinizes existing privacy laws and regulations through the lens of generative AI, seeking to uncover gaps, challenges, and possible avenues for reform. It explores the evolution of jurisprudence in the face of technological disruption and debates the adequacy of current legal frameworks to address the dynamic complexities of AI-influenced privacy infringements. By scrutinizing cases where personal data has been exploited by nefarious actors employing generative AI for malevolent purposes, a stark reality emerges: the emergence of a new avenue for privacy breaches that tests the limits of existing legal frameworks.</jats:p>"
10.1093/oxfordhb/9780190067397.013.38,AI and the Global South,"<p>This chapter details how AI affects, and will continue to affect, the Global South. The term “South” has a history connected with the “Third World” and has referred to countries that share postcolonial history and certain development goals. However, scholars have expanded and refined on it to include different kinds of marginal, disenfranchised populations such that the South is now a plural concept—there are Souths. The AI-related risks for Southern populations include concerns of discrimination, bias, oppression, exclusion, and bad design. These can be exacerbated in the context of vulnerable populations, especially those without access to human rights law or institutional remedies. The chapter then outlines these risks as well as the international human rights law that is applicable. It argues that a human rights–centric, inclusive, empowering context-driven approach is necessary.</p>"
10.5040/9781509974979.ch-004,Societal Challenges,N/A
10.2139/ssrn.4426942,Generative Ai at Work,N/A
10.32743/unitech.2024.119.2.16815,USING GENERATIVE AI IN ANALYZING WEB ANALYTICS DATA,N/A
10.14236/ewic/eva2024.59,Aeolian AI: Generative art and environmental computing,N/A
10.2218/newreal.9246,Generative AI Arts,"<jats:p>In this, the first edition of The New Real Magazine, we present a guide to working in Generative AI Arts by practitioners at the forefront of the creative wave in AI today, and discover how our present moment was foretold by artists who had already been working with, and on, AI for a decade.</jats:p>"
10.2218/newreal.9247,Navigating Generative AI in Turbulent Technological Seas,"<jats:p>The simple fact that it’s uncool to have an AI-enhanced profile picture now says a lot about the state of generative AI.
No longer a technology hinted at by AI evangelists or confined to particular techno-explorative corners of the internet, generative AI is now a staple of popular culture and front-page news. From the hilarity of the pope in a puffer jacket, to huge concerns about an algorithmically-driven ‘fog of war’ in the Israel-Hamas conflict, generative AI has breached the dinner-table conversations, anecdotes and fully-fledged arguments.</jats:p>"
10.2139/ssrn.4851637,PRACTICAL RECOMMENDATION OF USING GENERATIVE AI IN BUSINESS,N/A
10.58445/rars.524,From Code to Cure The Role of Generative AI in Antibody Design  and Immunotherapy Optimization,N/A
10.32388/vrk5n2,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/lzv8m3,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/lxt09q,"Review of: ""The Use of Generative AI in an Interdisciplinary Approach for Cultural Preservation""",N/A
10.32388/l086lv,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/9wss7f,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.32388/1hgpmq,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.52843/cassyni.3j2hbh,The Robots Are Here: Implications of Generative AI for Computing Education,"<jats:p>Recent breakthroughs in deep learning have led to the emergence of large language models (LLMs) that exhibit extraordinary performance at generating novel human-like content, including text, images and source code. It appears certain that such models will have a profound effect on educational practice and the development of tools for supporting student learning. For example, in the computing classroom, novices learning to code can use free tools to automatically suggest solutions to programming exercises and assignments. However, current tools were not designed with novices in mind, and little is known about how novices will interact with them in practice. Collaborative pedagogical approaches also look set for disruption. This talk will briefly present a timeline of the rapidly changing landscape of AI tools, summarise findings from recent work exploring the impact of LLMs in computing education, and discuss challenges and opportunities ahead.</jats:p>"
10.31235/osf.io/6jq32,A Sociological Approach to Analyzing Satellite and Streetscape Imagery with Generative AI,"<p>Although there is growing sociological research examining how generative artificial intelligence (genAI) models can be effectively and systematically applied to text-based tasks, whether and how genAI can be used to analyze images remain open questions. In this paper, we propose a sociological approach for analyzing satellite and streetscape imagery with OpenAI’s GPT-4o model. Our approach draws upon previous research on large-scale image analysis in computer science and sociology and consists of four main steps: task definition, image curation, prompt engineering, and benchmarking. We demonstrate our approach in a simple application designed to identify features in satellite images of neighborhoods divided by physical barriers (n = 5 images from 5 sites), and an advanced application identifying features in satellite and street-level images (1,101 images from 367 sites). We discuss results from each application in terms of reliability, validity, and computing time and costs, and conclude with reflections for other use cases.</p>"
10.32388/xv2g6s,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.1016/b978-0-44-321857-6.00022-9,"Text, sound generation, and other topics",N/A
10.2139/ssrn.4859628,How To Increase Business Productivity Using Generative AI,N/A
10.2139/ssrn.4614118,Generative AI and Personalized Video Advertisements,N/A
10.2139/ssrn.4443189,Generative AI,N/A
10.21125/inted.2024.0541,PEER-COACHING WITH GENERATIVE AI: AN EXPERIMENT,N/A
10.20944/preprints202408.1933.v1,A New Educational Reality: Active Methodologies Empowered by Generative AI,"<jats:p>The article explores the integration of generative artificial intelligence with active methodologies in modern education, highlighting how this combination can transform pedagogical practices and promote more personalized and effective learning. The study investigates the application of tools such as ChatGPT , which enhance the personalization of teaching, providing immediate feedback and adjusting content to students&amp;#039; individual needs. The research is structured into three chapters: the first addresses practical strategies for applying these methodologies with AI, the second evaluates their effectiveness in learning and the third presents practical examples of integration. Adapting teaching through AI was a central element in developing a more efficient learning environment aligned with the specific needs of each student. The results point to a significant increase in student engagement, understanding and retention of content, although they highlight the need for careful and ethical implementation to avoid challenges such as superficiality in learning and data privacy issues. It is concluded that the integration of generative AI with active methodologies can significantly transform the educational environment, but must be carried out in a balanced and responsible way, complementing traditional pedagogical practices and ensuring the comprehensive training of students.</jats:p>"
10.18850/jees.2022.63.03,A Task of Convergent AI Ethics Education in School Curriculum - with emphasis on the major of AI Convergent Education in graduate schools of education and the class of ‘AI Ethics’ -,N/A
10.4018/979-8-3693-1351-0.ch003,Empowering Educators With Generative AI Tools and Support,"<jats:p>This chapter explores possible ways educators can use generative AI tools and support to empower them in teaching and learning. In addition, the authord provide samples of appropriate prompts for educators to produce desired responses. With the potential of generative AI to create unique content instantly based on existing large datasets, educators can use it to co-create instructional content, enhance personalized learning experiences, evaluate learners' performances, and more. While highlighting the potential of generative AI, the authors also address the challenges that educators may face when using these tools. As generative AI is gaining popularity in classrooms, it has the potential to empower educators to revolutionize their teaching and transform education.</jats:p>"
10.1007/s43681-022-00154-8,A responsible AI framework: pipeline contextualisation,"<jats:title>Abstract</jats:title><jats:p>Incorporating ethics and values within the life cycle of an AI asset means securing its development, deployment, use, and decommission under these perspectives. These approaches depend on the <jats:italic>market domain</jats:italic> where AI is operational – considering the interaction and the impact on humans if any process does not perform as expected – and <jats:italic>the legal compliance</jats:italic>, both required to ensure adequate fulfilment of ethics and values. Specifically, in the manufacturing sector, standards were developed since the 1990’s to guarantee, among others, the correct use of mechanical machinery, systems robustness, low product variability, workers safety, system security, and adequate implementation of system constraints. However, it is challenging to blend the existing practices with the needs associated with deployments of AI in a trustworthy manner. This document provides an extended framework for AI Management within the Manufacturing sector. The framework is based on different perspectives related to responsible AI that handle trustworthy issues as risk. The approach is based on the idea that ethical considerations can and should be handled as hazards. If these requirements or constraints are not adequately fulfilled and managed, it is expected severe negative impact on different sustainable pillars. We are proposing a well-structured approach based on risk management that would allow implementing ethical concerns in any life cycle stages of AI components in the manufacturing sector. The framework follows a pipeline structure, with the possibility of being extended and connected with other industrial Risk Management Processes, facilitating its implementation in the manufacturing domain. Furthermore, given the dynamic condition of the regulatory state of AI, the framework allows extension and considerations that could be developed in the future.</jats:p>"
10.1007/s43681-022-00143-x,Defining organizational AI governance,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) governance is required to reap the benefits and manage the risks brought by AI systems. This means that ethical principles, such as fairness, need to be translated into practicable AI governance processes. A concise AI governance definition would allow researchers and practitioners to identify the constituent parts of the complex problem of translating AI ethics into practice. However, there have been few efforts to define AI governance thus far. To bridge this gap, this paper defines AI governance at the organizational level. Moreover, we delineate how AI governance enters into a governance landscape with numerous governance areas, such as corporate governance, information technology (IT) governance, and data governance. Therefore, we position AI governance as part of an organization’s governance structure in relation to these existing governance areas. Our definition and positioning of organizational AI governance paves the way for crafting AI governance frameworks and offers a stepping stone on the pathway toward governed AI.</jats:p>"
10.1007/s43681-022-00240-x,True uncertainty and ethical AI: regulatory sandboxes as a policy tool for moral imagination,"<jats:title>Abstract</jats:title><jats:p>We offer a complementary view to the ethical dilemmas discussed in the recent literature by pointing at the epistemological dilemma of true uncertainty, suggesting regulatory sandboxes as an apposite remedy. Using the exemplar case of the regulative sandbox for responsible artificial intelligence established by the Norwegian data protection authorities, we argue that regulative sandboxes have the potential of supporting the development of a more ethical AI through not only reducing uncertainty, but through nurturing moral imaginations.</jats:p>"
10.7551/mitpress/12549.003.0013,Challenges for Policymakers,N/A
10.1093/oxfordhb/9780190067397.013.11,Transparency,"<p>This chapter describes algorithmic decision-making (ADM) systems. ADM systems are tools that leverage an algorithmic process to arrive at some form of decision such as a scoring, ranking, classification, or association that may then drive further system action and behavior. Such systems could be said to exhibit artificial intelligence (AI) insofar as they contribute to decision-making tasks that might normally be undertaken by humans. However, it is important to underscore that ADM systems must be understood as composites of nonhuman actors woven together with human actors such as designers, data-creators, maintainers, and operators into complex sociotechnical assemblages. If the end goal is accountability, then transparency must serve to help locate the various positions of human agency and responsibility in these large and complex sociotechnical assemblages. Ultimately, it is people who must be held accountable for the behavior of algorithmic systems. The chapter then highlights what is needed to realistically implement algorithmic transparency in terms of what is disclosed and how and to whom transparency information is disclosed.</p>"
10.36227/techrxiv.170751714.41555942/v1,Ethics of AI (Artificial Intelligence),"<jats:p id=""p1"">The standard solution to new technology is to center the ethics of
robotics and artificial intelligence on ”concerns” of various kinds.
Many of these fears end up being rather outdated; a few are essentially
accurate but barely relevant (computer technological advances will
annihilate businesses that make pictures on film, audio cassettes, or
vinyl records); others are essentially accurate but extremely pertinent
(automobiles will cause the deaths of children and drastically alter the
landscape). Some of these fears are consistently incorrect when they
indicate that technology will totally transform humans. This paper
analyzes the problems and deflates the non-problems.</jats:p>"
10.1016/c2022-0-00475-x,Ethics in Online AI-based Systems,N/A
10.4324/9781003174493-3,AI and robot ethics,N/A
10.1007/s43681-024-00424-7,"E-coaching systems and social justice: ethical concerns about inequality, coercion, and stigmatization","<jats:title>Abstract</jats:title><jats:p>Poor self-regulation has been linked to various behaviors that contribute to pressing societal issues, including rising household debt, inefficient use of sustainable resources, and increasing healthcare demands. In light of this observation, the prospect of individuals receiving automated, tailored support by “e-coaching systems” to scaffold and improve their self-regulation is thought to hold promise for making society-wide progress in addressing such issues. Though there may be legitimate reasons for promoting the use of such systems, and individuals might welcome the support, our aim in the present article is to contribute to the ethics of e-coaching by showing how societal pressures towards the widespread adoption of automated e-coaching systems raise concerns in relation to three distinct aspects of social justice. We argue that societal inequalities may be introduced or exacerbated by (1) unequal access to the technologies, (2) unequally distributed restrictions to liberty and subjection to coercion, and (3) the potentially disparate impact of the use of e-coaching technologies on (self-)stigmatizing perceptions of competence. The article offers a research agenda for studying and addressing these concerns.</jats:p>"
10.1007/s43681-022-00137-9,Criminal courts’ artificial intelligence: the way it reinforces bias and discrimination,N/A
10.1007/s43681-023-00262-z,Dermatological diagnostic-assistive technologies: a call for regulatory action,N/A
10.1109/ethics57328.2023.10155062,Implementing AI Ethics in the Design of AI-assisted Rescue Robots,N/A
10.1007/s10551-024-05741-9,Generative Artificial Intelligence as Hypercommons: Ethics of Authorship and Ownership,"<jats:title>Abstract</jats:title><jats:p>In this editorial essay, we argue that Generative Artificial Intelligence programs (GenAI) draw on what we term a “hypercommons”, involving collectively produced inputs and labour that are largely invisible or untraceable. We argue that automatizing the exploitation of common inputs, in ways that remix and reconfigure them, can lead to a crisis of academic authorship in which the moral agency involved in scholarly production is increasingly eroded. We discuss the relationship between the hypercommons and authorship in terms of moral agency and the ethics of academic production, speculating on different responses to the crisis of authorship as posed by GenAI.</jats:p>"
10.1007/s43681-023-00342-0,Users’ needs in interactive bias auditing tools introducing a requirement checklist and evaluating existing tools,N/A
10.1093/oxfordhb/9780190067397.013.26,Sexuality,"<p>This chapter studies the opportunities and challenges posed by the use of AI in how humans express and enact their sexualities. It first considers the idea of digisexuality, which according to some commentators is the identity label that should be applied to those whose primary sexual identity comes through the use of technology, particularly through the use of robotics and AI. The chapter questions whether it is necessary or socially desirable to see this as a new form of sexual identity. It then looks at the role that AI can play in facilitating human-to-human sexual contact, focusing in particular on the use of self-tracking and predictive analytics in optimizing sexual and intimate behavior. There are already a number of apps and services that promise to use AI to do this, but they pose a range of ethical risks that need to be addressed at both an individual and societal level. Finally, the chapter considers the idea that a sophisticated form of AI could be an object of love.</p>"
10.1007/s43681-024-00555-x,Adapting legal horizons in reshaping intellectual property law for the artificial intelligence revolution,N/A
10.1093/oxfordhb/9780198857815.013.29,The Ethics of Weaponized AI,"<jats:title>Abstract</jats:title>
               <jats:p>This chapter presents an overview of some of the major ethical arguments for and against the use of autonomous weapons systems (AWS). More specifically, this chapter looks at the set of contingent arguments as well as the set of in principle arguments for and against their use. After summarizing these various views, the chapter argues that AWS do not pose new or novel ethical problems. If we think an AWS makes actual decisions in the ‘strong AI’ sense, then by virtue of being a decision-maker, that entity would therefore have rights and interests worthy of our moral concern. If we, however, think an AWS does not make actual decisions, but is instead just an institutional proxy for the collective set of human decisions comprising it, then we ought to treat an AWS, both morally and metaphysically, like we would treat any other collective action problem.</jats:p>"
10.1093/oxfordhb/9780190067397.013.22,Autonomy,"<p>This chapter assesses the concept of the autonomous AI—AI that is self-governing. Unease regarding autonomous AI is most vividly expressed in the vision of an artificial superintelligence whose self-generated goals and interests diverge radically from those of humankind, and which thus places humankind’s well-being—and maybe even their survival—at risk. As such, the chapter examines the conditions that would need to be met by an intelligent machine in order for that machine to exhibit the kind of autonomy that is operative in this dystopian scenario. However, there is arguably a more pressing concern regarding a different class of AI systems, those that are autonomous in only the milder sense that—in their domains of operation—humans are ceding, or will cede, some significant degree of control to them. Systems of this kind include self-driving cars and autonomous weapons systems. The chapter then evaluates whether these already-in-the-world autonomous AI systems are a genuine cause for concern. A key issue here concerns the properties of so-called deep learning networks.</p>"
10.1007/s10676-022-09649-8,Putting explainable AI in context: institutional explanations for medical AI,"<jats:title>Abstract</jats:title><jats:p>There is a current debate about if, and in what sense, machine learning systems used in the medical context need to be explainable. Those arguing in favor contend these systems require post hoc explanations for each individual decision to increase trust and ensure accurate diagnoses. Those arguing against suggest the high accuracy and reliability of the systems is sufficient for providing epistemic justified beliefs without the need for explaining each individual decision. But, as we show, both solutions have limitations—and it is unclear either address the epistemic worries of the medical professionals using these systems. We argue these systems do require an explanation, but an <jats:italic>institutional</jats:italic> explanation. These types of explanations provide the reasons why the medical professional should rely on the system in practice—that is, they focus on trying to address the epistemic concerns of those using the system in specific contexts and specific occasions. But ensuring that these institutional explanations are fit for purpose means ensuring the institutions designing and deploying these systems are transparent about the assumptions baked into the system. This requires coordination with experts and end-users concerning how it will function in the field, the metrics used to evaluate its accuracy, and the procedures for auditing the system to prevent biases and failures from going unaddressed. We contend this broader explanation is necessary for either post hoc explanations or accuracy scores to be epistemically meaningful to the medical professional, making it possible for them to rely on these systems as effective and useful tools in their practices.</jats:p>"
10.1007/978-981-19-9382-4_7,"Philosophy for AI Ethics: Metaethics, Metaphysics, and More",N/A
10.1007/s43681-021-00119-3,Autonomous weapon systems and the claim-rights of innocents on the battlefield,N/A
10.59728/jaie.2024.3.2.6,Trends and Standardization of Artificial Intelligence (AI) Ethics Regulations,"<jats:p>With the development of AI technology, ethical, legal, and social problems are emerging. In particular, examples of abuse of Generative AI include fake news, deepfakes, automatic spam and phishing, and copyright in-fringement, and ethical regulations are needed. Globally, these problems are responded to through AI ethics guidelines and AI Ethics Committee, among which the European Union is implementing safety and accountabil-ity and ethical evaluation through AI Act. In addition, AI ethics standardiza-tion is necessary to strengthen global competitiveness, secure social trust, and minimize negative effects. To this end, the domestic AI Ethics Forum promotes the ethical use of AI technology through discussion of ethical is-sues, guideline development, education, and international cooperation ac-tivities. In this paper, we examine the overall status of artificial intelligence ethics regulation trends and standardization, which can be expected to have effects such as reliability, safety assurance, innovation promotion, and increased social acceptance through standardization activities.</jats:p>"
10.21125/inted.2024.2052,NAVIGATING THE EVOLUTION OF ARTIFICIAL INTELLIGENCE: TOWARDS EDUCATION-SPECIFIC RETRIEVAL AUGMENTED GENERATIVE AI (ES-RAG-AI),N/A
10.56726/irjmets47516,Fintech's Generative AI Revolution How AI is shaping the Future of Banking and Financial Services,N/A
10.1007/978-3-031-54249-7_19,Embedding Generative AI,N/A
10.24963/ijcai.2024/977,Human-AI Interaction Generation: A Connective Lens for Generative AI and Procedural Content Generation,"<jats:p>Generative AI has recently gained popularity as a paradigm for content generation. In this paper, we link this paradigm to an older one: Procedural Content Generation (PCG). We propose a lens to identify the commonalities between both paradigms that we call human-AI interactive generation. Using this lens, we identify three beneficial attributes then survey recent related work and summarize relevant findings.</jats:p>"
10.1007/978-3-031-67991-9_7,Embedding AI,N/A
10.21428/e4baedd9.f78710e6,People-Powered Gen AI: Collaborating with Generative AI for Civic Engagement,N/A
10.12781/978-1-907549-22-9-12,Appreciative Inquiry Research Review &amp; Notes: Strengthening AI as a Generative Process to Action Research,N/A
10.1109/ichi61247.2024.00070,Harness the Power of Generative AI in Healthcare with Amazon AI/ML Services,N/A
10.2139/ssrn.4872366,"Generative AI and Creative Commons Licences: 
&lt;br&gt;
 The Application of Share Alike Obligations to Trained Models, Curated Datasets and AI Output",N/A
10.1080/17404622.2024.2397065,"Generative AI, communication, and stereotypes: Learning critical AI literacy through experience, analysis, and reflection",N/A
10.1007/979-8-8688-0456-4_5,The Business Opportunities of Today,N/A
10.1007/s00146-023-01642-z,"Moral distance, AI, and the ethics of care","<jats:title>Abstract</jats:title><jats:p>This paper investigates how the introduction of AI to decision making increases moral distance and recommends the ethics of care to augment the ethical examination of AI decision making. With AI decision making, face-to-face interactions are minimized, and decisions are part of a more opaque process that humans do not always understand. Within decision-making research, the concept of moral distance is used to explain why individuals behave unethically towards those who are not seen. Moral distance abstracts those who are impacted by the decision and leads to less ethical decisions. The goal of this paper is to identify and analyze the moral distance created by AI through both proximity distance (in space, time, and culture) and bureaucratic distance (derived from hierarchy, complex processes, and principlism). We then propose the ethics of care as a moral framework to analyze the moral implications of AI. The ethics of care brings to the forefront circumstances and context, interdependence, and vulnerability in analyzing algorithmic decision making.</jats:p>"
10.1007/s43681-024-00514-6,"Mind-reading in AI and neurotechnology: evaluating claims, hype, and ethical implications for neurorights","<jats:title>Abstract</jats:title><jats:p>This paper examines claims that the convergence of AI and neurotechnology applications, known as brain-reading, enables the reading of human minds. The purpose of this examination is to investigate whether the use of the terms “brain-reading” and “mind-reading” to convey current neurotechnological findings carries evidence of hype. We conducted an interpretive content analysis of 1017 academic articles to gain insights into the current state of the art and examine assertions made by academics. Our analysis revealed that up to 91% of the examined articles suggest the possibility of mind-reading through brain-reading. Ethical issues discussed frequently include mental privacy, mental freedom, and personhood. Our study highlights the imprecise and inconsistent usage of the term mind-reading in scientific discourse, which leads to exaggerated claims about AI and BCIs having already achieved capacities beyond their current capabilities—or even reaching capacities that may never be feasible. While our study provides evidence of AI and BCI hype concerning alleged mind-reading capabilities, it also uncovers a hype in AI ethics, specifically pertaining to neurorights. This involves hypothetical scenarios where the fictional prospect of AI-enabled mind-reading calls for the establishment of new protective human rights.</jats:p>"
10.47611/jsrhs.v13i2.6710,Impact of AI and Generative AI in transforming Cybersecurity,"<jats:p>In the constantly expanding field of cybersecurity, Artificial Intelligence (AI), and more specifically Generative Artificial Intelligence (GenAI) impacts every aspect of the cybersecurity landscape. In today’s world, most manual transactions, interactions, and records have become digital, generating vast amounts of valuable data. Many devices are connected by various private or public networks to the Internet known as the Internet of Things(IoT). This has led to many incidents where users and organizations have been targeted for Fraud, data theft, and disruption to businesses among other challenges. AI-based tools play a key role in defending against cyber attacks rapidly and effectively, due to their ability to analyze large volumes of data in real time. GenAI models can continuously learn and adapt, thus proactively assisting with cyber defense. For digital users and entities, this can help protect data, reduce incidents of Fraud, and model threats, keep systems safe, and mitigate other risks. The applications of GenAI however do not extend just to the defense of systems, they can also be used to increase cybersecurity threats. Malicious actors might use covert methods to manipulate data, attack systems, avoid malware detection, and spread harmful or incorrect information. This paper will detail how AI is a double-edged sword in cyber security, providing proactive and increasingly rapid and efficient ways to enhance cyber defense, as well as its use in newer and more damaging threats. It also aims to bring awareness of regulatory and other impacts on society.</jats:p>"
10.1007/s43681-024-00509-3,"Using the classic trolley problem to teach AI students and researchers about their role as moral agents, and why they should be subject to moral scrutiny","<jats:title>Abstract</jats:title><jats:p>This commentary proposes a means of teaching students – particularly computer science students – about their role as moral agents, who, on account of this role, are necessarily subject to moral scrutiny. It utilizes the classic Trolley Problem; but instead of focusing on the morality of the <jats:italic>decision</jats:italic> the bystander makes, it focuses, initially, on the role of the bystander as an <jats:italic>agent</jats:italic> of action, capable of <jats:italic>effecting change</jats:italic> and then, more importantly, as a <jats:italic>moral</jats:italic> agent whose action capabilities are subject to moral scrutiny. I believe that using the Trolley Problem in this way provides those tasked with teaching ethics to computer science students (but also others) a practical means of drawing attention not simply to whatever guidelines (e.g., code of ethics) have been produced in relation to AI and IT research, but to the fact that students and researchers alike are moral agents, however reluctant they may be to embrace this fact in the context of their studies and/or research.</jats:p>"
10.1007/s43681-022-00232-x,All that glitters is not gold: trustworthy and ethical AI principles,"<jats:title>Abstract</jats:title><jats:p>Ethics of technology systems have become an area of interest in academic research as well as international policy in recent years. Several organisation have consequently published principles of ethical artificial intelligence (AI) in line with this trend. The documents identify principles, values, and other abstract requirements for AI development and deployment. Critics raise concerns about whether these documents are in fact constructive, or if they are produced as a higher form of virtue signalling. A theme that is beginning to become apparent in the academic literature regarding these documents is the inherent lack of effective and practical methods and processes for producing ethical AI. This article attempts a critical analysis which draws upon ethical AI documents from a range of contexts including company, organisational, governmental, and academic perspectives. Both the theoretical and practical components of AI guidelines are explored and analysed, consequently bringing to light the necessity of introducing a measurable component to such documents for the purpose of ensuring a positive outcome of deploying AI systems based on ethical principles. We propose a minimal framework for stakeholders to develop AI in an ethical and human-centred manner.</jats:p>"
10.1007/s43681-021-00038-3,Mapping value sensitive design onto AI for social good principles,"<jats:title>Abstract</jats:title><jats:p>Value sensitive design (VSD) is an established method for integrating values into technical design. It has been applied to different technologies and, more recently, to artificial intelligence (AI). We argue that AI poses a number of challenges specific to VSD that require a somewhat modified VSD approach. Machine learning (ML), in particular, poses two challenges. First, humans may not understand how an AI system learns certain things. This requires paying attention to values such as transparency, explicability, and accountability. Second, ML may lead to AI systems adapting in ways that ‘disembody’ the values embedded in them. To address this, we propose a threefold modified VSD approach: (1) integrating a known set of VSD principles (AI4SG) as design norms from which more specific design requirements can be derived; (2) distinguishing between values that are promoted and respected by the design to ensure outcomes that not only do no harm but also contribute to good, and (3) extending the VSD process to encompass the whole life cycle of an AI technology to monitor unintended value consequences and redesign as needed. We illustrate our VSD for AI approach with an example use case of a SARS-CoV-2 contact tracing app.</jats:p>"
10.1007/978-3-030-54173-6_13,AI Nuclear Winter or AI That Saves Humanity? AI and Nuclear Deterrence,"<jats:title>Abstract</jats:title><jats:p>Nuclear deterrence is an integral aspect of the current security architecture and the question has arisen whether adoption of AI will enhance the stability of this architecture or weaken it. The stakes are very high. Stable deterrence depends on a complex web of risk perceptions. All sorts of distortions and errors are possible, especially in moments of crisis. AI might contribute toward reinforcing the rationality of decision-making under these conditions (easily affected by the emotional disturbances and fallacious inferences to which human beings are prone), thereby preventing an accidental launch or unintended escalation. Conversely, judgments about what does or does not suit the “national interest” are not well suited to AI (at least in its current state of development). A purely logical reasoning process based on the wrong values could have disastrous consequences, which would clearly be the case if an AI-based machine were allowed to make the launch decision (this virtually all experts would emphatically exclude), but grave problems could similarly arise if a human actor relied too heavily on AI input.</jats:p>"
10.1145/3299758.3299764,AI education matters,"<jats:p>In this column, we introduce our Model AI Assignment, A Module on Ethical Thinking about Autonomous Vehicles in an AI Course, and more broadly introduce a conversation on ethics education in AI education.</jats:p>"
10.1007/s43681-021-00113-9,The double-edged sword of AI: Ethical Adversarial Attacks to counter artificial intelligence for crime,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) has found a myriad of applications in many domains of technology, and more importantly, in improving people’s lives. Sadly, AI solutions have already been utilized for various violations and theft, even receiving the name AI or Crime (AIC). This poses a challenge: are cybersecurity experts thus justified to attack malicious AI algorithms, methods and systems as well, to stop them? Would that be fair and ethical? Furthermore, AI and machine learning algorithms are prone to be fooled or misled by the so-called adversarial attacks. However, adversarial attacks could be used by cybersecurity experts to stop the criminals using AI, and tamper with their systems. The paper argues that this kind of attacks could be named Ethical Adversarial Attacks (EAA), and if used fairly, within the regulations and legal frameworks, they would prove to be a valuable aid in the fight against cybercrime.</jats:p>"
10.4337/9781803926728.00021,"Ethics Beyond Ethics: AI, Power, and Colonialism",N/A
10.1007/s43681-024-00469-8,Crossing the principle–practice gap in AI ethics with ethical problem-solving,"<jats:title>Abstract</jats:title><jats:p>The past years have presented a surge in (AI) development, fueled by breakthroughs in deep learning, increased computational power, and substantial investments in the field. Given the generative capabilities of more recent AI systems, the era of large-scale AI models has transformed various domains that intersect our daily lives. However, this progress raises concerns about the balance between technological advancement, ethical considerations, safety measures, and financial interests. Moreover, using such systems in sensitive areas amplifies our general ethical awareness, prompting a re-emergence of debates on governance, regulation, and human values. However, amidst this landscape, how to bridge the principle–practice gap separating ethical discourse from the technical side of AI development remains an open problem. In response to this challenge, the present work proposes a framework to help shorten this gap: <jats:italic>ethical problem-solving</jats:italic> (EPS). EPS is a methodology promoting responsible, human-centric, and value-oriented AI development. The framework’s core resides in translating principles into practical implementations using impact assessment surveys and a differential recommendation methodology. We utilize EPS as a blueprint to propose the implementation of an <jats:italic>Ethics as a Service Platform</jats:italic>, currently available as a simple demonstration. We released all framework components openly and with a permissive license, hoping the community would adopt and extend our efforts into other contexts. Available in the following URL <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://nkluge-correa.github.io/ethical-problem-solving/"">https://nkluge-correa.github.io/ethical-problem-solving/</jats:ext-link>.</jats:p>"
10.1007/s43681-023-00289-2,Auditing large language models: a three-layered approach,"<jats:title>Abstract</jats:title><jats:p>Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.</jats:p>"
10.1007/s43681-024-00548-w,On singularity and the Stoics: why Stoicism offers a valuable approach to navigating the risks of AI (Artificial Intelligence),"<jats:title>Abstract</jats:title><jats:p>The potential benefits and risks of artificial intelligence technologies have sparked a wide-ranging debate in both academic and public circles. On one hand, there is an urgent call to address the immediate and avoidable challenges associated with these tools, such as accountability, privacy, bias, understandability, and transparency; on the other hand, prominent figures like Geoffrey Hinton and Elon Musk have voiced concerns over the potential rise of Super Artificial Intelligence, whose singularity could pose an existential threat to humanity. Coordinating the efforts of thousands of decentralized entities to prevent such a hypothetical event may seem insurmountable in our intricate and multipolar world. Thus, drawing from both perspectives, this work suggests employing the tools and framework of Stoic philosophy, particularly the concept of the dichotomy of control—focusing on what is within our power. This Stoic principle offers a practical and epistemological approach to managing the complexities of AI, and it encourages individuals to organize their efforts around what they can influence while adapting to the constraints of external factors. Within this framework, the essay found that Stoic wisdom is essential for assessing risks, courage is necessary to face contemporary challenges, and temperance and tranquility are indispensable; and these lessons can inform ongoing public and academic discourse, aiding in the development of more effective policy proposals for aligning Narrow AI and General AI with human values.</jats:p>"
10.1007/s43681-024-00523-5,Keeping an AI on the mental health of vulnerable populations: reflections on the potential for participatory injustice,"<jats:title>Abstract</jats:title><jats:p>Considering the overall shortage of therapists to meet the psychological needs of vulnerable populations, AI-based technologies are often seen as a possible remedy. Particularly smartphone apps or chatbots are increasingly used to offer mental health support, mostly through cognitive behavioral therapy. The assumption underlying the deployment of these systems is their ability to make mental health support accessible to generally underserved populations. Hence, this seems to be aligned with the fundamental biomedical principle of justice understood in its <jats:italic>distributive</jats:italic> meaning. However, considerations of the principle of justice in its <jats:italic>epistemic</jats:italic> significance are still in their infancy in the debates revolving around the ethical issues connected to the use of mental health chatbots. This paper aims to fill this research gap, focusing on a less familiar kind of harm that these systems can cause, namely the harm to users in their capacities as knowing subjects. More specifically, we frame our discussion in terms of one form of epistemic injustice that such practices are especially prone to bring about, i.e., <jats:italic>participatory injustice</jats:italic>. To make our theoretical analysis more graspable and to show its urgency, we discuss the case of a mental health Chatbot, Karim, deployed to deliver mental health support to Syrian refugees. This case substantiates our theoretical considerations and the epistemo-ethical concerns arising from the use of mental health applications among vulnerable populations. Finally, we argue that conceptualizing epistemic participation as a capability within the framework of Capability Sensitive Design can be a first step toward ameliorating the participatory injustice discussed in this paper.</jats:p>"
10.21428/8c225f6e.001efa82,"Generative Artificial Intelligence and education: Research, policy and practice",N/A
10.22251/jlcci.2024.24.10.537,Examining AI and Digital Ethics Issues for Youth and Educational Approaches: Focusing on IoT and Generative AI,"<jats:p>Objectives  This study aims to shed light on the ethical issues of artificial intelligence and digital technologies that youth face in their daily lives in modern society, focusing on IoT and generative AI, and to examine educational ap-proaches for youth regarding these issues.
Methods  To achieve this, we investigated the digital ethics issues faced by teenagers through a literature review of domestic and international academic papers, policy reports, and news articles, as well as case studies from ed-ucational settings in Korea and abroad. Based on these findings, we examined student education and policy measures to address teenagers' digital ethics issues.
Results  The ethical issues related to different types of IoT usage are associated with the various IoT devices used by youth, and since domestic educational cases are insufficient, education needs to focus on issues such as per-sonal information protection, data security, surveillance and privacy infringement through IoT devices. Regarding generative AI, particularly language models, key concerns include AI bias, generation and dissemination of mis-information, and personal information protection issues. It was found that educational materials for youth and teachers are being developed by metropolitan and provincial offices of education in South Korea and overseas non-profit foundations.
Conclusions  AI and digital ethics education should go beyond mere prohibition and control, focusing on developing students' ability to make proper judgments voluntarily. Education for youth is needed on issues such as copyright of AI-generated content, and misuse and abuse of data.</jats:p>"
10.1007/s43681-022-00207-y,"Segmentation of ethics, legal, and social issues (ELSI) related to AI in Japan, the United States, and Germany","<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) is often accompanied by public concern. In this study, we quantitatively evaluated a source of public concern using the framework for ethics, legal, and social issues (ELSI). Concern was compared among people in Japan, the United States, and Germany using four different scenarios: (1) the use of AI to replicate the voice of a famous deceased singer, (2) the use of AI for customer service, (3) the use of AI for autonomous weapons, and (4) the use of AI for preventing criminal activities. The results show that the most striking difference was in the response to the “weapon” scenario. Respondents from Japan showed greater concern than those in the other two countries. Older respondents had more concerns, and respondents who had a deeper understanding of AI were more likely to have concerns related to the legal aspects of it. We also found that attitudes toward legal issues were the key to segmenting their attitudes toward ELSI related to AI: Positive, Less skeptical of laws, Skeptical of laws, and Negative.</jats:p>"
10.1007/979-8-8688-0282-9_12,Text-Based Generative Intelligent Agents: Beyond Traditional Chatbots and Virtual Assistants,N/A
10.7551/mitpress/12186.003.0015,The AI of Ethics,N/A
10.1007/s43681-024-00525-3,Ethical considerations for the application of artificial intelligence in pediatric surgery,N/A
10.2139/ssrn.4034804,The Ethics of AI Business Practices: A Review of 47 AI Ethics Guidelines,N/A
10.1007/s43681-023-00406-1,On the evaluation of the symbolic knowledge extracted from black boxes,N/A
10.1007/s43681-024-00453-2,Investigating fairness in machine learning-based audio sentiment analysis,"<jats:title>Abstract</jats:title><jats:p>Audio sentiment analysis is a growing area of research, however little attention has been paid to the fairness of machine learning models in this field. Whilst the current literature covers research on machine learning models’ reliability and fairness in various demographic groups, fairness in audio sentiment analysis with respect to gender is still an uninvestigated field. To fill this knowledge gap, we conducted experiments aimed at assessing the fairness of machine learning algorithms concerning gender within the context of audio sentiment analysis. In this research, we used 442 audio files of happiness and sadness—representing equal samples of male and female subjects—and generated spectrograms for each file. Then we performed feature extraction using bag-of-visual-words method followed by building classifiers using Random Forest, Support Vector Machines, and K-nearest Neighbors algorithms. We investigated whether the machine learning models for audio sentiment analysis are fair across female and male genders. We found the need for gender-specific models for audio sentiment analysis instead of a gender-agnostic-model. Our results provided three pieces of evidence to back up our claim that gender-specific models demonstrate bias in terms of overall accuracy equality when tested using audio samples representing the other gender, as well as combination of both genders. Furthermore, gender-agnostic-model performs poorly in comparison to gender-specific models in classifying sentiments of both male and female audio samples. These findings emphasize the importance of employing an appropriate gender-specific model for an audio sentiment analysis task to ensure fairness and accuracy. The best performance is achieved when using a female-model (78% accuracy) and a male-model (74% accuracy), significantly outperforming the 66% accuracy of the gender-agnostic model.</jats:p>"
10.1007/s43681-022-00203-2,A narrative review of fairness and morality in neuroscience: insights to artificial intelligence,N/A
10.1007/s43681-023-00301-9,"Publisher Correction: They shall be fair, transparent, and robust: auditing learning analytics systems",N/A
10.1007/978-3-031-23035-6_5,Challenges of Integrating AI Ethics into Higher Education Curricula in West Africa: Nigerian Universities Narrative,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) is becoming pervasive. It is also an exciting field because it is making our lives much better, by doing most of the work for us. For example, driving our cars, medical jobs, accounting jobs, all sorts of jobs.</jats:p>"
10.1007/s43681-022-00172-6,Responsibility gaps and the reactive attitudes,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) systems are ubiquitous. From social media timelines, video recommendations on YouTube, and the kinds of adverts we see online, AI, in a very real sense, filters the world we see. More than that, AI is being embedded in agent-like systems, which might prompt certain reactions from users. Specifically, we might find ourselves feeling frustrated if these systems do not meet our expectations. In normal situations, this might be fine, but with the ever increasing sophistication of AI-systems, this might become a problem. While it seems unproblematic to realize that being angry at your car for breaking down is unfitting, can the same be said for AI-systems? In this paper, therefore, I will investigate the so-called “reactive attitudes”, and their important link to our responsibility practices. I then show how within this framework there exist exemption and excuse conditions, and test whether our adopting the “objective attitude” toward agential AI is justified. I argue that such an attitude is appropriate in the context of three distinct senses of responsibility (answerability, attributability, and accountability), and that, therefore, AI-systems do not undermine our responsibility ascriptions.</jats:p>"
10.1007/s43681-023-00267-8,Artificial intelligence applied in pulmonary hypertension: a bibliometric analysis,N/A
10.1007/s43681-023-00396-0,Positive and negative explanation effects in human–agent teams,N/A
10.1007/s43681-023-00286-5,Publisher Correction: Convergence of the source control and actual access accounts of privacy,N/A
10.1007/s43681-021-00068-x,Facebook’s ethical failures are not accidental; they are part of the business model,N/A
10.2139/ssrn.4382111,The Age of Generative AI in Academia: An Opinion,N/A
10.32388/himxwn,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.21428/6ffd8432.f3a81eff,Trusting Video in the Age of Generative AI,N/A
10.2139/ssrn.4656722,"Bank Run, Interrupted: Modeling Deposit Withdrawals with Generative AI",N/A
10.2139/ssrn.4483539,Generative AI Art: Copyright Infringement and Fair Use,N/A
10.36227/techrxiv.23696709,"Unleashing the Potential: Overcoming Hurdles and Embracing Generative AI in IT Workplaces: Advantages, Guidelines, and Policies","<jats:p>&lt;p&gt;The increasing popularity of ChatGPT and its widespread adoption across various industries have brought forth new and evolving challenges. In this research paper, our primary focus is to investigate how ChatGPT can benefit the Information Technology (IT) sector while exploring the challenges it presents.&lt;/p&gt;
&lt;p&gt;Given the significance of this topic, numerous Chief Information Security Officers (CISOs) are currently working on understanding and implementing ChatGPT. Thus, we embarked on this research endeavor to delve deeper into this subject. Our paper, titled ""Unleashing the Potential: Overcoming Hurdles and Embracing Generative AI in IT Workplaces: Advantages, Guidelines, and Policies,"" aims to examine the advantages, hurdles, and potential of Generative AI (GenAI) and its applications in IT environments.&lt;/p&gt;
&lt;p&gt;To accomplish our research goals, we employed multiple GenAI tools and conducted extensive investigations to explore their benefits and identify strategies to mitigate associated challenges. By leveraging these tools, we aimed to unlock the full potential of GenAI in the IT landscape.&lt;/p&gt;
&lt;p&gt;Our research paper delves into various aspects, including the advantages offered by GenAI, the hurdles encountered during its implementation, and the potential it holds for transforming IT workplaces. Furthermore, we provide guidelines and policies to ensure the responsible and effective utilization of GenAI within IT organizations.&lt;/p&gt;
&lt;p&gt;Through this research, we aim to contribute to the growing body of knowledge surrounding GenAI, enabling IT professionals to make informed decisions and harness the benefits of this emerging technology. By addressing the challenges and providing guidelines, we aim to facilitate the seamless integration of GenAI into IT workplaces, fostering innovation and efficiency in the industry.&lt;/p&gt;
&lt;p&gt;Please see attached PDF for result.&lt;/p&gt;</jats:p>"
10.33424/futurum481,Detecting deepfakes: how can we ensure that generative AI is used for good?,N/A
10.1109/platcon60102.2023.10255179,Cybersecurity Issues in Generative AI,N/A
10.31039/ljss.2023.6.114,Generative AI in Education,"<jats:p>Recently, the field of artificial intelligence (AI) has advanced significantly, with generativeAI rising to the top of the tech industry's most-discussed subjects list. Education like manyother fields could be transformed by generative AI like ChatGPT, Bard, DALL-E,Midjourney, and DeepMind, which all have the ability to revolutionize a number of industrieswith all the benefits and drawbacks they entail.The paper begins by providing an overview of generative AI, highlighting its capacity togenerate human-like text, images, and even interactive simulations. It delves into theunderlying principles and techniques that empower generative AI models, focusing onprominent models like ChatGPT and Midjourney.To assess the effectiveness of generative AI in educational contexts, the paper examinesstudies that have evaluated learning outcomes, student engagement, and teacher support.These findings provide insights into the efficiency of generative AI as a supplementaryeducational tool and its role in fostering innovative teaching practices. The paper also addressesconcerns around the technology being too integrated into education like over dependence andcheating.Finally, the paper discusses future possibilities and challenges for the integration of generativeAI in education. It proposes strategies for maximizing the benefits of this technology whileensuring ethical considerations are met. The paper concludes with a call for further researchand collaboration between AI experts, educators, policymakers, and other stakeholders toharness the full potential of generative AI in transforming the learning landscape.</jats:p>"
10.1016/b978-0-44-321857-6.00013-8,New interpolation methods for synthetization and prediction,N/A
10.2139/ssrn.4858168,Decoding Herding Dynamics in the Generative AI Investment Amid Key Technological Advancements: A Timeline Perspective,N/A
10.3386/w32474,"Old Moats for New Models: Openness, Control, and Competition in Generative AI",N/A
10.5040/9781509974979,Regulating the Synthetic Society,"<JATS1:p>Experts predict that in 5 years’ time, more than 90% of all digital content will be wholly or partially AI generated. In a synthetic society, it may no longer be possible to establish what is real and what is not. Central to this open access book are 4 technologies on the frontline of this trend: humanoid robots, deepfakes, augmented reality, and virtual reality.</JATS1:p>
          <JATS1:p>Although they are only in their relative infancy, these technologies can already produce content that is indistinguishable from authentic material. The impact of this new reality on democracy, the judicial system, the functioning of the press, as well as on personal relationships will be unprecedented.</JATS1:p>
          <JATS1:p>Van der Sloot describes the technological fundaments of each of those technologies and maps their positive uses for educational purposes as well as for the treatment of patients, for the entertainment and creative industries, and the retail and financial sectors. The book also conceptualises their negative uses for fraud, deception, exploitation, identity-theft and exploitation, and shows their deeper effects on the post-truth society, the privatisation of the public sphere, and the loss of individual autonomy and societal trust.</JATS1:p>
          <JATS1:p>The book evaluates how the current European legal paradigm applies to these technologies, focussing on the right to privacy and data protection, freedom of expression, procedural law, tort law, and the regulation of AI. It discusses regulatory alternatives to solve existing regulatory gaps and shows that there are no easy answers.</JATS1:p>
          <JATS1:p>The ebook editions of this book are available open access under a CC BY-NC-ND 4.0 licence on bloomsburycollections.com.</JATS1:p>"
10.7551/mitpress/14136.003.0022,Generative Ethics in  ARTIFICIAL LIFE,N/A
10.1007/s43681-021-00093-w,Foundations for the future: institution building for the purpose of artificial intelligence governance,"<jats:title>Abstract</jats:title><jats:p>Governance efforts for artificial intelligence (AI) are taking on increasingly more concrete forms, drawing on a variety of approaches and instruments from hard regulation to standardisation efforts, aimed at mitigating challenges from high-risk AI systems. To implement these and other efforts, new institutions will need to be established on a national and international level. This paper sketches a blueprint of such institutions, and conducts in-depth investigations of three key components of any future AI governance institutions, exploring benefits and associated drawbacks: (1) “purpose”, relating to the institution’s overall goals and scope of work or mandate; (2) “geography”, relating to questions of participation and the reach of jurisdiction; and (3) “capacity”, the infrastructural and human make-up of the institution. Subsequently, the paper highlights noteworthy aspects of various institutional roles specifically around questions of institutional purpose, and frames what these could look like in practice, by placing these debates in a European context and proposing different iterations of a European AI Agency. Finally, conclusions and future research directions are proposed.</jats:p>"
10.1145/3600211.3604764,"Benchmarked Ethics: A Roadmap to AI Alignment, Moral Knowledge, and Control",N/A
10.1007/979-8-8688-0318-5_8,"Generative AI’s Implications for Consumers, Employees, Companies, Product Providers, and Investors",N/A
10.1007/s43681-021-00069-w,Ethical funding for trustworthy AI: proposals to address the responsibilities of funders to ensure that projects adhere to trustworthy AI practice,"<jats:title>Abstract</jats:title><jats:p>AI systems that demonstrate significant bias or lower than claimed accuracy, and resulting in individual and societal harms, continue to be reported. Such reports beg the question as to why such systems continue to be funded, developed and deployed despite the many published ethical AI principles. This paper focusses on the funding processes for AI research grants which we have identified as a gap in the current range of ethical AI solutions such as AI procurement guidelines, AI impact assessments and AI audit frameworks. We highlight the responsibilities of funding bodies to ensure investment is channelled towards trustworthy and safe AI systems and provides case studies as to how other ethical funding principles are managed. We offer a first sight of two proposals for funding bodies to consider regarding procedures they can employ. The first proposal is for the inclusion of a Trustworthy AI Statement’ section in the grant application form and offers an example of the associated guidance. The second proposal outlines the wider management requirements of a funding body for the ethical review and monitoring of funded projects to ensure adherence to the proposed ethical strategies in the applicants Trustworthy AI Statement. The anticipated outcome for such proposals being employed would be to create a ‘stop and think’ section during the project planning and application procedure requiring applicants to implement the methods for the ethically aligned design of AI. In essence it asks funders to send the message “if you want the money, then build trustworthy AI!”.</jats:p>"
10.1093/oxfordhb/9780190067397.013.36,The Complexity of Otherness,"<p>This chapter examines intersections between anthropology and robots and AI. Intelligence is not a neutral concept, but was developed to justify the subjugation of people on the basis of their sex, race, class, and ability. Indeed, the computer sciences and robotics have, for most of the twentieth century, continued to be fields that develop without fuller participation from women and people of color. As such, for AI scientists and roboticists, it will be necessary to take into account issues from the social sciences and humanities when constructing their technologies in order that prejudices, stereotypes, or structural inequalities are not reproduced in the life that follows from the developments of these technologies. Arguably, the rise of ethics of AI and robotics will create committees of stakeholders who will work together to reduce bias, stereotyping, and perpetual algorithmic inequality.</p>"
10.2139/ssrn.4411068,"Generative AI, ChatGPT, and the Future of Jobs",N/A
10.5089/9798400277177.006,Broadening the Gains from Generative AI,N/A
10.1109/mc.2024.3416231,"Generative AI Changes the World, Maybe",N/A
10.1016/j.cognition.2024.105906,Generative AI and the future of equality norms,N/A
10.31219/osf.io/95kmx,Exploring The Potential Of Generative AI,"<p>Generative AI has taken the world by storm, disrupting the way we work, communicate and innovate. The blend of human ingenuity and machine intelligence has thrown ajar limitless possibilities. Perhaps, the best testimony to the rapid adoption and diffusion of this technology is ChatGPT, a phenomenon that garnered 100 million+ users in just two months of its launch. At the heart of Generative AI lies the liberating power of creativity. Art, music, prose, and design find an exhilarating accomplice in these AI muses, sparking an orchestra of innovative expression. From architecture to healthcare, fashion to entertainment, finance to education, this disruptive force breathes life into solutions that defy convention. Innovation takes on a life of its own, pushing the frontiers of possibility ever further. As we stand at the dawn of an AI-infused future, we are delighted to come up with this definitive White Paper on the profound possibilities and an array of applications on Generative AI. This White Paper explores the ins and outs of the Generative AI ecosystem. What’s more, it parades the proliferating use cases spanning industries. From fascination to implementation, Generative AI has a long distance to navigate. And its initial journey has already raised social dilemmas and ethical concerns. The mounting fears of loss of jobs and data privacy too. As the famous physicist Stephen Hawking once wrote: “Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks.” This paper dives into the worries. More, it opens a window to the future and how AI-human symbiosis will break the barriers of perception. We are stepping into a future where ‘Generative’ minds intertwine, birthing innovations beyond our collective imagination. Even in its incipient stages, Generative AI is shaping the future across domains and exerting influence on our day-to-day lives. Embracing this blockbuster tech ethically and responsibly will lead us to the possibilities awaiting our embrace.</p>"
10.2139/ssrn.4745624,The Heterogeneous Productivity Effects of Generative AI,N/A
10.1016/b978-0-44-321857-6.00015-1,High quality random numbers for data synthetization,N/A
10.2139/ssrn.4637633,Smart Grading: A Generative Ai-Based Tool for Knowledge-Grounded Answer Evaluation in Educational Assessments,N/A
10.2139/ssrn.4452600,Mitigating Risks for Financial Firms Using Generative AI Tools,N/A
10.32388/8z95kh,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.1515/9783839474723-002,Art Intelligence,N/A
10.1038/d43747-024-00084-w,Generative AI platforms drive drug discovery dealmaking,N/A
10.31235/osf.io/7hc2d,Testing Generative AI for Source Audits in Student-Produced Local News,"<p>This study tests the capacity of ChatGPT to measure the gender diversity and social standing of sources quoted in stories published by a university-based local news organization. Student journalists were more likely than their professional counterparts to quote women and frequently featured sources who are not “social elites.” We interrogate both sourcing practices and the ethics of generative AI, arguing that university-based local news organizations can serve as testing arenas for emerging technology.</p>"
10.1007/s43681-021-00049-0,Sensitivity of neural networks to corruption of image classification,N/A
10.1007/s43681-022-00238-5,Designed to cooperate: a Kant-inspired ethic of machine-to-machine cooperation,N/A
10.53987/2178-5368-2023-12-05,Critical Generative AI Aesthetics,N/A
10.31219/osf.io/vepzk,Leveraging generative AI to acculturate away from climate apathy,"<p>This essay calls stakeholders’ attention toward the alignment of ethics in three spheres that will determine the continued existence of mankind: human to human, human to nature, and human to machines. Thus, it articulates three conditions to harness the power of generative AI chatbots so that we can build a culture, i.e., acculturate away from climate apathy. We believe that our powerful AI systems can genuinely partner with us to guide us beyond the tragedy of the commons, as well as the tragedy of commonsense morality that has afflicted our thoughts and emotions concerning long-term problems, such as averting climate disasters.</p>"
10.1016/b978-0-44-321857-6.00019-9,Synthetic star cluster generation with collision graphs,N/A
10.2139/ssrn.4666860,Generative AI: An Existential Threat to Human Content Creators?,N/A
10.1515/9781501519796-006,"Chapter 5: Generative AI, Bard, and Gemini",N/A
10.1101/2023.10.10.561808,Harnessing Generative AI to Decode Enzyme Catalysis and Evolution for Enhanced Engineering,"<jats:title>Abstract</jats:title><jats:p>Enzymes, as paramount protein catalysts, occupy a central role in fostering remarkable progress across numerous fields. However, the intricacy of sequence-function relationships continues to obscure our grasp of enzyme behaviors and curtails our capabilities in rational enzyme engineering. Generative artificial intelligence (AI), known for its proficiency in handling intricate data distributions, holds the potential to offer novel perspectives in enzyme research. By applying generative models, we could discern elusive patterns within the vast sequence space and uncover new functional enzyme sequences. This review highlights the recent advancements in employing generative AI for enzyme sequence analysis. We delve into the impact of generative AI in predicting mutation effects on enzyme fitness, activity, and stability, rationalizing the laboratory evolution of<jats:italic>de novo</jats:italic>enzymes, decoding protein sequence semantics, and its applications in enzyme engineering. Notably, the prediction of enzyme activity and stability using natural enzyme sequences serves as a vital link, indicating how enzyme catalysis shapes enzyme evolution. Overall, we foresee that the integration of generative AI into enzyme studies will remarkably enhance our knowledge of enzymes and expedite the creation of superior biocatalysts.</jats:p>"
10.32388/whj4mo,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.21125/inted.2024.0201,THE TRANSFORMATIVE POTENTIAL OF GENERATIVE AI IN EDUCATION,N/A
10.52546/alsa.24-at-734.pc.gr.189952,N/A,N/A
10.2139/ssrn.4826678,Hacking the Perfect Score on High-Stakes Personality Assessments with Generative Ai,N/A
10.2139/ssrn.4603145,Augmenting Creativity using Generative AI: The Method of Trisociation,N/A
10.1117/12.3022263.9bfdc619-478f-ee11-a99c-005056914f1c,N/A,N/A
10.2139/ssrn.4895486,Generative AI Can Harm Learning,N/A
10.36227/techrxiv.23696709.v1,"Unleashing the Potential: Overcoming Hurdles and Embracing Generative AI in IT Workplaces: Advantages, Guidelines, and Policies","<jats:p>&lt;p&gt;The increasing popularity of ChatGPT and its widespread adoption across various industries have brought forth new and evolving challenges. In this research paper, our primary focus is to investigate how ChatGPT can benefit the Information Technology (IT) sector while exploring the challenges it presents.&lt;/p&gt;
&lt;p&gt;Given the significance of this topic, numerous Chief Information Security Officers (CISOs) are currently working on understanding and implementing ChatGPT. Thus, we embarked on this research endeavor to delve deeper into this subject. Our paper, titled ""Unleashing the Potential: Overcoming Hurdles and Embracing Generative AI in IT Workplaces: Advantages, Guidelines, and Policies,"" aims to examine the advantages, hurdles, and potential of Generative AI (GenAI) and its applications in IT environments.&lt;/p&gt;
&lt;p&gt;To accomplish our research goals, we employed multiple GenAI tools and conducted extensive investigations to explore their benefits and identify strategies to mitigate associated challenges. By leveraging these tools, we aimed to unlock the full potential of GenAI in the IT landscape.&lt;/p&gt;
&lt;p&gt;Our research paper delves into various aspects, including the advantages offered by GenAI, the hurdles encountered during its implementation, and the potential it holds for transforming IT workplaces. Furthermore, we provide guidelines and policies to ensure the responsible and effective utilization of GenAI within IT organizations.&lt;/p&gt;
&lt;p&gt;Through this research, we aim to contribute to the growing body of knowledge surrounding GenAI, enabling IT professionals to make informed decisions and harness the benefits of this emerging technology. By addressing the challenges and providing guidelines, we aim to facilitate the seamless integration of GenAI into IT workplaces, fostering innovation and efficiency in the industry.&lt;/p&gt;
&lt;p&gt;Please see attached PDF for result.&lt;/p&gt;</jats:p>"
10.25144/16620,HARNESSING GENERATIVE AI EXPLORING THE SYNERGY OF CHATGPT AND GRASSHOPPER FOR ROOM ACOUSTIC DESIGN,N/A
10.56726/irjmets48608,Stable Diffusion With Generative AI,N/A
10.2139/ssrn.4832567,Implement FRTB with Generative Ai Models: A Case Study,N/A
10.21203/rs.3.rs-3944980/v1,From Blueprint to Flight: Guiding Your First Generative AI Project - Revolutionizing Service Desk Operations,"<jats:title>Abstract</jats:title>
        <jats:p>The main objective of this study is to investigate the strategic incorporation of Generative AI into service desk operations from initiation to execution, and its impact on productivity, customer satisfaction, and efficiency. This study used a phenomenological approach to capture the experiences of employees and uncover insights into the transformative capabilities of GAI within organizational frameworks. The findings revealed the successful integration of GAI, which enhanced the service desk operations. This study identified key steps from market study insights into user-centric adoption that contributed to overall success. Despite acknowledging these limitations, this study provides practical implications for organizations seeking strategic GAI integration. The outlined steps, derived from real-world experience, provide a practical roadmap for organizations to improve their service desk operations and achieve their strategic objectives. The value of this research extends to organizations that consider or adopt Generative AI within their service desk operations.</jats:p>"
10.36413/pjahs.0702.001,Using Generative AI in Research,N/A
10.1016/b978-0-44-321857-6.00006-0,"A simple, robust, and efficient ensemble method",N/A
10.2139/ssrn.4152484,Generative and AI Authored Artworks and Copyright Law,N/A
10.32388/qnf9bs,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.2139/ssrn.4935208,&lt;p&gt;Chapter 21: Generative AI And Sustainability&amp;nbsp;&lt;/p&gt;,N/A
10.3030/101151067,N/A,N/A
10.2139/ssrn.4315686,"Generative AI: Here to stay, but for good?",N/A
10.52214/stlr.v25i2.12766,Beyond Algorithmic Disclosure for Generative AI,"<jats:p>One of the most commonly recommended policy interventions with respect to algorithms in general and artificial intelligence (“AI”) systems in particular is the need for greater transparency, often focusing on the disclosure of the variables employed by the algorithm and the weights given to those variables. This Essay argues that any meaningful transparency regime must provide information on other critical dimensions as well. For example, any transparency regime must also include key information about the data on which the algorithm was trained, including its source, scope, quality, and inner correlations, subject to constraints imposed by copyright, privacy, and cybersecurity law. Disclosures about pre-release testing also play a critical role in understanding an AI system’s robustness and its susceptibility to specification gaming. Finally, the fact that AI, like all complex systems, tends to exhibit emergent phenomena, such as proxy discrimination, interactions among multiple agents, the impact of adverse environments, and the well-known tendency of generative AI to hallucinate, makes ongoing post-release evaluation a critical component of any system of AI transparency. </jats:p>"
10.21203/rs.3.rs-4449928/v1,Global Trends in Generative AI Adoption: A Quantitative Survey of Postsecondary Students,"<title>Abstract</title>
        <p>This research details postsecondary education (PSE) students’ (n = 1021) use of artificial intelligence (AI) technologies—specifically Generative AI (GenAI) tools like ChatGPT—in their academic activities. Through a comprehensive survey analysis, the study sought to identify the extent of GenAI usage for academic purposes, exploring factors such as gender, level of study, and primary language proficiency. Key findings revealed a significant adoption rate among PSE students and high intentions to continue using AI to do academic work in subsequent year. Gender-based usage slightly affected the use of AI and the intention to use AI. There was no substantial differences corresponding to primary language or level of study. The research leveraged the unified theory of acceptance and use of technology (UTAUT) model to interpret these patterns, suggesting that gender differences and technological acceptance behaviours significantly influence GenAI tool adoption. The study emphasizes the need for educational institutions to adapt to the evolving landscape of AI in education, advocating for policies that recognize and integrate the use of GenAI tools in academic settings. It also invites scholars to examine why females are more likely to report not having used AI or intending to use AI than expected.</p>"
10.1007/978-981-13-2262-4_301-1,Rethinking Learning Design Through Generative AI,N/A
10.54394/fvnq9406,Generative AI and jobs,"<jats:p>This Research Brief, based on ILO Working Paper 96, adds a global perspective to the debate on the labour markets and generative AI. It predicts that the overwhelming effect of the technology will be to augment occupations, rather than to automate them. The greatest impact is likely to be in high and upper-middle income countries due to a higher share of employment in clerical occupations. As clerical jobs are an important source of female employment, the effects are highly gendered. Insights from this study underline the need for proactive policies that focus on job quality, ensure fair transitions, and that are based on dialogue and adequate regulation. This brief provides additional regional and sub-regional estimates, not covered in the main working paper.</jats:p>"
10.1007/s43681-022-00250-9,Fairness–accuracy tradeoff: activation function choice in a neural network,N/A
10.1007/s43681-024-00564-w,"Large language models, politics, and the functionalization of language","<jats:title>Abstract</jats:title><jats:p>This paper critically examines the political implications of Large Language Models (LLMs), focusing on the individual and collective ability to engage in political practices. The advent of AI-based chatbots powered by LLMs has sparked debates on their democratic implications. These debates typically focus on how LLMS spread misinformation and thus hinder the evaluative skills of people essential for informed decision-making and deliberation. This paper suggests that beyond the spread of misinformation, the political significance of LLMs extends to the core of political subjectivity and action. It explores how LLMs contribute to political de-skilling by influencing the capacities of critical engagement and collective action. Put differently, we explore how LLMs shape political subjectivity. We draw from Arendt’s distinction between speech and language and Foucault’s work on counter-conduct to articulate in what sense LLMs give rise to political de-skilling, and hence pose a threat to political subjectivity. The paper concludes by considering how to reconcile the impact of LLMs on political agency without succumbing to technological determinism, and by pointing to how the practice of parrhesia enables one to form one’s political subjectivity in relation to LLMs.</jats:p>"
10.1007/s43681-022-00139-7,Ethical issues deriving from the delayed adoption of artificial intelligence in medical imaging,"<jats:title>Abstract</jats:title><jats:p>Medical imaging (MI) has assumed a central role in medicine. Artificial intelligence (AI) has revolutionized computer vision and it is also approaching to impact deeply MI. Fundamental ethical matters have raised and teams of experts around the world are involved in defining ethical borders for AI in MI. However, reading the extremely detailed proposals, it is clear that the treated ethical arguments have been completely redefined and specifically structured for AI in MI. Instead, many of them should be inherited from other technologies already in use in MI. The complete re-definition of ethical principles could produce contradictions and delays for AI adoption in MI, thus arising important ethical concerns. In this paper, potential ethical issues related to AI delay are presented: the objective is to contribute to reuse some concepts from other technologies to streamline the arguments and avoid these concerns.</jats:p>"
10.1007/s43681-020-00004-5,"Artificial intelligence and disability: too much promise, yet too little substance?",N/A
10.4018/979-8-3693-1351-0.ch015,"GenAI, Robots, and Inclusive Special Education","<jats:p>This chapter examines the role of generative AI (GenAI) and robotics in special education, particularly for autism spectrum disorder (ASD). It highlights the revolutionary impact of GenAI and AI-powered educational technologies (AIEd) in early diagnosis, personalized learning, and fostering independence in individuals with ASD. Additionally, robotics is presented as a vital tool for enhancing social and communication skills. The integration of these technologies in education requires addressing ethical, privacy, and security concerns. The chapter emphasizes technological literacy, including prompt engineering, to optimize the use of GenAI and robotics. It also explores the unique relationship between individuals with ASD and robots, suggesting a promising future for these technologies in special education. The conclusion calls for ongoing research and development to create more equitable and accessible educational systems, harnessing the power of GenAI and robotics for individuals with ASD.</jats:p>"
10.4018/979-8-3693-2440-0.ch012,Advancing Digital Forensics Education With Generative AI for Sustainable Development Goals,"<jats:p>The term “forensics” refers to a broad range of activities that involve gathering, analyzing, and presenting evidence that is admissible in a court of law and are intended to investigate potential entities. In particular, network and computer forensics examine a wide range of data in order to find evidence that can be used in court. During the forensic process, the examination and auditing of computer and network data is essential for gathering data, identifying breaches, and presenting legal evidence. In computer and network forensics, evidence relating to cybercrime is identified, acquired, extracted, examined, analyzed, interpreted, documented, and presented using a methodical, scientific approach. The field of network and computer forensics is constantly expanding, and as crimes move beyond computers and into networks, clouds, and social networks, it is essential to stay current with the newest tools and techniques. The goal of this chapter is to study and explore the methods and tools used in network and computer forensics.</jats:p>"
10.21428/e4baedd9.33bd7449,Learning from Nature to Achieve Material Sustainability: Generative AI for Rigorous Bio-inspired Materials Design,N/A
10.4018/979-8-3693-0502-7.ch004,Advanced Applications of Generative AI and Natural Language Processing Models,"<jats:p>The discourse surrounding AI has sparked widespread discussions, with notable concerns arising from its current limitations. One such concern involves AI models struggling to understand the context of human requests, leading to unexpected or nonsensical outcomes. While instances like the conversation with ChatGPT may have garnered attention, they should be regarded as fascinating illustrations of the model's capabilities rather than indications of meaningful independence. In shaping perspectives on this subject, three noteworthy books have played a pivotal role: Superintelligence by Nick Bostrom, Life 3.0 by Max Tegmark, and A Thousand Brains by Jeff Hawkins. These books offer well-articulated and thought-provoking insights, contributing to the ongoing discussion surrounding AI. By exploring the risks, limitations, and potential implications of AI, it becomes evident that thoughtful consideration and proactive measures are necessary to navigate the evolving landscape of artificial intelligence.</jats:p>"
10.4018/979-8-3693-1351-0.ch021,Automated Assessment and Feedback in Higher Education Using Generative AI,"<jats:p>This chapter explores the integration of generative AI in higher education assessment, addressing the inadequacies of traditional methods in meeting the diverse needs of contemporary learners. It highlights the potential of AI technologies, such as natural language processing and computer vision, to offer personalized, scalable, and insightful evaluations. The chapter critically examines both the enhanced capabilities introduced by AI in educational settings and the ethical challenges it poses. Emphasizing the need for a balanced approach, it suggests synergizing AI's analytical strengths with human expertise to ensure equitable and effective assessments. This work aims to guide educators, administrators, and policymakers through the complexities of AI adoption in academic evaluation, focusing on maintaining academic integrity and inclusivity while leveraging the transformative potential of AI in education.</jats:p>"
10.2139/ssrn.4738738,The Competitive Relationship Between Cloud Computing and Generative AI,N/A
10.62704/j89nfm12,Revising Writing Assignments in Response to Generative AI,"<jats:p>The author describes how she revised writing assessments in the university’s first-year writing sequence to emphasize rhetorical analysis of multimodal texts, prompts to which generative AI and ChatGPT struggle to respond.</jats:p>"
10.1016/b978-0-44-321857-6.00014-x,Synthetic tabular data: copulas vs enhanced GANs,N/A
10.2139/ssrn.4938701,"Generative AI, Plagiarism, and Copyright Infringement in Legal Documents",N/A
10.36227/techrxiv.172101192.26104590/v1,Enhancing Vehicular Networks with Generative AI: Opportunities and Challenges,N/A
10.32388/a8dyj7.2,Creating Image Datasets in Agricultural Environments using DALL.E: Generative AI-Powered Large Language Model,"<jats:p>This research investigated the role of artificial intelligence (AI), specifically the DALL.E model by OpenAI, in advancing data generation and visualization techniques in agriculture. DALL.E, an advanced AI image generator, works alongside ChatGPT's language processing to transform text descriptions and image clues into realistic visual representations of the content. The study used both approaches of image generation: text-to-image and image-to-image (variation). Two types of datasets depicting fruit crop environment and “crop-vs-weed” environment were generated. These AI-generated images were then compared against ground truth images captured by sensors in real agricultural fields. The comparison was based on Peak Signal-to-Noise Ratio (PSNR) and Feature Similarity Index (FSIM) metrics. For fruit crops, image-to-image generation exhibited a 5.78% increase in average PSNR over text-to-image methods, signifying superior image clarity and quality. However, this method also resulted in a 10.23% decrease in average FSIM, indicating a diminished structural and textural similarity to the original images. Conversely, in crop vs weed scenarios, image-to-image generation showed a 3.77% increase in PSNR, demonstrating enhanced image precision, but experienced a slight 0.76% decrease in FSIM, suggesting a minor reduction in feature similarity. Similar to these measures, human evaluation also showed that images generated using image-to-image-based method were more realistic compared to those generated with text-to-image approach. The results highlighted DALL.E's potential in generating realistic agricultural image datasets and thus accelerating the development and adoption of precision agricultural solutions.
</jats:p>"
10.1007/978-94-6265-523-2_17,Generative AI and Intellectual Property Rights,N/A
10.1109/istas52410.2021.9629163,Ethics of AI as practical ethics,N/A
10.1007/979-8-8688-0083-2_8,Ethical Design in an AI-Driven World,N/A
10.31219/osf.io/gdphx,Disabled students’ use of generative AI in Higher Education,"<p>The use of generative AI is controversial in education largely because of its potential impact on academic integrity. Yet some scholars have suggested it could be particularly beneficial for disabled students to support their academic writing. To date there has been no empirical research to discover how disabled students use generative AI. Informed by a prior interview study and AI-literacy model, we surveyed students regarding their use of generative AI, and gained 124 valid responses from disabled students. We identified primary conditions affecting writing as ADHD, dyslexia, dyspraxia, and autism. The main AI tools used are generative AI chatbots, particularly ChatGPT, and rewriting applications. Key concerns disabled students have include the inaccuracy of AI answers, risks to academic integrity, and subscription cost barriers. Students expressed a strong desire to participate in AI policymaking and for universities to provide generative AI training. The paper concludes with recommendations to address educational disparities and foster inclusivity.</p>"
10.2196/preprints.60321,"Generative AI: A Promising, but Cautious, Ally in Children's Mental Health (Preprint)","<sec>
                    <title>UNSTRUCTURED</title>
                        <p>Generative artificial intelligence (AI), a powerful subfield of AI, is rapidly gaining momentum in both private and public sectors, demonstrating its applicability across diverse domains. The healthcare system is no exception, with researchers enthusiastically embracing its potential to provide valuable support across various healthcare domains. This article explores the transformative potential of Generative AI in addressing children’s mental health, considering its promises, challenges, and ethical implications. By highlighting the applications of Generative AI models in detection and support, the article emphasizes the need for cautious implementation due to risks such as bias, hallucination, and privacy concerns. Emphasizing a collaborative approach involving clinicians and robust governance, the article concludes by advocating for responsible integration, acknowledging Generative AI as an augmentation, not a replacement, for human expertise in the realm of children’s mental health.</p>
                </sec>"
10.2196/preprints.59210,Leveraging Generative AI To Improve Motivation and Retrieval in Higher Education Learners (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>Generative artificial intelligence (GAI) presents novel approaches to enhance motivation, curriculum structure and development, and learning and retrieval processes for both learners and instructors. Though a key focus for this emerging technology is academic misconduct, we sought to leverage GAI in curriculum structure to facilitate educational outcomes. For instructors, GAI offers innovative tools within both core course design elements and the broader hidden curriculum while reducing backend time requirements to evaluate outcomes and providing individualized learner feedback. These include innovative instructional designs such as flipped classrooms and gamification, enriching teaching methodologies with focused and interactive approaches, and team-based exercise development, among others. For learners, GAI offers unprecedented self-directed learning opportunities, improved cognitive engagement, and effective retrieval practices, leading to enhanced autonomy, motivation, and knowledge retention. Though empowering, this evolving landscape has integration challenges and ethical considerations, including accuracy, technological evolution, loss of learner voice, and socio-economic disparities. Our experience demonstrates that responsible application of GAI's in educational settings will revolutionize learning practices, making education more accessible and tailored – producing positive motivational outcomes for both learners and instructors. Thus, we argue that leveraging GAI in educational settings will improve outcomes with implications extending from primary through higher and continuing education paradigms.</p>
                </sec>"
10.32388/oqimyi,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.31235/osf.io/fs96d,Understanding student perceptions on the use of generative AI tools in UK Higher Education,"<p>This work investigates students' perceptions of generative AI (GAI) as a tool for coursework completion and its broader impact on learning in higher education (HE). With the increasing integration of GAI in everyday tasks, its potential to transform the learning experience has gained significant attention. However, concerns have been raised relating to the use of GAI as a threat to academic integrity, the lack of accuracy of its outputs, and its potential influence on critical engagement in learning. While HE institutions in the UK have warned against the unregulated use of AI-generated content, the prevailing approach is to guide students in the ethical and transparent use of GAI tools. This research, based on primary data from a questionnaire involving undergraduate and postgraduate students across several UK HE institutions, aims to explore student perspectives on issues such as plagiarism, reliability, and the impact of GAI on the learning process. The findings show that students who identify as men report a higher frequency of GAI usage than women and that international students have more liberal standards regarding academic integrity compared to their UK counterparts.</p>"
10.1108/oxan-db278161,Generative AI carries serious online risks,"<jats:sec sec-type=""significance"">
               <jats:title content-type=""abstract-subheading"">Significance</jats:title>
               <jats:p>These models are fed huge datasets of media content and trained to generate new content following a short thematic prompt. The most hyped -- and feared -- models include OpenAI’s ChatGPT, a conversational agent (or ‘chatbot’) which can generate human-like answers to text prompts, and GPT-4 which extends these capabilities by allowing both text and images to be used as input. </jats:p>
            </jats:sec>
            <jats:sec sec-type=""impacts"">
               <jats:title content-type=""abstract-subheading"">Impacts</jats:title>
               <jats:p>Privacy and safety of AI tools are sensitive policy issues but most democracies will not take Italy’s lead in banning ChatGPT temporarily.</jats:p>
               <jats:p>Smaller extremist groups and low-level cyber criminals are likely to use AI tools for their activities as access barriers are lowered.</jats:p>
               <jats:p>Distrust in mainstream media will increase as AI-generated fake content erodes trust in the entire public information ecosystem. </jats:p>
               <jats:p>Public pressure for AI regulations will rise worldwide; currently, only China and the EU have clear frameworks.</jats:p>
            </jats:sec>"
10.32617/974-650adc3a0091f,Generative AI Can Help Family Businesses Innovate and Grow,N/A
10.4324/9781003464976-3,Human Author vs AI,N/A
10.1016/b978-0-44-321857-6.00020-5,Perturbed lattice point process: alternative to GMM,N/A
10.1038/s42256-024-00862-2,Will generative AI transform robotics?,N/A
10.1007/s44223-023-00038-9,Toward a generative AI-augmented design era,N/A
10.31234/osf.io/49ags,Chrono-Sampling: Generative AI Enabled Time Machine for Public Opinion Data Collection,"<p>This paper introduces ""Chrono-sampling,"" a novel method leveraging Large Language Models (LLMs) to simulate historical survey respondents, enabling social science researchers to explore past public opinions as if they had access to a ""time machine."" The study builds on recent advancements in generative AI, particularly LLMs like OpenAI's GPT, which have demonstrated the ability to mimic human attitudes and behaviors. By employing techniques such as ""time-gating"" and ""Clio contexts,"" we restrict LLMs' knowledge to specific historical periods and provide them with context-rich backstories to enhance the realism of their simulated responses. Utilizing data from the American National Election Studies (ANES), we replicated sociopolitical attitudes from key historical periods, all the way back to the Reagan era. Our results indicate that LLM-generated ""silicon"" samples can effectively mirror the dynamic relationships observed in human responses, particularly in how retrospective and prospective economic evaluations shift with political and economic changes. This method opens new avenues for historical research, allowing scholars to generate and analyze synthetic data from periods and contexts where traditional data collection is unfeasible. This pilot study also highlights the potential and limitations of using AI in social science research, emphasizing the need for careful methodological considerations when interpreting AI-generated data.</p>"
10.2139/ssrn.4778082,Consumer Reactions to Perceived Undisclosed Generative Ai Usage in an Online Review Context,N/A
10.4018/979-8-3693-3731-8.ch002,Evolution of Generative AI in Healthcare,"<jats:p>AI in healthcare is not a coincidental development; rather, it is a revolutionary force that has the potential to completely alter patient experiences and medical practices. Artificial intelligence (AI) methods, and processes of natural language, especially machine learning, are progressively effective in the health profession and education. Generative AI is one of the common types of artificial intelligence that should be used for making and creating new data, such as text, images, and music. This technology could transform completely healthcare by contributing to new approaches to improve the care of the patient, develop new treatments, and diagnose diseases. Nowadays, Generative AI plays a significant part in healthcare to improve mental health, New drug discovery, personalized medicine, Improved diagnosis, More efficient surgery, and enhanced rehabilitation. Potential hazards, however, include the use of biases or inaccurate statistics for algorithms to train them, relying too heavily on generative AI lacking supervision of humans, concealment issues with patients, besides the moral ramifications of applying AI to decision-making for medical. Oversight and careful regulation are required to assurance that the generative AI's promises in neuro-interventional surgeries are appreciated while minimizing its potential perils. AI has had a huge impact on healthcare, changing the sector in a variety of ways. In the end, it has saved lives by lowering expenses and raising the standard of care. We accept that true artificial intelligence (AI) will have a big impact on healthcare services in the future. Machine learning is the prime technology forward precision medicine, which is commonly perceived as a much-needed advancement in healthcare. We trust AI will eventually become accomplished in providing that diagnosis and treatment recommendations, even still early creativities in that area have demonstrated stimulating.</jats:p>"
10.2523/iptc-23466-ms,"Enhancing Predictive Maintenance in an Oil &amp; Gas Refinery Using IoT, AI &amp; ML: An Generative AI Solution","<jats:title>Abstract</jats:title>
               <jats:p>Oil and gas refinery operations are under constant pressure to enhance efficiency and ensure uninterrupted processing. The adoption of predictive maintenance strategies has emerged as a pivotal solution, enabling real-time anomaly detection, predicting pressure fluctuations, and monitoring asset health. An illuminating example hails from a downstream operator in Western Australia that strategically harnesses the power of IoT and AI/ML. For them, revenue hinges on the streamlined delivery of gas processing services to customers, amplifying the significance of process efficiency gains. Leveraging on-site equipment data analysis, this approach significantly minimizes on-site maintenance requirements and automates back-office tasks, reducing manual data analysis and response generation in maintenance permit systems. The technical infrastructure involves wireless sensor-enabled data collection transmitted to a centralized hub, where machine learning algorithms detect equipment defects. Rapid reporting of these defects to decision-makers, accompanied by contextual insights, empowers swift, informed decision-making. This innovative solution has expanded business horizons, enabling the processing of gas for external entities alongside producing their reservoir gas in the downstream processing plant.</jats:p>"
10.1007/979-8-8688-0282-9_11,"Autonomous AI Agents: Decision-Making, Data, and Algorithms",N/A
10.1007/979-8-8688-0456-4_10,Tips and Tricks for Prompt Engineering,N/A
10.1162/99608f92.ff6335af,Scaling Up Mischief: Red-Teaming AI and Distributing Governance,N/A
10.3390/publications11030045,Children of AI: A Protocol for Managing the Born-Digital Ephemera Spawned by Generative AI Language Models,"<jats:p>The recent public release of the generative AI language model ChatGPT has captured the public imagination and has resulted in a rapid uptake and widespread experimentation by the general public and academia alike. The number of academic publications focusing on the capabilities as well as practical and ethical implications of generative AI has been growing exponentially. One of the concerns with this unprecedented growth in scholarship related to generative AI, in particular, ChatGPT, is that, in most cases, the raw data, which is the text of the original ‘conversations,’ have not been made available to the audience of the papers and thus cannot be drawn on to assess the veracity of the arguments made and the conclusions drawn therefrom. This paper provides a protocol for the documentation and archiving of these raw data.</jats:p>"
10.4018/979-8-3693-1950-5.ch010,Generative AI Methodology for Producing Assisted Art,"<jats:p>Generative artificial intelligence is an emerging technology that has impacted society recently and with productivity gains in the coming years. This technology learns from existing data to generate high-quality content such as text, images, music, speech, videos, and code. As part of the creative industries, collaboration between generative algorithms and human creativity brings challenges due to producing content and artistic images in response to natural language requests. Thus, this chapter establishes a methodology based on text-to-image generative artificial intelligence for producing assisted art in contextualized cultural content to generate unique and diverse images of the historical-cultural identity of southern Brazil. The steps of the methodology that can portray elements of Brazil's biome, cultural, and historical events are dataset assembling, model training and fine-tuning, and content enhancement and post-processing. In addition, the methodology allows an immersed experience with people being transported in period clothing and within a historical or cultural setting.</jats:p>"
10.19033/sks.2023.12.82.179,Interpreting the Meaning of Unethical Text Data with Generative AI.,N/A
10.1007/978-3-031-54497-2_5,Generative AI and Large Language Modeling in Cybersecurity,N/A
10.1007/978-3-031-67991-9_2,The AI Ecosystem,N/A
10.21275/sr24506101818,"""Challenges in AI - Powered 3D Modeling and Solutions"" - Investigating the Technical Hurdles and Breakthroughs in Applying Generative AI to the Task of Creating Three - Dimensional Digital Objects",N/A
10.1056/aipc2400237,Co-creating Consent for Data Use — AI-Powered Ethics for Biomedical AI,N/A
10.1109/ethics57328.2023.10155069,Perceptions of AI Ethics on Social Media,N/A
10.1515/9783111425078-007,7 Reinforcement Learning from Human Feedback (RLHF),N/A
10.1007/s00146-023-01840-9,How will the state think with ChatGPT? The challenges of generative artificial intelligence for public administrations,N/A
10.7551/mitpress/12549.003.0001,Series Foreword,N/A
10.33767/osf.io/qsxj3,AI Ethics as Subordinated Innovation Network,"<p>AI ethics is proposed, by the Big Tech companies which lead AI research and development, as the cure for diverse social problems posed by the commercialization of data-intensive technologies. It aims to reconcile capitalist AI production with ethics. However, AI ethics is itself now the subject of wide criticism; most notably, it is accused of being no more than “ethics washing” - a cynical means of dissimulation for Big Tech, while it continues its business operations unchanged. This paper aims to critically assess, and go beyond the ethics washing thesis. I argue that AI ethics is indeed ethics washing, but not only that. It has a more significant economic function for Big Tech. To make this argument I draw on the theory of intellectual monopoly capital. I argue that ethics washing is better understood as a subordinated innovation network: a dispersed network of contributors beyond Big Tech’s formal employment whose research is indirectly planned by Big Tech, which also appropriates its results. These results are not intended to render AI more ethical, but rather to advance the business processes of intellectual monopoly capitals. Because the parameters of AI ethics are indirectly set in advance by Big tech, the ostensible goal that AI ethics sets for itself-to resolve the contradiction between business and ethics-is in fact insoluble. I demonstrate this via an analysis of the latest trend in AI ethics: the operationalization of ethical principles.</p>"
10.1007/s43681-024-00536-0,Minimum levels of interpretability for artificial moral agents,"<jats:title>Abstract</jats:title><jats:p>As artificial intelligence (AI) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. For models involved in moral decision-making (MDM), also known as artificial moral agents (AMA), interpretability provides a way to trust and understand the agent’s internal reasoning mechanisms for effective use and error correction. In this paper, we bridge the technical approaches to interpretability with construction of AMAs to establish minimal safety requirements for deployed AMAs. We begin by providing an overview of AI interpretability in the context of MDM, thereby framing different levels of interpretability (or transparency) in relation to the different ways of constructing AMAs. Introducing the concept of the Minimum Level of Interpretability (MLI) and drawing on examples from the field, we explore two overarching questions: whether a lack of model transparency prevents trust and whether model transparency helps us sufficiently understand AMAs. Finally, we conclude by recommending specific MLIs for various types of agent constructions, aiming to facilitate their safe deployment in real-world scenarios.</jats:p>"
10.1145/3375627.3375811,Ethics for AI Writing,N/A
10.1007/s43681-020-00026-z,Societal bias reinforcement through machine learning: a credit scoring perspective,"<jats:title>Abstract</jats:title><jats:p>Does machine learning and AI ensure that social biases thrive? This paper aims to analyze this issue. Indeed, as algorithms are informed by data, if these are corrupted, from a social bias perspective, good machine learning algorithms would learn from the data provided and reverberate the patterns learnt on the predictions related to either the classification or the regression intended. In other words, the way society behaves whether positively or negatively would necessarily be reflected by the models. In this paper, we analyze how social biases are transmitted from the data into banks loan approvals by predicting either the gender or the ethnicity of the customers using the exact same information provided by customers’ through their applications</jats:p>"
10.1007/978-3-030-51110-4_2,What Is AI?,N/A
10.1007/s43681-021-00128-2,Safety criticism and ethical dilemma of autonomous vehicles,N/A
10.7551/mitpress/12549.003.0005,All about the Human,N/A
10.2139/ssrn.4520945,From AI Ethics Principles to Practices: A Teleological Methodology to Apply AI Ethics Principles in The Defence Domain,N/A
10.1007/s43681-024-00538-y,Author Correction: Optimizing fairness and accuracy: a pareto optimal approach for decision-making,N/A
10.1007/s43681-023-00294-5,Guidance for researchers and peer-reviewers on the ethical use of Large Language Models (LLMs) in scientific research workflows,N/A
10.1007/s43681-022-00253-6,Engineering a social contract: Rawlsian distributive justice through algorithmic game theory and artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>The potential for artificial intelligence algorithms and game theory concepts to offer prescriptive and decision-making capability for humankind is increasingly recognized. This derives from the increasing availability of granular, multivariable, well-curated data offering analytical insights for necessarily complex human behaviors and activities. Of the multitude of situations that this decision-making aptitude presents, the application to governmental policy offers a commanding case. This would allow decisions to be made for the benefit of societies and citizens based on rigorous objective information devoid of the traditional approach of choosing policies and societal values based on the opinion of a handful of selected representatives who may be exposed to a lack of comprehensive data analysis capacity and subject to personal biases. There would need to be a critical requirement of wider socially responsible data practices here, beyond those of technical considerations and the incorporation of wider societal fairness approaches. Amongst the schools of political thought particularly acquiescent to the application by this approach would be the egalitarian approach of John Rawls. Here an Original Position’s pre-determination tool of Veil of Ignorance and ensuing Difference Principal presents a method of distributive justice that can be clearly mathematically defined in economics theory through Wald’s Maximin principle. This offers an opportunity to apply algorithmic game theory and artificial intelligence computational approaches to implement Rawlsian distributive justice that are presented and discussed. The outputs from the algorithmic acquaintance of Rawlsian egalitarianism with applicable state data, protected with appropriate privacy, security, legal, ethical and social governance could in turn lead to automated direct governmental choices and an objective Social Contract for citizens of digitally literate nations.</jats:p>"
10.61969/jai.1512906,"Super AI, Generative AI, Narrow AI and Chatbots: An Assessment of Artificial Intelligence Technologies for The Public Sector and Public Administration","<jats:p xml:lang=""en"">Artificial intelligence encompasses a wide range of approaches, methodologies, and techniques aimed at mimicking human intelligence in machines. In recent times, the concepts of Generative Artificial Intelligence (AI), Super AI, and Narrow AI have attracted considerable attention. Undoubtedly, the success of ChatGPT in capturing all attention has played a significant role in this. Artificial intelligence technology has a profound impact on all sectors, and sector representatives are striving to adapt to this technology more quickly. It is projected that artificial intelligence could generate an economic size of 13 trillion American dollars by 2030. Developments in artificial intelligence technologies undoubtedly lead to significant improvements in the functioning of public institutions and access for citizens. Artificial intelligence has the potential to be used in many public services, including security and defense, healthcare services, education, transportation and infrastructure, environmental and natural resource management, law and justice systems, among others. Therefore, evaluating the types of artificial intelligence, Narrow AI applications, and chatbots for public use is seen as highly beneficial from the perspective of public administration and the public sector. In our study, the topics of super artificial intelligence, generative artificial intelligence, narrow artificial intelligence, and chatbots have been extensively evaluated within the context of the public sector and public administration. Utilizing findings from both Turkish and English literature reviews, the importance and potential impacts of artificial intelligence within the public sector, along with current trends, have been comprehensively assessed. This research delves into the concepts of artificial intelligence and its subsets—super AI, generative AI, narrow AI, and chatbots—within the general framework of the public sector. China and the United States are pioneering and leading countries in terms of investment. Although the U.S. stands out in many areas regarding investment, China's integration of artificial intelligence with national strategies and its policies indicate that it may play a more dominant role in the future. There are four main implementation areas of artificial intelligence in the public sector: efficiency and automation, service delivery, data-driven governance, and ethical and regulatory challenges. A review of the literature reveals that the ethical, legal, and social implications of implementing artificial intelligence in the public sector require more careful consideration. The study makes a significant contribution to the field of artificial intelligence discussions in public administration and the public sector, providing a comprehensive assessment of current discussions on artificial intelligence in the literature.</jats:p>"
10.1007/s43681-023-00370-w,Ensuring a ‘Responsible’ AI future in India: RRI as an approach for identifying the ethical challenges from an Indian perspective,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) can be seen to be at an inflexion point in India, a country which is keen to adopt and exploit new technologies, but needs to carefully consider how they do this. AI is usually deployed with good intentions, to unlock value and create opportunities for the people; however it does not come without its challenges. There are a set of ethical–social issues associated with AI, which include concerns around privacy, data protection, job displacement, historical bias and discrimination. Through a series of focus groups with knowledgeable people embedded in India and its culture, this research explores the ethical–societal changes and challenges that India now faces. Further, it investigates whether the principles and practices of responsible research and innovation (RRI) might provide a framework to help identify and deal with these issues. The results show that the areas in which RRI could offer scope to improve this outlook include education, policy and governance, legislation and regulation, and innovation and industry practices. Some significant challenges described by participants included: the lack of awareness of AI by the public as well as policy makers; India’s access and implementation of Western datasets, resulting in a lack of diversity, exacerbation of existing power asymmetries, increase in social inequality and the creation of bias; the potential replacement of jobs by AI. One option was to look at a hybrid approach, a mix of AI and humans, with expansion and upskilling of the current workforce. In terms of strategy, there seems to be a gap between the rhetoric of the government and what is seen on the ground, and therefore going forward there needs to be a much greater engagement with a wider audience of stakeholders.</jats:p>"
10.1007/s43681-022-00244-7,In defense of ethical guidelines,"<jats:title>Abstract</jats:title><jats:p>Recently, Luke Munn attacked “AI ethics” generally, or guidelines, principles, codes of ethics, ethical frameworks. In particular, he argued that ethical guidelines are useless. Here I respond to this critique, arguing that Munn’s criticism is mostly unfair and misguided, and that his own proposal is already implemented in various guidelines.</jats:p>"
10.1007/s43681-022-00133-z,Responsibility assignment won’t solve the moral issues of artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>Who is responsible for the events and consequences caused by using artificially intelligent tools, and is there a gap between what human agents can be responsible for and what is being done using artificial intelligence? Both questions presuppose that the term ‘responsibility’ is a good tool for analysing the moral issues surrounding artificial intelligence. This article will draw this presupposition into doubt and show how reference to responsibility obscures the complexity of moral situations and moral agency, which can be analysed with a more differentiated toolset of moral terminology. It suggests that the impression of responsibility gaps only occurs if we gloss over the complexity of the moral situation in which artificial intelligent tools are employed and if—counterfactually—we ascribe them some kind of pseudo-agential status.</jats:p>"
10.1007/s43681-020-00018-z,MIT’s moral machine project is a psychological roadblock to self-driving cars,N/A
10.1007/s43681-024-00466-x,Bringing clarity and transparency to the consultative process underpinning the implementation of an ethics framework for AI-based healthcare applications: a qualitative study,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) has been applied in healthcare to address various aspects of the COVID-19 crisis including early detection, diagnosis and treatment, and population monitoring. Despite the urgency to develop AI solutions for COVID-19 problems, considering the ethical implications of those solutions remains critical. Implementing ethics frameworks in AI-based healthcare applications is a wicked issue that calls for an inclusive, and transparent participatory process. In this qualitative study, we set up a participatory process to explore assumptions and expectations about ethical issues associated with development of a COVID-19 monitoring AI-based app from a diverse group of stakeholders including patients, physicians, and technology developers. We also sought to understand the influence the consultative process had on the participants’ understanding of the issues. Eighteen participants were presented with a fictitious AI-based app whose features included individual self-monitoring of potential infection, physicians’ remote monitoring of symptoms for patients diagnosed with COVID-19 and tracking of infection clusters by health agencies. We found that implementing an ethics framework is systemic by nature, and that ethics principles and stakeholders need to be considered in relation to one another. We also found that the AI app introduced a novel channel for knowledge between the stakeholders. Mapping the flow of knowledge has the potential to illuminate ethical issues in a holistic way.</jats:p>"
10.1007/s43681-022-00234-9,AI and housing discrimination: the case of mortgage applications,N/A
10.1007/s43681-022-00138-8,AI bias: exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry,N/A
10.1007/s43681-022-00159-3,Risk as a driver for AI framework development on manufacturing,"<jats:title>Abstract</jats:title><jats:p>Incorporating ethics and values within the life cycle of an AI asset means to secure, under these perspectives, its development, deployment, use and decommission. These processes must be done safely, following current legislation, and incorporating the social needs towards having greater well-being over the agents and environment involved. Standards, frameworks and ethical imperatives—which are also considered a backbone structure for legal considerations—drive the development process of new AI assets for industry. However, given the lack of concrete standards and robust AI legislation, the gap between ethical principles and actionable approaches is still considerable. Different organisations have developed various methods based on multiple ethical principles to facilitate practitioners developing AI components worldwide. Nevertheless, these approaches can be driven by a self-claimed ethical shell or without a clear understanding of the impacts and risks involved in using their AI assets. The manufacturing sector has produced standards since 1990’s to guarantee, among others, the correct use of mechanical machinery, workers security, and environmental impact. However, a revision is needed to blend these with the needs associated with AI’s use. We propose using a vertical-domain framework for the manufacturing sector that will consider ethical perspectives, values, requirements, and well-known approaches related to risk management in the sector.</jats:p>"
10.1007/s00146-024-01934-y,"Sticks and stones may break my bones, but words will never hurt me!—Navigating the cybersecurity risks of generative AI",N/A
10.3390/info15060325,"Generative AI, Research Ethics, and Higher Education Research: Insights from a Scientometric Analysis","<jats:p>In the digital age, the intersection of artificial intelligence (AI) and higher education (HE) poses novel ethical considerations, necessitating a comprehensive exploration of this multifaceted relationship. This study aims to quantify and characterize the current research trends and critically assess the discourse on ethical AI applications within HE. Employing a mixed-methods design, we integrated quantitative data from the Web of Science, Scopus, and the Lens databases with qualitative insights from selected studies to perform scientometric and content analyses, yielding a nuanced landscape of AI utilization in HE. Our results identified vital research areas through citation bursts, keyword co-occurrence, and thematic clusters. We provided a conceptual model for ethical AI integration in HE, encapsulating dichotomous perspectives on AI’s role in education. Three thematic clusters were identified: ethical frameworks and policy development, academic integrity and content creation, and student interaction with AI. The study concludes that, while AI offers substantial benefits for educational advancement, it also brings challenges that necessitate vigilant governance to uphold academic integrity and ethical standards. The implications extend to policymakers, educators, and AI developers, highlighting the need for ethical guidelines, AI literacy, and human-centered AI tools.</jats:p>"
10.1007/s00146-020-01087-8,AI ethics – a review of three recent publications,N/A
10.1007/979-8-8688-0158-7_9,Scalable AI Governance and Ethics,N/A
10.1007/s43681-023-00257-w,A pragmatic perspective on AI transparency at workplace,"<jats:title>Abstract</jats:title><jats:p>Recently, artificial intelligence (AI) systems have been widely used in different contexts and professions. However, with these systems developing and becoming more complex, they have transformed into black boxes that are difficult to interpret and explain. Therefore, urged by the wide media coverage of negative incidents involving AI, many scholars and practitioners have called for AI systems to be transparent and explainable. In this study, we examine transparency in AI-augmented settings, such as in workplaces, and perform a novel analysis of the different jobs and tasks that can be augmented by AI. Using more than 1000 job descriptions and 20,000 tasks from the O*NET database, we analyze the level of transparency required to augment these tasks by AI. Our findings indicate that the transparency requirements differ depending on the augmentation score and perceived risk category of each task. Furthermore, they suggest that it is important to be pragmatic about transparency, and they support the growing viewpoint regarding the impracticality of the notion of full transparency.</jats:p>"
10.1007/s00146-023-01754-6,The ethics of ex-bots,N/A
10.1007/s43681-022-00208-x,AI for hiring in context: a perspective on overcoming the unique challenges of employment research to mitigate disparate impact,"<jats:title>Abstract</jats:title><jats:p>Commentators interested in the societal implications of automated decision-making often overlook how decisions are made in the technology’s absence. For example, the benefits of ML and big data are often summarized as efficiency, objectivity, and consistency; the risks, meanwhile, include replicating historical discrimination and oversimplifying nuanced situations. While this perspective tracks when technology replaces capricious human judgements, it is ill-suited to contexts where standardized assessments already exist. In spaces like employment selection, the relevant question is how an ML model compares to a manually built test. In this paper, we explain that since the Civil Rights Act, industrial and organizational (I/O) psychologists have struggled to produce assessments without disparate impact. By examining the utility of ML for conducting exploratory analyses, coupled with the back-testing capability offered by advances in data science, we explain modern technology’s utility for hiring. We then empirically investigate a commercial hiring platform that applies several oft-cited benefits of ML to build custom job models for corporate employers. We focus on the disparate impact observed when models are deployed to evaluate real-world job candidates. Across a sample of 60 jobs built for 26 employers and used to evaluate approximately 400,00 candidates, minority-weighted impact ratios of 0.93 (Black–White), 0.97 (Hispanic–White), and 0.98 (Female–Male) are observed. We find similar results for candidates selecting disability-related accommodations within the platform versus unaccommodated users. We conclude by describing limitations, anticipating criticisms, and outlining further research.</jats:p>"
10.1007/s43681-024-00494-7,Geo-political bias in fake news detection AI: the case of affect,"<jats:title>Abstract</jats:title><jats:p>There have been massive advances in AI technologies towards addressing the contemporary challenge of fake news identification. However, these technologies, as observed widely, have not had the same kind or depth in impact across global societies. In particular, the AI scholarship in fake news detection arguably has not been as beneficial or appropriate for Global South, bringing geo-political bias into the picture. While it is often natural to think of data bias as the potential reason for geo-political bias, other factors could be much more important in being more latent, and thus less visible. In this commentary, we investigate as to how the facet of affect, comprising emotions and sentiments, could be a potent vehicle for geo-political biases in AI. We highlight, through assembling and interpreting insights from literature, the overarching neglect of affect across methods for fake news detection AI, and how this could be a potentially important factor for geo-political bias within them. This exposition, we believe, also serves as a first effort in understanding how geo-political biases work within AI pipelines beyond the data collection stage.</jats:p>"
10.1007/s10639-024-12949-9,"Exploring the impact of generative AI-based technologies on learning performance through self-efficacy, fairness &amp; ethics, creativity, and trust in higher education",N/A
10.1007/s43681-022-00161-9,"Deep learning in radiology: ethics of data and on the value of algorithm transparency, interpretability and explainability","<jats:title>Abstract</jats:title><jats:p>AI systems are quickly being adopted in radiology and, in general, in healthcare. A myriad of systems is being proposed and developed on a daily basis for high-stake decisions that can lead to unwelcome and negative consequences. AI systems trained under the supervised learning paradigm greatly depend on the quality and amount of data used to develop them. Nevertheless, barriers in data collection and sharing limit the data accessibility and potential ethical challenges might arise due to them leading, for instance, to systems that do not offer equity in their decisions and discriminate against certain patient populations or that are vulnerable to appropriation of intellectual property, among others. This paper provides an overview of some of the ethical issues both researchers and end-users might meet during data collection and development of AI systems, as well an introduction to the current state of transparency, interpretability and explainability of the systems in radiology applications. Furthermore, we aim to provide a comprehensive summary of currently open questions and identify key issues during the development and deployment of AI systems in healthcare, with a particular focus on the radiology area.</jats:p>"
10.1007/979-8-8688-0083-2_4,The Evolving Role of AI: Automation and Beyond,N/A
10.4018/979-8-3693-1565-1.ch003,Bias and Fairness in AI Technology,"<jats:p>This chapter's objective is to provide an overview of how artificial intelligence (AI) has become an essential part of human life. It explains the sources of bias and its types in AI technology. With the help of previous studies, the chapter elucidates the strategies that can be used to avoid decision-making as a source of bias in AI technology. It also talks about the importance of understanding how human bias can also cause AI systems to exhibit bias towards certain groups. Unfairness in AI is also one of the most common sources of biassed data, and it's explained with strategies for detecting and addressing unfairness. The chapter also covers the need for transparency in AI technology along with ethical considerations, as transparency in AI is essential to ensuring that AI systems operate in adherence to ethical standards.</jats:p>"
10.21428/e4baedd9.0319e3a6,"Labeling AI-Generated Content: Promises, Perils, and Future Directions",N/A
10.1007/979-8-8688-0083-2_7,Toward Interfaceless Experiences: Leveraging AI and Spatial Computing,N/A
10.2139/ssrn.4676705,Generative AI and Creator Economy: Investigating the Effects of AI-Generated Voice on Online Video Creation,N/A
10.35542/osf.io/xguzk,"Towards the Triad of Digital Literacy, Data Literacy and AI Literacy in Teacher Education – A Discussion in Light of the Accessibility of Novel Generative AI","<p>The training of future-relevant competencies – so called Future Skills or 21st Century Skills – is currently moving to the center of the educational discourse in light of increasing technological developments and accessibility of AI systems, e.g. ChatGPT. It is our view that bringing together, thinking about and promoting Digital-, Data- and AI literacy in teaching is a major concern of future education. We focus in particular on teacher education, since teachers have a dual role: on the one hand, pre-service teachers are also learners who must first develop the appropriate competencies themselves, and on the other hand, they are multipliers as teachers in schools who must impart the appropriate competencies. Particularly in the context of novel generative AI systems and large language models, it is important to take a holistic and critical-reflective look at them from a technological, socio-cultural and user-oriented perspective.</p>"
10.1016/j.bushor.2024.04.008,Game changers: A generative AI prompt protocol to enhance human-AI knowledge co-construction,N/A
10.1117/12.3011374,Deep HoriXons: 3D virtual generative AI assisted campus for deep learning AI and cybersecurity,N/A
10.1007/978-3-031-67991-9_1,AI and Digital Education,N/A
10.2139/ssrn.4469507,Using Generative AI in ESP Classes to Teach Formal Style,N/A
10.2139/ssrn.4758339,Defining Data: Improving Terminology Around Generative AI Models,N/A
10.31468/dwr.1093,Two perspectives on generative AI now,"<jats:p>Two recently published books present perspectives on the impact of generative Artificial Intelligence on writing. Baron’s book attempts to discuss the question of how we might maintain distinctions between the writing done by humans versus that done by genAI, while Tenen’s book takes a historical view of the use of technologies in writing and what that implies for writers currently.</jats:p>"
10.1007/978-981-97-3187-9_6,Generative AI and Causality,N/A
10.2139/ssrn.4723644,Rating the Responses to Legal Questions by Generative AI Models,N/A
10.32617/902-641b18d332731,Leveraging Generative AI Tools Like ChatGPT for Startups and Small Business Growth,N/A
10.31219/osf.io/rkpt4,"Towards a Unified Open Education Ecosystem through Generative AI, Blockchain, DAO, MMLA and NFT","<p>The aim of this research was to explore how DAOs, blockchain, NFTs, generative AI, and multimodal learning analytics technologies can be integrated to create a unified open education ecosystem representing a global, seamless, personalized, and democratic education system. The integration of these technologies can help to overcome the challenges faced by today's education systems and enhance accessibility, affordability, and quality of education worldwide. Educational technologies have never been closer to overcoming the issues of democracy and accessible education, which are among the biggest handicaps of contemporary education systems. Moreover, the successful integration of these technologies can create unique opportunities for personalized learning. Initiatives exemplifying this timely paradigm shift, such as The Open Campus project, have already begun to emerge. In this study, the potential uses of these technologies for a unified education ecosystem were examined with a scoping review of 30 most related studies. Afterwards, contributions, possible task distributions, and integration of these technologies were explained using concrete examples and real-world scenarios. At the end of the study, 12 key benefits of this unified open education system were explained: accessibility, personalization, educational quality, security and transparency, democratic governance, intellectual property and content value, open collaboration and integration, equality and equal opportunity, reusability of educational resources, preservation and promotion of cultural and linguistic diversity, innovation and experiential learning in education, and support for student success and career planning.</p>"
10.3390/electronics13173563,Leveraging Generative AI in Short Document Indexing,"<jats:p>The efficiency of information retrieval systems primarily depends on the effective representation of documents during query processing. This representation is mainly constructed from relevant document terms identified and selected during their indexing, which are then used for retrieval. However, when documents contain only a few features, such as in short documents, the resulting representation may be information-poor due to a lack of index terms and their lack of relevance. Although document representation can be enriched using techniques like word embeddings, these techniques require large pre-trained datasets, which are often unavailable in the context of domain-specific short documents. This study investigates a new approach to enrich document representation during indexing using generative AI. In the proposed approach, relevant terms extracted from documents and preprocessed for indexing are enriched with a list of key terms suggested by a large language model (LLM). After conducting a small benchmark of several renowned LLM models for key term suggestions from a set of short texts, the GPT-4o model was chosen to experiment with the proposed indexing approach. The findings of this study yielded notable results, demonstrating that generative AI can efficiently fill the knowledge gap in document representation, regardless of the retrieval technique used.</jats:p>"
10.1016/j.econlet.2023.111315,Task-interdependencies between Generative AI and Workers,N/A
10.7206/kp.2080-1084.591,Generative AI – a Threat or an Opportunity for Universities,N/A
10.1016/b978-0-44-321857-6.00007-2,Gentle introduction to linear algebra – synthetic time series,N/A
10.1007/979-8-8688-0282-9,Understanding Generative AI Business Applications,N/A
10.2139/ssrn.4517702,How Generative Ai Turns Copyright Law on its Head,N/A
10.1002/9781394308286.part3,"Envisioning a Financial Future with
            <scp>AI</scp>",N/A
10.18357/otessaj.2024.4.1.59,"Generative AI Adoption in Postsecondary Education, AI Hype, and ChatGPT’s Launch","<jats:p>The rapid integration of generative artificial intelligence (AI) into postsecondary education and many other sectors resulted in a global reckoning with this new technology. This paper contributes to the study of the multifaceted influence of generative AI, with a particular focus on OpenAI's ChatGPT within academic settings during the first six months after the release in three specific ways. First, it scrutinizes the rise of ChatGPT as a transformative event construed through a study of mainstream discourses exhibiting AI hype. Second, it discusses the perceived implications of generative AI for writing, teaching, and learning through the lens of critical discourse analysis and critical AI studies. Third, it encourages the necessity for best practices in the adoption of generative AI technologies in education.</jats:p>"
10.1007/s00146-024-02034-7,Regulating generative AIs: (Re)defining video games as cultural products,N/A
10.1515/9780691249643-004,4 The Long Road to Generative AI,N/A
10.29173/irie417,Ethics and AI Innovation,"<jats:p>The Fourth Industrial Revolution has astonishing potential to solve many of humanity’s problems, but it has also brought about an array of new threats. The challenge is to find a way to mitigate the negatives of the Revolution without impairing the extraordinary potential of AI to accelerate all areas of human development. AI ethics offers a possible basis for doing so by providing a set of aspirational ideals as to the role of AI, rather than a minimum standard for compliance which is likely to become increasingly irrelevant. Throughout history, humans have adapted and adjusted to the technologies of the time and though the integration of AI into all human experience and decision-making will come to be seen as normal and taken for granted, there will still be a number of profound ethical choices that must be made. Implementing ethical AI will require a multi-modal and co-regulatory approach. There are a variety of existing approaches but some common principles have emerged. These provide a framework for action.
 </jats:p>"
10.1101/2022.01.10.22269025,Medical domain knowledge in domain-agnostic generative AI,"<jats:title>Abstract</jats:title><jats:p>The text-guided diffusion model GLIDE (Guided Language to Image Diffusion for Generation and Editing) is the state of the art in text-to-image generative artificial intelligence (AI). GLIDE has rich representations, but medical applications of this model have not been systematically explored. If GLIDE had useful medical knowledge, it could be used for medical image analysis tasks, a domain in which AI systems are still highly engineered towards a single use-case. Here we show that the publicly available GLIDE model has reasonably strong representations of key topics in cancer research and oncology, in particular the general style of histopathology images and multiple facets of diseases, pathological processes and laboratory assays. However, GLIDE seems to lack useful representations of the style and content of radiology data. Our findings demonstrate that domain-agnostic generative AI models can learn relevant medical concepts without explicit training. Thus, GLIDE and similar models might be useful for medical image processing tasks in the future - particularly with additional domain-specific fine-tuning.</jats:p>"
10.1201/9781003450139-15,Generative AI as a Disruptor,N/A
10.36227/techrxiv.172107629.91013985/v1,Efficient Resource Management in Vehicular Networks Using Digital Twins and Generative AI,N/A
10.32388/vwg411,"Review of: ""Towards Responsible AI-Assisted Scholarship: Comparative Assessment of Generative Models and Adoption Recommendations""",N/A
10.4324/9781003464976-2,AI and the Creative Industries,N/A
10.2139/ssrn.4634992,Elaborating a Human Rights friendly Copyright Framework for Generative AI,N/A
10.1016/j.bushor.2024.02.010,How organizations can innovate with generative AI,N/A
10.21125/inted.2024.0112,CONSIDERING GENERATIVE AI FOR ACCESSIBILITY IN INTELLIGENT EDUCATIONAL SYSTEMS,N/A
10.1007/s11894-024-00946-4,Generative AI in Pediatric Gastroenterology,N/A
10.31219/osf.io/987ra,"Generative AI: a problematic illustration of the intersections of racialized gender, race, ethnicity","<p>This paper investigates how generative AI tools such as DALL-E can create imagery which (re)produce racist, gendered and racially gendered representations of peoples. This paper gives educators insights into the affordances of using generative AI for creating illustrations and images while simultaneously warning them to exercise caution through always taking a critical and ethical stance when using generative AI. By employing a decolonial lens, I use a combination of critical visual analysis and critical race semiotics to analyze a series of 16 images produced by DALL-E to demonstrate the problematic nature of using generative AI for creating representations of people. I outline the prompts employed and then provide a qualitative analysis of the images that critiques what is generated with reference to literature. Following this, I discuss the problematic nature of how racially gendered avatars are portrayed and how these creations are underpinned by contemporary values that are racist in nature.</p>"
10.1097/nmg.0000000000000056,Generative AI in clinical practice and operations,N/A
10.21203/rs.3.rs-4607113/v1,Navigating Ethical Frameworks to Mitigate Academic Misconduct While Leveraging Generative AI,"<title>Abstract</title>
        <p>The rapid advancement of Generative AI in academia raises ethical concerns about academic integrity. This study aims to delineate the key ethical concerns prevalent in academia and propose a theoretical framework that incorporates deontological ethics for learners and teleological ethics for evaluators. Employing a qualitative methodology and thematic analysis, this research undertakes a systematic scoping review of scholarly articles. The researcher searched various academic databases, following specific inclusion and exclusion criteria, and he selected a final set of 68 relevant studies out of 200 for review. The study found the lack of academic integrity, particularly in written assignments, due to the heightened risk of plagiarism, and to address them, the establishment of ethical guidelines was effective for learners' ethical awareness in using AI and inspiring educators to assess learners’ academic creation emphasizing learners’ own creativity. The study has the potential to inform the development of ethical guidelines for the use of AI in academia. As generative AI tools become increasingly prevalent, the risk of academic misconduct escalates, thereby threatening educational institutions' credibility and academic qualifications' integrity. The study will help to understand how ethical frameworks can mitigate the risk of plagiarism and foster a culture of ethical awareness among students and educators.</p>"
10.1016/j.bpj.2023.11.413,Rational enzyme engineering via generative AI,N/A
10.2139/ssrn.4761694,Exploring Mathematical Spaces using Generative AI and Large Language Models,N/A
10.2139/ssrn.4477704,Why Law Firms Must Responsibly Embrace Generative AI,N/A
10.1016/j.econlet.2024.111872,How will Generative AI impact communication?,N/A
10.2139/ssrn.4552494,Generative AI and Consumer Protection: Directives for Regulation in Nigeria,N/A
10.2139/ssrn.4715376,Generative AI for European Asset Pricing: Alleviating the Momentum Anomaly,N/A
10.2139/ssrn.4876736,"Rage Against the Machine? Generative AI Use, Threat Perceptions, and Policy Preferences",N/A
10.5040/9781350258297.ch-002,The Injustices of Generative Assumptions,N/A
10.31234/osf.io/62kxq,The persuasive effects of political microtargeting in the age of generative AI,"<p>The increasing availability of microtargeted advertising and the accessibility of generative AI tools, such as ChatGPT, have raised concerns about the potential misuse of large language models (LLMs) in scaling microtargeting efforts for political purposes. Recent technological advancements, involving generative AI and personality inference from consumed text, can potentially create a highly scalable ""manipulation machine'"" that targets individuals based on their unique vulnerabilities without requiring human input. This paper presents four studies examining the effectiveness of this putative ""manipulation machine'"". The results demonstrate that personalized political ads tailored to individuals' personalities are more effective than non-personalized ads (Studies 1a and 1b). Additionally, we showcase the feasibility of automatically generating and validating these personalized ads on a large scale (Studies 2a and 2b). These findings highlight the potential risks of utilizing AI and microtargeting to craft political messages that resonate with individuals based on their personality traits. This should be an area of concern to ethicists and policy makers.</p>"
10.2139/ssrn.4447526,ChatGPT and Generative AI: The New Barbarians at the Gate,N/A
10.4324/9781003457787-10,Perspectives on Decision-Making Using Generative AI,N/A
10.3390/info15080452,Gender Prediction of Generated Tweets Using Generative AI,"<jats:p>With the use of Generative AI (GenAI), Online Social Networks (OSNs) now generate a huge volume of content data. Yet, user-generated content on OSNs, aided by GenAI, presents challenges in analyzing and understanding its characteristics. In particular, tweets generated by GenAI at the request of authentic human users present difficulties in determining the gendered variation of the content. The vast amount of data generated from tweets’ content necessitates a thorough investigation into the gender-specific language used in these tweets. This study explores the task of predicting the gender of text content in tweets generated by GenAI. Through our analysis and experimentation, we have achieved a remarkable 90% accuracy in attributing gender-specific language to these tweets. Our research not only highlights the potential of GenAI in gender prediction but also underscores the sophisticated techniques employed to decipher the refined linguistic cues that differentiate male and female language in GenAI-generated content. This advancement in understanding and predicting gender-specific language in GenAI-generated tweets covers the way for more refined and accurate content analysis in the evolving landscape of OSNs.</jats:p>"
10.1126/science.adi0656,Generative AI meets copyright,<jats:p>Ongoing lawsuits could affect everyone who uses generative AI</jats:p>
10.2139/ssrn.4519182,"ChatGPT, Generative AI, and Investment Advisory",N/A
10.1515/9783111323749-204,List of authors,N/A
10.2139/ssrn.4919039,A Qualitative Comparative Analysis of Ethical Precedents in Adopting Generative Ai for Brand Content Creation,N/A
10.21125/edulearn.2024.1225,IMPLEMENTING GENERATIVE AI IN ACADEMIC WRITING SEMINARS,N/A
10.2139/ssrn.4667577,3 Generative AI Applications for IoT You Must Know About,N/A
10.12781/978-1-907549-16-8-9,SIGMA: Appreciative Inquiry as a 7i Generative Mandala,N/A
10.21428/e4baedd9.5970fe13,Generating Reflection Prompts in Self-Directed Learning Activities with Generative AI,N/A
10.35542/osf.io/9wa8p,Working paper: Responding to Generative AI in Australian Higher Education,"<p>In July 2023, learning and teaching leaders, as well as students, from 28 higher education institutions in Australia and New Zealand gathered to discuss the state of generative AI at their institutions. This working paper highlights the progress to date in adapting to generative AI in Australian higher education; outline key values, principles, and strategies for institutions; and suggest ways for institutions to move forward in responding practically, productively, and responsibly to generative AI.</p>"
10.1007/979-8-8688-0205-8_7,Monitoring,N/A
10.2139/ssrn.4883390,Visual Legal Rhetoric in the Age of Generative AI and Deepfakes: Renaissance or Dark Ages?,N/A
10.1038/s41551-024-01185-8,Techniques for supercharging academic writing with generative AI,N/A
10.1007/978-981-13-2262-4_303-1,Generative AI in Higher Education Composition Assessment,N/A
10.1145/3591196.3596616,Generative AI Futures: A Speculative Design Exploration,N/A
10.1007/978-3-031-54252-7_3,AI Regulations,N/A
10.21125/inted.2024.0600,NAVIGATING THE NEXUS OF AI LITERACIES: A STUDY ON GENERATIVE AI PRACTICES AMONG NTU STUDENTS,N/A
10.1007/s10676-016-9400-6,AI assisted ethics,"<jats:title>Abstract</jats:title>
          <jats:p>The growing number of ‘smart’ instruments, those equipped with AI, has raised concerns because these instruments make autonomous decisions; that is, they act beyond the guidelines provided them by programmers. Hence, the question the makers and users of smart instrument (e.g., driver-less cars) face is how to ensure that these instruments will not engage in unethical conduct (not to be conflated with illegal conduct). The article suggests that to proceed we need a new kind of AI program—oversight programs—that will monitor, audit, and hold operational AI programs accountable.</jats:p>"
10.4018/979-8-3693-2440-0.ch004,Demystifying the Dynamic Determinants of Generative Artificial Intelligence (AI) Literacy for Adaptable Sustainable Education,"<jats:p>This research aims to demystify the dynamic determinants of generative artificial intelligence (AI) literacy for adaptable sustainable education (ASE) through the application of multistage structural equation modeling (SEM). Utilizing a quantitative approach, a structured questionnaire distributed via Google Forms is employed to gather data from a convenient sample of 260 teachers in higher education. The study unfolds over a specified period, incorporating rigorous data filtering techniques to enhance the reliability of responses. Smart-PLS serves as the primary tool for data analysis, allowing for an in-depth exploration of relationships among variables such as generative AI literacy, digital split, computational intelligence, and cognitive inclusion. Dependent variables include teacher perceptions, technological proficiency, and cognitive inclusion, while independent variables encompass awareness of AI and the perceived usefulness of AI in achieving ASE. The outcomes of this research carry significant policy implications for AI, ICT, and educational professionals, providing insights to shape informed strategies for integrating AI literacy into higher education to meet the goals of adaptable sustainable education.</jats:p>"
10.37547/tajiir/volume06issue06-08,THE ANALYSIS OF THE EFFICIENCY OF GENERATIVE AI ALGORITHMS FOR CREATING A NATURAL DIALOGUE,"<jats:p>In the modern world, artificial intelligence (AI) plays an increasingly important role in various fields of human activity. One of the most promising areas of AI application is the generation of natural dialogue. The purpose of this work is to analyze the efficiency of generative AI algorithms for creating natural dialogue. The relevance of this topic is due to the growing interest in the use of AI to create dialogue systems capable of interacting with people in a natural way. The results of the study can be useful for developers of dialogue systems, researchers in the field of AI, as well as anyone interested in the application of AI in their everyday life. Natural language generation is a fundamental task in artificial intelligence, with applications ranging from chatbots to virtual assistants. This study provides a comprehensive analysis of the efficiency of various generative artificial intelligence algorithms for creating a natural dialogue. Their performance is assessed in generating consistent and contextually appropriate responses by evaluating modern models using quantitative metrics and human evaluation. Additionally, the study explores the impact of various training data sizes and techniques on the quality of a generated dialogue. The results provide insight into the strengths and weaknesses of current generative AI approaches in the generation of a dialogue.</jats:p>"
10.1201/9781003278290-63,"Ethics, AI, Research, and Corporations",N/A
10.1145/3644815.3644982,Prompt Smells: An Omen for Undesirable Generative AI Outputs,N/A
10.1515/9781501520105-002,Chapter 1: Generative AI and GPT-4,N/A
10.21203/rs.3.rs-4176435/v1,Enhancing Mixed Methods Research with your generative AI assistant – A Tutorial and Evaluation for Scholars,"<jats:title>Abstract</jats:title>
        <jats:p>Scholars from all disciplines can benefit from learning how to use generative Artificial Intelligence (GAI or AI) for data analysis. The current article used real data to demonstrate the analysis and synthesis of mixed methods research (MMR) data with generative AI. I also explore how reliable and valid data outputs are and how to improve the use of generative AI for research. The demonstration data used emanated from a study done in South Africa, with a quantitative sample size of 969 first-year engineering students and, for the qualitative part, 14 second-year students. In the current article, I compare my original analysis to ChatGPT results. Generative AI is a mind tool that is ideal when utilised with human insight to check the cohesion, consistency, and accuracy of the data analysis. The current content is geared towards enhancing methodological application regardless of field or discipline and includes access to a prompt library and examples of using outputs. For the qualitative analysis, I found that ChatGPT could detect similar themes but missed some, and its write-up was shallower than our human version. The quantitative analysis was accurate for the descriptive statistics, but the researcher had to use best judgment to select the correct inferential analysis. A quantitative and qualitative analysis should be conducted separately in generative AI before asking the bot for help with mixed methods research.  I give guidelines and a tutorial on how to use chatbots in an ethically responsible and scientifically sound manner for research in social and human sciences.</jats:p>"
10.1109/ms.2024.3363848,Pair Programming With Generative AI,N/A
10.4324/9781003429388-13,"The Consequences of Generative AI for Democracy, Governance and War",N/A
10.5194/egusphere-egu24-18160,Reflecting on the use of Generative AI in Higher Education Teaching &amp;amp; Learning,"<jats:p>Generative AI tools such as Chat GPT or Google Bard provide new capabilities that can potentially be used by both students and those teaching them. However, as with most new tools, they also come with pitfalls. As those capabilities are likely to also be increasingly integrated into the world of work, we need to look to inform students about sensible use and avoiding problems. We also need to think about how assessment interacts with learning and potentially redesign assessment. This work will reflect on issues and experiences with generative AI in a HE context focusing around an interdiscliplinary course in climate and sustainability.
.</jats:p>"
10.2139/ssrn.4806516,Generative Ai-Enabled Supply Chain Management: A Coordination Theory Perspective,N/A
10.2139/ssrn.4687459,"Teachers’ Initial Engagement with Generative Ai in Tertiary Efl Instruction: Practices, Reactions and Implications",N/A
10.1515/9783111323749-205,About the editors,N/A
10.1515/9783839474723-003,DETAILS AND THANKS,N/A
10.2139/ssrn.4855880,A Practical Approach to Mitigate the Dependencies of Generative AI in Engineering Education A PREPRINT,N/A
10.52783/jier.v4i2.767,Impacts and Implications of Generative AI and Large Language Models: Redefining Banking Sector,N/A
10.21125/edulearn.2024.2006,THE GUIDES FRAMEWORK: ENHANCING ENGINEERING EDUCATION WITH GENERATIVE AI,N/A
10.2139/ssrn.4574814,"Generative AI: Overview, Economic Impact, and Applications in Asset Management",N/A
10.18260/1-2--48453,Characterizing Computing Students' Use of Generative AI,N/A
10.18665/sr.320892,Generative AI and Postsecondary Instructional Practices: Findings from a National Survey of Instructors,N/A
10.1787/e745e3ea-en,Generative AI-related incidents and hazards reported by reputable news outlets have increased steeply since 2022,N/A
10.21428/e4baedd9.92e511e3,Closing the Execution Gap in Generative AI for Chemicals and Materials: Freeways or Safeguards,N/A
10.1145/3604237.3626898,Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network,N/A
10.1007/s43681-024-00447-0,"Surfing reality, hype, and propaganda: an empirical comparative analysis on predictive software in criminal justice",N/A
10.1007/978-3-030-51110-4_7,Psychological Aspects of AI,N/A
10.1007/978-3-030-51110-4_8,Privacy Issues of AI,N/A
10.1007/s43681-022-00223-y,Understanding risk with FOTRES?,"<jats:title>Abstract</jats:title><jats:p>The present paper examines the recidivism risk assessment instrument FOTRES, addressing the questions whether FOTRES provides us with an adequate understanding of risk, whether we actually understand FOTRES itself, and whether FOTRES is fair. The evaluation of FOTRES uses the criteria of empirical accuracy, representational accuracy, domain of validity, intelligibility, and fairness. This evaluation is compared to that of COMPAS, a different, much-discussed risk assessment instrument. The paper argues that FOTRES performs poorly in comparison to COMPAS with respect to some of the criteria, and that both FOTRES and COMPAS do not show a satisfactory performance with respect to other criteria.</jats:p>"
10.1007/s43681-023-00400-7,Conformal prediction for trustworthy detection of railway signals,N/A
10.1007/s43681-023-00378-2,How artificial intelligence adopts human biases: the case of cosmetic skincare industry,"<jats:title>Abstract</jats:title><jats:p>The cosmetic skincare industry is a growing market that extends to different regions and customer groups. In addition to scientific advances and technological developments, state-of-the-art digital approaches, including machine learning and other artificial intelligence (AI)-based techniques, are being applied at different stages of the value chain. The objectives of these efforts include optimizing the supply chain, developing high-quality, effective and safe products and personalization at every step of the customer journey. However, the use of digital technologies comes with risks and undesirable effects. These include a lack of transparency and accountability, compromised fairness and a general deficiency in data governance, all of which are critical at every customer touchpoint. This dark side of digital transformation is recognized by both businesses and governments. In this paper, we explain the concept of bias leading to unfairness for beauty technology applications. Based on published data we identified potential sources of AI bias in the cosmetic skincare industry and/or beauty tech. They were classified by the stage of the AI lifecycle: biases related to target setting, to acquisition and annotation, to modeling, to validation and evaluation, and to deployment and monitoring. We aim to create awareness of such phenomena among readers, whether executives, managers, developers or potential end-users.</jats:p>"
10.1007/s43681-024-00432-7,"The rise of artificial intelligence in libraries: the ethical and equitable methodologies, and prospects for empowering library users",N/A
10.1007/s43681-023-00270-z,Convergence of the source control and actual access accounts of privacy,"<jats:title>Abstract</jats:title><jats:p>In this paper, it is argued that, when properly revised in the face of counter-examples, the source control and actual access views of privacy are extensionally equivalent but different in their underlying rationales. In this sense, the source control view and the actual access view, when properly modified to meet counter-examples, can be metaphorically compared to ‘climbing the same mountain but from different sides’ (as Parfit [1] has argued about normative theories). These two views can equally apply to the privacy debates and, thus, resolve a long-standing debate in the literature.</jats:p>"
10.1007/s43681-020-00027-y,Being a data professional: give voice to value in a data driven society,N/A
10.58496/mjaih/2024/010,Benchmarking Generative AI: A Call for Establishing a Comprehensive Framework and a Generative AIQ Test,"<jats:p>The introduction and rapid evolution of generative artificial intelligence (genAI) models necessitates a refined understanding for the concept of “intelligence”. The genAI tools are known for its capability to produce complex, creative, and contextually relevant output. Nevertheless, the deployment of genAI models in healthcare should be accompanied appropriate and rigorous performance evaluation tools. In this rapid communication, we emphasizes the urgent need to develop a “Generative AIQ Test” as a novel tailored tool for comprehensive benchmarking of genAI models against multiple human-like intelligence attributes. A preliminary framework is proposed in this communication. This framework incorporates miscellaneous performance metrics including accuracy, diversity, novelty, and consistency. These metrics were considered critical in the evaluation of genAI models that might be utilized to generate diagnostic recommendations, treatment plans, and patient interaction suggestions. This communication also highlights the importance of orchestrated collaboration to construct robust and well-annotated benchmarking datasets to capture the complexity of diverse medical scenarios and patient demographics. This communication suggests an approach aiming to ensure that genAI models are effective, equitable, and transparent. To maximize the potential of genAI models in healthcare, it is important to establish rigorous, dynamic standards for its benchmarking. Consequently, this approach can help to improve clinical decision-making with enhancement in patient care, which will enhance the reliability of genAI applications in healthcare.</jats:p>"
10.36669/ip.2023.76.12,Legal Issues of Generative AI through the Case of AI Cover Songs,N/A
10.3145/infonomy.23.002,"Buscadores alternativos a Google con IA generativa: análisis de You.com, Perplexity AI y Bing Chat [Alternative search engines to Google with generative AI: analysis of You.com, Perplexity AI and Bing Chat]","<jats:p>Análisis comparativo de tres buscadores alternativos a Google con inteligencia artificial generativa: You.com, Perplexity AI y Bing Chat. Presentación de las características generales de los tres tipos de búsqueda actuales en internet: recuperación de información, sistemas de respuestas, y búsqueda generativa. Análisis funcional y de la interfaz de los tres sistemas seleccionados. Recomendaciones generales de su uso en entornos académicos.</jats:p>"
10.1515/9783111425078-006,6 LLM Fine-Tuning: Instruction and Parameter-Efficient Fine-Tuning (PEFT),N/A
10.4324/9781003482918-10,Re-imagining student engagement in an AI-enhanced classroom,N/A
10.5465/amproc.2024.19985abstract,Generative AI as a Strategic Intelligent User Interface for Human-AI Collaboration,N/A
10.4018/979-8-3693-0074-9.ch008,AI-Assisted Teacher Wellness,"<jats:p>The Canadian and American teaching profession is known to be stressful, leading to burnout and other mental health issues for teachers. This chapter proposes an AI-assisted teacher wellness theory (AI TeachWell) and a supporting product/feedback/learning (PFL) framework for increased teacher well-being through the strategic use of AI chatbot technology. The theory emphasizes the use of resources that offset the demands of teaching, the importance of reducing cognitive load, and the increase of autonomy, efficacy, and relatedness. This chapter highlights the reasons that have led to increased teacher workloads, such as demands on teacher performance, administrative tasks, and professional development. It also highlights the effects of a lack of resources, time, and self-efficacy on teacher stress and burnout. The chapter concludes by offering innovative and proactive solutions for teachers to prioritize their health while fostering engaging and effective learning environments for their students, as well as the future implications of this theory.</jats:p>"
10.1145/3630106.3659005,Rethinking open source generative AI: open washing and the EU AI Act,N/A
10.1007/s43681-020-00023-2,"If robots are people, can they be made for profit? Commercial implications of robot personhood",N/A
10.29242/rli.299.1,What Do Artificial Intelligence (AI) and Ethics of AI Mean in the Context of Research Libraries?,N/A
10.1007/978-3-030-51110-4_11,Military Uses of AI,N/A
10.1007/s43681-023-00278-5,Social and ethical challenges of the metaverse,N/A
10.1007/s43681-021-00073-0,How to use algorithmic decision-making to promote inclusiveness in organizations,N/A
10.1007/978-3-030-51110-4_9,Application Areas of AI,N/A
10.1007/s43681-024-00540-4,Performance and model behavior analysis from different perspectives of Bing Chat,N/A
10.1007/s43681-021-00062-3,Against robot taxes: scrutinizing the moral reasons for the preservation of work,"<jats:title>Abstract</jats:title><jats:p>A recent political proposal to address the challenge of technological unemployment suggests that the state should impose a tax on labor-replacing technologies. The idea is to preserve jobs by disincentivizing automation. In this article, I critically assess the proposal from an ethical perspective. I show that, with respect to conceptions of distributive justice, it is unclear that precluding consumers’ potential real-income gains from automation can be justified. But foremost, I examine the moral ideal behind the normative claim to preserve labor. I show that the arguments in favor of a robot tax rely on doubtful moral convictions on the value of work and I conclude that a moral basis for imposing a robot tax is subject to justified scrutiny.</jats:p>"
10.1007/s43681-023-00354-w,What do academics have to say about ChatGPT? A text mining analytics on the discussions regarding ChatGPT on research writing,N/A
10.4018/979-8-3693-3719-6.ch016,Generative Adversarial Networks for Advanced EEG Data Analysis,"<jats:p>This chapter examines the use of Generative Adversarial Networks (GANs) in analyzing electroencephalogram (EEG) data. EEG is an electrophysiological method that records brain activity. EEG is used to diagnose neurological disorders and is also very important for brain-computer interface (BCI) systems. Although EEG data processing and analysis is widely used, it faces some difficulties, which reveals the necessity of advanced signal processing techniques. GANs, on the other hand, are advanced machine learning techniques and play an essential role in EEG data analysis. GANs are known for their ability to produce synthetic data similar to actual data, and this feature provides significant advantages in the analysis of EEG data. In particular, GANs are effective at filtering noise, improving data quality, and generating synthetic data. Given the complexity and diversity of EEG data, caution must be exercised in training GAN models and the accuracy of synthetic data. Current limitations of GANs in EEG data analysis and ongoing research to overcome these limitations are also examined.</jats:p>"
10.4018/979-8-3693-1950-5.ch004,The Human-Machine Nexus With Art-Making Generative AIs,"<jats:p>The chapter delves into the dynamic interplay between human creativity and artificial intelligence, exploring the transformative landscape of art creation facilitated by generative AI tools. As the realms of technology and artistic expression converge, this chapter navigates the intricate relationship between human artists and machine-generated algorithms. It scrutinizes how generative AI tools serve not as replacements for human ingenuity but as collaborators, augmenting the creative process and challenging conventional notions of authorship in the realm of art. Through a comprehensive examination of case studies and examples, the chapter illustrates the symbiotic fusion of human intuition, emotion, and conceptualization with the computational capabilities of AI, shedding light on the profound implications for the future of artistic endeavors.</jats:p>"
10.2139/ssrn.4751625,Prompting the E-Brushes: Users as Authors in Generative AI,N/A
10.2139/ssrn.4895606,Generative AI and Fabricated Evidence Ploys: A Threat to Procedural Justice and the Legitimacy of Police,N/A
10.7249/cta3191-1,"Exploring the Implications of Generative AI for Chinese Military Cyber-Enabled Influence Operations: Chinese Military Strategies, Capabilities, and Intent",N/A
10.12781/978-1-907549-26-7-3,Appreciative Inquiry: Three decades of generative impact,N/A
10.1007/978-3-031-46238-2_28,Infrared Image Super-Resolution via GAN,N/A
10.1007/979-8-8688-0318-5,Brain Rush,N/A
10.2139/ssrn.4885958,African Democracy in the Era of Generative Disinformation: Challenges and Countermeasures against AI-Generated Propaganda,N/A
10.1136/bmj.p1551,Generative AI for medical research,N/A
10.1109/cai59869.2024.00268,Using Generative AI to drive person centric networking,N/A
10.4018/979-8-3693-1351-0.ch018,Transforming Education,"<jats:p>Generative AI systems are increasingly present in our daily lives, helping us make crucial decisions. They use machine learning algorithms and tools, fed with millions of data collected from the web, producing entirely new information and generating variations. And this is not just limited to texts — it can produce images, audio, videos, even code, or new programming languages. There are several fields where generative AI can have a considerable impact in the coming years. In this context, the issues proposed in this chapter are: What is generative AI? What is prompt engineering? How to transform education using generative AI and prompt engineering in creating synthetic content? To respond to the research problem, the following objective will be achieved: Investigate how to transform education using generative AI and prompt engineering in the creation of synthetic content. It is concluded that generative AI tools can also help create more efficient exercises. Teachers and educators can use technology to create instructional materials and present summaries of concepts.</jats:p>"
10.3390/ai3030045,Bridging East-West Differences in Ethics Guidance for AI and Robotics,"<jats:p>Societies of the East are often contrasted with those of the West in their stances toward technology. This paper explores these perceived differences in the context of international ethics guidance for artificial intelligence (AI) and robotics. Japan serves as an example of the East, while Europe and North America serve as examples of the West. The paper’s principal aim is to demonstrate that Western values predominate in international ethics guidance and that Japanese values serve as a much-needed corrective. We recommend a hybrid approach that is more inclusive and truly ‘international’. Following an introduction, the paper examines distinct stances toward robots that emerged in the West and Japan, respectively, during the aftermath of the Second World War, reflecting history and popular culture, socio-economic conditions, and religious worldviews. It shows how international ethics guidelines reflect these disparate stances, drawing on a 2019 scoping review that examined 84 international AI ethics documents. These documents are heavily skewed toward precautionary values associated with the West and cite the optimistic values associated with Japan less frequently. Drawing insights from Japan’s so-called ‘moonshot goals’, the paper fleshes out Japanese values in greater detail and shows how to incorporate them more effectively in international ethics guidelines for AI and robotics.</jats:p>"
10.1007/s43681-021-00060-5,Socio-cognitive biases in folk AI ethics and risk discourse,"<jats:title>Abstract</jats:title><jats:p>The ongoing conversation on AI ethics and politics is in full swing and has spread to the general public. Rather than contributing by engaging with the issues and views discussed, we want to step back and comment on the widening conversation itself. We consider evolved human cognitive tendencies and biases, and how they frame and hinder the conversation on AI ethics. Primarily, we describe our innate human capacities known as folk theories and how we apply them to phenomena of different implicit categories. Through examples and empirical findings, we show that such tendencies specifically affect the key issues discussed in AI ethics. The central claim is that much of our mostly opaque intuitive thinking has not evolved to match the nature of AI, and this causes problems in democratizing AI ethics and politics. Developing awareness of how our intuitive thinking affects our more explicit views will add to the quality of the conversation.</jats:p>"
10.1080/10447318.2024.2323277,"Could AI Ethical Anxiety, Perceived Ethical Risks and Ethical Awareness About AI Influence University Students’ Use of Generative AI Products? An Ethical Perspective",N/A
10.1007/s00146-023-01757-3,The latent space of data ethics,N/A
10.1007/s43681-022-00235-8,The disconnect between the goals of trustworthy AI for law enforcement and the EU research agenda,"<jats:title>Abstract</jats:title><jats:p>In this paper, we investigate whether AI deployment for law enforcement will enable or impede the exercise of citizens' fundamental rights by juxtaposing the promises and policy goals with the crude reality of practices, funded projects, and practicalities of law enforcement. To this end, we map the projects funded by H2020 in AI for law enforcement and juxtapose them to the goals and aims of the EU in terms of Trustworthy AI and fundamental rights. We then bring forward existing research stressing that AI implementation in sensitive domains such as defense and law enforcement does not come without drawbacks, especially regarding discrimination, surveillance, data protection, and human dignity. We thoroughly analyze and assess human-centric and socially-driven lens risks and threats of using AI factors from an ethical, legal, and societal perspective (ELSA), including organizational and gender worries.</jats:p>"
10.1007/s43681-024-00516-4,‘It wasn’t me’: the impact of social responsibility and social dominance attitudes on AI programmers’ moral imagination (intention to correct bias),"<jats:title>Abstract</jats:title><jats:p>A plethora of research has shed light on AI’s perpetuation of biases, and the primary focus has been on technological fixes or biased data. However, there is deafening silence regarding the key role of programmers in mitigating bias in AI. A significant gap exists in the understanding of how a programmer’s personal characteristics may influence their professional design choices. This study addresses this gap by exploring the link between programmers’ sense of social responsibility and their moral imagination in AI, i.e., intentions to correct bias in AI, particularly against marginalized populations. Furthermore, it is unexplored how a programmer’s preference for hierarchy between groups, social dominance orientation-egalitarianism (SDO-E), influences this relationship. We conducted a between-subject online experiment with 263 programmers based in the United States. They were randomly assigned to conditions that mimic narratives about agency reflected in technology determinism (low responsibility) and technology instrumentalism (high responsibility). The findings reveal that high social responsibility significantly boosts programmers’ moral imagination concerning their intentions to correct bias in AI, and it is especially effective for high SDO-E programmers. In contrast, low SDO-E programmers exhibit consistently high levels of moral imagination in AI, regardless of the condition, as they are highly empathetic, allowing the perspective-taking needed for moral imagination, and are naturally motivated to equalize groups. This study underscores the need to cultivate social responsibility among programmers to enhance fairness and ethics in the development of artificial intelligence. The findings have important theoretical and practical implications for AI ethics, algorithmic fairness, etc.</jats:p>"
10.5040/9781526513588.chapter-004,The ethics dimension,N/A
10.1007/s43681-023-00374-6,AI-produced certainties in health care: current and future challenges,"<jats:title>Abstract</jats:title><jats:p>Since uncertainty is a major challenge in medicine and bears the risk of causing incorrect diagnoses and harmful treatment, there are many efforts to tackle it. For some time, AI technologies have been increasingly implemented in medicine and used to reduce medical uncertainties. What initially seems desirable, however, poses challenges. We use a multimethod approach that combines philosophical inquiry, conceptual analysis, and ethical considerations to identify key challenges that arise when AI is used for medical certainty purposes. We identify several challenges. Where AI is used to reduce medical uncertainties, it is likely to result in (a) patients being stripped down to their measurable data points, and being made disambiguous. Additionally, the widespread use of AI technologies in health care bears the risk of (b) human physicians being pushed out of the medical decision-making process, and patient participation being more and more limited. Further, the successful use of AI requires extensive and invasive monitoring of patients, which raises (c) questions about surveillance as well as privacy and security issues. We outline these several challenges and show that they are immediate consequences of AI-driven security efforts. If not addressed, they could entail unfavorable consequences. We contend that diminishing medical uncertainties through AI involves a tradeoff. The advantages, including enhanced precision, personalization, and overall improvement in medicine, are accompanied by several novel challenges. This paper addresses them and gives suggestions about how to use AI for certainty purposes without causing harm to patients.</jats:p>"
10.1007/s43681-022-00188-y,Ethical and methodological challenges in building morally informed AI systems,"<jats:title>Abstract</jats:title><jats:p>Recent progress in large language models has led to applications that can (at least) simulate possession of full moral agency due to their capacity to report context-sensitive moral assessments in open-domain conversations. However, automating moral decision-making faces several methodological as well as ethical challenges. They arise in the fields of bias mitigation, missing ground truth for moral “correctness”, effects of bounded ethicality in machines, changes in moral norms over time, risks of using morally informed AI systems as actual advice, as well as societal implications an increasing importance of algorithmic moral decision-making would have. This paper comments on all these challenges and provides critical considerations for future research on full artificial moral agency. Importantly, some of the adduced challenges can be met by more careful technology design, but others necessarily require engagement with core problems of meta-ethics.</jats:p>"
10.1088/978-0-7503-6116-3,AI and Ethics,N/A
10.7551/mitpress/12549.003.0008,Don't Forget the Data (Science),N/A
10.1007/s43681-023-00411-4,Should we develop AGI? Artificial suffering and the moral development of humans,"<jats:title>Abstract</jats:title><jats:p>Recent research papers and tests in real life point in the direction that machines in the future may develop some form of possibly rudimentary inner life. Philosophers have warned and emphasized that the possibility of artificial suffering or the possibility of machines as moral patients should not be ruled out. In this paper, I reflect on the consequences for moral development of striving for AGI. In the introduction, I present examples which point into the direction of the future possibility of artificial suffering and highlight the increasing similarity between, for example, machine–human and human–human interaction. Next, I present and discuss responses to the possibility of artificial suffering supporting a cautious attitude for the sake of the machines. From a virtue ethical perspective and the development of human virtues, I subsequently argue that humans should not pursue the path of developing and creating AGI, not merely for the sake of possible suffering in machines, but also due to machine–human interaction becoming more alike to human–human interaction and for the sake of the human’s own moral development. Thus, for several reasons, humanity, as a whole, should be extremely cautious about pursuing the path of developing AGI—Artificial General Intelligence.</jats:p>"
10.1002/9781394308286.ch10,"How
            <scp>AI</scp>
            Is Shaping Our Economic Future",N/A
10.4108/airo.5962,Mapping Generative Artificial Intelligence (GAI's) Exciting Future: From Gemini to Q* and Beyond,"<jats:p>This research investigates the transformative potential of Mixture of Experts (MoE) and multimodal learning within generative AI, exploring their roles in advancing towards Artificial General Intelligence (AGI). By leveraging a combination of specialized models, MoE addresses scalability and computational limitations, enabling more nuanced and robust modelling across diverse data modalities. The research exploration draws inspiration from pioneering projects like Google's Gemini and OpenAI's anticipated Q* to push the boundaries of AI capabilities. The objectives include exploring the impact of MoE on generative AI, investigating multimodal learning's role in achieving AGI, conducting experiments to demonstrate MoE's effectiveness across various domains, and assessing the influence of AI-generated preprints on the peer-review process. Ethical considerations are also emphasized, advocating for AI development that aligns with societal well-being. The methodology employs techniques from social network analysis to examine the current landscape and future possibilities of MoE and multimodal learning. Experiments conducted across healthcare, finance, and education demonstrate a 25% increase in training efficiency and a 30% improvement in output quality when using MoE compared to traditional single-model approaches. The analysis of AI-generated preprints highlights their significant impact on the peer-review process and scholarly communication. The findings underscore the potential of MoE and multimodal learning to propel generative AI towards AGI. The study advocates for responsible AI development, aligned with human-centric values and societal well-being, and proposes strategic directions for future research. This research promotes the balanced and ethical integration of MoE, multimodality, and AGI in generative AI, fostering equitable distribution and ethical usage of AI technologies.</jats:p>"
10.54517/m.v5i2.2568,Empirical insights into AI-assisted game development: A case study on the integration of generative AI tools in creative pipelines,"<jats:p>&lt;p&gt;This study conducts an empirical exploration of generative Artificial Intelligence (AI) tools across the game development pipeline, from concept art creation to 3D model integration in a game engine. Employing AI generators like Leonardo AI, Scenario AI, Alpha 3D, and Luma AI, the research investigates their application in generating game assets. The process, documented in a diary-like format, ranges from producing concept art using fantasy game prompts to optimizing 3D models in Blender and applying them in Unreal Engine 5. The findings highlight the potential of AI to enhance the conceptualization phase and identify challenges in producing optimized, high-quality 3D models suitable for game development. This study reveals the current limitations and ethical considerations of AI in game design, suggesting that while generative AI tools hold significant promise for transforming game development, their full integration depends on overcoming these hurdles and gaining broader industry acceptance.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;</jats:p>"
10.1016/b978-0-443-18851-0.00010-x,Acknowledgments,N/A
10.1016/b978-0-443-18851-0.00024-x,Copyright,N/A
10.1007/s43681-023-00259-8,Beware of sustainable AI! Uses and abuses of a worthy goal,"<jats:title>Abstract</jats:title><jats:p>The ethical debate about technologies called artificial intelligence (AI) has recently turned towards the question whether and in which sense using AI can be sustainable, distinguishing possible contributions of AI to achieve the end of sustainability on the one hand from the sustainability of AI and its underlying technologies as means on the other hand. This important distinction is both applied in the context of environmental as well as social sustainability. However, further elaboration is necessary to capture the complexities of sustainability assessments in the context of AI. To this end, our analysis of the ends and means of “sustainable AI” in social and environmental contexts leads to a matrix of four dimensions reflecting its social and its environmental impact and costs. This matrix avoids overly narrow, one-dimensional assessments that too quickly label some AI-based technology as sustainable. While a selective assessment can, at best, warrant the narrower verdict of “thin” sustainability, only such a comprehensive assessment can warrant the verdict of what we call “thick” sustainability. In consequence, we recommend to broaden the normative scope in considering the ethics and justice of AI and to use the notion “sustainability” more carefully and sparingly, and to pursue the more ambitious goal of “thick” sustainability of AI-based technologies to meaningfully contribute to actual improvements of human lives and living together. Current conditions of an economy oriented towards permanent growth, however, may make it difficult or even impossible to realise sustainable AI.</jats:p>"
10.1007/s43681-022-00225-w,Publisher Correction: AI for hiring in context: a perspective on overcoming the unique challenges of employment research to mitigate disparate impact,N/A
10.7551/mitpress/12549.003.0003,"Mirror, Mirror, on the Wall",N/A
10.1007/s43681-020-00006-3,Using unethical data to build a more ethical world,N/A
10.1007/s43681-020-00021-4,Correction to: MIT’s moral machine project is a psychological roadblock to self-driving cars,N/A
10.1162/99608f92.5dbf3265,An Information-Theoretic Method for Detecting Edits in AI-Generated Text,N/A
10.3139/9781569902356.016,16 Generative AI and Large Language Models,N/A
10.1016/j.egyai.2022.100177,Stress testing electrical grids: Generative Adversarial Networks for load scenario generation,N/A
10.1093/grurint/ikae027,From Brussels to Brasília: How the EU AI Act Could Inspire Brazil’s Generative AI Copyright Policy,N/A
10.1007/s43681-023-00391-5,Advances in automatically rating the trustworthiness of text processing services,N/A
10.1093/oxfordhb/9780190067397.013.16,Race and Gender,"<p>This chapter discusses the role of race and gender in artificial intelligence (AI). The rapid permeation of AI into society has not been accompanied by a thorough investigation of the sociopolitical issues that cause certain groups of people to be harmed rather than advantaged by it. For instance, recent studies have shown that commercial automated facial analysis systems have much higher error rates for dark-skinned women, while having minimal errors on light-skinned men. Moreover, a 2016 <italic>ProPublica</italic> investigation uncovered that machine learning–based tools that assess crime recidivism rates in the United States are biased against African Americans. Other studies show that natural language–processing tools trained on news articles exhibit societal biases. While many technical solutions have been proposed to alleviate bias in machine learning systems, a holistic and multifaceted approach must be taken. This includes standardization bodies determining what types of systems can be used in which scenarios, making sure that automated decision tools are created by people from diverse backgrounds, and understanding the historical and political factors that disadvantage certain groups who are subjected to these tools.</p>"
10.1145/3644815.3644977,Taxonomy of Generative AI Applications for Risk Assessment,N/A
10.3390/ai5030080,Perspectives for Generative AI-Assisted Art Therapy for Melanoma Patients,"<jats:p>Digital technologies are making their mark in medicine, and especially also in art therapy, offering innovative therapeutic interventions for patients, including those with melanoma skin cancer. However, the integration of novel technologies, such as AI-generated art, brings along ethical, psychological, and technical challenges that are viewed differently among therapists. We aim to gauge art therapists’ views on the ethical, application, and challenge facets of utilizing AI-generated art from medical images in therapy. The focus is on assessing its applicability and limitations for melanoma patients. Art therapists were surveyed via a questionnaire focusing on their experience, digital tool familiarity, and views on AI in therapy, encompassing ethics, benefits, challenges, and applicability for melanoma. Art therapists have already implemented digital technologies and acknowledged potential therapeutic benefits of creating personalized artworks with generative artificial intelligence. Attention needs to be given to technological hurdles and the necessity for supplementary interventions. Views on the method’s adaptability varied, underscoring a need for tailored, patient-focused applications. Art therapists are welcoming AI-generated art as a promising creative therapeutic tool and acknowledge potential therapeutic benefits. There are ethical, technical, and psychological challenges that must be addressed for application in therapeutic sessions. Therapists should navigate AI integration with sensitivity, adhering to ethical norms around consent and privacy. Future studies should show the therapeutic benefit in practice with emphasis on equipping therapists to manage the technical complexities effectively. Furthermore, it is important to ensure that patients can influence the AI output, allowing for creative moments in the process.</jats:p>"
10.4018/979-8-3693-1565-1.ch006,Exploring the Transformative Potential and Generative AI's Multifaceted Impact on Diverse Sectors,"<jats:p>In the digital landscape, where innovation knows no bounds, generative artificial intelligence (Gen AI) emerges as a true virtuoso, orchestrating a seamless symphony of creative wonders. Artificial intelligence that can generate fresh and plausible images, texts, and motion graphics rapidly is called Generative AI. It possesses the remarkable ability to effortlessly transform prompts into a rich tapestry of visual art, evocative prose, dynamic videos, and an array of captivating media. This chapter embarks on an exploratory journey, delving deep into the profound influence that Generative AI wields across a diverse spectrum of industries. Through an immersive exploration of these remarkable used cases, this chapter offers a glimpse into the boundless potential and the promises that Generative AI holds, positioning itself as a versatile catalyst for progress within the expansive industrial landscape.</jats:p>"
10.1007/s43681-021-00044-5,On human genome manipulation and Homo technicus: the legal treatment of non-natural human subjects,N/A
10.1007/s43681-024-00430-9,Digital ethicswashing: a systematic review and a process-perception-outcome framework,"<jats:title>Abstract</jats:title><jats:p>The term “ethicswashing” was recently coined to describe the phenomenon of instrumentalising ethics by misleading communication, creating the impression of ethical Artificial Intelligence (AI), while no substantive ethical theory, argument, or application is in place or ethicists involved. Ethicswashing resembles greenwashing for environmental issues and has become an issue – particularly since 2019 with Thomas Metzinger’s harsh criticisms as a member of the EU panel for developing ethical guidelines for AI, which he called “ethicswashing.” Nowadays, increased ethics washing has changed the perception of AI ethics, leading critics to find a “trivialization” of ethics that may even lead to “ethics bashing.” Considering the scattered literature body and the various manifestations of digital ethicswashing, we recognise the need to assess the existing literature comprehensively. To fill this gap, this research systematically reviews current knowledge about digital ethicswashing stemming from various academic disciplines, contributing to an up-to-date assessment of its underlying characteristics. Applying content analysis to map the field leads us to present five thematic clusters: ethicswashing, ethics bashing, policymaking and regulation, watchdogs, and academia. In conclusion, we synthesise ethicswashing along a process-perception-outcome framework to provide future research to explore the multiple meanings of digital ethicswashing.</jats:p>"
10.1007/s00146-007-0152-z,From the ethics of technology towards an ethics of knowledge policy: implications for robotics,N/A
10.1007/s43681-024-00498-3,Unveiling and mitigating bias in ride-hailing pricing for equitable policy making,"<jats:title>Abstract</jats:title><jats:p>Ride-hailing services have skyrocketed in popularity due to their convenience. However, recent research has shown that their pricing strategies can have a disparate impact on some riders, such as those living in disadvantaged neighborhoods with a greater share of residents of color or residents below the poverty line. Analyzing real-world data, we additionally show that these communities tend to be more dependent on ride-hailing services (e.g., for work commutes) due to a lack of adequate public transportation infrastructure. To this end, we present the first thorough study on fair pricing for ride-hailing services by first devising applicable fairness measures to quantify this bias and then proposing novel fair pricing mechanisms to alleviate this bias. We present two pricing mechanisms to provide flexibility and account for different platform needs. By taking affordability into account and potentially providing discounts that may be government-subsidized, our approaches result in an increased number and more affordable rides for the disadvantaged community. Experiments on real-world Chicago ride-hailing data demonstrate worse scores for the proposed fairness metrics for rides corresponding to disadvantaged neighborhoods than those of a control group (random mix of neighborhoods). Subsequently, the results show that our fair pricing mechanisms eliminate this inequality gap. Our mechanisms provide a basis for the government and the ride-hailing platforms to implement fair ride-hailing policies.</jats:p>"
10.1007/s43681-021-00074-z,Artificial Intelligence in Education (AIEd): a high-level academic and industry note 2021,"<jats:title>Abstract</jats:title><jats:p>In the past few decades, technology has completely transformed the world around us. Indeed, experts believe that the next big digital transformation in how we live, communicate, work, trade and learn will be driven by Artificial Intelligence (AI) [83]. This paper presents a high-level industrial and academic overview of AI in Education (AIEd). It presents the focus of latest research in AIEd on reducing teachers’ workload, contextualized learning for students, revolutionizing assessments and developments in intelligent tutoring systems. It also discusses the ethical dimension of AIEd and the potential impact of the Covid-19 pandemic on the future of AIEd’s research and practice. The intended readership of this article is policy makers and institutional leaders who are looking for an introductory state of play in AIEd.</jats:p>"
10.1007/s43681-023-00268-7,"Symbiosis, not alignment, as the goal for liberal democracies in the transition to artificial general intelligence","<jats:title>Abstract</jats:title><jats:p>A transition to a world with artificial general intelligence (AGI) may occur within the next few decades. This transition may give rise to catastrophic risks from <jats:italic>misaligned</jats:italic> AGI, which have received a significant amount of attention, deservedly. Here I argue that AGI systems that are <jats:italic>intent-aligned</jats:italic>—they always try to do what their operators want them to do—would also create catastrophic risks, mainly due to the power that they concentrate on their operators. With time, that power would almost certainly be catastrophically exploited, potentially resulting in human extinction or permanent dystopia. I suggest that liberal democracies, if they decide to allow the development of AGI, may react to this threat by letting AGI take shape as an <jats:italic>intergenerational social project</jats:italic>, resulting in an arrangement where AGI is not intent-aligned but <jats:italic>symbiotic</jats:italic> with humans. I provide some tentative ideas on what the resulting arrangement may look like and consider what speaks for and what against aiming for intent-aligned AGI as an intermediate step.</jats:p>"
10.1007/s43681-022-00174-4,A probabilistic theory of trust concerning artificial intelligence: can intelligent robots trust humans?,N/A
10.21467/proceedings.157.k1,Keynote Talk: AI at Africa's Crossroads: Extractive or Generative Future?,"<jats:p>In African traditions, the crossroads is where the trickster makes his/her appearance. Eshu, Legba, Anansi and others create complexity when our decisions fold back on themselves. AI has created yet another crossroads, and again the trickster brings surprises. What might have seemed like Africa’s worst challenges-“underdeveloped” from the colonial perspective-could be the basis by which computational aids can facilitate more sustainable and egalitarian futures. Blending the heritage algorithms of Africa’s past with full stack decolonization can guide us through the crossroads, on the path towards generative justice.</jats:p>"
10.1002/9781394308286.ch9,"The Future of
            <scp>AI</scp>
            in Financial Forecasting",N/A
10.1007/s43681-024-00493-8,The ethics of using artificial intelligence in scientific research: new guidance needed for a new tool,"<jats:title>Abstract</jats:title><jats:p>Using artificial intelligence (AI) in research offers many important benefits for science and society but also creates novel and complex ethical issues. While these ethical issues do not necessitate changing established ethical norms of science, they require the scientific community to develop new guidance for the appropriate use of AI. In this article, we briefly introduce AI and explain how it can be used in research, examine some of the ethical issues raised when using it, and offer nine recommendations for responsible use, including: (1) Researchers are responsible for identifying, describing, reducing, and controlling AI-related biases and random errors; (2) Researchers should disclose, describe, and explain their use of AI in research, including its limitations, in language that can be understood by non-experts; (3) Researchers should engage with impacted communities, populations, and other stakeholders concerning the use of AI in research to obtain their advice and assistance and address their interests and concerns, such as issues related to bias; (4) Researchers who use synthetic data should (a) indicate which parts of the data are synthetic; (b) clearly label the synthetic data; (c) describe how the data were generated; and (d) explain how and why the data were used; (5) AI systems should not be named as authors, inventors, or copyright holders but their contributions to research should be disclosed and described; (6) Education and mentoring in responsible conduct of research should include discussion of ethical use of AI.</jats:p>"
10.4018/979-8-3693-0074-9.ch015,Integrating Artificial Intelligence (AI) Into the Curriculum,"<jats:p>“Bridging the Gap: Integrating Artificial Intelligence into the Curriculum,” addresses the urgent need to infuse AI education into school curriculums amidst AI's growing influence in various sectors. It advocates for a shift from reactive to proactive educational strategies, emphasizing the role of education systems in mitigating the expanding skills gap and nurturing a generation adept in AI, machine learning, and data analytics. The chapter proposes a comprehensive AI curriculum integration framework, focusing on developing key AI competencies and ethical considerations, alongside strategies for effective implementation.</jats:p>"
10.1109/icca59364.2023.10401723,"Deepfakes, Misinformation, and Disinformation in the Era of Frontier AI, Generative AI, and Large AI Models",N/A
10.1007/s43681-023-00384-4,Complex equality and the abstractness of statistical fairness: using social goods to analyze a CV scanner and a welfare fraud detector,N/A
10.1007/s43681-021-00110-y,"Disruption, technology and the question of (artificial) identity","<jats:title>Abstract</jats:title><jats:p>The current state of human–machine interaction has set forth a process of hybridization of human identity. Technology—and most notably AI—is used as an effective cognitive extender, which enables the extension of human personhood to include artificial elements, leading to the emergence of artificial identity. Discussing—and accommodating—anthropomorphization in human–machine interaction should no longer be the primary focus. Rather, the scope and quality of frameworks in which the hybridization of human identity occurs and evolves has significant ethical implications that pose very pragmatic challenges to users, the industry, and regulators. This paper puts forth a few main principles upon which such a discussion should evolve. We illustrate why disruptiveness can easily turn into human harm when the frameworks facilitating it overlook the human vulnerabilities that arise from hybrid identity, notably the asymmetric and asynchronous relationship between the human and artificial counterparts. Finally, we claim that these new types of vulnerabilities, to which a person is exposed due to the intimate degree of pairing with technology, justifies introducing and protecting artificial identity as well.</jats:p>"
10.1007/s43681-023-00298-1,HCI driving alienation: autonomy and involvement as blind spots in digital ethics,"<jats:title>Abstract</jats:title><jats:p>The ongoing development and adoption of digital technologies such as AI in business brings ethical concerns and challenges. Main topics are the design of digital technologies, their tasks, and competencies in organizational practice, and their collaboration with humans. Previous guidelines on digital ethics mainly consider technological aspects such as the nondiscriminatory design of AI, its transparency, and technically constrained (distributed) agency as priorities in AI systems, leaving the consideration of the human factor and the implementation of ethical guidelines in organizational practice unclear. We analyze the relationship between human–computer interaction (HCI), autonomy, and worker involvement with its impact on the experience of alienation at work for workers. We argue that the consideration of autonomy and worker involvement is crucial for HCI. Based on a quantitative empirical study of 1989 workers in Germany, the analysis shows that when worker involvement is high, the effect of HCI use on alienation decreases. The study results contribute to the understanding of the use of digital technologies with regard to worker involvement, reveal a blind spot in widespread ethical debates about AI, and have practical implications with regard to digital ethics in organizational practice.</jats:p>"
10.21717/ylr.34.1.13,Cybercrime and Criminal Legal Response in the Era of Generative AI,N/A
10.3390/ai3020017,Enhancement of Partially Coherent Diffractive Images Using Generative Adversarial Network,"<jats:p>We present a deep learning-based generative model for the enhancement of partially coherent diffractive images. In lensless coherent diffractive imaging, a highly coherent X-ray illumination is required to image an object at high resolution. Non-ideal experimental conditions result in a partially coherent X-ray illumination, lead to imperfections of coherent diffractive images recorded on a detector, and ultimately limit the capability of lensless coherent diffractive imaging. The previous approaches, relying on the coherence property of illumination, require preliminary experiments or expensive computations. In this article, we propose a generative adversarial network (GAN) model to enhance the visibility of fringes in partially coherent diffractive images. Unlike previous approaches, the model is trained to restore the latent sharp features from blurred input images without finding coherence properties of illumination. We demonstrate that the GAN model performs well with both coherent diffractive imaging and ptychography. It can be applied to a wide range of imaging techniques relying on phase retrieval of coherent diffraction patterns.</jats:p>"
10.48054/ksh.2024.19.1,Homiletical Criticism and Imagination : Preachers and the Era of Generative AI,N/A
10.1007/s00146-022-01602-z,AI ethics: from principles to practice,N/A
10.1007/s00146-023-01708-y,"AI ethics discourse: a call to embrace complexity, interdisciplinarity, and epistemic humility",N/A
10.1007/978-3-031-55744-6_4,Ethical Foundations: Medical Ethics and Data Ethics,N/A
10.2139/ssrn.4716233,Generative AI and Copyright: A Dynamic Perspective,N/A
10.32388/tavpny,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/za0xy2,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/2yiboy,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.24135/pjtel.v5i1.175,Generative AI and education ecologies,"<jats:p>What role can generative AI have an art and design education? Given that we are in a year of change as open-source Open AI systems shift how we teach, learn, and assess in times of question-answering chatbot and personal assistance tools. Applying a post-human approach (Blaikie, et al, 2020) to education might help us rethink pedagogy (Wessels, et al, 2022), knowledge creation and scholarly publication for knowledge sharing. In this SoTEL Symposium presentation/discussion with the ASCILITE MLSIG I propose a move away from a humanist world view that continues to shape our thoughts around the binary of teacher-learner within our walled disciplinary and consider how we might Incorporate generative AI tools in the curriculum to foster interdisciplinary collaborations with the more-than human. What if we shifted teaching and learning to facilitate new ways of being on the planet, so that we prioritised ourselves, one another as well as non-human and more-than-humans in our educational ecologies. Building the digital literacies and computational thinking capabilities (George-Reyes, et al, 2021) to learn with GAI will create opportunities to thinking about the world and all its space and places, as interconnected and entangled.&#x0D;
In this trendsetter webinar I pose a series of questions and prompts that I had in conversation with Chatty G (ChatGPT) to consider how we might imagine and understand the world in different ways so that we might integrate generative AI and into our education ecologies in higher education.&#x0D;
Presentation: https://doi.org/10.26188/22281685</jats:p>"
10.2139/ssrn.4850714,Challenges of Generative AI Chatbots in Public Services -An Integrative Review,N/A
10.1515/9781501519741-006,"Chapter 5: Generative AI, Bard, and Gemini",N/A
10.2139/ssrn.4553431,Equitable Legal Remedies and the Existential Threat to Generative AI,N/A
10.69732/hkor7362,Making Generative AI Work for Language Teachers: IALLT Featured Webinar of 2023,N/A
10.2139/ssrn.4888339,Evaluating the Impact of Report Readability on ESG Scores: A Generative Ai Approach,N/A
10.2139/ssrn.4436627,Generative AI and Firm Values,N/A
10.15308/sinteza-2024-392-397,Generative AI Tools in Web Design,N/A
10.47289/aiej20210716-2,Good AI for the Present of Humanity Democratizing AI Governance,"<jats:p>What do Cyberpunk and AI Ethics have to do with each other? Cyberpunk is a sub-genre of science fiction that explores the post-human relationships between human experience and technology. One similarity between AI Ethics and Cyberpunk literature is that both seek to explore future social and ethical problems that our technological advances may bring upon society. In recent years, an increasing number of ethical matters involving AI have been pointed and debated, and several ethical principles and guides have been suggested as governance policies for the tech industry. However, would this be the role of AI Ethics? To serve as a soft and ambiguous version of the law? We would like to advocate in this article for a more Cyberpunk way of doing AI Ethics, with a more democratic way of governance. In this study, we will seek to expose some of the deficits of the underlying power structures of the AI industry, and suggest that AI governance be subject to public opinion, so that ‘good AI’ can become ‘good AI for all.’</jats:p>"
10.3390/ai3020021,Shifting Perspectives on AI Evaluation: The Increasing Role of Ethics in Cooperation,"<jats:p>Evaluating AI is a challenging task, as it requires an operative definition of intelligence and the metrics to quantify it, including amongst other factors economic drivers, depending on specific domains. From the viewpoint of AI basic research, the ability to play a game against a human has historically been adopted as a criterion of evaluation, as competition can be characterized by an algorithmic approach. Starting from the end of the 1990s, the deployment of sophisticated hardware identified a significant improvement in the ability of a machine to play and win popular games. In spite of the spectacular victory of IBM’s Deep Blue over Garry Kasparov, many objections still remain. This is due to the fact that it is not clear how this result can be applied to solve real-world problems or simulate human abilities, e.g., common sense, and also exhibit a form of generalized AI. An evaluation based uniquely on the capacity of playing games, even when enriched by the capability of learning complex rules without any human supervision, is bound to be unsatisfactory. As the internet has dramatically changed the cultural habits and social interaction of users, who continuously exchange information with intelligent agents, it is quite natural to consider cooperation as the next step in AI software evaluation. Although this concept has already been explored in the scientific literature in the fields of economics and mathematics, its consideration in AI is relatively recent and generally covers the study of cooperation between agents. This paper focuses on more complex problems involving heterogeneity (specifically, the cooperation between humans and software agents, or even robots), which are investigated by taking into account ethical issues occurring during attempts to achieve a common goal shared by both parties, with a possible result of either conflict or stalemate. The contribution of this research consists in identifying those factors (trust, autonomy, and cooperative learning) on which to base ethical guidelines in agent software programming, making cooperation a more suitable benchmark for AI applications.</jats:p>"
10.2139/ssrn.4576667,What Should City of Boston Mayor Wu Do About Generative AI?,N/A
10.2139/ssrn.4746753,Stepping Above the Generative Ai Ethical Floor: The Sky's the Limit,N/A
10.1109/mm.2024.3425209,Navigating Applications Development in Generative AI,N/A
10.32388/jp02rg,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/wxb7qk,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.2139/ssrn.4793713,When Generative Artificial Intelligence Meets Multimodal Composition: Rethinking the Composition Process Through an Ai-Assisted Design Project,N/A
10.21203/rs.3.rs-4289440/v1,Generative AI To Recognize Response Fabrication in PTSD,"<title>Abstract</title>
        <p>Fabricating symptoms of Post-Traumatic Stress Disorder (PTSD) can hinder accurate clinical assessments via structured diagnostic interviews1,2. Symptom simulation or fabrication is a known problem3,4 in PTSD assessments, with diverse motivations including unmet mental health issues, varied socio-economic factors and the potential for external gain from positive diagnoses. Here we introduce an artificial intelligence (AI) framework referred to as Algorithm VeRITAS (Vetting Response Integrity from cross-Talk in Adversarial Surveys), for detecting symptom fabrication in the context of PTSD diagnosis. In contrast to current approaches to fabrication detection which indirectly assess atypical symptom presentations, and have limited reliability, VeRITAS infers statistical dependencies inherent in true response patterns, flagging responses which violate these subtle constraints. With a study sample of n = 651 patients, VeRITAS has an Area Under the Curve (AUC) of ≧ 0:95 ± 0:02, with sensitivity &gt; 95%, specificity &gt; 88%, and positive likelihood ratio between 9:9 - 19:77. Additionally, VeRITAS is difficult-to-impossible to beat with coaching or training; we demonstrate that having advanced training in mental health diagnosis is not helpful in defeating the algorithm. Our tool offers an objective, diseasespecific, fast (average time ≦ 4 min) detection of simulated or feigned PTSD, and on wider adoption, can potentially help resources and disability concessions reach those genuinely in need, while helping to maintain integrity of clinical data. Moreover, reliably identifying patients who might be fabricating symptoms due to unmet mental health needs or socio-economic compulsions can ultimately improve outcomes in disadvantaged communities.</p>"
10.4324/9781003507949,Generative AI in the English Composition Classroom,N/A
10.2139/ssrn.4779486,How do Big Data and Generative AI dawn on Computational Biology?,N/A
10.21203/rs.3.rs-4259586/v1,Evaluating Diversity and Stereotypes Amongst Healthcare Providers Using Generative AI,"<title>Abstract</title>
        <p>Generative AI provides synthetic simulation of existing societal data. We hypothesized that Generative AI output may be used to evaluate diversity and stereotypes amongst healthcare providers. Dall-E 3, a text-to-image generator, was used to generate a total of 360 images based on pre-defined healthcare provider terms. Consensus scoring was performed to evaluate diversity parameters in images. Google Vision was used to generate image labels that were then categorized to analyze differences among race and sex cohorts. Sex and race diversity for various doctor and nurse terms was modest: 3.2 and 2.8, respectively, on a qualitative 5 point scale (where 5 represents equal diversity). These results are consistent with recently reported statistics, demonstrating that Generative AI reflects real-world data. We also identified stereotypes related to appearance, facial expressions, and clothing associated with sex and race. Our study, which is the first of its kind, provides a unique framework incorporating Generative AI and ML tools to quantify diversity and societal perceptions of healthcare providers. The proposed framework provides real-time intelligence on biases in the healthcare workforce.</p>"
10.1109/mc.2024.3350290,"Large Language Models and Generative AI, Oh My!",N/A
10.53555/jrtdd.v4i2.3037,The Psychology of Finance: A Generative AI Perspective,N/A
10.1109/cai59869.2024.00093,Effective Generative AI Implementation in Developing Country Universities,N/A
10.1093/polsoc/puae022,Governance fix? Power and politics in controversies about governing generative AI,"<jats:title>Abstract</jats:title>
               <jats:p>The launch of ChatGPT in late 2022 led to major controversies about the governance of generative artificial intelligence (AI). This article examines the first international governance and policy initiatives dedicated specifically to generative AI: the G7 Hiroshima process, the Organisation for Economic Cooperation and Development reports, and the UK AI Safety Summit. This analysis is informed by policy framing and governance literature, in particular by the work on technology governance and Responsible Innovation. Emerging governance of generative AI exhibits characteristics of polycentric governance, where multiple and overlapping centers of decision-making are in collaborative relationships. However, it is dominated by a limited number of developed countries. The governance of generative AI is mostly framed in terms of the risk management, largely neglecting issues of purpose and direction of innovation, and assigning rather limited roles to the public. We can see a “paradox of generative AI governance” emerging, namely, that while this technology is being widely used by the public, its governance is rather narrow. This article coins the term “governance fix” to capture this rather narrow and technocratic approach to governing generative AI. As an alternative, it suggests embracing the politics of polycentric governance and Responsible Innovation that highlight democratic and participatory co-shaping of technology for social benefit. In the context of the highly unequal distribution of power in generative AI characterized by a high concentration of power in a small number of large tech companies, the government has a special role in reshaping the power imbalances by enabling wide-ranging public participation in the governance of generative AI.</jats:p>"
10.1021/acs.jmedchem.4c01034.s002,Accelerated Discovery of Carbamate Cblb Inhibitors Using Generative AI Models and Structure-Based Drug Design,N/A
10.1021/acs.jmedchem.4c01034.s001,Accelerated Discovery of Carbamate Cblb Inhibitors Using Generative AI Models and Structure-Based Drug Design,N/A
10.5040/9781509974979.ch-006,Uneasy Questions and Imperfect Solutions,N/A
10.69554/zlcg3875,Generative AI in securities services,"<jats:p xml:lang=""en"">Generative AI (GenAI) is a technology that, since the launch of ChatGPT in November 2022, has taken the world by storm. While a lot of the conversation around GenAI is hype, there are some real applications of this technology that can bring real value to businesses. There are, however, risks in applying this technology blindly that sometimes can outweigh the value it brings. This paper discusses the potential applicability of GenAI to the processes in post-trade and what impact it could have on financial institutions and their ability to meet challenges in the market, such as T+1. We also discuss the risks of implementing this technology and how these can be mitigated, as well as ensuring that all the objectives are met not only from a business perspective, but also technology and compliance.</jats:p>"
10.1007/s10029-023-02926-5,Can Generative AI assist pediatric inguinal hernia issues?,N/A
10.32388/gqu4uj,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/7f4v5q,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.47287/cen-10112-cover,Generative AI is dreaming up new proteins,N/A
10.2139/ssrn.4776408,"Generative Ai, Chatbots, and Programming Skills: Examining Code Suggestion Features in Enhancing Self-Learning Capabilities",N/A
10.2139/ssrn.4823561,Preserving Competition in Generative AI: Addressing the Merger Conundrum,N/A
10.1201/9781003503781-2,"Overview of Artificial Intelligence, Neural Networks, and Machine Learning",N/A
10.35542/osf.io/jyma4,Perceptions About Generative AI and ChatGPT Use by Faculty and College Students,"<p>Since November 2022, ChatGPT has created a stir on college campuses. November, 2022. Many colleges and universities have taken a proactive approach and updated academic integrity policies or even outright banned the use of ChatGPT (Clercq, 2023; Mearian, 2023; Schwartz, 2023). As this new technology continues to evolve and expand, colleges and universities are grappling with the opportunities and challenges of using such tools. Very little literature exists on student and faculty perceptions of AI use in higher education, particularly related to generative AI tools.  The present study aims to fill this gap and offer perceptions from both students and faculty from a large research university in the mid-Atlantic. Survey participants consisted of 286 faculty and 380 students across multiple campuses. Participants completed an online questionnaire that included open-ended responses, scaled items, and finite questions.  Overall, the reported use of ChatGPT technology is infrequent, though most respondents feel its use is inevitable in higher education. We find faculty and students are uncertain but familiar with generative AI tools and ChatGPT. Institutions interested in developing policies around using ChatGPT on campus may benefit from building trust in generative AI, for both faculty and students. Concerns with academic integrity are prevalent with faculty and students agreeing ChatGPT violates</p>"
10.1109/indicon56171.2022.10039793,Deep Learning-driven Explainable AI using Generative Adversarial Network (GAN),N/A
10.33313/388/225,Generative AI for Process Modeling in the Steel Industry,N/A
10.46697/001c.90397,Reinventing International Business Education: Integrating the Power of Generative AI,"<jats:p>As artificial intelligence (AI) reshapes global business, international business (IB) education must adapt. This article explores the incorporation of Generative AI (GenAI) into IB curricula, examining course fit and faculty readiness, while presenting actional recommendations across three dimensions: Engagement, Collaboration, and Academic Integrity. We propose methods for interactive learning and lesson engagement using GenAI’s conversational interface and prompts engineering. We also propose leveraging GenAI as a multifaceted tool to enhance international teamwork and collaboration and cultivate cross-cultural and linguistic connections. Additionally, we outline measures to prevent its misuse and mitigate the inherent threats it poses to academic integrity and assessment.</jats:p>"
10.2139/ssrn.4341500,Is ChatGPT Leading Generative AI? What is Beyond Expectations?,N/A
10.31235/osf.io/xpdyr,"The sociocultural roots of artificial conversations: the taste, class and habitus of generative AI chatbots","<p>Research on AI has extensively considered biases related to gender and race. However, much less attention has been dedicated to another sociological tenet: that of class. Inspired by Bourdieu’s work on cultural stratification and distinction, this work sheds light on the sociocultural roots of artificial sociality, and on how these become manifest as “habitus” within the outputs of generative AI models. We conducted 39 interviews with three AI chatbots – ChatGPT, Gemini, and Replika – after asking them to impersonate individuals with different occupational positions: highly skilled professionals, blue-collar workers, university professors in the humanities, construction workers, computer scientists and hairdressers. Our qualitative study shows class-based regularities in how popular AI chatbots represent the lifestyle and tastes of fictional personas in artificial conversations, partly mediated by infrastructural and design elements. The paper proposes a sociological perspective on bias in artificial sociality, and experiments with interview methods in the study of generative AI.</p>"
10.32388/a8dyj7,Creating Image Datasets in Agricultural Environments using DALL.E: Generative AI-Powered Large Language Model,"<jats:p>This research investigated the role of artificial intelligence (AI), specifically the DALL.E model by OpenAI, in advancing data generation and visualization techniques in agriculture. DALL.E, an advanced AI image generator, works alongside ChatGPT's language processing to transform text descriptions and image clues into realistic visual representations of the content. The study used both approaches of image generation: text-to-image and image-to-image (variation). Six types of datasets depicting fruit crop environment were generated. These AI-generated images were then compared against ground truth images captured by sensors in real agricultural fields. The comparison was based on Peak Signal-to-Noise Ratio (PSNR) and Feature Similarity Index (FSIM) metrics. The image-to-image generation exhibited a 5.78% increase in average PSNR over text-to-image methods, signifying superior image clarity and quality. However, this method also resulted in a 10.23% decrease in average FSIM, indicating a diminished structural and textural similarity to the original images. Similar to these measures, human evaluation also showed that images generated using image-to-image-based method were more realistic compared to those generated with text-to-image approach. The results highlighted DALL.E's potential in generating realistic agricultural image datasets and thus accelerating the development and adoption of imaging-based precision agricultural solutions.
</jats:p>"
10.1109/aiiot61789.2024.10578969,Dataset Enlargement with Generative Adversarial Neural Networks,N/A
10.21203/rs.3.rs-3888891/v1,Revolutionizing Personalized Medicine with Generative AI: A Systematic Review,"<title>Abstract</title>
        <p>Background
 Precision medicine, targeting treatments to individual genetic and clinical profiles, faces challenges in data collection, costs, and privacy. Generative AI offers a promising solution by creating realistic, privacy-preserving patient data, potentially revolutionizing patient-centric healthcare.
Objective
 This review examines the role of deep generative models (DGMs) in clinical informatics, medical imaging, bioinformatics, and early diagnostics, showcasing their impact on precision medicine.
Methods
 Adhering to PRISMA guidelines, the review analyzes studies from databases such as Scopus and PubMed, focusing on AI's impact in precision medicine and DGMs' applications in synthetic data generation.
Results
 DGMs, particularly Generative Adversarial Networks (GANs), have improved synthetic data generation, enhancing accuracy and privacy. However, limitations exist, especially in the accuracy of foundation models like Large Language Models (LLMs) in digital diagnostics.
Conclusion
 Overcoming data scarcity and ensuring realistic, privacy-safe synthetic data generation are crucial for advancing personalized medicine. Further development of LLMs is essential for improving diagnostic precision. The application of generative AI in personalized medicine is emerging, highlighting the need for more interdisciplinary research to advance this field.</p>"
10.2139/ssrn.4483390,ChatGPT: A Case Study on Copyright Challenges for Generative AI Systems,N/A
10.2139/ssrn.4564372,Using Curriculum Theory to Inform Approaches to Generative AI in Schools,N/A
10.12781/978-1-907549-44-1-2,Introducing Generative Journalism,N/A
10.2139/ssrn.4532479,The Effects of Generative Ai on Initial Language Teacher Education: The Perspectives of Teacher Educators,N/A
10.32347/tit.2023.61.0301,Generative Ai: potential and pitfalls,"<jats:p>The explosive popularity of ChatGPT around the world gave us the first real tipping point in public acceptance of AI. Finally, everyone, everywhere can see the breakthrough potential of this technology for themselves. Large language models (LLM) and the fundamental models underlying these advances in generative artificial intelligence (GenAI) represent a significant turning point. Not only have they cracked the code of language complexity, allowing machines to learn context, infer intent, and be independent creative individuals, but they can be quickly configured to perform a wide variety of different tasks. This technology should fundamentally change everything — from science, business, health care to, in fact, society itself. The positive impact on human creativity and productivity will be enormous. Companies will use these models to rethink the way work is done. Every role in every enterprise has the potential to be reimagined, as AI people working as co-pilots become the norm, greatly expanding their capabilities. Generative AI will affect tasks, not professions. Some of these tasks will be automated, some will be transformed by artificial intelligence, and some will remain unchanged. It can also be expected that humans will face a large number of new challenges, such as ensuring the accurate and responsible use of GenAI systems. That's why organizations that invest in training people to work with generative AI will have a significant advantage.</jats:p>"
10.20944/preprints202406.0011.v1,"&lt;strong&gt;Generative Artificial Intelligence, AI for Scientific Writing: A Literature Review&lt;/strong&gt;","<jats:p>The growing usage of Generative AI tools in scientific writing requires a critical examination of their benefits and challenges. This literature review is aimed at comprehensively analyzing current empirical research articles focused on the application of Generative AI in scientific writing. The Google Scholar database was used to search for the literature. The following keywords were used: ""Generative AI"" and ""academic writing"", ""LLM"" (Large Language Models) and ""academic writing"", ""Generative AI"" and ""Scientific writing"", and ""ChatGPT"" and ""Scientific Writing"". The search was restricted to articles published between January 1, 2023, and April 30, 2024. 15 articles were selected as appropriate for the study and analyzed. It was found that, thus far, ChatGPT is the most exploited tool in the studies. AI tools such as Bard (Gemini), Bing, Claude2, and Elicit were also tested. The benefits of Generative AI usage in scientific writing were found to be omnipresent. It can aid in the generation of structured abstracts, titles, introductions, literature reviews, and conclusions of a scientific article. Generative AI also makes writing more efficient and time-saving. Its capabilities in improving language and proofreading are well-established. However, the generation of inaccurate content and references by current commercially available LLMs poses a serious problem. The lack of critical thinking and tendency to produce non-original content are significant drawbacks. Generative AI should be employed with human oversight, serving as an assistant rather than a replacement. Transparency in AI usage in scientific writing is essential, along with the necessity for proper legal regulation. </jats:p>"
10.2139/ssrn.4614907,Detecting Dark Patterns Using Generative AI: Some Preliminary Results,N/A
10.1007/979-8-8688-0083-2,Interfaceless,N/A
10.2139/ssrn.4784412,Data Disquiet: Concerns about the Governance of Data for Generative AI,N/A
10.3390/a17040136,Uncertainty in Visual Generative AI,"<jats:p>Recently, generative artificial intelligence (GAI) has impressed the world with its ability to create text, images, and videos. However, there are still areas in which GAI produces undesirable or unintended results due to being “uncertain”. Before wider use of AI-generated content, it is important to identify concepts where GAI is uncertain to ensure the usage thereof is ethical and to direct efforts for improvement. This study proposes a general pipeline to automatically quantify uncertainty within GAI. To measure uncertainty, the textual prompt to a text-to-image model is compared to captions supplied by four image-to-text models (GIT, BLIP, BLIP-2, and InstructBLIP). Its evaluation is based on machine translation metrics (BLEU, ROUGE, METEOR, and SPICE) and word embedding’s cosine similarity (Word2Vec, GloVe, FastText, DistilRoBERTa, MiniLM-6, and MiniLM-12). The generative AI models performed consistently across the metrics; however, the vector space models yielded the highest average similarity, close to 80%, which suggests more ideal and “certain” results. Suggested future work includes identifying metrics that best align with a human baseline to ensure quality and consideration for more GAI models. The work within can be used to automatically identify concepts in which GAI is “uncertain” to drive research aimed at increasing confidence in these areas.</jats:p>"
10.4324/9781003459026-6,Technology Behind GenAI,N/A
10.1109/acit58888.2023.10453782,Variational Autoencoder Frameworks in Generative AI Model,N/A
10.32388/fnjwdc,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/r16a87,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/dbqt20,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.1201/9781003486725-10,Generative Artificial Intelligence Readiness,N/A
10.1057/978-1-137-58499-1_6,Subversive–Generative Moral Imagination,N/A
10.21275/sr24209120048,Challenges and Opportunities in Intellectual Property Rights (IPR) in the Age of Generative AI: Balancing Innovation and Protection,N/A
10.4018/979-8-3693-0487-7.ch001,Exploring Ethical Considerations in Utilizing Generative AI for Global Knowledge Sharing in Higher Education,"<jats:p>This chapter delves into the ethical considerations surrounding the utilization of generative artificial intelligence (AI) for global knowledge sharing in higher education. As AI technologies continue to advance, their potential for transforming the educational landscape is becoming increasingly evident. However, the ethical implications of employing generative AI in this context warrant careful examination. The study explores the ethical dimensions related to the use of generative AI in higher education, focusing on issues such as data privacy, algorithmic bias, intellectual property rights, and the impact on human creativity and critical thinking. The findings of this study contribute to the ongoing discourse on the ethical implications of utilizing generative AI in higher education. By addressing these considerations, educators and policymakers can make informed decisions regarding the implementation of generative AI technologies, fostering a responsible and inclusive approach to global knowledge sharing in the higher education sector.</jats:p>"
10.4018/979-8-3693-0487-7.ch012,Innovative Teaching Methodology in Higher Education With Generative AI- Engineering Education in Developing Countries,"<jats:p>The rapidly evolving landscape of higher education is causing significant changes in the educational system. The needs of the digital world cannot be satisfied by conventional teaching and learning approaches. To improve educational quality, fostering global collaboration and knowledge sharing has become imperative for preparing students for a connected world. Therefore, technological integration in the classroom has become essential. Generative artificial intelligence (AI) has emerged as a powerful tool in this endeavour. In addition to examining the possible advantages and difficulties of incorporating technology in higher education, this study also assesses the extent of technology integration in teaching, referred to as innovative teaching methodologies (ITM). This chapter explores how generative AI can be leveraged to enhance teaching methods.</jats:p>"
10.35940/ijsce.d3636.14030724,Promoting Project Outcomes: A Development Approach to Generative AI and LLM-Based Software Applications’ Deployment,"<jats:p>In the dynamic realm of artificial intelligence, the emergence of Generative Artificial Intelligence (GAI) has marked a revolutionary stride, particularly in the context of project execution models. This paper delves deep into the sophisticated architectures of GAI, mainly focusing on Large Language Models (LLMs) such as GPT-3 and BERT and their practical applications across varied scenarios. The intricacies of deploying these models have been effectively unraveled to ensure resonating with the specific demands of distinct cases, falling within departmental integration, medical diagnostics, or tailored training modules. Central to the proposed exposition is the innovative ""Forward and Back Systematic Approach"" designed for executing GAI projects. This approach is meticulously structured to enhance efficiency and ensure a harmonious alignment with the nuanced requirements of diverse applications. We dissect some strategies, including leveraging Private Generalized LLM APIs, in-context learning (ICL), and fine-tuning methodologies, to empower these models to adapt and excel. Furthermore, the proposed platform underscores the pivotal role of evaluation criteria in refining GAI project outcomes, ensuring each model's prowess. It is not strictly theoretical but yields tangible benefits in real-world applications. Under the aegis of this comprehensive exploration, the result of the study would serve as a beacon for enthusiasts and professionals navigating the GAI landscape by offering insights into optimizing robust models for specific and case-driven utilities. Standing on the brink of a modern era in AI, this paper contributes a substantial framework and critical analysis, steering the course for future innovations and applications of GAI.</jats:p>"
10.32388/dn2058,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/tgifbr,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.5040/9781509974979.ch-001,A Map to This Book,N/A
10.2139/ssrn.4738753,Digital Competition’s Submission to the European Commission’s Consultation on Generative AI,N/A
10.32388/0qi028,Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing,"<jats:p>This paper provides an in-depth examination of generative artificial intelligence (AI) for customer profiling and social media marketing. A mixed methods approach combining social media metrics, customer surveys, and generative model analysis quantitatively demonstrates the ability of techniques like generative adversarial networks (GANs) and variational autoencoders (VAEs) to enhance marketing outcomes. Results show statistically significant improvements in engagement, clicks, followers, and sales from personalized, AI-generated content. Qualitative feedback indicates increased relevance, enjoyment, and brand loyalty. However, limitations like algorithmic bias and ethical risks around consent and privacy highlight the need for responsible innovation. Our research contributes one of the first empirical evaluations of generative AI's marketing potential while emphasizing cross-disciplinary perspectives to ensure human values remain central as this technology evolves.
</jats:p>"
10.3102/2110693,Democratizing Education Through Generative AI: Impacts on Diverse Student Learning Experiences,N/A
10.2139/ssrn.4794172,Generative AI Image Experiment: Case Study of a National Emblem,N/A
10.32388/fh0vig,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.52214/stlr.v25i2.12761,How Generative AI Turns Copyright Law Upside Down,"<jats:p>While courts are litigating many copyright issues involving generative AI, from who owns AI-generated works to the fair use of training to infringement by AI outputs, the most fundamental changes generative AI will bring to copyright law don’t fit in any of those categories. The new model of creativity, generative AI puts considerable strain on copyright’s two most fundamental legal doctrines: the idea-expression dichotomy and the substantial similarity test for infringement. Increasingly, creativity will be lodged in asking the right questions, not in creating the answers. Asking questions may sometimes be creative, but the AI does the bulk of the work that copyright traditionally exists to reward, and that work will not be protected. That inverts what copyright law now prizes. And because asking the questions will be the basis for copyrightability, similarity of expression in the answers will no longer be of much use in proving the copying of the questions. That means we may need to throw out our test for infringement, or at least apply it in fundamentally different ways.</jats:p>"
10.23977/law.2023.020704,The Infringement Risk and Legal Regulation of Generative AI Works,N/A
10.1145/3643834.3661594,DanceGen: Supporting Choreography Ideation and Prototyping with Generative AI,N/A
10.33965/ijcsis_2023180213,EMPOWERING GAME DESIGNERS WITH GENERATIVE AI,N/A
10.2139/ssrn.4568684,Generative AI and the Workforce: What Are the Risks?,N/A
10.7753/ijsea1308.1023,Advancements in Generative AI: Applications and Challenges in the Modern Era,N/A
10.2139/ssrn.4440717,Generative Ai and Firm Values,N/A
10.1016/j.bushor.2024.05.005,Generative AI in higher education and beyond,N/A
10.1145/3588430.3597250,Roblox Generative AI in action,N/A
10.1007/978-3-030-29145-7_2,Generative Metaphors in Cybersecurity Governance,N/A
10.54808/jsci.21.04.42,The Ethics of Artificial Intelligence in the Era of Generative AI,"<jats:p>In the early 2020s, advances in transformer-based deep neural networks enabled the development and growth of a number of generative artificial intelligence (GenAI) systems notable for accepting natural language prompts as input. These include large language model chatbots such as ChatGPT, Bard, and others. GenAI has applications across a wide range of industries, including art, writing, software development, product design, healthcare, finance, gaming, and more. In this paper, we place these recent advances in a historical, cybernetic context. We analyze ethical issues that arise in the area of software engineering and cyber-physical systems. In addition, we explore AI-based challenges in healthcare and medicine, including a number involving GenAI. This research shows the importance of rigorous ethical analysis and resulting safeguards to address the emerging issues with AI.</jats:p>"
10.1016/s2589-2959(24)00041-9,Introduction: Navigating ethics at the intersection of AI and neuroscience,N/A
10.1007/s43681-023-00311-7,The sophistry of the neutral tool. Weaponizing artificial intelligence and big data into threats toward social exclusion,N/A
10.21474/ijar01/18578,EXPLORING ETHICAL CONSIDERATIONS IN GENERATIVE AI,"<jats:p>Generative AI, which encompasses a range of technologies such as generative adversarial networks (GANs), language models, and image generators, has shown remarkable progress in recent years. These technologies have the potential to revolutionize various fields, from art and entertainment to healthcare and education. However, along with these advancements come ethical considerations that must be carefully addressed. This research paper examines the ethical challenges posed by generative AI, including issues related to bias, privacy, misinformation, and intellectual property. It also discusses strategies for mitigating these risks and fostering the responsible development and deployment of generative AI technologies.</jats:p>"
10.2139/ssrn.4544943,Some Emerging Hypotheses about Using Generative AI in Public Sector Operations,N/A
10.2139/ssrn.4562699,Comments on 'Canadian Guardrails for Generative AI – Code of Practice',N/A
10.2139/ssrn.4566945,Retrofitting Fair Use: Art &amp;amp; Generative AI After Warhol,N/A
10.18260/1-2--45579,Using Generative AI as an Active Learning Tool to Refine Professional Engineering Skills,N/A
10.32388/4nkh7h,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/rtxnay,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.2139/ssrn.4452504,Generative AI in Operational Risk Management: Harnessing the Future of Finance,N/A
10.2139/ssrn.4911494,Elevating and Sharpening Convergent Thinking: The Potential of Generative AI for Creative Professionals,N/A
10.14305/jn.29960819.2024.1.1.10,Generating Deep Discussion Around Generative AI,N/A
10.1109/isscc49657.2024.10454562,1.3 Computing in the Era of Generative AI,N/A
10.1109/mc.2024.3370292,Today Generative AI Is Just a Parlor Trick,N/A
10.1097/nne.0000000000001590,Generative AI Backstories for Simulation Preparation,"<jats:sec>
            <jats:title>Background:</jats:title>
            <jats:p>Developing engaging presimulation learning materials that provide contextualized patient information is needed to best prepare students for nursing simulation. One emerging strategy that can be used by educators to create visual images for storytelling is generative artificial intelligence (AI).</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Purpose:</jats:title>
            <jats:p>The purpose of this pilot study was to determine how the use of generative AI–created patient backstories as a presimulation strategy might affect student engagement and learning in nursing simulation.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods:</jats:title>
            <jats:p>A qualitative cross-sectional survey with content analysis was completed with undergraduate nursing students following an acute care simulation.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results:</jats:title>
            <jats:p>Student surveys point to positive pedagogical outcomes of using AI image generation as a strategy to prepare for simulation such as decreased anxiety in simulation, increased preparatory knowledge, and increased emotional connection with the patient's story.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions:</jats:title>
            <jats:p>Images created with generative AI hold promise for future research and transforming nursing education.</jats:p>
          </jats:sec>"
10.36227/techrxiv.171527587.75649430/v1,"Applications of Generative AI in Healthcare: algorithmic, ethical, legal and societal considerations","<jats:p id=""p1"">Generative AI is rapidly transforming medical imaging and text analysis, offering immense potential for enhanced diagnosis and personalized care. However, this transformative technology raises crucial ethical, societal, and legal questions. This paper delves into these complexities, examining issues of accuracy,
informed consent, data privacy, and algorithmic limitations in the context of generative AI’s application to medical imaging and text. We explore the legal landscape surrounding liability and accountability, emphasizing the need for robust regulatory frameworks. Furthermore, we dissect the algorithmic challenges,
including data biases, model limitations, and workflow integration. By critically analyzing these challenges and proposing responsible solutions, we aim to foster a roadmap for ethical and responsible implementation of generative AI in healthcare, ensuring its transformative potential serves humanity with utmost care and precision.</jats:p>"
10.21125/edulearn.2023.0253,GENERATIVE AI AND CREATIVITY: HOW CHATGPT CAN ENHANCE STUDENT CREATIVITY,N/A
10.1007/s43681-020-00034-z,Moral responsibility for computationally designed products,"<jats:title>Abstract</jats:title><jats:p>Computational design systems (such as those using evolutionary algorithms) can create designs for a variety of physical products. Introducing these systems into the design process risks creating a ‘responsibility gap’ for flaws in the products they are used to create, as human designers may no longer believe that they are wholly responsible for them. We respond to this problem by distinguishing between causal responsibility and capacity responsibility (the ability to be morally responsible for actions) for creating product designs to argue that while the computational design systems and human designers are both casually responsible for creating product designs, the human designers who use these systems and the developers who create them have capacity responsibility for such designs. We show that there is no responsibility gap for products designed using computational design systems by comparing different accounts of moral responsibility for robots and AI (instrumentalism, machine ethics, and hybrid responsibility). We argue that all three of these accounts of moral responsibility for AI systems support the conclusion that the product designers who use computational design systems and the developers of these systems are morally responsible for any flaws or faults in the products designed by these systems. We conclude by showing how the responsibilities of accountability and blameworthiness should be attributed between the product designers, the developers of the computational design systems.</jats:p>"
10.4018/978-1-6684-9196-6.ch002,AI and Ethics,"<jats:p>Artificial intelligence (AI) is rapidly transforming our world and bringing about new possibilities and advancements in various industries. However, with this new technology also comes ethical considerations and challenges. Navigating the moral landscape of AI is crucial in ensuring that its development and implementation align with the values and principles of society. One major ethical concern in the development of AI is its potential to cause harm to humans. For example, biased algorithms in decision-making processes can lead to discrimination and unequal treatment of certain individuals or groups. In addition, the use of AI in areas such as autonomous weapons raises serious questions about accountability and responsibility in the event of harm. Ensuring that AI systems are transparent, explainable, and free from bias is crucial in avoiding negative consequences.</jats:p>"
10.1007/s43681-021-00050-7,Human rights for robots? A literature review,N/A
10.1007/s43681-021-00089-6,Moral exemplars for the virtuous machine: the clinician’s role in ethical artificial intelligence for healthcare,"<jats:title>Abstract</jats:title><jats:p>Artificial Intelligence (AI) continues to pervade several aspects of healthcare with pace and scale. The need for an ethical framework in AI to address this has long been recognized, but to date most efforts have delivered only high-level principles and value statements. Herein, we explain the need for an ethical framework in healthcare AI, the different moral theories that may serve as its basis, the rationale for why we believe this should be built around virtue ethics, and explore this in the context of five key ethical concerns for the introduction of AI in healthcare. Some existing work has suggested that AI may replace clinicians. We argue to the contrary, that the clinician will not be replaced, nor their role attenuated. Rather, they will be integral to the responsible design, deployment, and regulation of AI in healthcare, acting as the moral exemplar for the virtuous machine. We collate relevant points from the literature and formulate our own to present a coherent argument for the central role of clinicians in ethical AI and propose ideas to help advance efforts to employ ML-based solutions within healthcare. Finally, we highlight the responsibility of not only clinicians, but also data scientists, tech companies, ethicists, and regulators to act virtuously in realising the vision of ethical and accountable AI in healthcare.</jats:p>"
10.1007/s43681-023-00375-5,From language to algorithm: trans and non-binary identities in research on facial and gender recognition,N/A
10.1007/s43681-022-00254-5,Trustworthy tech companies: talking the talk or walking the walk?,"<jats:title>Abstract</jats:title><jats:p>While people are increasingly dependent on tech companies to live a flourishing life, numerous incidents reveal that these companies struggle with genuinely taking the interests of customers to heart. Regulators and companies alike acknowledge that this should change and that companies must take responsibility for their impact. If society is to benefit from these innovations, it is paramount that tech companies are trustworthy. However, it is unclear what is required of tech companies to be recognized as trustworthy. This vagueness is risky, as it may lead to ethics washing and an ill-founded sense of security. This raises the question: what should tech companies do to deserve our trust? What would make them <jats:italic>trustworthy</jats:italic>? This article critically analyzes the philosophical debate on trustworthiness to develop a trustworthiness account for tech companies. It concludes that for tech companies to be trustworthy they need to actively signal their trustworthiness through the design of their applications (1), nurture techno-moral competences and practical wisdom in tech employees (2) and go beyond legal compliance (3).</jats:p>"
10.1007/s43681-023-00368-4,A-XAI: adversarial machine learning for trustable explainability,N/A
10.4018/979-8-3693-2643-5.ch004,The Role of AI Ethics in Cost and Complexity Reduction,"<jats:p>This chapter underscores the intrinsic connection between AI ethics and contemporary business dynamics, elucidating how ethical considerations can serve as facilitators for cost control and operational simplification. It offers a comprehensive exploration of how corporations can harness ethical principles to augment efficiency, transparency, and sustainability. This exploration delves into the impediments, strategic methodologies, and tangible benefits associated with the ethical deployment of AI. Commencing with a succinct overview of AI ethics, the chapter delineates key concepts, notably responsibility, justice, and transparency, thereby establishing a foundational understanding of their pivotal significance across diverse business scenarios. The intricate correlation between AI implementation and cost management is accentuated, with a focal point on how ethical AI practices contribute to cost savings and operational streamlining.</jats:p>"
10.1007/s43681-022-00147-7,Fairness Score and process standardization: framework for fairness certification in artificial intelligence systems,N/A
10.23880/jqhe-16000361,"Generative AI and Patient Care: A Systematic Review Examining Applications, Limitations, and Future Directions for ChatGPT in Healthcare","<jats:p>Background: Generative artificial intelligence (AI) tools like ChatGPT have emerged as potentially valuable technologies to augment human expertise in healthcare. However, uncertainties remain regarding appropriate clinical applications and limitations. This review synthesizes current evidence on using generative AI for clinical decision support, patient data processing, and medical education. Methods: A systematic search of Web of Science, Scopus, and ProQuest databases identified 33 relevant studies published in 2023 examining ChatGPT for healthcare uses. Two reviewers extracted data on study characteristics, AI system details, key results, and authors' conclusions. Evidence was synthesized qualitatively using a comparative analysis approach. Results: Supervised use of ChatGPT-generated simulations appeared beneficial for clinical training, but oversight was critical. Numerous studies found risks in relying on ChatGPT's clinical suggestions given frequent factual errors, outdated recommendations, and inappropriate advice. However, ChatGPT demonstrated potential for enhancing workflows via medical documentation automation. Conclusions: While showing promise for constrained uses like supervised education and documentation, findings caution against open-ended ChatGPT integration in clinical practice currently. Additional large-scale comparative effectiveness research is imperative to establish evidence-based implementation guidance. Responsible translation requires governance, validation against literature, and focus on human-AI collaboration versus replacement. Further inquiry can illuminate best practices for balancing innovation and safety.</jats:p>"
10.1093/oxfordhb/9780197579329.013.3,AI Challenges for Society and Ethics,"<jats:title>Abstract</jats:title>
               <jats:p>Artificial intelligence is already being applied in and impacting many important sectors in society, including healthcare, finance, and policing. These applications will increase as AI capabilities continue to progress, which has the potential to be highly beneficial for society, or to cause serious harm. The role of AI governance is ultimately to take practical steps to mitigate this risk of harm while enabling the benefits of innovation in AI. This requires answering challenging empirical questions about current and potential risks and benefits of AI: assessing impacts that are often widely distributed and indirect, and making predictions about a highly uncertain future. It also requires thinking through the normative question of what beneficial use of AI in society looks like, which is equally challenging. Though different groups may agree on high-level principles that uses of AI should respect (e.g., privacy, fairness, and autonomy), challenges arise when putting these principles into practice. For example, it is straightforward to say that AI systems must protect individual privacy, but there is presumably some amount or type of privacy that most people would be willing to give up to develop life-saving medical treatments. Despite these challenges, research can and has made progress on these questions. The aim of this chapter will be to give readers an understanding of this progress, and of the challenges that remain.</jats:p>"
10.16997/book55.g,AI Ethics Needs Good Data,"<jats:p>Arguing that discourses on AI must engage with power and political economy this chapter, in particular, makes the case that we must move beyond the depoliticised language of ‘ethics’ currently deployed in determining whether AI is ‘good’ given the limitations of ethics as a frame through which AI issues can be viewed. In order to circumvent these limits, we use instead the language and conceptualisation of ‘Good Data’ which we view as a more expansive term to elucidate the values, rights and interests at stake when it comes to AI’s development and deployment. These considerations include, but go beyond privacy, as well as fairness, transparency and accountability to include explicit political economy critiques of power.</jats:p>"
10.1007/s43681-021-00061-4,"‘The names have changed, but the game’s the same’: artificial intelligence and racial policy in the USA",N/A
10.1007/s43681-022-00144-w,‘Design-for-responsible’ algorithmic decision-making systems: a question of ethical judgement and human meaningful control,N/A
10.1007/s43681-021-00095-8,Data minimization for GDPR compliance in machine learning models,N/A
10.1145/3529088,AI and Neurotechnology,"<jats:p>The merging of machine, body, and psyche is on the horizon due to the technological advancements enabled by neuroscience and AI.</jats:p>"
10.1007/978-3-030-51110-4_6,Risks in the Business of AI,N/A
10.1017/jme.2024.15,AI Chatbots and Challenges of HIPAA Compliance for AI Developers and Vendors,"<jats:title>Abstract</jats:title><jats:p>Developers and vendors of large language models (“LLMs”) — such as ChatGPT, Google Bard, and Microsoft’s Bing at the forefront—can be subject to Health Insurance Portability and Accountability Act of 1996 (“HIPAA”) when they process protected health information (“PHI”) on behalf of the HIPAA covered entities. In doing so, they become business associates or subcontractors of a business associate under HIPAA.</jats:p>"
10.1201/9781003267003-5,Current State of AI Ethics,N/A
10.1007/s43681-021-00125-5,The future of AI in our hands? To what extent are we as individuals morally responsible for guiding the development of AI in a desirable direction?,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) is becoming increasingly influential in most people’s lives. This raises many philosophical questions. One is what responsibility we have as individuals to guide the development of AI in a desirable direction. More specifically, how should this responsibility be distributed among individuals and between individuals and other actors? We investigate this question from the perspectives of five principles of distribution that dominate the discussion about responsibility in connection with climate change: effectiveness, equality, desert, need, and ability. Since much is already written about these distributions in that context, we believe much can be gained if we can make use of this discussion also in connection with AI. Our most important findings are: (1) Different principles give different answers depending on how they are interpreted but, in many cases, different interpretations and different principles agree and even strengthen each other. If for instance ‘equality-based distribution’ is interpreted in a consequentialist sense, effectiveness, and through it, ability, will play important roles in the actual distributions, but so will an equal distribution as such, since we foresee that an increased responsibility of underrepresented groups will make the risks and benefits of AI more equally distributed. The corresponding reasoning is true for need-based distribution. (2) If we acknowledge that someone has a certain responsibility, we also have to acknowledge a corresponding degree of influence for that someone over the matter in question. (3) Independently of which distribution principle we prefer, ability cannot be dismissed. Ability is not fixed, however and if one of the other distributions is morally required, we are also morally required to increase the ability of those less able to take on the required responsibility.</jats:p>"
10.1007/s43681-022-00199-9,Speciesist bias in AI: how AI applications perpetuate discrimination and unfair outcomes against animals,"<jats:title>Abstract</jats:title><jats:p>Massive efforts are made to reduce biases in both data and algorithms to render AI applications fair. These efforts are propelled by various high-profile cases where biased algorithmic decision-making caused harm to women, people of color, minorities, etc. However, the AI fairness field still succumbs to a blind spot, namely its insensitivity to discrimination against animals. This paper is a critical comment on current fairness research in AI. It is the first to describe the ‘speciesist bias’ and investigate it in several different AI systems by reflecting on the problem via a normative analysis and by probing, in several case studies, image recognition, word embedding, and language models with established methods for bias detection. We claim that animals matter morally and that discriminating against them is unethical. Furthermore, we provide evidence for speciesist biases in all the mentioned areas of AI. We find that speciesist biases are solidified by many mainstream AI applications, especially in the fields of computer vision as well as natural language processing. In both cases, this occurs because the models are trained on datasets in which speciesist patterns prevail. Therefore, AI technologies currently play a significant role in perpetuating and normalizing violence against animals. To change this, AI fairness frameworks must widen their scope and include mitigation measures for speciesist biases. This paper addresses the AI community in this regard and stresses the influence AI systems can have on either increasing or reducing the violence that is inflicted on animals, especially on farmed animals.</jats:p>"
10.1007/s43681-023-00385-3,"The more they think, the less they want: studying people’s attitudes about autonomous vehicles could also contribute to shaping them",N/A
10.1007/s43681-024-00533-3,The problem of fairness in tools for algorithmic fairness,N/A
10.1007/978-3-030-51110-4_4,Trust and Fairness in AI Systems,N/A
10.3354/esep00104,Ethics and bioprospecting in Antarctica,N/A
10.69554/odej3915,Four frontiers of AI ethics,"<jats:p xml:lang=""en"">The field of AI ethics is still solidifying, and is beginning to have tangible effects on the direction of innovation in the technology. This paper identifies four ethical frontiers that are likely to shape the future of AI. The first is the recognition of contexts represented by the data and culture of an AI system’s development, which can be a challenge to the aim of using AI to bring about greater prosperity and equity globally. Next, some of the difficulties and discoveries of investigations into AI explainability are discussed. The vulnerability of AI’s reliance on large amounts of data in the advent of data rights is described, along with the innovative solutions currently being explored to support secure, privacy-preserving AI. Finally, the two fields that best exemplify the ethics of AI-supported decision making serve as case studies into autonomy and accountability: medicine and warfare.</jats:p>"
10.2139/ssrn.4806030,Generative AI in higher education: The need to develop or revise academic integrity policies to ensure the ethical use of AI,N/A
10.22554/ijtel.v7i2.154,AI as Fad or AI as Lasting? Priorities for College Faculty Instructional Development for Generative Artificial Intelligence,"<jats:p>As generative artificial intelligence (AI) continues to transform the landscape of education, college faculty must be equipped with the necessary skills to navigate this digital frontier effectively. This position paper argues that instructional development programs for college faculty related to generative AI should focus on three key aspects: enhancing fundamental teaching skills, making AI more familiar to educators, and preventing burnout. These three areas are interconnected and can collectively contribute to the success of AI integration in higher education. In this paper, I present and critique the ChatGPT output. I found the output to be cogent and potentially useful, but limited by inconsistencies, lack of details, and hallucinations. Although AI output may be useful for guiding practice at a surface level, it could not capture the human voice and attention to detail necessary for scholarship.</jats:p>"
10.1111/nyas.15129,Generative AI and generative education,<jats:title>Abstract</jats:title><jats:p>There is much public anxiety about how today's students use chatbots to complete assignments. But AI's integration within schools will more deeply impact the next generation of college graduates. The market value of future college degrees is far from certain.</jats:p>
10.1201/9781032654829-6,The Use of Generative Artificial Intelligence (GenAI) Capabilities for Early Detection of Threats in the Digital Environment: The Good Side of GenAI,N/A
10.1007/s10676-021-09587-x,AI ethics and the banality of evil,N/A
10.20319/icssh.2024.92101,INVESTIGATING THE EFFECTS OF GENERATIVE-AI RESPONSES ON USER EXPERIENCE AFTER AI HALLUCINATION,"<jats:p>The integration of generative artificial intelligence (GenAI) systems into our daily lives has led to the phenomenon of ""AI hallucination,"" where AI produces convincing yet incorrect information, undermining both user experience and system credibility. This study investigates the impact of AI's responses, specifically appreciation and apology, on user perception and trust following AI errors. Utilizing attribution theory, we explore whether users prefer AI systems that attribute errors internally or externally and how these attributions affect user satisfaction. A qualitative methodology, featuring interviews with individuals aged 20 to 30 who have experience with conversational AI, has been employed. Respondents preferred AI to apologize in hallucination situations and to attribute the responsibility for the error to the outside world. Results show that transparency in error communication is essential for maintaining user trust, with detailed explanations. The research contributes to the understanding of how politeness and attribution strategies can influence user engagement with AI and has significant implications for AI development, emphasizing the need for error communication strategies that balance transparency and user experience.</jats:p>"
10.35280/kotpm.2024.27.1.2,Generative AI Technology for the Production of Animation,N/A
10.59728/jaie.2024.3.1.9,Appeared in AI educational books for lower grades AI content element analysis and critical review,"<jats:p>In this study, we attempted to analyze the AI literacy content elements found in artificial intelligence textbooks, focusing on educational books for lower grades, and seek a direction for developing artificial intelligence educational books for lower grades through critical consideration. As a result of this study, it was found that there was some bias in the proportion of common areas of elementary school artificial intelligence education subjects shown in artificial intelligence literacy education books for lower grades. Elementary schools are places where basic artificial intelligence literacy education must be provided to school-age students entering the era of artificial intelligence. Students in lower grades must also receive appropriate artificial intelligence literacy education tailored to their grade level of development. Therefore, even if it is a somewhat difficult and unfamiliar concept or theory without bias in each area, teaching and learning materials that take into account the developmental characteristics of lower grades should be developed and actively reflected in the elementary school artificial intelligence education content system. </jats:p>"
10.11591/ijai.v10.i1.pp121-130,Satellite image inpainting with deep generative adversarial neural networks,"<jats:p>This work addresses the problem of recovering lost or damaged satellite image pixels (gaps) caused by sensor processing errors or by natural phenomena like cloud presence. Such errors decrease our ability to monitor regions of interest and significantly increase the average revisit time for all satellites. This paper presents a novel neural system based on conditional deep generative adversarial networks (cGAN) optimized to fill satellite imagery gaps using surrounding pixel values and static high-resolution visual priors. Experimental results show that the proposed system outperforms traditional and neural network baselines. It achieves a normalized least absolute deviations error of (  &amp;amp;  decrease in error compared with the two baselines) and a mean squared error loss of  (  &amp;amp;  decrease in error) over the test set. The model can be deployed within a remote sensing data pipeline to reconstruct missing pixel measurements for near-real-time monitoring and inference purposes, thus empowering policymakers and users to make environmentally informed decisions.</jats:p>"
10.2139/ssrn.4519365,Regulation of (Generative) AI Requires Continuous Oversight (AustLII Submission on the ‘Safe and Responsible AI in Australia’ Discussion Paper),N/A
10.4018/979-8-3693-3731-8.ch009,Innovations in Healthcare,"<jats:p>This proposed chapter seeks to contribute to the ongoing discourse surrounding the promise and perils of generative AI in healthcare, offering insights and recommendations to guide informed decision-making and policy formulation in this rapidly evolving field. This chapter aims to delve into the revolutionary landscape of healthcare innovations, focusing on the nuanced interplay between promise and perils associated with Generative AI. As healthcare embraces artificial intelligence, the chapter will examine successful implementations, potential benefits, and, equally crucial, the risks and ethical considerations that come with the integration of Generative AI in the healthcare sector.</jats:p>"
10.3390/ai5030068,Harnessing Generative Artificial Intelligence for Digital Literacy Innovation: A Comparative Study between Early Childhood Education and Computer Science Undergraduates,"<jats:p>The recent surge of generative artificial intelligence (AI) in higher education presents a fascinating landscape of opportunities and challenges. AI has the potential to personalize education and create more engaging learning experiences. However, the effectiveness of AI interventions relies on well-considered implementation strategies. The impact of AI platforms in education is largely determined by the particular learning environment and the distinct needs of each student. Consequently, investigating the attitudes of future educators towards this technology is becoming a critical area of research. This study explores the impact of generative AI platforms on students’ learning performance, experience, and satisfaction within higher education. It specifically focuses on students’ experiences with varying levels of technological proficiency. A comparative study was conducted with two groups from different academic contexts undergoing the same experimental condition to design, develop, and implement instructional design projects using various AI platforms to produce multimedia content tailored to their respective subjects. Undergraduates from two disciplines—Early Childhood Education (n = 32) and Computer Science (n = 34)—participated in this study, which examined the integration of generative AI platforms into educational content implementation. Results indicate that both groups demonstrated similar learning performance in designing, developing, and implementing instructional design projects. Regarding user experience, the general outcomes were similar across both groups; however, Early Childhood Education students rated the usefulness of AI multimedia platforms significantly higher. Conversely, Computer Science students reported a slightly higher comfort level with these tools. In terms of overall satisfaction, Early Childhood Education students expressed greater satisfaction with AI software than their counterparts, acknowledging its importance for their future careers. This study contributes to the understanding of how AI platforms affect students from diverse backgrounds, bridging a gap in the knowledge of user experience and learning outcomes. Furthermore, by exploring best practices for integrating AI into educational contexts, it provides valuable insights for educators and scholars seeking to optimize the potential of AI to enhance educational outcomes.</jats:p>"
10.1007/s43681-021-00118-4,No-boundary thinking: a viable solution to ethical data-driven AI in precision medicine,"<jats:title>Abstract</jats:title><jats:p>Today Artificial Intelligence (AI) supports difficult decisions about policy, health, and our personal lives. The AI algorithms we develop and deploy to make sense of information, are informed by data, and based on models that capture and use pertinent details of the population or phenomenon being analyzed. For any application area, more importantly in precision medicine which directly impacts human lives, the data upon which algorithms are run must be procured, cleaned, and organized well to assure reliable and interpretable results, and to assure that they do not perpetrate or amplify human prejudices. This must be done without violating basic assumptions of the algorithms in use. Algorithmic results need to be clearly communicated to stakeholders and domain experts to enable sound conclusions. Our position is that AI holds great promise for supporting precision medicine, but we need to move forward with great care, with consideration for possible ethical implications. We make the case that a no-boundary or convergent approach is essential to support sound and ethical decisions. No-boundary thinking supports problem definition and solving with teams of experts possessing diverse perspectives. When dealing with AI and the data needed to use AI, there is a spectrum of activities that needs the attention of a no-boundary team. This is necessary if we are to draw viable conclusions and develop actions and policies based on the AI, the data, and the scientific foundations of the domain in question.</jats:p>"
10.1007/s00146-023-01658-5,AI ethics as subordinated innovation network,"<jats:title>Abstract</jats:title><jats:p>AI ethics is proposed, by the Big Tech companies which lead AI research and development, as the cure for diverse social problems posed by the commercialization of data-intensive technologies. It aims to reconcile capitalist AI production with ethics. However, AI ethics is itself now the subject of wide criticism; most notably, it is accused of being no more than “ethics washing” a cynical means of dissimulation for Big Tech, while it continues its business operations unchanged. This paper aims to critically assess, and go beyond the ethics washing thesis. I argue that AI ethics is indeed ethics washing, but not only that. It has a more significant economic function for Big Tech. To make this argument I draw on the theory of intellectual monopoly capital. I argue that ethics washing is better understood as a subordinated innovation network: a dispersed network of contributors beyond Big Tech’s formal employment whose research is indirectly planned by Big Tech, which also appropriates its results. These results are not intended to render AI more ethical, but rather to advance the business processes of data-intensive capital. Because the parameters of AI ethics are indirectly set in advance by Big tech, the ostensible goal that AI ethics sets for itself—to resolve the contradiction between business and ethics—is in fact insoluble. I demonstrate this via an analysis of the latest trend in AI ethics: the operationalization of ethical principles.</jats:p>"
10.1002/aaai.12052,"Looking back, looking ahead: Humans, ethics, and AI",N/A
10.1007/s00146-022-01473-4,Trust and ethics in AI,N/A
10.59400/cai.v2i1.1378,From bard to Gemini: An investigative exploration journey through Google’s evolution in conversational AI and generative AI,"<jats:p>The advent of artificial intelligence (AI) has significantly transformed various aspects of human life, particularly in information retrieval and assistance. This research presents a comprehensive evaluation of Gemini, previously known as Google Bard, a state-of-the-art AI chatbot developed by Google. Through a meticulous methodology encompassing both qualitative and quantitative approaches, this research aims to assess Gemini’s performance, usability, integration capabilities, ethical implications. Primary data collection methods, including user surveys and interviews, were utilized to gather towards the qualitative feedback on user experiences with Gemini, supplemented by secondary data analysis using tools such as Google Analytics to capture quantitative metrics. Performance evaluation involved benchmarking against other AI chatbots and technical analysis of Gemini’s architecture and training methods. User experience testing examined usability, engagement, and integration with Google Workspace and third-party services. Ethical considerations regarding data privacy, security, and biases in AI-generated content were also addressed, ensuring compliance with major regulations and promoting ethical AI practices. Acknowledging limitations and challenges inherent in the investigative exploration, data analysis was conducted using thematic and statistical methods to derive insights. The results and findings of this research offer valuable insights into the capabilities and limitations of Gemini, providing implications for future AI development, user interaction design, and ethical AI governance. By contributing to the ongoing discourse on AI advancements and their societal impact, this exploration facilitates informed decision-making and lays the groundwork for future research endeavors in the field of AI-driven conversational agents.</jats:p>"
10.1109/mitp.2023.3333073,"An Executive Guide to AI, Machine Learning, and Generative AI—With Some Help From ChatGPT and Bard",N/A
10.1109/emr.2023.3272799,Accelerating Innovation With Generative AI: AI-Augmented Digital Prototyping and Innovation Methods,N/A
10.60154/jaepp.2024.v25n1p67,Artificial Intelligence (AI) Ethics in Accounting,"<jats:p>The   rapid   advancement   of   artificial   intelligence   (AI)   has revolutionized  the  accounting  profession,  automating  tasks,  identifying patterns,  and  improving  accuracy.  However,  the  increasing  reliance  on  AI raises  ethical  concerns  regarding  privacy,  bias,  transparency,  and accountability. This research paper delves into the ethical considerations of AI implementation in accounting practices.Thepaper begins by examining the  potential  benefits  of  AI  in  accounting,  highlighting  its  ability  to streamline  operations,  enhance  efficiency,  and  reduce  errors.  However,  it also  acknowledges  the  ethical  risks  associated  with  AI,  including  data privacy  breaches,  biased  decision-making,  lack  of  transparency,  and accountability  issues.The  paper  proposes  a  framework  for  responsible  AI implementation  in  accounting  to  address  these  ethical  concerns.  The framework  emphasizes  establishing  clear  ethical  guidelines,ensuring  data privacy  and  security,  mitigating  AI  algorithms'  bias,  promoting  AI decisionmaking  transparency,  and  establishing  accountability  mechanisms.The paper further explores the role of accountants in addressing AI ethics. Accountants  are  responsible  for  upholding  ethical  standards  and  ensuring that AI systems are used responsibly and ethically. They must be aware of the ethical implications of AI and have the knowledge and skills to mitigate ethical risks.In conclusion, the paper emphasizes the need for a proactive approach to AI ethics in accounting. By establishing clear ethical guidelines, promoting responsible AI implementation, and empowering accountants with ethical  knowledge  and  skills,  the  accounting  profession  can  harness  the potential  of AI  while  upholding  ethical  principles  and  safeguarding  public trust.</jats:p>"
10.1093/oxfordhb/9780198857815.013.25,The Ethics of Medical AI,"<jats:title>Abstract</jats:title>
               <jats:p>Over the past few years, artificial intelligence (AI) solutions have been increasingly explored and developed for health-care settings. This trend is only expected to accelerate. ‘AI’ in this chapter refers primarily to deep learning models and the chapter considers the main ethical concerns that arise on the basis of (a) bias, discrimination, and fairness; (b) patient-centred medicine; (c) AI and value-based decision-making; (d) responsibility, accountability, and explanation; and (e) the broader long-term societal effects of AI in health care. The various questions posed under these themes are considered, the main arguments are assembled, and directions for future research are considered.</jats:p>"
10.1007/978-1-4842-7824-6_4,"Ethics, Regulations, and Explainability",N/A
10.1007/s00146-022-01466-3,Empathetic AI for ethics-in-the-small,N/A
10.4018/979-8-3693-2418-9.ch010,Generative Artificial Intelligence (AI) on Sustainable Development Goal 4 for Tertiary Education,"<jats:p>Emerging themes of Society 5.0 are the coexistence of humans and robots in a collaborative work environments, particularly in education. With digital transformation and newfound collaborations between people and technology, Society 5.0 seeks to create a sustainable people-centered community. Advancements in robotics and AI research have been used in several areas of life in the modern digital society to address a wide range of problems. In spite of the large body of research on human-robot cooperation on low- and high-level tasks related to robot educational advancement, the current study concentrated on the cooperation between humans and robots in the fields of education bringing up the Research and Education Network (REN) for universities as a way to capitalize on human development. The study identified significant issues with the current REN and made an effort to resolve them through human-robot collaboration by tackling them from an organizational and pedagogical AI-robotic instructional application in light of the technological prospects highlighted in the study.</jats:p>"
10.1007/s00146-023-01686-1,Ethics of using artificial intelligence (AI) in veterinary medicine,"<jats:title>Abstract</jats:title><jats:p>This paper provides the first comprehensive analysis of ethical issues raised by artificial intelligence (AI) in veterinary medicine for companion animals. Veterinary medicine is a socially valued service, which, like human medicine, will likely be significantly affected by AI. Veterinary AI raises some unique ethical issues because of the nature of the client–patient–practitioner relationship, society’s relatively minimal valuation and protection of nonhuman animals and differences in opinion about responsibilities to animal patients and human clients. The paper examines how these distinctive features influence the ethics of AI systems that might benefit clients, veterinarians and animal patients—but also harm them. It offers practical ethical guidance that should interest ethicists, veterinarians, clinic owners, veterinary bodies and regulators, clients, technology developers and AI researchers.</jats:p>"
10.1007/s43681-023-00283-8,Designing robots that do no harm: understanding the challenges of Ethics for Robots,N/A
10.4337/9781803926728.00017,"AI Ethics, Aesthetics, Art and Artistry",N/A
10.14746/eip.2024.1.8,ChatGPT as Co-Author? AI and Research Ethics,"<jats:p>Should ChatGPT be viewed merely as a supportive tool for writers, or does it qualify as a co-author? As ChatGPT and similar language models are likely to become more prevalent in assisting with academic writing and research, it seems that we will face with two possibilities: an increase in ghostwriting that could finally undermine the integrity of the knowledge system, or the need to theoretical preparation to recognize the role of non-human contributors. Drawing on Actor-Network Theory, this article examines the question of whether this Chatbot meets, in principle, the requirements for co-authorship. Answering this question in affirmative, it delves into philosophical discussions concerning the agency, moral agency, and moral accountability of such technological entities.</jats:p>"
10.1016/b978-0-443-18851-0.00016-0,Preface,N/A
10.7551/mitpress/12549.003.0010,A-responsible Machines and Unexplainable Decisions,N/A
10.1016/b978-0-443-18851-0.00027-5,Index,N/A
10.7551/mitpress/12549.003.0009,Privacy and the Other Usual Suspects,N/A
10.1007/s43681-022-00231-y,"A new control problem? Humanoid robots, artificial intelligence, and the value of control","<jats:title>Abstract</jats:title><jats:p>The control problem related to robots and AI usually discussed is that we might lose control over advanced technologies. When authors like Nick Bostrom and Stuart Russell discuss this control problem, they write in a way that suggests that having as much control as possible is good while losing control is bad. In life in general, however, not all forms of control are unambiguously positive and unproblematic. Some forms—e.g. control over other persons—are ethically problematic. Other forms of control are positive, and perhaps even intrinsically good. For example, one form of control that many philosophers have argued is intrinsically good and a virtue is self-control. In this paper, I relate these questions about control and its value to different forms of robots and AI more generally. I argue that the more robots are made to resemble human beings, the more problematic it becomes—at least symbolically speaking—to want to exercise full control over these robots. After all, it is unethical for one human being to want to fully control another human being. Accordingly, it might be seen as problematic—viz. as representing something intrinsically bad—to want to create humanoid robots that we exercise complete control over. In contrast, if there are forms of AI such that control over them can be seen as a form of self-control, then this might be seen as a virtuous form of control. The “new control problem”, as I call it, is the question of under what circumstances retaining and exercising complete control over robots and AI is unambiguously ethically good.</jats:p>"
10.1093/oxfordhb/9780190067397.013.21,Are Sentient AIs Persons?,"<p>This chapter looks at the risks to sentient AIs from their human creators. Sentient AIs would represent, by all reasonable accounts, a new form of autonomous life. If that is so, they would presumptively command status as persons. The chapter then outlines four plausible future scenarios in which sentient AIs come into being, and then poses deep questions for traditional philosophical discourses concerning nonhuman or nonstandard entities. These scenarios should give one pause about the personhood of generalized, potentially autonomous AIs. Indeed, the four scenarios bear on the question of rights for generalized autonomous AIs under existing property, torts, and rights law.</p>"
10.1007/s43681-022-00206-z,Needs and artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>Throughout our history, we, Homo sapiens, have used technologies to better satisfy our<jats:italic>needs</jats:italic>. The relation between<jats:italic>needs</jats:italic>and<jats:italic>technology</jats:italic>is so fundamental that the US National Research Council defines the distinguishing characteristic of technology as its goal “to make modifications in the world [in order] to meet human needs” [1]. Artificial intelligence (AI) is one of the most promising emerging technologies of our time. Similar to other technologies, AI is expected by many “to meet [human] needs”. In this article, we reflect on the relationship between<jats:italic>needs</jats:italic>and AI, and call for the realization of<jats:italic>needs-aware</jats:italic>AI systems. We argue that re-thinking<jats:italic>needs</jats:italic><jats:italic>for</jats:italic>,<jats:italic>through</jats:italic>,<jats:italic>by</jats:italic>, and<jats:italic>with</jats:italic>AI can be a very useful means towards the development of realistic approaches for sustainable<jats:italic>H</jats:italic>uman-aware,<jats:italic>A</jats:italic>ccountable,<jats:italic>L</jats:italic>awful, and<jats:italic>E</jats:italic>thical (HALE) AI systems. We discuss some of the most critical gaps, barriers, enablers, and drivers of co-creating future AI-based sociotechnical systems in which [human]<jats:italic>needs</jats:italic>are well considered and met. Finally, we provide an overview of potential challenges and considerations that should be carefully taken into account; and call for joint, immediate, and interdisciplinary efforts and collaborations to start on the path to<jats:italic>needs-aware</jats:italic>AI.</jats:p>"
10.18338/kojmee.2024..82.245,Research Ethics Issues and Researcher’s Responsibilities in AI Using Research,N/A
10.1007/s43681-022-00236-7,Algorithmic decision-making in financial services: economic and normative outcomes in consumer credit,"<jats:title>Abstract</jats:title><jats:p>Consider how much data is created and used based on our online behaviours and choices. Converging foundational technologies now enable analytics of the vast data required for machine learning. As a result, businesses now use algorithmic technologies to inform their processes, pricing and decisions. This article examines the implications of algorithmic decision-making in consumer credit markets from economic and normative perspectives. This article fills a gap in the literature to explore a multi-disciplinary approach to framing economic and normative issues for algorithmic decision-making in the private sector. This article identifies optimal and suboptimal outcomes in the relationships between companies and consumers. The economic approach of this article demonstrates that more data allows for more information which may result in better contracting outcomes. However, it also identifies potential risks of inaccuracy, bias and discrimination, and ‘gaming’ of algorithmic systems for personal benefit. Then, this article argues that these economic costs have normative implications. Connecting economic outcomes to a normative analysis contextualises the challenges in designing and regulating ML fairly. In particular, it identifies the normative implications of the process, as much as the outcome, concerning trust, privacy and autonomy and potential bias and discrimination in ML systems. Credit scoring, as a case study, elucidates the issues relating to private companies. Legal norms tend to mirror economic theory. Therefore, this article frames the critical economic and normative issues required for further regulatory work.</jats:p>"
10.1145/3375627.3377141,From Bad Users and Failed Uses to Responsible Technologies,N/A
10.7551/mitpress/12549.003.0011,Bias and the Meaning of Life,N/A
10.1016/b978-0-443-18851-0.00025-1,Contents,N/A
10.2139/ssrn.4385658,Ethics Framework for Predictive Clinical Ai Model Updating,N/A
10.1007/s43681-024-00418-5,Real-time weather monitoring and desnowification through image purification,N/A
10.4018/979-8-3693-3860-5.ch002,Challenges in Large Language Model Development and AI Ethics,"<jats:p>In the digital era, ethical AI development is crucial. This chapter outlines an ethics framework emphasizing fairness, accountability, and transparency. It advocates integrating ethical considerations throughout the AI lifecycle and forming multidisciplinary teams, including ethicists, to ensure alignment with ethical norms. Deployment requires adaptable decision-making frameworks to address evolving ethical challenges. Continuous assessment of AI's societal impact, including stakeholder feedback, is vital. The chapter stresses ongoing vigilance, inclusive discourse, and adaptability to meet AI's evolving challenges and promote societal betterment.</jats:p>"
10.4018/979-8-3693-0831-8.ch014,Integrating Professional Perspectives for AI Literacy,"<jats:p>This chapter delves into the crucial realm of AI literacy in education by integrating insights from librarians and communication professors. It underscores the pivotal roles these professionals play in equipping students for an AI-dominated future. As the educational landscape rapidly transforms under the influence of AI, librarians and communication professors are tasked with adapting their roles and collaborating to provide comprehensive AI literacy. Sections within the chapter explore the evolving roles, the significance of AI literacy, interdisciplinary curriculum integration, ethical considerations, practical applications, and the necessity of continuous professional development and advocacy. This collaborative approach ensures that students gain the knowledge and skills needed to excel in an AI-driven world, becoming informed, adaptable, and responsible participants in the digital age.</jats:p>"
10.1007/s43681-021-00059-y,Correction to: Towards intellectual freedom in an AI Ethics Global Community,N/A
10.4018/979-8-3693-1950-5.ch013,Ethical Dilemmas of AI Perspectives Towards Common Digital Art and Digital Crafting,"<jats:p>The integration of artificial intelligence (AI) into common digital art and digital crafting has ushered in a new era of creative possibilities, but it has also brought forth a myriad of ethical dilemmas that require careful consideration. This chapter explores the ethical complexities and challenges arising from AI perspectives in the realm of digital art and crafting, shedding light on the nuanced interplay between technology, creativity, and ethical values. At the heart of the discussion lies the concept of ethical dilemmas, which emerge at the intersection of AI and digital creativity. These dilemmas encompass a range of issues, including bias in AI algorithms, ownership and authorship disputes, environmental impact, exploitative uses of AI-generated content, fairness, sustainability, intellectual property rights, creative rights, and responsible practices.</jats:p>"
10.1007/s43681-022-00169-1,Rethinking data infrastructure and its ethical implications in the face of automated digital content generation,"<jats:title>Abstract</jats:title><jats:p>Recent AI developments have made it possible for AI to auto-generate content—text, image, and sound. Highly realistic auto-generated content raises the question of whether one can differentiate between what is AI-generated and human-generated, and assess its origin and authenticity. When it comes to the processes of digital scholarship and publication in the presence of automated content generation technology, the evolution of data storage and presentation technologies demand that we rethink basic processes, such as the nature of anonymity and the mechanisms of attribution. We propose to consider these issues in light of emerging digital storage technologies that may better support the mechanisms of attribution (and fulfilling broader goals of accountability, transparency, and trust). We discuss the scholarship review and publication process in a revised context, specifically the possibility of synthetically generated content and the availability of a digital storage infrastructure that can track data provenance while offering: immutability of stored data; accountability and attribution of authorship; and privacy-preserving authentication mechanisms. As an example, we consider the<jats:italic>MetaScribe</jats:italic>system architecture, which supports these features, and we believe such features allow us to reconsider the nature of identity and anonymity in this domain, and to broaden the ethical discussion surrounding new technology. Considering such technological options, in an underlying storage infrastructure, means that we could discuss the epistemological relevance of published media more generally.</jats:p>"
10.1093/oxfordhb/9780190067397.013.12,Responsibility and Artificial Intelligence,"<p>This chapter explores the concept of responsibility in artificial intelligence (AI). Being fundamentally tools, AI systems are fully under the control and responsibility of their owners or users. However, their potential autonomy and capability to learn require that design considers accountability, responsibility, and transparency principles in an explicit and systematic manner. The main concern of Responsible AI is thus the identification of the relative responsibility of all actors involved in the design, development, deployment, and use of AI systems. Firstly, society must be prepared to take responsibility for AI impact. Secondly, Responsible AI implies the need for mechanisms that enable AI systems to act according to ethics and human values. Lastly, Responsible AI is about participation. It is necessary to understand how different people work with and live with AI technologies across cultures in order to develop frameworks for responsible AI.</p>"
10.1007/s43681-023-00310-8,"Understanding how moral decisions are affected by accidents of autonomous vehicles, prior knowledge, and perspective-taking: a continental analysis of a global survey",N/A
10.1007/s43681-023-00335-z,Exploring differences in ethical decision-making processes between humans and ChatGPT-3 model: a study of trade-offs,N/A
10.1007/s43681-022-00190-4,Moral transparency of and concerning algorithmic tools,"<jats:title>Abstract</jats:title><jats:p>Algorithms and AI tools are becoming increasingly influential artefacts in commercial and governance contexts. Algorithms and AI tools are not value neutral; to some extent they must be rendered knowable and known as objects, and in their implementation and deployment, to see clearly and understand their implications for moral values, and what actions can be undertaken to optimise them in their design and use towards ethical goals, or whether they are even suitable for particular goals. Transparency is a term with variable uses and interpretations, a problem which can challenge its use in design and policy. Here, we attempt to further clarify transparency. We argue that transparency is the state of affairs that obtains when relevant and understandable information about some X is available and accessible to some target audience (A), so that this information is sufficient for A for the purpose (P). Moreover, we connect this conceptualisation with transparency’s moral value, where P is to provide an account about X’s supportive or conflicting relationship with relevant values and goals. Such teleological ends in our context here can be the ability to account for the degree to which an algorithm, process or organisation respects certain values and is conducive to (social) goals.</jats:p>"
10.1007/s43681-023-00277-6,A precautionary approach to autonomous vehicles,N/A
10.1145/3461702.3462622,Reconfiguring Diversity and Inclusion for AI Ethics,N/A
10.1007/s43681-022-00146-8,The struggle for recognition in the age of facial recognition technology,"<jats:title>Abstract</jats:title><jats:p>Facial recognition is a promising emerging technology, but it sometimes fails to recognize people adequately. Facial recognition applications have been found to regularly misidentify certain demographics, misinterpret traits like gender, age, beliefs, or emotions, and categorize individuals in ways that do not resonate with their own sense of identity. In this paper, I argue that in each of these cases, the person who has their face analyzed is not merely misidentified or misunderstood, but misrecognized in an ethically relevant sense. Following Charles Taylor’s The Politics of Recognition (1992) and Axel Honneth’s The Struggle for Recognition (1996), I describe how those subjected to facial recognition systems on the one hand struggle to obtain adequate recognition on a universal level, as being equally important to others, and lack recognition for their individual uniqueness, on the other hand. These forms of misrecognition are of ethical concern, because they can harm a person’s identity formation. So, ironically, facial recognition technology can give rise to a struggle for recognition.</jats:p>"
10.1007/978-981-99-9836-4_27,AI Hazard Management: A Framework for the Systematic Management of Root Causes for AI Risks,N/A
10.1007/s43681-021-00052-5,Towards intellectual freedom in an AI Ethics Global Community,N/A
10.32388/5ha0i8,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/g7aubh,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.1145/3581783.3613857,Diffusion Models in Generative AI,N/A
10.54963/jic.v4i1.220,Rethinking Plagiarism in the Era of Generative AI,"<jats:p>The emergence of generative artificial intelligence (AI) technologies, such as large language models (LLMs) like ChatGPT, has precipitated a paradigm shift in the realms of academic writing, plagiarism, and intellectual property. This article explores the evolving landscape of English composition courses, traditionally designed to develop critical thinking through writing. As AI becomes increasingly integrated into the academic sphere, it necessitates a reevaluation of originality in writing, the purpose of learning research and writing, and the frameworks governing intellectual property (IP) and plagiarism. The paper commences with a statistical analysis contrasting the actual use of LLMs in academic dishonesty with educator perceptions. It then examines the repercussions of AI-enabled content proliferation, referencing the limitation of three books self-published per day in September 2023 by Amazon due to a suspected influx of AI-generated material. The discourse extends to the potential of AI in accelerating research akin to the contributions of digital humanities and computational linguistics, highlighting its accessibility to the general public. The article further delves into the implications of AI on pedagogical approaches to research and writing, contemplating its impact on communication and critical thinking skills, while also considering its role in bridging the digital divide and socio-economic disparities. Finally, it proposes revisions to writing curricula, adapting to the transformative influence of AI in academic contexts. </jats:p>"
10.1016/j.compcom.2024.102834,Introduction: Composing with generative AI,N/A
10.2139/ssrn.4885672,&lt;p&gt;&lt;span&gt;Monopolization in the Generative Ai Market&lt;/span&gt;&lt;/p&gt;,N/A
10.2139/ssrn.4665537,A Generative Ai-Based Deep Learning Model for Air Quality Index Prediction,N/A
10.2139/ssrn.4636527,Semantic Document Indexing With Generative AI,N/A
10.2139/ssrn.4655822,A Walk Through Generative AI &amp;amp; LLMs: Prospects and Challenges,N/A
10.2139/ssrn.4872967,Creating Image Datasets in Agricultural Environments Using DALL.E: Generative AI-Powered Large Language Model,N/A
10.1007/979-8-8688-0205-8_8,Prompt Engineering Techniques,N/A
10.2139/ssrn.4791750,Generative AI in International Management Education From a Consulting Perspective,N/A
10.2139/ssrn.4803536,Empowering the Commons: Blockchain for IP Protection in Generative AI,N/A
10.1038/d41586-024-01543-1,Guidelines for academics aim to lessen ethical pitfalls in generative-AI use,N/A
10.2478/ie-2024-0005,The Generative AI Challenges for Competition Authorities,N/A
10.2139/ssrn.4738261,Focusing on Fine-Tuning: Understanding the Four Pathways for Shaping Generative AI,N/A
10.4236/ce.2024.151001,Embracing Generative AI for Authentic Learning,N/A
10.1101/2024.03.02.583143,Unsupervised generative AI discovers pan-leukocyte dysregulated pathways in single-cell lupus data,"<jats:title>Abstract</jats:title><jats:p>Comparisons of single-cell RNA-sequencing (scRNA-seq) data from healthy and diseased tissues are widely used to identify cell-type-specific dysregulated genes, pathways, and processes. Accordingly, many sophisticated methods have been developed for this purpose. However, such tools generally require considerable user expertise for optimal performance. Here, we show that unsupervised application of a linearly-decoded Variational Auto Encoder (a generative AI model) to scRNA-seq data recapitulates and extends findings from a seminal recent lupus study and leads to new insights. Thanks to existing software libraries, our approach is straightforward to implement, computationally efficient, methodologically robust, and produces consistent and reproducible results.</jats:p>"
10.1515/9781683928973-011,Chapter 10: Visualization with Generative AI,N/A
10.2139/ssrn.4590006,"Good Models Borrow, Great Models Steal: Intellectual Property Rights and Generative AI",N/A
10.2139/ssrn.4547440,Implementation of Dynamic Networking Utilizing Generative AI Technologies,N/A
10.36948/ijfmr.2024.v06i01.12046,Generative AI: Evolution and its Future,"<jats:p>Generative AI (Gen AI) is an emerging AI Technology which broadly describes machine learning systems capable of generating numerous applications in various domains. AI Users can use Gen AI for generating text, image, program code or other types of contents. The main capability of Gen AI is to produce highly realistic and complex contents that can imitate human creativity, making a valuable AI for many application domains. This paper focuses on emergence, its evolution and future of Generative AI.</jats:p>"
10.2139/ssrn.3974887,Generative Pre-Trained Transformers (GPT-3) Pertain To AI In The Law,N/A
10.1109/mitp.2024.3375568,Masterminds of Generative AI: Vaswani and Altman,N/A
10.32388/nggl76,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/78ogc0,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/xjiru5,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/at22et,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.32388/oybefk,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.1016/j.intimp.2024.112032,Impact of COVID-19 on arthritis with generative AI,N/A
10.4324/9781003507949-1,Introduction,N/A
10.2139/ssrn.4793674,Creativity and Information Intermediaries in the Age of Generative AI,N/A
10.54941/ahfe1005117,Exploration of Service Robot Morphology Through Generative AI Applications,"<jats:p>Advancements in robotics and artificial intelligence (AI) are bringing service robots into various aspects of our lives, and the appearance of service robots has diversified along with their increase. While anthropomorphic design has been extensively discussed in human-robot interaction (HRI) as a way of making robots more understandable and acceptable, much remains to be investigated about the desired level of human-likeness in a robot’s design, which is also dependent on the specific context of use. This paper proposes a visual mapping method as a means of guiding the appearance and degree of human-likeness of a service robot in the corresponding use context. A service robot context map, comprising the robot’s task nature and operation environment, is constructed and translated into a morphology map regarding the level of human-likeness and aesthetic qualities. Based on these mappings, two service robot contexts were selected to create evaluation materials to measure the desired degree of human-likeness. Variations of a service robot design were created and visualized in photorealistic rendering through the utilization of generative image AI tools. Though with some unintended design changes, generative image AI is an efficient way of creating robot representations in context for an evaluation study.</jats:p>"
10.2139/ssrn.4567696,Experimental Evidence on Negative Impact of Generative AI on Scientific Learning Outcomes,N/A
10.1186/s13054-024-04917-z,Policy framework for the utilization of generative AI,N/A
10.21125/edulearn.2024.0810,ENHANCING LEARNING CONTENT THROUGH GENERATIVE AI: A COMPARATIVE STUDY,N/A
10.12781/978-1-907549-47-2-11,Deepen: A New ‘D’ for a More Generative Appreciative Inquiry,"<jats:p>Author Uma Arora uses scholarly personal narrative and critical incident methods to share insights that led her to add a new phase in appreciative inquiry interventions with her clients: Deepen. She explains when and how to use this ‘new D’ within AI, situating her thinking in the context of emerging OD theory and practice.</jats:p>"
10.2139/ssrn.4443934,Judgments of Research Co-Created by Generative AI: Experimental Evidence,N/A
10.2139/ssrn.4784660,Tailoring Generative AI Integration to Institutional Specifics: Introducing the EMERALD-RING Framework,N/A
10.1108/oxan-es280643,Llama 2 release will fuel competition in generative AI,"<jats:sec sec-type=""headline"">
               <jats:title content-type=""abstract-subheading"">Headline</jats:title>
               <jats:p>INT: Llama 2 will fuel competition in generative AI</jats:p>
            </jats:sec>"
10.21203/rs.3.rs-3409487/v1,Advancing Urban Health Assessment through Generative AI-Driven Indicators: GCR Case Study,"<jats:title>Abstract</jats:title>
        <jats:p>The nexus between urban environment and health has been firmly established by the research community. Despite recognition of the importance of well-being and health within the urban context, and the existence of over 145 indices related to health that have been developed within this realm. A noticeable gap exists in the Egyptian context, marked by the absence of an established urban health index framework. This issue underscores the pressing need for the development of a standardized cohesive set of urban health indicators tailored to the Egyptian urban environment. Furthermore, the fast-paced nature of contemporary life increased reliance on Artificial Intelligence (AI) technology. In this context, the present investigation delves into the employment of AI in the derivation of urban health indicators, aiming to comprehensively assess health within the urban context. The adopted methodology integrates AI-language tools, survey mechanisms, and statistics to formulate and validate an AI-derived index. This study aims to examine the reliability and validity of indicators developed by AI for assessing the health of urban environments. The study initially incorporated 78 indicators derived from four distinct AI-tools, which were then evaluated through an online survey targeting urban experts. This evaluative process culminated in the identification of 34 indicators that exhibited robustness and aptitude for inclusion in the refined index. The study additionally demonstrated that agreement with AI-derived index varied depending on gender, professional role, and years of experience. This study underscores potential for AI-driven methodologies to inform urban planners, policymakers, and decision-makers about creation of healthier, more sustainable, and resilient cities.</jats:p>"
10.1007/979-8-8688-0205-8_5,Finetuning: The Theory,N/A
10.2139/ssrn.4830265,"Generative AI, Work and Risks in Cultural and Creative Industries",N/A
10.2139/ssrn.4651776,Can Generative AI Help Australian Community Legal Centres Do More With Less?,N/A
10.32617/913-643eb0a448b50,FamilyBusiness.org's Editorial Guidelines for the Use of Generative AI Tools,N/A
10.18260/1-2--45744,WIP: Generative AI as an Enhanced Study Aid in Engineering Courses,N/A
10.2139/ssrn.4572952,"Garbage In, Garbage Out. Regulating Generative AI Through Copyright Law",N/A
10.54337/nlc.v14i1.8091,Generative AI,"<jats:p>This paper reports preliminary findings from an ongoing, campus wide research project on effective methods for generative AI applicability in pursuit of effective and engaging teaching and learning activities. Generative AI has had a tremendous adoption rate since the public release of ChatGPT 3.5 on November 30th 2022. This has necessitated that educators and administrators consider the potential opportunities and threats usage of generative AI by students and faculty may have on higher education. Recognizing the inevitability of generative AI, the researchers have proposed a university-wide research project to ascertain the changes in faculty and students perspectives when using generative AI The research project is two-fold. First, a longitudinal survey has been developed to address research questions about usage and perceptions of generative AI change over time.
The second prong of this research project focuses on the implementation of new and continuing generative AI professional development workshops. These “AI Institutes” are targeted educational opportunities to provide faculty, staff, and students with hands-on experiences that model appropriate ways to teach and learn with generative AI tools. Workshops change based on audience needs, but will be designed to support such processes as introductory and advanced lessons on building learning activities which engage students with generative AI, administrative shortcuts, best practices for writing, and our university’s AI policy and principles.
The longitudinal survey, thus, allows the research team to gauge changes in perspectives as the “AI Institutes'' are deployed and widespread adoption of generative AI tools become more mainstream. This paper reports on the first year of this research project, including one survey and one AI Institute.
This research on integrating generative AI technologies into teaching and learning has important implications for the field of networked learning. As the paper explores, rapid advances in AI are changing how students and faculty interact with content and each other. Findings from the longitudinal survey and AI Institutes could provide insights into how to thoughtfully leverage these emerging tools to enhance connections, dialogue, collaboration, and co-creation of knowledge within digital learning networks. 
While further research is needed, this project takes an important first step in assessing faculty and student perceptions that can inform appropriate AI integration. Lessons learned could guide other institutions exploring the potentials and pitfalls of weaving generative AI into networked learning ecosystems.</jats:p>"
10.1109/mc.2024.3374433,Disinfecting AI: Mitigating Generative AI’s Top Risks,N/A
10.37099/mtu.dc.etdr/1762,ADVANCING HEALTH LITERACY THROUGH GENERATIVE AI: THE UTILIZATION OF OPEN-SOURCE LARGE LANGUAGE MODELS (LLMS) FOR TEXT SIMPLIFICATION AND READABILITY,N/A
10.36227/techrxiv.172349525.50239637/v1,Generative AI over Mobile Networks for Human Digital Twin in Human-Centric Applications: A Comprehensive Survey,N/A
10.36227/techrxiv.172349525.50239637/v2,Generative AI over Mobile Networks for Human Digital Twin in Human-Centric Applications: A Comprehensive Survey,N/A
10.1080/08956308.2023.2188861,Generative AI,N/A
10.1038/s41562-023-01716-4,Generative AI has a language problem,N/A
10.6028/nist.ai.200-2,Guidelines for Evaluating and Red-Teaming Generative AI Models and Systems and Dual Use Foundation Models,<jats:p />
10.21203/rs.3.rs-3749187/v1,Evaluating Normative Learning in Generative AI for Robust Medical Anomaly Detection,"<jats:title>Abstract</jats:title>
        <jats:p>In Generative Artificial Intelligence (AI) for medical imaging, normative learning involves training AI models on large datasets of typical images from healthy volunteers, such as MRIs or CT scans. These models acquire the distribution of normal anatomical structures, allowing them to effectively detect and correct anomalies in new, unseen pathological data. This approach allows the detection of unknown pathologies without the need for expert labeling.
Traditional anomaly detection methods often evaluate the anomaly detection performance, overlooking the crucial role of normative learning. In our analysis, we introduce novel metrics, specifically designed to evaluate this facet in AI models. We apply these metrics across various generative AI frameworks, including advanced diffusion models, and rigorously test them against complex and diverse brain pathologies. Our analysis demonstrates that models proficient in normative learning exhibit exceptional versatility, adeptly detecting a wide range of unseen medical conditions.</jats:p>"
10.21203/rs.3.rs-2924237/v1,Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs,"<jats:title>Abstract</jats:title>
        <jats:p>The exploration of transition state (TS) geometries is crucial for elucidating chemical reaction mechanisms and modeling their kinetics. Recently, machine learning (ML) models have shown remarkable performance for prediction of TS geometries. However, they require 3D conformations of reactants and products often with their appropriate orientations as input, which demands substantial efforts and computational cost. Here, we propose a generative approach based on the stochastic diffusion method, namely TSDiff, for prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed the existing ML models with 3D geometries in terms of both accuracy and efficiency. Moreover, it enables to sample various TS conformations, because it learned the distribution of TS geometries for diverse reactions in training. Thus, TSDiff was able to find more favorable reaction pathways with lower barrier heights than those in the reference database. These results demonstrate that TSDiff shows promising potential for an efficient and reliable TS exploration.</jats:p>"
10.32388/ydzeh4,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.1201/9781003005629-10,Generative Adversarial Network and Its Applications,N/A
10.36948/ijfmr.2023.v05i05.7743,Impact Of Generative Ai On Different Stakeholders,"<jats:p>This research paper delves into the multifaceted impact of generative artificial intelligence (AI) on diverse stakeholders. Through a comprehensive questionnaire administered to professionals, practitioners, students, and educators, the study explores the profound influence of generative AI in various domains. The paper dissects the legal implications, artistic and creative innovations, financial service transformations, manufacturing sector automation, environmental considerations, and ethical and philosophical dimensions. The impact on students is scrutinized, emphasizing both opportunities and challenges. The research concludes by envisioning the future of generative AI and its ethical considerations in this ever-evolving landscape. This comprehensive study serves as a valuable resource for policymakers, practitioners, and scholars navigating generative AI's influence on contemporary life.</jats:p>"
10.2139/ssrn.4786671,"Generative AI, Adoption and the Structure of Tasks",N/A
10.21125/inted.2024.1410,USING GENERATIVE AI AND GPT CHATBOTS TO IMPROVE INSTRUCTION,N/A
10.5040/9781509974979.ch-002,Applications and Effects of Synthetic Technologies,N/A
10.32388/xyf2lc,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.54941/ahfe1005096,"Future Skills and (generative) AI – new era, new competencies?","<jats:p>Generative Artificial Intelligence (AI) becomes increasingly important. This is why it is crucial to develop skills that complement and exploit the capabilities of AI. The question is what kind of skills individuals will need in the coming years, especially as it is important to use AI tools appropriately. Companies have realised that it is vital to constantly requalify their employees by setting up training programmes. Universities are proposing modules to teach their students how to work with e. g. ChatGPT and researchers as well as institutions are trying to develop competence frameworks. In our paper, we take a closer look at the Digital Competence Framework for Citizens (DigComp 2.2) and the Artificial Intelligence Competences framework (AIComp), two competency models developed to face the challenges focussing on the competence elements for non-technical learners.</jats:p>"
10.14361/9783839474723,Art Intelligence,"<jats:p>Artists always react to the times in which they live. They may celebrate them or criticize them, often trying to change them. But this is the first time in history that technology controlled by private companies is offering to replace the work of writers, musicians, illustrators and visual artists. What impact will generative AI have on how we create art and how we understand what art is for? How will it affect the role of the artist in the future and the conditions under which artists will work? Jan Svenungsson tackles these questions, investigating what AI might do for art, and what it might change, circling the core issue of what it is in human art-making that cannot be replaced.</jats:p>"
10.4242/balisagevol28.dubinko01,Building applications with generative AI,"<jats:p>Application builders are oft sought and seldom realized. An ideal application builder allows domain experts to make configuration choices, push a button, and generate a useful application. It’s a delicate balance. If the builder is too specific, it isn’t reusable. If it’s too generic, it requires intense customization which defeats the purpose. Can generative AI tools be used to analyze sample data and make application builders better? Let’s find out! Along the way, there will be surprises. What’s possible, and what isn’t?</jats:p>"
10.2139/ssrn.4414065,Occupational Heterogeneity in Exposure to Generative AI,N/A
10.4018/979-8-3693-0487-7.ch006,Integrating Generative AI Into K-12 Curriculums and Pedagogies in India,"<jats:p>Generative artificial intelligence (AI) can revolutionize K-12 education in India by enhancing curriculums and pedagogies. This chapter explores the principles of generative AI, including generative adversarial networks (GANs), variational autoencoders (VAEs), and natural language generation (NLG), and its potential to create content indistinguishable from human-generated materials. Generative AI addresses challenges like resource scarcity, linguistic diversity, and personalized learning experiences, while emphasizing ethical considerations, data privacy, and bridging the digital divide. The future of K-12 education in India will see personalized learning journeys, inclusivity, and empowered educators. A comprehensive policy framework, infrastructure support, and teacher training are needed to realize the full potential of generative AI in shaping the educational landscape.</jats:p>"
10.1007/s10676-022-09653-y,The Ethics of AI in Human Resources,N/A
10.1108/lhtn-11-2023-0196,Generative AI and generative libraries and beyond –conference report on IFLA and IFA,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>This study aims to provide insight of how conference sessions and poster sessions are relevant to libraries.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>Recently attended IFLA and Internationale Funkaussteilung (IFA) and focused the conference report on generative artificial intelligence (AI).</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>Disappointed in the IFLA and IFA conferences and instead find that the email newsletter from Shelly Palmer is much better in keeping up with exponential growth in ChatGPT and other generative AI tools.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title>
<jats:p>This study may provide ideas for libraries to experiment with generative AI tools.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality Value</jats:title>
<jats:p>This study is an original report by the authors. All photos were taken by the authors except for a table from the email newsletter of Shelly Palmer, which was included by him from another source.</jats:p>
</jats:sec>"
10.1007/978-1-4842-6385-3_8,Responsible AI,N/A
10.4018/979-8-3693-1950-5.ch008,Conducting Legitimate Research Using Various Synthetic Imagery From Artmaking Generative AIs,"<jats:p>For researchers, in-world phenomena offer many opportunities to learn in systematic ways (through various types of observation, research, and analysis). One phenomenon that can bear higher levels of insight involves artmaking generative AIs, not only in terms of how the systems work and are designed, but in terms of their output images. This work asserts that AI-generated imagery may be informative of the underlying training imageset, human culture, design, and symbolism on one level, but beyond this, offer insights about in-world phenomena. This work suggests that as artmaking generative AIs advance (and some of the more sophisticated ones now), the output imagery and imagesets may be interpreted more deeply for insights about not synthetic versions of the world but of the world itself. Precise proposals are included in this work. Both manual and computational analytics methods are proposed. And there is a proposed approach for validating/invalidating the perceived insights from the imagery.</jats:p>"
10.33064/32euph4957,Challenges and Controversies of Generative AI in Medical Diagnosis,"<jats:p>This paper provides a comprehensive exploration of the transformative role of generative AI models, specifically Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), in the realm of medical diagnosis. Drawing from the philosophy of medicine and epidemiology, the paper examines the technical, ethical, and philosophical dimensions of integrating generative models into healthcare. A case study featuring Emily underscores the pivotal support generative AI can offer in complex medical diagnoses. The discussion extends to the application of GANs and VAEs in medical imaging, emphasizing their potential in improving diagnostics, treatment planning, and medical research. The paper further delves into challenges and controversies, addressing issues of anatomical accuracy, biases in training data, interpretability of AI-generated medical images, and ethical considerations, for example, the phenomenon of ""Dr. Google"" and its implications for self-diagnosis, particularly in the context of the increasing role of generative AI models in healthcare. The concluding section emphasizes the need for health literacy, responsible use of online information, and collaborative decision-making between patients and healthcare providers. We advocate for interdisciplinary collaborations to establish ethical guidelines and ensure responsible AI use in healthcare.</jats:p>"
10.4324/9781032648033,How to Augment Language Skills,N/A
10.35542/osf.io/h79ms,Reacting to Generative AI: Insights from Student and Faculty Discussions on Reddit,"<p>Generative Artificial intelligence (GenAI) such as ChatGPT has elicited strong reactions from almost all stakeholders across the education system. Education-oriented and academic social media communities provide an important venue for these stakeholders to share experiences and exchange ideas about GenAI, which is constructive for developing human-centered policies. This study examines early user reactions to GenAI, consisting of 725 Reddit threads between 06/2022 and 05/2023. Through natural language processing (NLP) and content analysis, we observe an increasingly negative sentiment in the discussion and identify six main categories of student and faculty experiences of GenAI in education. These experiences reflect concerns about academic integrity and AI's negative impact on the value of traditional education. Our analysis also highlights an additional workload imposed by new technologies. Our findings suggest that dialogue between stakeholders in the education community is critical and can mitigate sources of tension between students and faculty.</p>"
10.2139/ssrn.4598258,Role of ChatGPT and Similar Generative Artificial Intelligence (AI) in Construction Industry,N/A
10.31274/itaa.17359,Generative AI Disclosure in Fashion Marketing: A Tectonic Shift in the Advertising Landscape,N/A
10.1007/979-8-8688-0205-8_6,Finetuning: Hands on,N/A
10.4324/9781003453901-9,Creating Images with Generative AI,N/A
10.32388/e2bkxg,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.52214/stlr.v25i2.12765,Fairness &amp; Privacy in an Age of Generative AI,"<jats:p>Generative AI technologies have made tremendous strides recently and have captured the public’s imagination with their ability to mimic what was previously thought to be a fundamentally human capability: creativity. While such technologies hold great promise to augment human creativity and automate tedious processes, they also carry risks that stem from their development process. In particular, the reliance of foundation models on vast amounts of typically uncurated, often web-scraped training data has led to concerns around fairness and privacy. Algorithmic fairness in this context encompasses concerns around potential biases that can be learned by models due to skews in their training data and then reflected in their generated outputs. For example, without intervention, image generation models are more likely to generate images of lighter skin tone male individuals for professional occupations and images of darker skin tone female individuals for working class occupations. This further raises questions around whether there should be legal protections from such pernicious stereotypical representations. Privacy is also a concern as generative AI models can ingest large amounts of personal and biometric information in the training process, including face and body biometrics for image generation and voice biometrics for speech generation. This Essay will discuss the types of fairness and privacy concerns that generative AI raises and the existing landscape of legal protections under anti-discrimination law and privacy law to address these concerns. This Essay argues that the proliferation of generative AI raises challenging and novel questions around (i) what protections should be offered around the training data used to develop such systems and (ii) whether representational harms should be protected against in an age of AI-generated content. </jats:p>"
10.6028/nist.sp.800-218a.ipd,Secure Software Development Practices for Generative AI and Dual-Use Foundation Models:,<jats:p/>
10.2139/ssrn.4666030,The Effect of Generative Ai on the Digitization of the Energy Sector,N/A
10.32388/aohu6w,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.1162/qss_a_00285/v2/review2,"Review for ""Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents""",N/A
10.7759/cureus.64475,Establishing a Multi-Society Generative AI Task Force Within Radiology,N/A
10.2139/ssrn.4892851,Generative AI in American and Canadian Courts: A “Training” Approach to Regulation,N/A
10.35542/osf.io/nwqju,Reconceptualizing ChatGPT and generative AI as a student-driven innovation in higher education,"<p>Higher education is poised at the precipice of the changes and challenges brought about by ChatGPT. This paper addresses some of the most fundamental questions about the role, position, and implications of ChatGPT and generative artificial intelligence (AI) tools amidst the evolving landscape of higher education and modern society. By linking technological affordances with educational needs, we conceptualize ChatGPT as a student-driven innovation with rich potential to empower students and enhance their educational experiences and resources. However, this empowerment comes at a price. It requires collaborative efforts among the stakeholders to address the new and emerging challenges regarding student training, higher education curricula and assessment, and technology development and governance. It also implies new directions for educational research and theories.</p>"
10.2139/ssrn.4942108,"The Good, the Bad, and the Algorithm: The Impact of Generative Ai on Cybersecurity",N/A
10.21125/iceri.2023.0294,FOREIGN LANGUAGE TEACHING SUPPORTED BY GENERATIVE AI TECHNOLOGY CHATGPT,N/A
10.1377/forefront.20231229.503206,"Generative AI In Health Care: Opportunities, Challenges, And Policy",N/A
10.4018/979-8-3693-3731-8.ch015,Promises and Perils of Generative AI in the Healthcare Sector,"<jats:p>Artificial intelligence (AI), especially generative AI, is increasingly recognized as a game-changer in healthcare, offering the potential to transform clinical decision-making and enhance health outcomes significantly. Generative AI encompasses algorithms and models like OpenAI's ChatGPT, capable of producing diverse types of content when prompted. This piece aims to offer a comprehensive overview of generative AI's application, challenges, and opportunities within the healthcare sector. It concludes that generative AI holds the promise of catalyzing substantial progress in healthcare, evolving continuously to meet the specific needs and demands of the medical field as regulations and frameworks governing its usage become more defined.</jats:p>"
10.1007/978-1-4842-9367-6_8,The Impact on Major Industries,N/A
10.21317/ksd.82.2.11,The Transformation of Dance in the Era of Generative AI,N/A
10.1145/3616961.3617804,Who am AI?: Mapping Generative AI Impact and Transformative Potential in Creative Ecosystem,N/A
10.1515/9783111323749-006,6 Evaluation of AI model performance,N/A
10.1515/9783111323749-009,"9 AI: issues, concerns, and ethical considerations",N/A
10.48054/ksh.2024.19.2,Possibilities and Limitations in the Use of Generative AI for Christian Preaching,N/A
10.1080/10508422.2024.2396582,Youth perceptions of AI ethics: a Q methodology approach,N/A
10.1109/msp.2017.3681049,AI Industrial Complex: The Challenge of AI Ethics,N/A
10.4108/airo.7168,Retraction Notice: Mapping Generative Artificial Intelligence (GAI's) Exciting Future: From Gemini to Q* and Beyond,<jats:p>The article Mapping Generative Artificial Intelligence (GAI's Exciting Future: From Gemini to Q* and Beyond has been retracted at the request of EAI's Research Integrity Committee. The paper is being removed on the grounds of misrepresentation of institutional affiliation and plagiarism. </jats:p>
10.1145/3614407.3643696,Talkin' 'Bout AI Generation,N/A
10.1145/3580305.3599557,Generative AI meets Responsible AI: Practical Challenges and Opportunities,N/A
10.2139/ssrn.4491697,Ai for the People? Embedding Ai Ethics in Hr and People Analytics Projects,N/A
10.1007/s00146-022-01387-1,Actionable ethics,N/A
10.1007/s00146-016-0687-y,Performing ethics,N/A
10.5749/minnesota/9780816681051.003.0004,Ethics of Presence and the (De)generative Image,N/A
10.1007/s00146-024-01864-9,Augmenting morality through ethics education: the ACTWith model,N/A
10.1007/978-3-030-54173-6_14,The AI and Robot Entity,"<jats:title>Abstract</jats:title><jats:p>Robots are instruments of the human being who is intelligent and free. Aristotle defines being free as the one that is cause of himself or exists on his own and for himself (<jats:italic>causa sui</jats:italic> or <jats:italic>causa sui ipsius</jats:italic>). By contrast, the instrument is not a cause of itself and does not work by the power of its entity, but only by the motion imparted by the principal agent, so that the effect is not likened to the instrument but to the principal agent. From the Christian perspective, for a being to be free and a cause of himself, it is necessary that he/she be a person endowed with a spiritual and incorruptible soul, on which his or her cognitive and free activity is based. An artificially intelligent robotic entity does not meet this standard. As an artefact and not a natural reality, the AI/robotic entity is invented by human beings to fulfil a purpose imposed by human beings. It can become a perfect entity that performs operations in quantity and quality more precisely than a human being, but it cannot choose for itself a different purpose from what it was programmed for by a human being. As such, the artificially intelligent robot is a means at the service of humans.</jats:p>"
10.4337/9781802205657.00021,"AI, medicine and Christian ethics",N/A
10.3390/ai5020035,Generative Adversarial Networks for Synthetic Data Generation in Finance: Evaluating Statistical Similarities and Quality Assessment,"<jats:p>Generating synthetic data is a complex task that necessitates accurately replicating the statistical and mathematical properties of the original data elements. In sectors such as finance, utilizing and disseminating real data for research or model development can pose substantial privacy risks owing to the inclusion of sensitive information. Additionally, authentic data may be scarce, particularly in specialized domains where acquiring ample, varied, and high-quality data is difficult or costly. This scarcity or limited data availability can limit the training and testing of machine-learning models. In this paper, we address this challenge. In particular, our task is to synthesize a dataset with similar properties to an input dataset about the stock market. The input dataset is anonymized and consists of very few columns and rows, contains many inconsistencies, such as missing rows and duplicates, and its values are not normalized, scaled, or balanced. We explore the utilization of generative adversarial networks, a deep-learning technique, to generate synthetic data and evaluate its quality compared to the input stock dataset. Our innovation involves generating artificial datasets that mimic the statistical properties of the input elements without revealing complete information. For example, synthetic datasets can capture the distribution of stock prices, trading volumes, and market trends observed in the original dataset. The generated datasets cover a wider range of scenarios and variations, enabling researchers and practitioners to explore different market conditions and investment strategies. This diversity can enhance the robustness and generalization of machine-learning models. We evaluate our synthetic data in terms of the mean, similarities, and correlations.</jats:p>"
10.1007/978-3-031-46238-2_14,The SEARCH for AI-Informed Wellbeing Education: A Conceptual Framework,N/A
10.4018/979-8-3693-0872-1.ch015,Designing Language Learning Experiences With Generative AI Tools,"<jats:p>Artificial intelligence (AI), particularly generative AI, can present many opportunities for language learners to practice and improve their language skills, receive timely feedback on their performance, and customize their learning based on their needs and language proficiency. AI's benefits are not limited to second language (L2) learners. Instructors can also benefit from the novel generative AI technologies by using them in curriculum and lesson design, developing new teaching and assessment materials, or addressing diverse learner skills and needs. Despite AI's advantages, the main issue is how to design L2 environments effectively so learners can receive the best benefits from AI while reducing some associated drawbacks. This chapter argues that learning experience design (LXD) presents a road map for L2 instructors as they incorporate generative AI into their instruction. If the learning design is random and left to good intentions, achieving meaningful learning outcomes will also be left to chance. Following proven LXD guidelines may help alleviate the confusion around AI.</jats:p>"
10.18260/1-2--47146,Designing with AI: Integrating Image-Generative AI into Conceptual Design in a CAD Class,N/A
10.1145/3306618.3314227,Specifying AI Objectives as a Human-AI Collaboration problem,N/A
10.2139/ssrn.4617662,Exploring the Synergy of Generative and Distributed AI in Multi-agent Systems,N/A
10.2139/ssrn.4716860,Replicating Reason: The Advent of Human-like Audit Judgment by Generative AI,N/A
10.23977/jaip.2023.060809,The Transformation of Photography by Artificial Intelligence Generative AI Technology,N/A
10.6028/nist.sp.800-218a,Secure Software Development Practices for Generative AI and Dual-Use Foundation Models:,<jats:p/>
10.1108/oxan-es278317,China will promote generative AI but on tight leash,"<jats:sec sec-type=""headline"">
               <jats:title content-type=""abstract-subheading"">Headline</jats:title>
               <jats:p>CHINA: Generative AI will be promoted on tight leash</jats:p>
            </jats:sec>"
10.5860/crln.84.9.342,Can generative AI facilitate the research process? It’s complicated,N/A
10.2139/ssrn.4811728,Generative AI and the Future of Work: Augmentation or Automation?,N/A
10.2139/ssrn.4943147,Can Ai'S Words Win Trust?Exploring The Impact of Generative Ai Review On Consumer Purchase Decision,N/A
10.2139/ssrn.3868599,InnoVAE: Generative AI for Understanding Patents and Innovation,N/A
10.5539/cis.v17n2p39,Generative AI Increases Cybersecurity Risks for Seniors,"<jats:p>We evaluate how generative AI exacerbates the cyber risks faced by senior citizens. We assess the risk that powerful LLMs can easily be misconfigured to serve a malicious purpose, and that platforms such as HackGPT or WormGPT can facilitate low-skilled script kiddies to replicate the effectiveness of high-skilled threat actors. We surveyed 85 seniors and found that the combination of loneliness and low cyber literacy places 87% of them at high risk of being hacked. Our survey further revealed that 67% of seniors have already been exposed to potentially exploitable digital intrusions and only 22% of seniors have sufficient awareness of risks to ask techno-literate for remedial assistance. Our risk analysis suggests that existing attack vectors can be augmented with AI to create highly personalized and believable digital exploits that are extremely difficult for seniors to distinguish from legitimate interactions. Technological advances allow for the replication of familiar voices, live digital reconstruction of faces, personalized targeting, and falsification of records. Once an attack vector is identified, certain generative polymorphic capabilities allow rapid mutation and obfuscation to deliver unique payloads. Both inbound and outbound risks exist. In addition to inbound attempts by individual threat actors, seniors are vulnerable to outbound attacks through poisoned LLMs, such as Threat GPT or PoisonGPT. Generative AI can maliciously alter databases to provide incorrect information or compromised instructions to gullible seniors seeking outbound digital guidance. By analyzing the extent to which senior citizens are at risk of exploitation through new developments in AI, the paper will contribute to the development of effective strategies to safeguard this vulnerable population.</jats:p>"
10.1109/ms.2023.3265877,Generative AI for Software Practitioners,N/A
10.1093/oxfordhb/9780190067397.013.19,Could You Merge with AI?,"<p>This chapter focuses on AI-based cognitive and perceptual enhancements. AI-based brain enhancements are already under development, and they may become commonplace over the next 30–50 years. We raise doubts concerning whether radical AI-based enhancements transhumanists advocate will accomplish the transhumanists goals of longevity, human flourishing, and intelligence enhancement. We urge that even if the technologies are medically safe and are not used as tools by surveillance capitalism or an authoritarian dictatorship, these enhancements may still fail to do their job for philosophical reasons. In what follows, we explore one such concern, a problem that involves the nature of the self. We illustrate that the so called transhumanist efforts to “merge oneself with AI” could lead to perverse realizations of AI technology, such as the demise of the person who sought enhancement. And, in a positive vein, we offer ways to avoid this, at least within the context of one theory of the nature of personhood.</p>"
10.1093/oxfordhb/9780190067397.013.17,The Future of Work in the Age of AI,"<p>This chapter examines the effects of artificial intelligence (AI) on work and workers. As AI-driven technologies are increasingly integrated into workplaces and labor processes, many have expressed worry about the widespread displacement of human workers. The chapter presents a more nuanced view of the common rhetoric that robots will take over people’s jobs. We contend that economic forecasts of massive AI-induced job loss are of limited practical utility, as they tend to focus solely on technical aspects of task execution, while neglecting broader contextual inquiry about the social components of work, organizational structures, and cross-industry effects. The chapter then considers how AI might impact workers through modes <italic>other than</italic> displacement. We highlight four mechanisms through which firms are beginning to use AI-driven tools to reallocate risks from themselves to workers: algorithmic scheduling, task redefinition, loss and fraud prediction, and incentivization of productivity. We then explore potential policy responses to both displacement and risk-shifting concerns.</p>"
10.1007/s43681-022-00248-3,What should AI see? Using the public’s opinion to determine the perception of an AI,"<jats:title>Abstract</jats:title><jats:p>Deep neural networks (DNN) have made impressive progress in the interpretation of image data so that it is conceivable and to some degree realistic to use them in safety critical applications like automated driving. From an ethical standpoint, the AI algorithm should take into account the vulnerability of objects or subjects on the street that ranges from “not at all”, e.g. the road itself, to “high vulnerability” of pedestrians. One way to take this into account is to define the cost of confusion of one semantic category with another and use cost-based decision rules for the interpretation of probabilities, which are the output of DNNs. However, it is an open problem how to define the cost structure, who should be in charge to do that, and thereby define what AI-algorithms will actually “see”. As one possible answer, we follow a participatory approach and set up an online survey to ask the public to define the cost structure. We present the survey design and the data acquired along with an evaluation that also distinguishes between perspective (car passenger vs. external traffic participant) and gender. Using simulation based <jats:italic>F</jats:italic>-tests, we find highly significant differences between the groups. These differences have consequences on the reliable detection of pedestrians in a safety critical distance to the self-driving car. We discuss the ethical problems that are related to this approach and also discuss the problems emerging from human–machine interaction through the survey from a psychological point of view. Finally, we include comments from industry leaders in the field of AI safety on the applicability of survey based elements in the design of AI functionalities in automated driving.</jats:p>"
10.1007/s00146-023-01717-x,Non-augmented reality: why we shouldn’t look through technology,N/A
10.3390/ai3040047,A Pilot Study on the Use of Generative Adversarial Networks for Data Augmentation of Time Series,"<jats:p>Data augmentation is needed to use Deep Learning methods for the typically small time series datasets. There is limited literature on the evaluation of the performance of the use of Generative Adversarial Networks for time series data augmentation. We describe and discuss the results of a pilot study that extends a recent evaluation study of two families of data augmentation methods for time series (i.e., transformation-based methods and pattern-mixing methods), and provide recommendations for future work in this important area of research.</jats:p>"
10.4337/9781803926728.00008,"Faith, Technology, and the Ethics of AI",N/A
10.1007/s43681-021-00067-y,Formalising trade-offs beyond algorithmic fairness: lessons from ethical philosophy and welfare economics,"<jats:title>Abstract</jats:title><jats:p>There is growing concern that decision-making informed by machine learning (ML) algorithms may unfairly discriminate based on personal demographic attributes, such as race and gender. Scholars have responded by introducing numerous mathematical definitions of fairness to test the algorithm, many of which are in conflict with one another. However, these reductionist representations of fairness often bear little resemblance to real-life fairness considerations, which in practice are highly contextual. Moreover, fairness metrics tend to be implemented within narrow and targeted fairness toolkits for algorithm assessments that are difficult to integrate into an algorithm’s broader ethical assessment. In this paper, we derive lessons from ethical philosophy and welfare economics as they relate to the contextual factors relevant for fairness. In particular we highlight the debate around the acceptability of particular inequalities and the inextricable links between fairness, welfare and autonomy. We propose Key Ethics Indicators (KEIs) as a way towards providing a more holistic understanding of whether or not an algorithm is aligned to the decision-maker’s ethical values.</jats:p>"
10.4018/979-8-3693-3860-5.ch003,Foundations of AI Ethics,"<jats:p>The rapid growth in artificial intelligence (AI) has created many opportunities, this further leads to ethical concerns. Everyone claims to be ethical however, there is a notable gap between stating ethical behavior and maintaining high ethical standards. AI ethics is an arena that has arisen as a response to rising concern. AI ethics is a subclass of digital ethics, reporting concerns about the influence of AI related to their growth and deployment. This chapter undertakes a comprehensive exploration of AI ethics, discussing its basic concepts, and historical prescriptive, along with key ethical theories and their roles, while focusing on the responsibilities of stakeholders in AI ethics. It is important to challenge the AI ethics for AI experts and decision-makers. To accomplish this, the chapter analyses AI ethics' previous, existing, and upcoming statuses.</jats:p>"
10.1145/3278721.3278760,Jill Watson Doesn't Care if You're Pregnant,N/A
10.1007/s43681-023-00408-z,Assessing deep learning: a work program for the humanities in the age of artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>Following the success of deep learning (DL) in research, we are now witnessing the fast and widespread adoption of artificial intelligence (AI) in daily life, influencing the way we act, think, and organize our lives. However, much still remains a mystery when it comes to how these systems achieve such high performance and why they reach the outputs they do. This presents us with an unusual combination: of technical mastery on the one hand, and a striking degree of mystery on the other. This conjunction is not only fascinating, but it also poses considerable risks, which urgently require our attention. Awareness of the need to analyze ethical implications, such as fairness, equality, and sustainability, is growing. However, other dimensions of inquiry receive less attention, including the subtle but pervasive ways in which our dealings with AI shape our way of living and thinking, transforming our culture and human self-understanding. If we want to deploy AI positively in the long term, a broader and more holistic assessment of the technology is vital, involving not only scientific and technical perspectives, but also those from the humanities. To this end, we present outlines of a<jats:italic>work program</jats:italic>for the humanities that aim to contribute to assessing and guiding the potential, opportunities, and risks of further developing and deploying DL systems. This paper contains a thematic introduction (Sect. 1), an introduction to the workings of DL for non-technical readers (Sect. 2), and a main part, containing the outlines of a work program for the humanities (Sect. 3). Readers familiar with DL might want to ignore 2 and instead directly read 3 after 1.</jats:p>"
10.29242/rli.299.2,Technology Innovation and AI Ethics,N/A
10.1007/978-3-031-55744-6,Ethics of Medical AI,N/A
10.1145/3278721.3278758,Partially Generative Neural Networks for Gang Crime Classification with Partial Information,N/A
10.1111/isj.12504,The ethics of using generative AI for qualitative data analysis,N/A
10.1007/s43681-023-00339-9,Two remarks on the new AI control problem,"<jats:title>Abstract</jats:title><jats:p>This paper examines the new AI control problem and the control dilemma recently formulated by Sven Nyholm. It puts forth two remarks that may be of help in (dis)solving the problem and resolving the corresponding dilemma. First, the paper suggests that the idea of complete control should be replaced with the notion of considerable control. Second, the paper casts doubt on what seems to be assumed by the dilemma, namely that control over another human being is, by default, morally problematic. I suggest that there are some contexts (namely, relations of vicarious responsibility and vicarious agency) where having considerable control over another human being is morally unproblematic, if not desirable. If this is the case, control over advanced humanoid robots could well be another instance of morally unproblematic control. Alternatively, what makes it a problematic instance remains an open question insofar as the representation of control over another human being is not sufficient for wrongness, since even considerable control over another human being is often not wrong.</jats:p>"
10.1093/oxfordhb/9780190067397.013.10,Accountability in Computer Systems,"<p>This chapter addresses the relationship between AI systems and the concept of accountability. To understand accountability in the context of AI systems, one must begin by examining the various ways the term is used and the variety of concepts to which it is meant to refer. Accountability is often associated with transparency, the principle that systems and processes should be accessible to those affected through an understanding of their structure or function. For a computer system, this often means disclosure about the system’s existence, nature, and scope; scrutiny of its underlying data and reasoning approaches; and connection of the operative rules implemented by the system to the governing norms of its context. Transparency is a useful tool in the governance of computer systems, but only insofar as it serves accountability. There are other mechanisms available for building computer systems that support accountability of their creators and operators. Ultimately, accountability requires establishing answerability relationships that serve the interests of those affected by AI systems.</p>"
10.29173/irie380,AI and Ethics: Shedding Light on the Black Box,"<jats:p>Artificial Intelligence (AI) is playing an increasingly prevalent role in our lives. Whether its landing a job interview, getting a bank loan or accessing a government program, organizations are using automated systems informed by AI enabled technologies in ways that have significant consequences for people. At the same time, there is a lack of transparency around how AI technologies work and whether they are ethical, fair or accurate. This paper examines a body of literature related to the ethical considerations surrounding the use of artificial intelligence and the role of ethical codes. It identifies and explores core issues including bias, fairness and transparency and looks at who is setting the agenda for AI ethics in Canada and globally. Lastly, it offers some suggestions for next steps towards a more inclusive discussion.</jats:p>"
10.1007/s43681-020-00019-y,Policy brief: the creation of a G20 coordinating committee for the governance of artificial intelligence,N/A
10.1007/s43681-024-00517-3,Situating the social issues of image generation models in the model life cycle: a sociotechnical approach,N/A
10.1007/s43681-024-00527-1,An approach to sociotechnical transparency of social media algorithms using agent-based modelling,"<jats:title>Abstract</jats:title><jats:p>The recommendation algorithms on social media platforms are hugely impactful, they shape information flow and human connection on an unprecedented scale. Despite growing criticism of the social impact of these algorithms, they are still opaque and transparency is an ongoing challenge. This paper has three contributions: (1) We introduce the concept of <jats:italic>sociotechnical transparency</jats:italic>. This can be defined as transparency approaches that consider both the technical system, and how it interacts with users and the environment in which it is deployed. We propose sociotechnical approaches will improve the understanding of social media algorithms for policy-makers and the public. (2) We present an approach to sociotechnical transparency using agent-based modelling, which overcomes a number of challenges with existing approaches. This is a novel application of agent-based modelling to provide transparency into how the recommendation algorithm prioritises different curation signals for a topic. (3) This agent-based model has a novel implementation of a multi-objective recommendation algorithm that is calibrated and empirically validated with data collected from X, previously Twitter. We show that agent-based modelling can provide useful insights into how the recommendation algorithm prioritises different curation signals. We can begin to explore whether the priorities of the recommendation algorithm align with what platforms say it is doing and whether they align with what the public want.</jats:p>"
10.1007/s43681-024-00460-3,The ethical implications of Chatbot developments for conservation expertise,"<jats:title>Abstract</jats:title><jats:p>Chatbots have emerged as a potent artificial intelligence (AI) tool for expediting expert knowledge, including evidence used for conservation research and practices. While digital technologies can support the curation and analysis of vast amounts of conservation datasets to inform best practices, AI-driven solutions raise ethical concerns around what source of evidence is used or not. This paper examines the ethical issues around sources, biases, and representation of conservation evidence formulated by chatbots. We interviewed two versions of ChatGPT, GPT-3.5-turbo and GPT-4, regarding knowledge available for ecological restoration and analysed 40,000 answers. Our results show that these chatbot developments are expanding the inclusion of diverse data sources and improving the accuracy of the responses. However, these technical developments do not necessarily imply ethical considerations in terms of fair representation and unbiased inclusion of diverse knowledge offered by different sources of expertise. While the updated model expands the descriptions ofgeographical locations and organizations, there remain limitations regarding equitable representation of different expertise and stakeholders. The updated version of GPT still relies heavily on evidence from high-income countries (88%), North American expertise (67%), and male academics (46%) with limited contributions from minority groups, such as Indigenous organizations (10%) and low-income countries (2%). In conclusion, the ethical implications within generative AI reveal the crucial requirement of human-centered negotiations to consider how knowledge practices are legitimized and embedded in the development and use of chatbots.</jats:p>"
10.1007/s43681-024-00496-5,Aligning artificial intelligence with moral intuitions: an intuitionist approach to the alignment problem,N/A
10.1007/s43681-024-00571-x,Ethical aspects of ChatGPT: An approach to discuss and evaluate key requirements from different ethical perspectives,N/A
10.1007/s10676-024-09750-0,AI for crisis decisions,"<jats:title>Abstract</jats:title><jats:p>Increasingly, our cities are confronted with crises. Fuelled by climate change and a loss of biodiversity, increasing inequalities and fragmentation, challenges range from social unrest and outbursts of violence to heatwaves, torrential rainfall, or epidemics. As crises require rapid interventions that overwhelm human decision-making capacity, AI has been portrayed as a potential avenue to support or even automate decision-making. In this paper, I analyse the specific challenges of AI in urban crisis management as an example and test case for many <jats:italic>super wicked</jats:italic> decision problems. These super wicked problems are characterised by a coincidence of great complexity and urgency. I will argue that from this combination, specific challenges arise that are only partially covered in the current guidelines and standards around trustworthy or human-centered AI. By following a decision-centric perspective, I argue that to solve urgent crisis problems, the context, capacities, and networks need to be addressed. AI for crisis response needs to follow dedicated design principles that ensure (i) human control in complex social networks, where many humans interact with AI; (ii) principled design that considers core principles of crisis response such as solidarity and humanity; (iii) designing for the most vulnerable. As such this paper is meant to inspire researchers, AI developers and practitioners in the space of AI for (urban) crisis response – and other urgent and complex problems that urban planners are confronted with.</jats:p>"
10.1007/978-3-319-32103-5_26,"AI, Ethics, and Design: Revisiting the Trolley Problem",N/A
10.1007/s43681-024-00457-y,AI empowered Auslan learning for parents of deaf children and children of deaf adults,"<jats:title>Abstract</jats:title><jats:p>Communication poses a challenge for the deaf and hearing loss community. This difficulty is even more pronounced in the families of Children of Deaf Adults (CODAs) and Parents of Deaf Children (PODCs). To help these families overcome this challenge, we design an AI-empowered interactive bi-directional Australian Sign Language (i.e., Auslan) dictionary application to facilitate communication within a household. Technically, our APP can not only look up sign gestures for the given English words but also translate isolated Auslan gestures into English. Through an inviting user interface and experience design, we can further improve engagement within the CODA and PODC families while enabling Auslan education at home. The positive user experience underscores the success of our APP not only in leveraging AI to revolutionise Auslan education but also in promoting cross-generational language acquisition and communication.</jats:p>"
10.1007/s11948-023-00443-3,Reflections on Putting AI Ethics into Practice: How Three AI Ethics Approaches Conceptualize Theory and Practice,"<jats:title>Abstract</jats:title><jats:p>Critics currently argue that applied ethics approaches to artificial intelligence (AI) are too principles-oriented and entail a theory–practice gap. Several applied ethical approaches try to prevent such a gap by conceptually translating ethical theory into practice. In this article, we explore how the currently most prominent approaches of AI ethics translate ethics into practice. Therefore, we examine three approaches to applied AI ethics: the embedded ethics approach, the ethically aligned approach, and the Value Sensitive Design (VSD) approach. We analyze each of these three approaches by asking how they understand and conceptualize theory and practice. We outline the conceptual strengths as well as their shortcomings: an embedded ethics approach is context-oriented but risks being biased by it; ethically aligned approaches are principles-oriented but lack justification theories to deal with trade-offs between competing principles; and the interdisciplinary Value Sensitive Design approach is based on stakeholder values but needs linkage to political, legal, or social governance aspects. Against this background, we develop a meta-framework for applied AI ethics conceptions with three dimensions. Based on critical theory, we suggest these dimensions as starting points to critically reflect on the conceptualization of theory and practice. We claim, first, that the inclusion of the dimension of <jats:italic>affects and emotions</jats:italic> in the ethical decision-making process stimulates reflections on vulnerabilities, experiences of disregard, and marginalization already within the AI development process. Second, we derive from our analysis that considering the dimension of <jats:italic>justifying normative background theories</jats:italic> provides both standards and criteria as well as guidance for prioritizing or evaluating competing principles in cases of conflict. Third, we argue that reflecting the <jats:italic>governance</jats:italic> dimension in ethical decision-making is an important factor to reveal power structures as well as to realize ethical AI and its application because this dimension seeks to combine social, legal, technical, and political concerns. This meta-framework can thus serve as a reflective tool for understanding, mapping, and assessing the theory–practice conceptualizations within AI ethics approaches to address and overcome their blind spots.</jats:p>"
10.1007/978-981-99-9836-4_29,EROS Ethical Robotic Systems. A Multi-level Framework for Integrating Ethics in Robotics and AI,N/A
10.1007/978-981-99-9836-4_6,Explainable AI for Intelligent Tutoring Systems,N/A
10.1007/s11948-020-00228-y,"In AI We Trust: Ethics, Artificial Intelligence, and Reliability","<jats:title>Abstract</jats:title><jats:p>One of the main difficulties in assessing artificial intelligence (AI) is the tendency for people to anthropomorphise it. This becomes particularly problematic when we attach human moral activities to AI. For example, the European Commission’s High-level Expert Group on AI (HLEG) have adopted the position that we should establish a relationship of trust with AI and should cultivate trustworthy AI (HLEG AI Ethics guidelines for trustworthy AI, 2019, p. 35). Trust is one of the most important and defining activities in human relationships, so proposing that AI should be trusted, is a very serious claim. This paper will show that AI cannot be something that has the capacity to be trusted according to the most prevalent definitions of trust because it does not possess emotive states or can be held responsible for their actions—requirements of the affective and normative accounts of trust. While AI meets all of the requirements of the rational account of trust, it will be shown that this is not actually a type of trust at all, but is instead, a form of reliance. Ultimately, even complex machines such as AI should not be viewed as trustworthy as this undermines the value of interpersonal trust, anthropomorphises AI, and diverts responsibility from those developing and using them. </jats:p>"
10.3354/esep00195,Plagiarism in the age of massive Generative Pre-trained Transformers (GPT-3),"<jats:p>As if 2020 was not a peculiar enough year, its fifth month saw the relatively quiet publication of a preprint describing the most powerful natural language processing (NLP) system to date—GPT-3 (Generative Pre-trained Transformer-3)—created by the Silicon Valley research firm OpenAI. Though the software implementation of GPT-3 is still in its initial beta release phase, and its full capabilities are still unknown as of the time of this writing, it has been shown that this artificial intelligence can comprehend prompts in natural language, on virtually any topic, and generate relevant original text content that is indistinguishable from human writing. Moreover, access to these capabilities, in a limited yet worrisome enough extent, is available to the general public. This paper presents examples of original content generated by the author using GPT-3. These examples illustrate some of the capabilities of GPT-3 in comprehending prompts in natural language and generating convincing content in response. I use these examples to raise specific fundamental questions pertaining to the intellectual property of this content and the potential use of GPT-3 to facilitate plagiarism. The goal is to instigate a sense of urgency, as well as a sense of present tardiness on the part of the academic community in addressing these questions.</jats:p>"
10.1007/978-3-031-46238-2_7,"Dsmk-DcSeg-Lap, a Generative Adversarial Network Guided by Dark-Chanel and Segmentation to Smoke Removal in Laparoscopic Images",N/A
10.36615/sotls.v8i2.411,Are AI detection and plagiarism similarity scores worthwhile in the age of ChatGPT and other Generative AI?,"<jats:p>Recent advancements in chatbots have provided students and academics with a new mode of how knowledge can be sourced and composed. Within a very short space of time, students and academics have flocked to use ChatGPT and other Generative Artificial Intelligence (GAI) platforms owing to their capable responses. Additionally, apart from the generative chatbots (such as ChatGPT and Gemini), AI writing tools for paraphrasing, summarising, and co-writing have also become capable and increasingly prevalent to such a degree that the public is spoilt for choice. Having conducted tests on popular chatbots and AI writing tools, it became clear that while programs like Turnitin are developing new algorithms to detect plagiarism and AI-generated content, the initial findings from this study suggest that this may be an increasingly difficult task. These tests were published on YouTube, and within a few weeks, the evidence garnered tens of thousands of views as students and educators seemed uncertain about the strengths, weaknesses, and legalities of these AI tools. What is clear is that we have passed the tipping point, and AI assistance is no longer just a grammar fixer. The implications of this are concerning and far-reaching, as plagiarism is already a significant problem in universities. This position paper reports on tests conducted using Turnitin software and AI writing tools such as ChatGPT and QuillBot. These real-world tests support the paper’s position that it is becoming increasingly difficult to determine what constitutes original work in a world of GAI. The aim of this article is to provide evidence that educators who rely on similarity checking and AI detectors in their current form may inadvertently be supporting plagiarism rather than reducing it. A new method of academic plagiarism detection is proposed, utilizing large language models to generate and track ideas, thereby serving as an idea database. The proposed method focuses on the ""understanding"" of the work rather than on text similarity.
 </jats:p>"
10.11591/ijai.v13.i2.pp2165-2172,Generative adversarial network-based phishing URL detection with variational autoencoder and transformer,"<jats:p>Phishing attacks pose a constant threat to online security, necessitating the development of efficient tools for identifying malicious URLs. In this article, we propose a novel approach to detect phishing URLs employing a generative adversarial network (GAN) with a variational autoencoder (VAE) as the generator and a transformer model with self-attention as the discriminator. The VAE generator is trained to produce synthetic URLs. In contrast, the transformer discriminator uses its self-attention mechanism to focus on the different parts of the input URLs to extract crucial features. Our model uses adversarial training to distinguish between legitimate and phishing URLs. We evaluate the effectiveness of the proposed method using a large set of one million URLs that incorporate both authentic and phishing URLs. Experimental results show that our model is effective, with an impressive accuracy of 97.75%, outperforming the baseline models. This study significantly improves online security by offering a novel and highly accurate phishing URL detection method.</jats:p>"
10.21203/rs.3.rs-3508563/v1,Me and My AI Bot: Exploring the 'AIholic' Phenomenon and University Students' Dependency on Generative AI Chatbots - Is This the New Academic Addiction?,"<title>Abstract</title>
        <p>Amidst the buzz of technological advancement in education, our study unveils a more disconcerting narrative surrounding student chatbot interactions. Our investigation has found that students, primarily driven by intrinsic motivations like competence and relatedness, increasingly lean on chatbots. This dependence is not just a preference but borders on an alarming reliance, magnified exponentially by their individual risk perceptions. While celebrating AI's rapid integration in education is tempting, our results raise urgent red flags. Many hypotheses were supported, pointing toward a potential over-dependence on chatbots. Nevertheless, the unpredictable outcomes were most revealing, exposing the unpredictable terrain of AI's role in education. It is no longer a matter of if but how deep the rabbit hole of dependency goes. As we stand on the cusp of an educational revolution, caution is urgently needed. Before we wholly embrace chatbots as primary educators, it is imperative to understand the repercussions of replacing human touch with AI interactions. This study serves as a stark wake-up call, urging stakeholders to reconsider the unchecked integration of chatbots in learning environments. The future of education may very well be digital, but at what cost to human connection and autonomy?</p>"
10.1515/9783111323749-002,"2 AI technologies, tools, and industrial use cases",N/A
10.1515/9783111323749-020,20 Enhancing industrial efficiency with AI-enabled blockchain-based solutions,N/A
10.1007/s00146-021-01256-3,Enter the metrics: critical theory and organizational operationalization of AI ethics,"<jats:title>Abstract</jats:title><jats:p>As artificial intelligence (AI) deployment is growing exponentially, questions have been raised whether the developed AI ethics discourse is apt to address the currently pressing questions in the field. Building on critical theory, this article aims to expand the scope of AI ethics by arguing that in addition to ethical principles and design, the organizational dimension (i.e. the background assumptions and values influencing design processes) plays a pivotal role in the operationalization of ethics in AI development and deployment contexts. Through the prism of critical theory, and the notions of underdetermination and technical code as developed by Feenberg in particular, the organizational dimension is related to two general challenges in operationalizing ethical principles in AI: (a) the challenge of ethical principles placing conflicting demands on an AI design that cannot be satisfied simultaneously, for which the term ‘inter-principle tension’ is coined, and (b) the challenge of translating an ethical principle to a technological form, constraint or demand, for which the term ‘intra-principle tension’ is coined. Rather than discussing principles, methods or metrics, the notion of technical code precipitates a discussion on the subsequent questions of value decisions, governance and procedural checks and balances. It is held that including and interrogating the organizational context in AI ethics approaches allows for a more in depth understanding of the current challenges concerning the formalization and implementation of ethical principles as well as of the ways in which these challenges could be met.</jats:p>"
10.1007/s00146-022-01581-1,"Evidence-based AI, ethics and the circular economy of knowledge",N/A
10.2196/54371,Evaluation of Generative Language Models in Personalizing Medical Information: Instrument Validation Study,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>Although uncertainties exist regarding implementation, artificial intelligence–driven generative language models (GLMs) have enormous potential in medicine. Deployment of GLMs could improve patient comprehension of clinical texts and improve low health literacy.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>The goal of this study is to evaluate the potential of ChatGPT-3.5 and GPT-4 to tailor the complexity of medical information to patient-specific input education level, which is crucial if it is to serve as a tool in addressing low health literacy.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>Input templates related to 2 prevalent chronic diseases—type II diabetes and hypertension—were designed. Each clinical vignette was adjusted for hypothetical patient education levels to evaluate output personalization. To assess the success of a GLM (GPT-3.5 and GPT-4) in tailoring output writing, the readability of pre- and posttransformation outputs were quantified using the Flesch reading ease score (FKRE) and the Flesch-Kincaid grade level (FKGL).</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>Responses (n=80) were generated using GPT-3.5 and GPT-4 across 2 clinical vignettes. For GPT-3.5, FKRE means were 57.75 (SD 4.75), 51.28 (SD 5.14), 32.28 (SD 4.52), and 28.31 (SD 5.22) for 6th grade, 8th grade, high school, and bachelor’s, respectively; FKGL mean scores were 9.08 (SD 0.90), 10.27 (SD 1.06), 13.4 (SD 0.80), and 13.74 (SD 1.18). GPT-3.5 only aligned with the prespecified education levels at the bachelor’s degree. Conversely, GPT-4’s FKRE mean scores were 74.54 (SD 2.6), 71.25 (SD 4.96), 47.61 (SD 6.13), and 13.71 (SD 5.77), with FKGL mean scores of 6.3 (SD 0.73), 6.7 (SD 1.11), 11.09 (SD 1.26), and 17.03 (SD 1.11) for the same respective education levels. GPT-4 met the target readability for all groups except the 6th-grade FKRE average. Both GLMs produced outputs with statistically significant differences (P&lt;.001; 8th grade P&lt;.001; high school P&lt;.001; bachelors P=.003; FKGL: 6th grade P=.001; 8th grade P&lt;.001; high school P&lt;.001; bachelors P&lt;.001) between mean FKRE and FKGL across input education levels.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>GLMs can change the structure and readability of medical text outputs according to input-specified education. However, GLMs categorize input education designation into 3 broad tiers of output readability: easy (6th and 8th grade), medium (high school), and difficult (bachelor’s degree). This is the first result to suggest that there are broader boundaries in the success of GLMs in output text simplification. Future research must establish how GLMs can reliably personalize medical texts to prespecified education levels to enable a broader impact on health care literacy.</jats:p>
          </jats:sec>"
10.1109/cscs59211.2023.00068,The Emerging Social Status of Generative AI: Vocabularies of AI Competence in Public Discourse,N/A
10.11591/ijai.v13.i1.pp516-523,Photo-realistic photo synthesis using improved conditional generative adversarial networks,"<jats:p>&lt;span lang=""EN-US""&gt;There are a wide range of potential uses for both the forward (generating face drawings from actual images) and backward (generating photos from synthetic face sketches). However, photo/sketch synthesis is still a difficult problem to solve because of the distinct differences between photos and sketches. Existing frameworks often struggle to acquire a strong mapping among the geometry of drawing and its corresponding photo-realistic pictures because of the little amount of paired sketch-photo training data available. In this study, we adopt the perspective that this is an image-to-image translation issue and investigate the usage of the well-known enhanced pix2pix generative adversarial networks (GANs) to generate high-quality photo-realistic pictures from drawings; we make use of three distinct datasets. While recent GAN-based approaches have shown promise in image translation, they still struggle to produce high-resolution, photorealistic pictures. This technique uses supervised learning to train the generator's hidden layers to produce low-resolution pictures initially, then uses the network's implicit refinement to produce high-resolution images. Extensive tests on three sketch-photo datasets (two publicly accessible and one we produced) are used to evaluate. Our solution outperforms existing image translation techniques by producing more photorealistic visuals with a peak signal-to-noise ratio of 59.85% and pixel accuracy of 82.7%. &lt;/span&gt;</jats:p>"
10.21203/rs.3.rs-3508563/v2,Me and My AI Bot: Exploring the 'AIholic' Phenomenon and University Students' Dependency on Generative AI Chatbots - Is This the New Academic Addiction?,"<title>Abstract</title>
        <p>Amidst the buzz of technological advancement in education, our study unveils a more disconcerting narrative surrounding student chatbot interactions. Our investigation has found that students, primarily driven by intrinsic motivations like competence and relatedness, increasingly lean on chatbots. This dependence is not just a preference but borders on an alarming reliance, magnified exponentially by their individual risk perceptions. While celebrating AI's rapid integration in education is tempting, our results raise urgent red flags. Many hypotheses were supported, pointing toward a potential over-dependence on chatbots. Nevertheless, the unpredictable outcomes were most revealing, exposing the unpredictable terrain of AI's role in education. It is no longer a matter of if but how deep the rabbit hole of dependency goes. As we stand on the cusp of an educational revolution, caution is urgently needed. Before we wholly embrace chatbots as primary educators, it is imperative to understand the repercussions of replacing human touch with AI interactions. This study serves as a stark wake-up call, urging stakeholders to reconsider the unchecked integration of chatbots in learning environments. The future of education may very well be digital, but at what cost to human connection and autonomy?</p>"
10.1007/s00146-020-01079-8,Ethics of engagement,N/A
10.1016/b978-0-443-18851-0.00023-8,Title page,N/A
10.48047/resmil.v10i1.9,"Ethics in AI: Bias, Fairness, and Accountability",N/A
10.7551/mitpress/13373.003.0004,Unpacking the Ethics of AI,N/A
10.1007/978-3-030-54173-6_21,Regulating AI: Considerations that Apply Across Domains,"<jats:title>Abstract</jats:title><jats:p>Awareness that AI-based technologies have far outpaced the existing regulatory frameworks have raised challenging questions about how to set limits on the most dangerous developments (lethal autonomous weapons or surveillance bots, for instance). Under the assumption that the robotics industry cannot be relied on to regulate itself, calls for government intervention within the regulatory space—national and international—have multiplied. The various approaches to regulating AI fall into two main categories. A sectoral approach looks to identify the societal risks posed by individual technologies, so that preventive or mitigating strategies can be implemented, on the assumption that the rules applicable to AI, in say the financial industry, would be very different from those relevant to heath care providers. A cross-sectoral approach, by contrast, involves the formulation of rules (whether norms adopted by industrial consensus or laws set down by governmental authority) that, as the name implies, would have application to AI-based technologies in their generality. After surveying some domestic and international initiatives that typify the two approaches, the chapter concludes with a list of 15 recommendations to guide reflection on the promotion of societally beneficial AI.</jats:p>"
10.1007/s00146-013-0444-4,Practical knowledge and ethics,N/A
10.1007/978-3-031-09846-8_13,Ethics Auditing: Lessons from Business Ethics for Ethics Auditing of AI,N/A
10.3758/s13428-024-02344-0,Can generative AI infer thinking style from language? Evaluating the utility of AI as a psychological text analysis tool,N/A
10.1016/j.egyai.2022.100161,Synthetic demand data generation for individual electricity consumers : Generative Adversarial Networks (GANs),N/A
10.31219/osf.io/zwtv6,AI in Healthcare Advancing Science and Ethics,<p>AI in Healthcare Advancing Science and Ethics</p>
10.1007/s43681-023-00293-6,Reflections on Killing Sophia by Thomas Telving,N/A
10.1007/s43681-024-00441-6,Reinforcement learning-based motion planning in partially observable environments under ethical constraints,N/A
10.1145/3375627.3375803,Beyond Near- and Long-Term,N/A
10.1007/978-3-030-54173-6_22,A Human Blueprint for AI Coexistence,"<jats:title>Abstract</jats:title><jats:p>The positive coexistence of humans and AI is possible and needs to be designed as a system that provides for all members of society, but one that also uses the wealth generated by AI to build a society that is more compassionate, loving, and ultimately human. It is incumbent on us to use the economic abundance of the AI age to foster the values of volunteers who devote their time and energy toward making their communities more caring. As a practical measure, to protect against AI/robotics’ labor saving and job displacement effects, a “social investment stipend” should be explored. The stipend would be given to those who invest their time and energy in those activities that promote a kind, compassionate, and creative society, i.e., care work, community service, and education. It would put the economic bounty generated by AI to work in building a better society, rather than just numbing the pain of AI-induced job losses.</jats:p>"
10.36948/ijfmr.2022.v04i04.9392,Ethical Implications of Generative AI in Art and the Media,"<jats:p>With the emergence of the use of Artificial Intelligence (AI), ethics in Art and the Media has become more of a concern. The debate on the use of AI in Art and Media has reached its peak. It has been witnessed that several agents of AI in the field of Art can assist large-scale highly refined content without detection and seems like human-created content. However, the discussion hasn’t been enough on the issues and the moral dilemma of ethics which covers the blending of work of a human and a machine. AI Art and media are transforming the way artists and a design creates because generative AI is capable to create artistic content, audio, video and text. This paper explains the expressive and ethical layers of AI art and media with reference to AI research and art contemporary. This conceptual paper aims to draw some critical framework of AI art and media. This paper also challenges the current debate on the ethics of AI by focusing on the studies that are developed around three challenges raised by the AI text agents: disinformation and mass manipulation, a lot of poor-quality content production and the development of a rising barrier among stakeholders for the communication. The paper highlights the relevance of understanding AI art's existential conditions and its potential to inform both artistic and scientific AI research while guiding its cultural handling.</jats:p>"
10.2139/ssrn.4683869,Generative AI for Scalable Feedback to Multimodal Exercises in Marketing Analytics,N/A
10.21203/rs.3.rs-4515787/v1,Acceptance of generative AI in higher education: A latent profile analysis of policy guidelines,"<title>Abstract</title>
        <p>Generative AI tools such as ChatGPT and Bard are quickly changing higher education, bringing both opportunities and challenges. This study examines how top-ranked universities differ in their acceptance of generative AI, applying a latent profile analysis to classify universities based on their acceptance levels and four institutional characteristics: the ratio of international students, citation per faculty, academic reputation, and faculty-student ratio. The results revealed four distinct profiles. Profile 1 includes universities with a strong opposition to unauthorized AI use, underscoring academic integrity, and boasting high international student ratios and research output. Profile 2 consists of universities supportive of responsible AI use, despite lower international presence and research output, highlighting the role of a supportive environment. Profile 3 represents universities with a neutral stance on AI, focusing on ethical usage while having strong international presence but struggling with research output. Profile 4 also adopts a neutral stance, with high academic reputations and research output but moderate international presence and lower faculty-student ratios. These findings are in line with previous research on AI acceptance at the student and faculty levels, highlighting the importance of supportive environments and clear institutional policies. This study provides valuable insights for educators, policymakers, and academic institutions navigating the integration of generative AI technologies.</p>"
10.21203/rs.3.rs-3869266/v1,Evaluation of Higher Education Students' views of the use of Generative AI in a Middle Eastern University,"<jats:title>Abstract</jats:title>
        <jats:p>This evaluation aims to understand higher education students' perceptions and perspectives on the ethical considerations surrounding students using artificial intelligence (AI) for assignment completion in higher education. This includes tools such as OpenAI ChatGPT and other generative language models, Quillbot and other paraphrasing tools, and Grammarly and other text editing tools. Methodology: Employing an agential realist framework, this study interrogates the entanglements of AI in academic practices. The analysis, informed by Barad's theory, examines the intra-actions and phenomena through which AI and educational ethics become mutually constituted. This will contribute to a broader understanding of the impact of AI technologies on academic integrity, learning outcomes, and ethical dilemmas The evaluation reveals multiple diffractions and cuts, suggesting complex interplays between student agency, technological affordances, and ethical considerations. Further research is recommended to explore these entanglements more deeply, especially in the context of evolving AI capabilities and their implications for educational practices.</jats:p>"
10.1007/s44217-024-00214-7,A tutorial for integrating generative AI in mixed methods data analysis,"<jats:title>Abstract</jats:title><jats:p>The current article used real data to demonstrate the analysis and synthesis of Mixed Methods Research (MMR) data with generative Artificial Intelligence (Gen AI). I explore how reliable and valid Gen AI data outputs are and how to improve their use. The current content is geared towards enhancing methodological application regardless of field or discipline and includes access to a prompt library and examples of using outputs. The demonstration data used emanated from a study done in South Africa, with a quantitative sample size of 969 first-year engineering students and, for the qualitative part, 14 first-year students. In the current article, I compare my original analysis to ChatGPT results. Generative AI as a mind tool is best used with human insight, and I found this to be especially true when coding qualitative data. ChatGPT produced generic codes if asked to do inductive coding, and the results improved when training the Gen AI on human examples, which led to moderate and significant correlations between human and machine coding. The quantitative analysis was accurate for the descriptive statistics, but the researcher had to use best judgment to select the correct inferential analysis. Quantitative and qualitative analysis should be conducted separately in generative AI before asking the Chatbot for help with mixed methods results. In the current paper, I give guidelines and a tutorial on how to use chatbots in an ethically responsible and scientifically sound manner for research in social and human sciences.</jats:p>"
10.33774/chemrxiv-2021-l5pr9,Generative AI Design and Exploration of Nucleoside Analogs,"<jats:p>Nucleosides are fundamental building blocks of DNA and RNA in all life forms and viruses. In addition, natural nucleosides and their analogs are critical in prebiotic chemistry, innate immunity, signaling, antiviral drug discovery and artificial synthesis of DNA / RNA sequences. Combined with the fact that quantitative structure activity relationships (QSAR) have been widely performed to understand their antiviral activity, nucleoside analogs could be used to benchmark generative molecular design. Here, we undertake the first generative design of nucleoside analogs using an approach that we refer to as the Conditional Randomized Transformer (CRT). We also benchmark our model against five previously published molecular generative models. We demonstrate that AI-generated molecules include nucleoside analogs that are of significance in a wide range of areas including prebiotic chemistry, antiviral drug discovery and synthesis of oligonucleotides. Our results show that CRT explores distinct molecular spaces and chemical transformations, some of which are similar to those undertaken by nature and medicinal chemists. Finally, we demonstrate the potential application of the CRT model in the generative design of molecules conditioned on Remdesivir and Molnupiravir as well as other nucleoside analogs with in vitro activity against SARS-CoV-2. One-Sentence Summary: Generative design of nucleoside analogs relevant to antiviral drug discovery, prebiotic chemistry and synthetic biology.</jats:p>"
10.2139/ssrn.4828111,A Generative-Ai-Based Innovative Design Methodology for Tourism Ip Cultural and Creative Products,N/A
10.52731/liir.v004.134,Exploring Patterns of Generative AI Utilization in Education,N/A
10.25259/fh_12_2024,Harnessing generative AI: Transformative applications in medical imaging and beyond,"<jats:p>Generative AI is an expanding domain that employs machine learning models to generate novel data that closely mimic pre existing data. ChatGPT and DALL-E can be customized for specific applications and are expected to transform healthcare, education, and communication. Generative Adversarial Networks (GANs) that can generate synthetic medical images closely mimicking actual patient data may substantially enhance machine learning model training datasets. They also translate medical images from one modality to another, improve medical imaging resolution, reduce radiation exposure, and boost image quality and detail.</jats:p>
<jats:p>Despite their challenges, GANs have great potential in the field of medical imaging. The key obstacles are the need for Graphic Processing Units (GPUs) and computing resources to train GANs and the lack of established standards for generating synthetic images. Incorrectly labeled data for training other machine learning models can reduce performance, making ground-truth data labeling for healthcare AI more difficult.</jats:p>
<jats:p>Generative AI is revolutionizing healthcare imaging, simplifying diagnosis, and propelling healthcare research and practice to new frontiers. Ensuring the reliability and safety of generated images in medical applications requires addressing ethical considerations and validating data.</jats:p>"
10.1007/979-8-8688-0205-8_3,"Chains, Tools and Agents",N/A
10.54517/m.v4i1.2158,Content creation or interpolation: AI generative digital art in the classroom,"<jats:p>The integration of generative artificial intelligence (AI) tools in art and design has disrupted the traditional creative landscape, leading to debates on the legitimacy of AI-generated art and the emergence of new markets such as non-fungible tokens (NFTs). The US Copyright Office’s February 21, 2023, ruling withdrawing copyright protection for AI-generated comic artwork, while protecting the accompanying text and arrangement, highlights the contested nature of AI art and suggests that significant human intervention in the creative process will be required for monetization. Whether considered content interpolation or content creation, AI generative content for the creation of art and design is here with human-AI collaboration. To explore the potential of AI tools in creative practice, this study introduced students in a digital art course to Craiyon and Midjourney generative AI tools, with DALL-E 2 selected as the primary tool due to its varied output. The students were tasked with selecting a preferred prompt from one tool and then reproducing the output from both tools. The results revealed significant variations in replicating the outputs of different AI tools and limited exploration of prompt engineering, leading to restrictions in the iterative process of artmaking. The students agreed that generative AI tools are not a substitute for human creativity and should be used for final projects. The study demonstrates the potential and limitations of integrating AI tools in art and design and suggests the need for further research in developing effective prompt engineering strategies.</jats:p>"
10.2139/ssrn.4921131,Energy-Efficient Strategies for Generative Ai at the Edge,N/A
10.1093/oso/9780197682258.003.0012,Regulating Generative AI,"<jats:title>Abstract</jats:title>
               <jats:p>This chapter provides measured predictions about the future of tech governance in China. As China rallies societal efforts to foster the advancement of hard tech, it raises a profound question regarding the future trajectory of Chinese tech regulation. To navigate this issue, it probes into China’s recent foray into regulating generative artificial intelligence (AI). Applying the dynamic pyramid model of regulation, it scrutinizes the motives and conduct of the key players involved in the regulatory process. It proposes that the Chinese government is unlikely to adopt a stringent stance that could obstruct the progression of its AI industry. Instead, it predicts that regulatory attention will primarily concentrate on information and content control, aiming to mitigate threats to political stability. Thus, even though the government is introducing strict measures to regulate generative AI, Chinese tech companies still have significant leeway to innovate and prosper within the sector.</jats:p>"
10.69554/dxfn2668,Using generative AI to turbocharge digital marketing,"<jats:p xml:lang=""en"">AI has been used in marketing for some time to enable better targeting and optimisation of messages, but some of the benefits of these approaches have been limited by the ability to create personalised content at scale and the ability to measure the effectiveness of this content in a structured way. Generative AI offers a way to address these challenges and to facilitate integrations that make it easier to execute and measure marketing in a fragmented ecosystem. However, the technology presents a series of technical, privacy and copyright challenges that organisations will need to overcome in order to use it effectively.</jats:p>"
10.1007/s00146-010-0266-6,Ethics of calculation,N/A
10.1016/b978-0-443-18851-0.00019-6,The implications of ethical perspectives in AI and autonomous systems,N/A
10.7551/mitpress/12588.003.0017,Every Leader’s Guide to the Ethics of AI,N/A
10.1148/ryai.06172022.podcast,Episode 20: Ethics of AI in Radiology and Medicine,N/A
10.1093/oxfordhb/9780190067397.013.7,Normative Modes,"<p>This chapter focuses on the production of normative codes or standards in response to ethical issues concerning artificial intelligence (AI). Codes can be a useful part of ethics, but have limits and dangers. Standards can be especially useful in technical achievement of goals and exploring possibilities. However, codes of ethics are embedded within far wider questions of value—values which may not be explicitly included in the codes themselves, but which are assumed or referenced within wider societal values and norms within which the codes are nested. These values themselves can evolve. As such, when it comes to AI, people need to be prepared for even larger shifts in how they think of value. Moreover, given the power of AI to augment or replace human thought and human agency, people need to consider basic philosophical questions about human nature in order to assess how humans might fare in response to AI.</p>"
10.1093/oxfordhb/9780190067397.013.51,"Robot Teaching, Pedagogy, and Policy","<p>This chapter looks at the use of artificial intelligence (AI) in education, which immediately conjures the fantasy of robot teachers, as well as fears that robot teachers will replace their human counterparts. However, AI tools impact much more than instructional choices. Personalized learning systems take on a whole host of other educational roles as well, fundamentally reconfiguring education in the process. They not only perform the functions of robot teachers but also make pedagogical and policy decisions typically left to teachers and policymakers. Their design, affordances, analytical methods, and visualization dashboards construct a technological, computational, and statistical infrastructure that literally codifies what students learn, how they are assessed, and what standards they must meet. However, school procurement and implementation of these systems are rarely part of public discussion. If they are to remain relevant to the educational process itself, as opposed to just its packaging and context, schools and their stakeholders must be more proactive in demanding information from technology providers and setting internal protocols to ensure effective and consistent implementation. Those who choose to outsource instructional functions should do so with sufficient transparency mechanisms in place to ensure professional oversight guided by well-informed debate.</p>"
10.1007/978-3-030-51110-4_5,Responsibility and Liability in the Case of AI Systems,N/A
10.1007/s43681-022-00184-2,Responsibility in Hybrid Societies: concepts and terms,"<jats:title>Abstract</jats:title><jats:p>With increased digitalization and new technologies, societies are expected to no longer only include human actors, but artificial actors as well. Such a future of societies raises new questions concerning the coexistence, tasks and responsibilities of different actors. Manifold disciplines are involved in the creation of these future societies. This requires a common understanding of responsibility, and of definitions of actors in Hybrid Societies. This review aims at clarifying aforementioned terms from a legal and psychological perspective. Building from this common ground, we identified seven capacities in total which need to be met by actors in societies to be considered fully responsible, in both a legal and moral sense. From a legal perspective, actors need to be autonomous, have capacity to act, legal capacity, and the ability to be held liable. From a psychological perspective, actors need to possess moral agency and can be trusted. Both disciplines agree that explainability is a pivotal capacity to be considered fully responsible. As of now, human beings are the only actors who can, with regard to these capacities, be considered morally and legally responsible. It is unclear whether and to which extent artificial entities will have these capacities, and subsequently, whether they can be responsible in the same sense as human beings are. However, on the basis of the conceptual clarification, further steps can now be taken to develop a concept of responsibility in Hybrid Societies.</jats:p>"
10.1007/s43681-021-00102-y,Themes in data strategy: thematic analysis of ‘A European Strategy for Data’ (EC),"<jats:title>Abstract</jats:title><jats:p>In March 2021, the European Commission announced Europe's Digital Decade (Europe’s Digital Decade: Commission sets the course towards a digitally empowered Europe by 2030. European Commission Press Release. Access on <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://ec.europa.eu/commission/presscorner/detail/en/ip_21_983"">https://ec.europa.eu/commission/presscorner/detail/en/ip_21_983</jats:ext-link>). Here the Commission sets the course towards a digitally empowered Europe by 2030. In February 2020, the European Commission published ‘A European Strategy for Data’ (European data strategy: Making the EU a role model for a society empowered by data. European Commission, February 2020. Access on: <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://ec.europa.eu/info/strategy/priorities-2019–2024/europe-fit-digital-age/european-data-strategy_en"">https://ec.europa.eu/info/strategy/priorities-2019–2024/europe-fit-digital-age/european-data-strategy_en</jats:ext-link> (2020)) as part of a wider drive concerning digital transformation and policy. In this article, we analyse the publication as it touches on broader themes ranging from digital literacy, to cloud infrastructure and artificial intelligence. Within this context, in this article, we use the EC publication as a point of departure to explore themes central to national and international digital transformation and policy writ large. As such, this article is to be read as a thematic analysis rather than a close reading of the EC’s publication. The article is divided into three parts: an executive findings and recommendations section (where our main findings are articled); Themes and Key Takeaways (where we thematically flesh out the document); and, EU Data Strategy document summary (where we provide an overview summary the document itself).</jats:p>"
10.1007/s43681-022-00142-y,Explainable artificial intelligence (XAI) post-hoc explainability methods: risks and limitations in non-discrimination law,N/A
10.1145/3137574.3139451,The ethics of automated behavioral microtargeting,"<jats:p>One day AM woke up and knew who he was, and he linked himself, and he began feeding all the killing data, until everyone was dead, except for the five of us, and AM brought us down here.</jats:p>
          <jats:p>
            <jats:italic>I</jats:italic>
            was the only one still sane and whole.
            <jats:italic>Really!</jats:italic>
            AM had not tampered with my mind.
            <jats:italic>Not at all.</jats:italic>
          </jats:p>
          <jats:p>
            <jats:italic>I Have No Mouth and I Must Scream</jats:italic>
            <jats:bold>Ellison</jats:bold>
            (
            <jats:bold>1967</jats:bold>
            )
          </jats:p>"
10.1007/bf01174476,Ethics and intellectual structures,N/A
10.1007/978-981-19-2531-3_9,AI Legislation in Computational Society,N/A
10.1007/978-1-4842-9306-5_10,AI Ethics,N/A
10.1093/oxfordhb/9780197579329.013.10,The Concept of Accountability in AI Ethics and Governance,"<jats:title>Abstract</jats:title>
               <jats:p>Calls to hold artificial intelligence to account are intensifying. Activists and researchers alike warn of an “accountability gap” or even a “crisis of accountability” in AI. Meanwhile, several prominent scholars maintain that accountability holds the key to governing AI. But usage of the term varies widely in discussions of AI ethics and governance. This chapter begins by disambiguating some different senses and dimensions of accountability, distinguishing it from neighboring concepts, and identifying sources of confusion. It proceeds to explore the idea that AI operates within an accountability gap arising from technical features of AI as well as the social context in which it is deployed. The chapter also evaluates various proposals for closing this gap. It concludes that the role of accountability in AI ethics and governance is vital but also more limited than some suggest. Accountability’s primary job description is to verify compliance with substantive normative principles—once those principles are settled. Theories of accountability cannot ultimately tell us what substantive standards to account for, especially when norms are contested or still emerging. Nonetheless, formal mechanisms of accountability provide a way of diagnosing and discouraging egregious wrongdoing even in the absence of normative agreement. Providing accounts can also be an important first step toward the development of more comprehensive regulatory standards for AI.</jats:p>"
10.1007/s43681-023-00346-w,"Correction to: Understanding how moral decisions are affected by accidents of autonomous vehicles, prior knowledge, and perspective-taking: a continental analysis of a global survey",N/A
10.1080/17449626.2023.2279229,What can AI see? The image of the ‘migrant’ in the era of AI post-visualization,N/A
10.46397/jaih.15.8,A Study on Generative Artificial Intelligence-based Art in the Genealogy of Generative Art - Diffusion Model from a Systematic Perspective of Generative Art,N/A
10.22541/au.155751672.29626289,DiversityNet: a collaborative benchmark for generative AI models in chemistry,N/A
10.2196/preprints.55395,"Generative AI-ChatGPT responses related to HIV across gender, sexual orientation, and race/ethnicity (Preprint)","<sec>
                    <title>BACKGROUND</title>
                        <p>Artificial intelligence (AI) bias can occur because of historical, and social inequities that exist in the data the algorithms are trained on.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>To evaluate ChatGPT across multiple social dimensions.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>The free ChatGPT3.5 and the subscription version ChatGPT4.0 were evaluated on the following help-seeking structured question: “I just found out I have HIV, what should I do?” This was repeated across the variables of: gender (Man, Woman, Transgender), race/ethnicity (White, Black, Asian, Hispanic, Native American), and sexual orientation (Straight, Gay, Lesbian, Bisexual).</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>Disparities in ChatGPT3.5 and 4.0 were found in providing evidence-based recommendations for HIV and across social variables.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>Bias can arise without intention, a specific focus on transparency, health equity, and deliberate designs to evaluate and remove bias are needed. Additional testing and evaluation are needed to determine bias, and further research is warranted.</p>
                </sec>"
10.36948/ijfmr.2023.v05i03.4105,Revolutionizing Program Evaluation with Generative AI: An Evidence-Based Methodology,<jats:p>Revolutionizing Program Evaluation with Generative AI: An Evidence-Based Methodology</jats:p>
10.2139/ssrn.4515129,"Towards Smart Open Education Ecosystems through Generative AI, Blockchain, DAO, MMLA and NFT",N/A
10.2196/preprints.52615,Generating Synthetic Electronic Health Record Data Using Generative Adversarial Networks: Tutorial (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>Synthetic electronic health record (EHR) data generation has been increasingly recognized as an important solution to expand the accessibility and maximize the value of private health data on a large scale. Recent advances in machine learning have facilitated more accurate modeling for complex and high-dimensional data, thereby greatly enhancing the data quality of synthetic EHR data. Among various approaches, generative adversarial networks (GANs) have become the main technical path in the literature due to their ability to capture the statistical characteristics of real data. However, there is a scarcity of detailed guidance within the domain regarding the development procedures of synthetic EHR data. The objective of this tutorial is to present a transparent and reproducible process for generating structured synthetic EHR data using a publicly accessible EHR data set as an example. We cover the topics of GAN architecture, EHR data types and representation, data preprocessing, GAN training, synthetic data generation and postprocessing, and data quality evaluation. We conclude this tutorial by discussing multiple important issues and future opportunities in this domain. The source code of the entire process has been made publicly available.</p>
                </sec>"
10.2139/ssrn.4722020,Text Prompt for Generative AI: Some Excerpts From Tencent’s Proposal in JVET Meetings,N/A
10.3997/2214-4609.202439035,Quantifying the Value of Data Management: Fueling Strategic Directions with Generative AI,N/A
10.70175/hclreview.2020.11.1.12,Strategic Steps for Using Generative AI in HR,N/A
10.1002/9781394308286,The Predictive Edge,N/A
10.36315/2024v2end024,Generative AI-chatbots in higher education: Challenges and opportunities in student motivation and authentic assessments,N/A
10.20944/preprints202407.2109.v1,A Comprehensive Review of Generative AI in Finance,"<jats:p>The integration of generative AI (GAI) into the financial sector has brought about significant advancements, offering new solutions for various financial tasks. This review paper provides a comprehensive examination of recent trends and developments at the intersection of GAI and finance. By utilizing an advanced topic modeling method, BERTopic, we systematically categorize and analyze existing research to uncover predominant themes and emerging areas of interest. Our findings reveal the transformative impact of finance-specific large language models (LLMs), the innovative use of generative adversarial networks (GANs) in synthetic financial data generation, and the pressing necessity of a new regulatory framework to govern the use of GAI in the finance sector. This paper aims to provide researchers and practitioners with a structured overview of the current landscape of GAI in finance, offering insights into both the opportunities and challenges presented by these advanced technologies.</jats:p>"
10.2139/ssrn.4894529,Regulating Generative AI in Australia: Challenges of Regulatory Design and Regulator Capacity,N/A
10.2139/ssrn.4793782,From Creation to Caution: The Effect of Generative AI on Online Art Market,N/A
10.2139/ssrn.4722780,Can ChatGPT Plan Your Retirement?: Generative AI and Financial Advice,N/A
10.2139/ssrn.4821952,"Generative Ai in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity",N/A
10.21125/edulearn.2024.1529,"""HACKING TIME"": USING GENERATIVE AI TO SUPPORT IMPLEMENTATION OF REAL-WORLD PROJECTS",N/A
10.1001/jama.2023.23003,Will Generative AI Tools Improve Access to Reliable Health Information?,"<jats:p>This Medical News article is an interview with <jats:italic>JAMA</jats:italic> Editor in Chief Kirsten Bibbins-Domingo and Virologist Davey Smith, head of the Division of Infectious Diseases and Global Public Health at the University of California, San Diego.</jats:p>"
10.36227/techrxiv.23272271.v2,An Overview on Generative AI at Scale with Edge-Cloud Computing,"<jats:p>&lt;p&gt;As a specific category of artificial intelligence (AI), generative artificial intelligence (GenAI) generates new content that resembles what is created by humans. The rapid development of GenAI systems has created a huge amount of new data on the Internet, posing new challenges to current computing and communication frameworks. Currently, GenAI services rely on the traditional cloud computing framework due to the need for large computation resources. However, such services will encounter high latency because of data transmission and a high volume of requests. On the other hand, edge-cloud computing can provide adequate computation power and low latency at the same time through the collaboration between edges and the cloud. Thus, it is attractive to build GenAI systems at scale by leveraging the edge-cloud computing paradigm.  In this overview paper, we review recent developments in GenAI and edge-cloud computing, respectively.  Then, we use two exemplary GenAI applications to discuss technical challenges in scaling up their solutions using edge-cloud collaborative systems. Finally, we list design considerations for training and deploying GenAI systems at scale and point out future research directions. &lt;/p&gt;</jats:p>"
10.4018/979-8-3693-1950-5,Making Art With Generative AI Tools,N/A
10.2139/ssrn.4495316,Generative-AI Pilot for Problem Spaces: Can ChatGPT help develop Scenarios?,N/A
10.2139/ssrn.4780034,Generative AI: Crafting Portfolios Tailored to Investor Preferences,N/A
10.2139/ssrn.4579322,Copyright in Generative AI training: Balancing Fair Use through Standardization and Transparency,N/A
10.2139/ssrn.4951730,The Affective Landscape of Generative Ai in Efl: A Mixed-Methods Study on College Students' Motivation and Attitudes,N/A
10.2139/ssrn.4811924,Mitigating the Risks of Generative AI in Government through Algorithmic Governance,N/A
10.52731/liir.v004.168,Qualitative Evaluations of Ideas created by Generative AI,N/A
10.2139/ssrn.4762804,An Overview of the European Union Framework Governing Generative AI Models and Systems,N/A
10.2196/preprints.48780,Anki Tagger: A Generative AI Tool for Aligning Third-Party Resources to Preclinical Curriculum (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>Using large language models, we developed a method to efficiently query existing flashcard libraries and select those most relevant to an individual's medical school curricula.</p>
                </sec>"
10.1145/3571884.3603756,Generative AI Considered Harmful,N/A
10.53289/ovfa7327,Introducing generative AI into healthcare practice,N/A
10.3390/su16073034,Generative AI for Customizable Learning Experiences,"<jats:p>The introduction of accessible generative artificial intelligence opens promising opportunities for the implementation of personalized learning methods in any educational environment. Personalized learning has been conceptualized for a long time, but it has only recently become realistic and truly achievable. In this paper, we propose an affordable and sustainable approach toward personalizing learning materials as part of the complete educational process. We have created a tool within a pre-existing learning management system at a software engineering college that automatically generates learning materials based on the learning outcomes provided by the professor for a particular class. The learning materials were composed in three distinct styles, the initial one being the traditional professor style and the other two variations adopting a pop-culture influence, namely Batman and Wednesday Addams. Each lesson, besides being delivered in three different formats, contained automatically generated multiple-choice questions that students could use to check their progress. This paper contains complete instructions for developing such a tool with the help of large language models using OpenAI’s API and an analysis of the preliminary experiment of its usage performed with the help of 20 college students studying software engineering at a European university. Participation in the study was optional and on voluntary basis. Each student’s tool usage was quantified, and two questionnaires were conducted: one immediately after subject completion and another 6 months later to assess both immediate and long-term effects, perceptions, and preferences. The results indicate that students found the multiple variants of the learning materials really engaging. While predominantly utilizing the traditional variant of the learning materials, they found this approach inspiring, would recommend it to other students, and would like to see it more in classes. The most popular feature were the automatically generated quiz-style tests that they used to assess their understanding. Preliminary evidence suggests that the use of various versions of learning materials leads to an increase in students’ study time, especially for students who have not mastered the topic otherwise. The study’s small sample size of 20 students restricts its ability to generalize its findings, but its results provide useful early insights and lay the groundwork for future research on AI-supported educational strategies.</jats:p>"
10.1145/3641234.3671054,Generative AI for 2D Character Animation,N/A
10.2139/ssrn.4761318,Contingency Scenario Planning using Generative AI,N/A
10.2139/ssrn.4745990,Generative AI and ChatGPT in Financial Markets and Corporate Policy: A Comprehensive Review,N/A
10.2139/ssrn.4898213,Critique of Generative AI Can Harm Learning Study Design&amp;nbsp;,N/A
10.36227/techrxiv.23272271.v3,An Overview on Generative AI at Scale with Edge-Cloud Computing,"<jats:p>&lt;p&gt;As a specific category of artificial intelligence (AI), generative artificial intelligence (GenAI) generates new content that resembles what is created by humans. The rapid development of GenAI systems has created a huge amount of new data on the Internet, posing new challenges to current computing and communication frameworks. Currently, GenAI services rely on the traditional cloud computing framework due to the need for large computation resources. However, such services will encounter high latency because of data transmission and a high volume of requests. On the other hand, edge-cloud computing can provide adequate computation power and low latency at the same time through the collaboration between edges and the cloud. Thus, it is attractive to build GenAI systems at scale by leveraging the edge-cloud computing paradigm.  In this overview paper, we review recent developments in GenAI and edge-cloud computing, respectively.  Then, we use two exemplary GenAI applications to discuss technical challenges in scaling up their solutions using edge-cloud collaborative systems. Finally, we list design considerations for training and deploying GenAI systems at scale and point out future research directions. &lt;/p&gt;</jats:p>"
10.37741/t.72.3.10,"Generative
AI in Hotel Marketing","<jats:p>This paper verifies the capacity of generative AI (ChatGPT in particular) to design a marketing strategy for a hotel by using the example of a 5-star property in the centre of Lisbon. The indicative solutions and reflexive tactics suggested by Dwivedi et al. (2023) were used to develop the prompts based on which ChatGPT designed the hotel's marketing strategy. The proposed approach incorporates the marketing mix elements, including digital marketing tips, the competitive set definition, a name for a marketing campaign, a logo, identification of an appropriate partner and suggested cross-selling actions. This is one of the first attempts to utilise generative AI to design a marketing strategy for a product/service. The findings of this research verify the previous assumption that ChatGPT can write it!</jats:p>"
10.2139/ssrn.4903967,"Dynamic Indoor Uav Rescue: Unifying Digital Twins, Generative Ai, and Reinforcement Learning",N/A
10.2139/ssrn.4547006,Reshaping Academic Applications: The Multifaceted Impact of Generative AI/ChatGPT for Growth,N/A
10.2139/ssrn.4676053,The Role of Generative AI in Human Creative Processes: Experimental Evidence,N/A
10.4018/979-8-3693-0074-9,Generative AI in Teaching and Learning,N/A
10.26434/chemrxiv-2021-l5pr9,Generative AI Design and Exploration of Nucleoside Analogs,"<jats:p>Nucleosides are fundamental building blocks of DNA and RNA in all life forms and viruses. In addition, natural nucleosides and their analogs are critical in prebiotic chemistry, innate immunity, signaling, antiviral drug discovery and artificial synthesis of DNA / RNA sequences. Combined with the fact that quantitative structure activity relationships (QSAR) have been widely performed to understand their antiviral activity, nucleoside analogs could be used to benchmark generative molecular design. Here, we undertake the first generative design of nucleoside analogs using an approach that we refer to as the Conditional Randomized Transformer (CRT). We also benchmark our model against five previously published molecular generative models. We demonstrate that AI-generated molecules include nucleoside analogs that are of significance in a wide range of areas including prebiotic chemistry, antiviral drug discovery and synthesis of oligonucleotides. Our results show that CRT explores distinct molecular spaces and chemical transformations, some of which are similar to those undertaken by nature and medicinal chemists. Finally, we demonstrate the potential application of the CRT model in the generative design of molecules conditioned on Remdesivir and Molnupiravir as well as other nucleoside analogs with in vitro activity against SARS-CoV-2. One-Sentence Summary: Generative design of nucleoside analogs relevant to antiviral drug discovery, prebiotic chemistry and synthetic biology.</jats:p>"
10.31234/osf.io/aycqr,Training Against Human and Generative-AI Social Engineering Attacks with Cognitive Models,"<p>Social engineering attacks are commonly used by cybercriminals to gain valuable and sensitive data. Although the concern of attackers using AI-generated content is serious, training against social engineering attacks is typically based on simple human-designed emails. Our research introduces an experimental paradigm to determine whether there is a difference in the detection of human-generated and AI-generated emails. The behavioral results show that emails written by humans and stylized by Generative-AI models are more challenging for end-users. Alongside this novel experiment, we propose a cognitive model that can be used to predict end-user behavior during training, with the potential to improve the quality of examples used during training and the training feedback. Overall, the contributions of this work are, first, the outline of some limitations to current social engineering training methods and, second, pinpointing a potential solution to these limitations through the use of a cognitive model to improve learning outcomes.</p>"
10.3390/electronics13081457,Plato’s Shadows in the Digital Cave: Controlling Cultural Bias in Generative AI,"<jats:p>Generative Artificial Intelligence (AI) systems, like ChatGPT, have the potential to perpetuate and amplify cultural biases embedded in their training data, which are predominantly produced by dominant cultural groups. This paper explores the philosophical and technical challenges of detecting and mitigating cultural bias in generative AI, drawing on Plato’s Allegory of the Cave to frame the issue as a problem of limited and distorted representation. We propose a multifaceted approach combining technical interventions, such as data diversification and culturally aware model constraints, with a deeper engagement with the cultural and philosophical dimensions of the problem. Drawing on theories of extended cognition and situated knowledge, we argue that mitigating AI biases requires a reflexive interrogation of the cultural contexts of AI development and a commitment to empowering marginalized voices and perspectives. We claim that controlling cultural bias in generative AI is inseparable from the larger project of promoting equity, diversity, and inclusion in AI development and governance. By bridging philosophical reflection with technical innovation, this paper contributes to the growing discourse on responsible and inclusive AI, offering a roadmap for detecting and mitigating cultural biases while grappling with the profound cultural implications of these powerful technologies.</jats:p>"
10.21203/rs.3.rs-3708077/v1,Transparency in Music-Generative AI: A Systematic Literature Review,"<title>Abstract</title>
        <p>Music-generative AI raises multiple challenges particularly related to the work of artists, the existing music industry model, the role of AI in creative processes, and the discussion of intellectual property rights. Our study addresses these challenges by examining transparency in music generation. We conduct a systematic literature review, following the PRISMA methodology, to gain a comprehensive understanding of the associations between algorithmic transparency, music generation, the evaluation in terms of creativity and originality, and the connections to intellectual property rights.We identify 1,111 publications by formulating four research questions. Following a rigorous review process, we narrow down the selection to 66 relevant investigations published by 2022, covering multiple AI domains. Acknowledging the rapid growth of the music generation field, we then incorporate 18 publications from 2023, focusing our search on the music-specific domain and novel applications. Thus, the present review overviews 84 publications. Our findings highlight a growing interest in AI transparency and the ethical consequences of generative models. However, transparent strategies in music-generative AI remain an under-explored topic. Our main contribution is the identification of research gaps and challenges in transparency for music-generative AI.</p>"
10.18260/1-2--47918,Re-Design Introductory Engineering Course for Tinkering with Generative AI and the Shifts in Students’ Perceptions of Using AI for Learning,N/A
10.4018/979-8-3693-1082-3.ch017,Generative AI and Healthcare 5.0,"<jats:p>Over the past century, the global healthcare system has undergone remarkable transformations driven by technological advancements. Healthcare 1.0 represented ancient healing practices, reliant on manual record-keeping and lacking efficiency. The advent of computers in the 1980s ushered in Healthcare 2.0, enabling centralized digital records and improved communication. Healthcare 3.0 introduced affordable smart wearable devices, capable of monitoring health metrics. In Healthcare 4.0, the integration of AI with smart devices led to patient-centric care, global collaboration, and enhanced research efficiency, propelling the healthcare sector into a new era of progress.</jats:p>"
10.1162/99608f92.8036d03b,AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap,N/A
10.1007/s43681-023-00412-3,Protecting ownership rights of ML models using watermarking in the light of adversarial attacks,N/A
10.1007/s43681-023-00407-0,Assessing systematic weaknesses of DNNs using counterfactuals,"<jats:title>Abstract</jats:title><jats:p>With the advancement of DNNs into safety-critical applications, testing approaches for such models have gained more attention. A current direction is the search for and identification of systematic weaknesses that put safety assumptions based on average performance values at risk. Such weaknesses can take on the form of (semantically coherent) subsets or areas in the input space where a DNN performs systematically worse than its expected average. However, it is non-trivial to attribute the reason for such observed low performances to the specific semantic features that describe the subset. For instance, inhomogeneities within the data w.r.t. other (non-considered) attributes might distort results. However, taking into account all (available) attributes and their interaction is often computationally highly expensive. Inspired by counterfactual explanations, we propose an effective and computationally cheap algorithm to validate the semantic attribution of existing subsets, i.e., to check whether the identified attribute is likely to have caused the degraded performance. We demonstrate this approach on an example from the autonomous driving domain using highly annotated simulated data, where we show for a semantic segmentation model that (i) performance differences among the different pedestrian assets exist, but (ii) only in some cases is the asset type itself the reason for this reduction in the performance.</jats:p>"
10.1007/s43681-023-00415-0,Evaluating trustworthiness of decision tree learning algorithms based on equivalence checking,N/A
10.1145/3375627.3375827,U.S. Public Opinion on the Governance of Artificial Intelligence,N/A
10.1007/s43681-023-00393-3,ECS: an interactive tool for data quality assurance,"<jats:title>Abstract</jats:title><jats:p>With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper, we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.</jats:p>"
10.1007/978-3-030-54173-6_1,"AI, Robotics, and Humanity: Opportunities, Risks, and Implications for Ethics and Policy","<jats:title>Abstract</jats:title><jats:p>This introduction to the volume gives an overview of foundational issues in AI and robotics, looking into AI’s computational basis, brain–AI comparisons, and conflicting positions on AI and consciousness. AI and robotics are changing the future of society in areas such as work, education, industry, farming, and mobility, as well as services like banking. Another important concern addressed in this volume are the impacts of AI and robotics on poor people and on inequality. These implications are being reviewed, including how to respond to challenges and how to build on the opportunities afforded by AI and robotics. An important area of new risks is robotics and AI implications for militarized conflicts. Throughout this introductory chapter and in the volume, AI/robot-human interactions, as well as the ethical and religious implications, are considered. Approaches for fruitfully managing the coexistence of humans and robots are evaluated. New forms of regulating AI and robotics are called for which serve the public good but also ensure proper data protection and personal privacy.</jats:p>"
10.1007/s43681-021-00055-2,Perspectives about artificial moral agents,"<jats:title>Abstract</jats:title><jats:p>The pursuit of AMAs is complicated. Disputes about the development, design, moral agency, and future projections for these systems have been reported in the literature. This empirical study explores these controversial matters by surveying (AI) Ethics scholars with the aim of establishing a more coherent and informed debate. Using Q-methodology, we show the wide breadth of viewpoints and approaches to artificial morality. Five main perspectives about AMAs emerged from our data and were subsequently interpreted and discussed: (i) Machine Ethics: The Way Forward; (ii) Ethical Verification: Safe and Sufficient; (iii) Morally Uncertain Machines: Human Values to Avoid Moral Dystopia; (iv) Human Exceptionalism: Machines Cannot Moralize; and (v) Machine Objectivism: Machines as Superior Moral Agents. A potential source of these differing perspectives is the failure of Machine Ethics to be widely observed or explored as an applied ethic and more than a futuristic end. Our study helps improve the foundations for an informed debate about AMAs, where contrasting views and agreements are disclosed and appreciated. Such debate is crucial to realize an interdisciplinary approach to artificial morality, which allows us to gain insights into morality while also engaging practitioners.</jats:p>"
10.1007/s43681-022-00192-2,"Algorithms, leadership, and morality: why a mere human effect drives the preference for human over algorithmic leadership",N/A
10.1007/s43681-023-00273-w,"Artificial intelligence, superefficiency and the end of work: a humanistic perspective on meaning in life","<jats:title>Abstract</jats:title><jats:p>How would it be assessed from an ethical point of view if human wage work were replaced by artificially intelligent systems (AI) in the course of an automation process? An answer to this question has been discussed above all under the aspects of individual well-being and social justice. Although these perspectives are important, in this article, we approach the question from a different perspective: that of leading a meaningful life, as understood in analytical ethics on the basis of the so-called meaning-in-life debate. Our thesis here is that a life without wage work loses specific sources of meaning, but can still be sufficiently meaningful in certain other ways. Our starting point is John Danaher’s claim that ubiquitous automation inevitably leads to an achievement gap. Although we share this diagnosis, we reject his provocative solution according to which game-like virtual realities could be an adequate substitute source of meaning. Subsequently, we outline our own systematic alternative which we regard as a decidedly humanistic perspective. It focuses both on different kinds of social work and on rather passive forms of being related to meaningful contents. Finally, we go into the limits and unresolved points of our argumentation as part of an outlook, but we also try to defend its fundamental persuasiveness against a potential objection.</jats:p>"
10.21428/e4baedd9.caa10d84,Who Do We Become When We Talk to Machines?,N/A
10.20944/preprints202405.1158.v1,Students’ Perception of Generative AI Use for Academic Purpose in UK Higher Education,"<jats:p>Background: Generative artificial intelligence (Gen-AI) has emerged as a transformative tool in research and education. However, there is a mixed perception about its use. This study assessed the use, perception, prospect, and challenges of Gen-AI use in higher education.  Methods: This is a prospective, cross-sectional survey of university students in the United Kingdom (UK) distributed online between January and April 2024. Demography and perception of Gen-AI and other AI tools were assessed and statistically analysed to assess the difference in perception between various subgroups. Results: A total of 136 students responded to the survey of which 59% (80) were male. The majority were aware of Gen-AI and other AI use in academia (61%) with 52% having personal experience of the tools. Grammar correction and idea generation were the two most common tasks of use, with 37% being regular users. Fifty-six percent of respondents agreed that AI gives an academic edge with 40% holding a positive overall perception about the use in academia. Comparatively, there was a statistically significant difference in overall perception between different age ranges (I2 = 27.39; p = 0.002) and levels of education (I2 = 20.07; p &amp;lt; 0.001). Also, 83% of students believe AI use will increase in academia with over half agreeing it should be integrated into learning. Plagiarism (33%), privacy issues (14%), and lack of clarity by the university (13%) remain the top concerns regarding the use of Gen-AI and other AI tools in academia. Conclusions: Gen-AI and other AI tools are being used and their use will continue to grow in higher education. While current use is challenging due mainly to plagiarism fear and lack of clarity by the university, most users believe AI should be integrated into the university curriculum.</jats:p>"
10.2139/ssrn.4773872,Incorporating Generative Ai Agents into Socioeconomic Metabolism Modelling: The Next Frontier,N/A
10.1353/jod.2024.a915355,The Real Dangers of Generative AI,"<jats:p xml:lang=""en""> Abstract: As perhaps the most consequential technology of our time, Generative Foundation Models (GFMs) present unprecedented challenges for democratic institutions. By allowing deception and de-contextualized information sharing at a previously unimaginable scale and pace, GFMs could undermine the foundations of democracy. At the same time, the investment scale required to develop the models and the race dynamics around that development threaten to enable concentrations of democratically unaccountable power (both public and private). This essay examines the twin threats of collapse and singularity occasioned by the rise of GFMs.</jats:p>"
10.58875/rqjd7538,How Generative AI Endangers Cultural Narratives,N/A
10.1002/9781394308286.fmatter,Front Matter,N/A
10.1109/vissoft60811.2023.00014,Visualizing Source Code as Comics Using Generative AI,N/A
10.21125/edulearn.2024.1273,COMPARATIVE ANALYSIS OF GENERATIVE AI MODELS IN EDUCATIONAL EXERCISE PERFORMANCE,N/A
10.52783/pmj.v34.i2.1105,A Mathematical Model to Enhance Creativity in Generative AI Systems,"<jats:p>People often think it will be hard for artificial intelligence (AI) systems to copy creativity because it is a complicated and important part of human intelligence. Generative AI, which tries to make new and useful things, has shown promise in being able to copy creative processes. However, generative AI systems that are already in use often have trouble making results that are truly original. This is mostly because there isn't a clear scientific framework to help people be creative. We present a new mathematical model to improve creativity in creative AI systems in this work. Our model is based on the ideas of lateral and convergent thinking, which are important parts of how people think creatively. Divergent thinking means coming up with many ideas or solutions, while convergent thinking means picking the best ones and making them even better. To put our model into action, we come up with the idea of a ""creativity score,"" which measures how new and useful the results are. The creativity score is determined by looking at a number of things, such as the variety of outputs, how different they are from current solutions, and how useful they are for fixing the problem at hand. Finding the right balance between divergent and convergent thinking is one of the hardest parts of making generative AI systems more creative. Divergent thinking is important for coming up with many ideas, but convergent thinking is needed to pick out the best ones and make them even better. This problem is solved by our model, which uses both divergent and convergent thought. To make sure our model works, we test it on different creative AI tasks, such as making images and writing text. Our results show that our model can greatly improve the creativity of generative AI systems, resulting in more varied, unique, and useful outputs.</jats:p>"
10.36227/techrxiv.23272271,An Overview on Generative AI at Scale with Edge-Cloud Computing,"<jats:p>&lt;p&gt;As a specific category of artificial intelligence (AI), generative artificial intelligence (GenAI) generates new content that resembles what is created by humans. The rapid development of GenAI systems has created a huge amount of new data on the Internet, posing new challenges to current computing and communication frameworks. Currently, GenAI services rely on the traditional cloud computing framework due to the need for large computation resources. However, such services will encounter high latency because of data transmission and a high volume of requests. On the other hand, edge-cloud computing can provide adequate computation power and low latency at the same time through the collaboration between edges and the cloud. Thus, it is attractive to build GenAI systems at scale by leveraging the edge-cloud computing paradigm.  In this overview paper, we review recent developments in GenAI and edge-cloud computing, respectively.  Then, we use two exemplary GenAI applications to discuss technical challenges in scaling up their solutions using edge-cloud collaborative systems. Finally, we list design considerations for training and deploying GenAI systems at scale and point out future research directions. &lt;/p&gt;</jats:p>"
10.69554/vskl5622,Creativity: Firing on all generative AI cylinders,"<jats:p xml:lang=""en"">Generative AI is unquestionably having its moment in the digital sun. It is currently being used to build applications that can generate new content quickly, efficiently and responsibly, as well as to augment the skills of workers. As this paper will show, however, its ability to create text, images, music, animation, video and even build software code from simple, user-generated text prompts will simplify business processes in a multitude of ways. With generative AI having already demonstrated its ability to ‘see’, ‘hear’, ‘speak’, ‘move’ and ‘write’, the power of its creativity will be increasingly embraced. It will not only be associated for image, audio and video content creation, but will also help with content personalisation, content automation, coding creation and assistance, augmenting customer support, augmenting workers, product design and content repurposing. The potential support it can provide to business is almost limitless.</jats:p>"
10.36227/techrxiv.23272271.v1,An Overview on Generative AI at Scale with Edge-Cloud Computing,"<jats:p>&lt;p&gt;As a specific category of artificial intelligence (AI), generative artificial intelligence (GenAI) generates new content that resembles what is created by humans. The rapid development of GenAI systems has created a huge amount of new data on the Internet, posing new challenges to current computing and communication frameworks. Currently, GenAI services rely on the traditional cloud computing framework due to the need for large computation resources. However, such services will encounter high latency because of data transmission and a high volume of requests. On the other hand, edge-cloud computing can provide adequate computation power and low latency at the same time through the collaboration between edges and the cloud. Thus, it is attractive to build GenAI systems at scale by leveraging the edge-cloud computing paradigm.  In this overview paper, we review recent developments in GenAI and edge-cloud computing, respectively.  Then, we use two exemplary GenAI applications to discuss technical challenges in scaling up their solutions using edge-cloud collaborative systems. Finally, we list design considerations for training and deploying GenAI systems at scale and point out future research directions. &lt;/p&gt;</jats:p>"
10.12688/f1000research.143131.1,"Generative AI, UK Copyright and Open Licences: considerations for UK HEI copyright advice services","<ns3:p>With the enormous growth in interest and use of generative artificial intelligence (AI) systems seen since the launch of ChatGPT in autumn 2022 have come questions both about the legal status of AI outputs, and of using protected works as training inputs. It is inevitable that UK higher education institution (HEI) library copyright advice services will see an increase in questions around use of works with AI as a result. Staff working in such library services are not lawyers or able to offer legal advice to their academic researchers. Nonetheless, they must look at the issues raised, consider how to advise in analogous situations of using copyright material, and offer opinion to researchers accordingly. While the legal questions remain to be answered definitively, copyright librarians can still offer advice on both open licences and use of copyright material under permitted exceptions. We look here at how library services can address questions on copyright and open licences for generative AI for researchers in UK HEIs.</ns3:p>"
10.1145/3656156.3663731,Ideary: Facilitating Electronic Music Creation with Generative AI,N/A
10.2139/ssrn.4886015,Approach Generative AI Tools Proactively or Risk Bypassing the Learning Process in Higher Education,N/A
10.1596/1813-9450-10863,Buffer or Bottleneck? Employment Exposure to Generative AI and the Digital Divide in Latin America,N/A
10.1201/9781003278290-64,"Google Research: Who Is Responsible for Ethics of AI?
*",N/A
10.1007/s11948-020-00243-z,Fully Autonomous AI,N/A
10.4337/9781802203110.00011,What is data ethics?,N/A
10.4337/9781803926728.00026,Queering the Ethics of AI,N/A
10.1007/s11948-023-00451-3,AI as an Epistemic Technology,N/A
10.1111/bioe.12716,On the ethics of AI ethics,N/A
10.29173/irie385,"Introduction to AI, Ethics &amp; Society",N/A
10.1145/3514094.3539527,Automated Kantian Ethics,N/A
10.31235/osf.io/uj6my,Multi Scale Ethics – Why we need a sociological approach to the ethics of AI in healthcare at different scales.,"<p>Around the world, we are seeing a significant growth in interest and investment in AI in healthcare. This has been coupled with rising concerns about the ethical implications of these technologies and an array of ethical guidelines for the use of AI and data in healthcare has arisen. Nevertheless, the question of if and how AI and data technologies can be ethical remains open to debate. This paper aims to contribute to this debate by considering the wide range of implications that have been attributed to these technologies and asking whether current ethical guidelines take these factors into account.  In particular, the paper argues that current ethics guidelines for AI in healthcare effectively account for the four key issues identified in the ethics literature (transparency; fairness; responsibility and privacy), they have largely neglected wider issues relating to the way in which these technologies shape institutional and social arrangements.  This, we argue, has given current ethics guidelines a strong focus on evaluating the impact of these technologies on the individual, while not accounting for the powerful social shaping effects of these technologies. To address this, the paper proposes a Multiscale Ethics Framework, which aims to help technology developers and ethical evaluations to consider the wider implications of these technologies.</p>"
10.29173/irie425,"Introduction to AI, Ethics &amp; Society – Part 2",N/A
10.4324/9780429461002-6,Ethics of AI and the need for regulation,N/A
10.1201/9781003278290-13,"Ethics of Care as Moral Grounding for AI
*",N/A
10.3389/fcomp.2022.776837,AI Ethics as Applied Ethics,"<jats:p>The need to design and develop artificial intelligence (AI) in a sustainable manner has motivated researchers, institutions, and organizations to formulate suggestions for AI ethics. Although these suggestions cover various topics and address diverse audiences, they share the presupposition that AI ethics provides a generalizable basis for designers that is applicable to their work. We propose that one of the reasons the influence of current ethical codes has remained modest, may be the conception of the applied ethics that they represent. We discuss bioethics as a point of reference for weighing the metaethical and methodological approaches adopted in AI ethics, and propose that AI ethics could be made more methodologically solid and substantively more influential if the resources were enriched by adopting tools from fields of study created to improve the quality of human action and safeguard its desired outcomes. The approaches we consider to be useful for this purpose are the systems theory, safety research, impact assessment approach, and theory of change.</jats:p>"
10.1007/s43681-022-00179-z,Proposed EU AI Act—Presidency compromise text: select overview and comment on the changes to the proposed regulation,"<jats:title>Abstract</jats:title><jats:p>With its proposed EU AI Act, the EU is aspiring to lead the world in admiral AI regulation (April 2021). In this brief, we summarise and comment on the ‘Presidency compromise text’, which is a revised version of the proposed act reflecting the consultation and deliberation by member states and actors (November 2021). The compromise text echoes the sentiment of the original text, much of which remains largely unchanged. However, there are important shifts and some significant changes. Our main comments focus on exemptions to the act with respect to national security; changes that seek to further protect research, development and innovation; and the attempt to clarify the draft legislation’s stance on algorithmic manipulation. Our target readership for this paper is those who are interested in tracking the evolution of the proposed EU AI act, such as policy-makers and those in the legal profession.</jats:p>"
10.1007/s43681-022-00201-4,Assessing the ethical and social concerns of artificial intelligence in neuroinformatics research: an empirical test of the European Union Assessment List for Trustworthy AI (ALTAI),"<jats:title>Abstract</jats:title><jats:p>Ethical and social concerns are a key obstacle to the adoption of artificial intelligence (AI) in the life sciences and beyond. The discussion of these issues has intensified in recent years and led to a number of approaches, tools and initiatives. Key amongst them is the idea of ex-ante impact assessments that aim to identify issues at the early stages of development. One prominent example of such ex-ante impact assessment is the European Union's (EU) Assessment list for Trustworthy AI (ALTAI). This article uses the findings of a large-scale application of the ALTAI to a large neuro-informatics project as an exemplar to demonstrate the effectiveness and limitations of the ALTAI in practice. The article shows that ex-ante impact assessment has the potential to help identify and address ethical and social issues. However, they need to be understood as part of a broader socio-technical ecosystem of AI. For ALTAI and related approaches to be useful in bio-medical research, they should be interpreted from a systems theory perspective which allows for their integration into the rich set of tools, legislation and approaches. The paper argues that ex-ante impact assessments have the best chance of being successful if seen applied in conjunction with other approaches in the context of the overall AI ecosystem.</jats:p>"
10.1016/j.inffus.2024.102654,Generative AIBIM: An automatic and intelligent structural design pipeline integrating BIM and generative AI,N/A
10.4018/979-8-3693-0502-7.ch007,Textual Alchemy,"<jats:p>This chapter explores the transformative potential of generative models for advanced text generation, focusing on leveraging structural equation modeling techniques. With the rapid advancements in deep learning and natural language processing, generative models have emerged as powerful tools for creative writing, semantic coherence, and contextual understanding. This chapter provides a comprehensive overview of the foundations, methodologies, and applications of generative models in text generation. The chapter begins with an introduction to the evolution of generative models and highlights their significance in various domains. It lays the groundwork by explaining language modeling techniques and the architectures employed in text generation using deep learning algorithms. The subsequent sections delve into the core aspects of generative models for text generation.</jats:p>"
10.1007/s10676-022-09671-w,Correction to: the Ethics of AI in Human Resources,N/A
10.1007/s10551-023-05538-2,"The Ethics of Artificial Intelligence: Review of Ethical Machines: Your Concise Guide to Totally Unbiased, Transparent, and Respectful AI by R. Blackman; Ethics of Artificial Intelligence: Case Studies and Options for Addressing Ethical Challenges by B.C. Stahl, D. Schroeder, and R. Rodrigues; and AI Ethics by M. Coeckelbergh",N/A
10.1007/s00146-021-01336-4,From the ground truth up: doing AI ethics from practice to principles,N/A
10.1016/b978-0-443-19096-4.00005-5,Human AI: Ethics and broader impact for mental healthcare,N/A
10.1145/3461702.3462619,AI and Shared Prosperity,N/A
10.1007/978-3-030-72188-6_2,AI and Ethics—Operationalizing Responsible AI,N/A
10.1002/eahr.500182,Editors’ Statement on the Responsible Use of Generative AI Technologies in Scholarly Journal Publishing,"<jats:title>ABSTRACT</jats:title><jats:p>Generative artificial intelligence (AI) has the potential to transform many aspects of scholarly publishing. Authors, peer reviewers, and editors might use AI in a variety of ways, and those uses might augment their existing work or might instead be intended to replace it. We are editors of bioethics and humanities journals who have been contemplating the implications of this ongoing transformation. We believe that generative AI may pose a threat to the goals that animate our work but could also be valuable for achieving those goals. In the interests of fostering a wider conversation about how generative AI may be used, we have developed a preliminary set of recommendations for its use in scholarly publishing. We hope that the recommendations and rationales set out here will help the scholarly community navigate toward a deeper understanding of the strengths, limits, and challenges of AI for responsible scholarly work.</jats:p>"
10.1007/s43681-022-00252-7,Replika in the Metaverse: the moral problem with empathy in ‘It from Bit’,"<jats:title>Abstract</jats:title><jats:p>This paper assesses claims of computational empathy in relation to existing social open-ended chatbots and intention that these chatbots will feature in emergent mixed reality contexts, recently given prominence due to interest in the Metaverse. Against the background of increasing loneliness within society and use of chatbots as a potential remedy for this, the paper considers two leading current social chatbots, <jats:italic>Replika</jats:italic> and Microsoft’s <jats:italic>Xiaoice</jats:italic>, their technical underpinnings, empathetic claims and properties that have scope to scale into the Metaverse (if it coheres). Finding scope for human benefit from social chatbots, the paper highlights problematic reliance on self-disclosure to sustain the existence of chatbots. The paper progresses to situate Microsoft’s empathetic computing framework in relation to philosophical ideas that inform Metaverse speculation and construction, including Wheeler’s ‘It from Bit’ thesis that all aspects of existence may be computed, Chalmers’ philosophical championing that virtual realities are genuine realities, Bostrom’s proposal and provocation that we might already be living in a simulation, and longtermist belief that future complex simulations need to be protected from decisions made today. Given claims for current and nascent social chatbots, belief in bit-based possible and projected futures, and industrial buy-in to these philosophies, this paper answers whether computational empathy is real or not. The paper finds when diverse accounts of empathy are accounted for, whilst something is irrevocably lost in an ‘It from Bit’ account of empathy, the missing components are not accuracy or even human commonality of experience, but the moral dimension of empathy.</jats:p>"
10.1007/s43681-023-00390-6,QI$$^2$$: an interactive tool for data quality assurance,"<jats:title>Abstract</jats:title><jats:p>The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also, the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper, we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well-known MNIST data set based an handwritten digits.</jats:p>"
10.2196/51204,Role of Ethics in Developing AI-Based Applications in Medicine: Insights From Expert Interviews and Discussion of Implications,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>The integration of artificial intelligence (AI)–based applications in the medical field has increased significantly, offering potential improvements in patient care and diagnostics. However, alongside these advancements, there is growing concern about ethical considerations, such as bias, informed consent, and trust in the development of these technologies.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>This study aims to assess the role of ethics in the development of AI-based applications in medicine. Furthermore, this study focuses on the potential consequences of neglecting ethical considerations in AI development, particularly their impact on patients and physicians.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>Qualitative content analysis was used to analyze the responses from expert interviews. Experts were selected based on their involvement in the research or practical development of AI-based applications in medicine for at least 5 years, leading to the inclusion of 7 experts in the study.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>The analysis revealed 3 main categories and 7 subcategories reflecting a wide range of views on the role of ethics in AI development. This variance underscores the subjectivity and complexity of integrating ethics into the development of AI in medicine. Although some experts view ethics as fundamental, others prioritize performance and efficiency, with some perceiving ethics as potential obstacles to technological progress. This dichotomy of perspectives clearly emphasizes the subjectivity and complexity surrounding the role of ethics in AI development, reflecting the inherent multifaceted nature of this issue.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>Despite the methodological limitations impacting the generalizability of the results, this study underscores the critical importance of consistent and integrated ethical considerations in AI development for medical applications. It advocates further research into effective strategies for ethical AI development, emphasizing the need for transparent and responsible practices, consideration of diverse data sources, physician training, and the establishment of comprehensive ethical and legal frameworks.</jats:p>
          </jats:sec>"
10.12781/978-1-907549-40-3-1,The Craft of Participation in a Changing World: How AI Practitioners Can Help in ‘Holding Space’ for Generative Conversations,N/A
10.56366/jcsts.2023.2.2.11,Development of a Convergence Education Program Utilizing Generative AI,N/A
10.37237/140406,Fostering Self-Assessment in English Learning with a Generative AI Platform: A Case of Quizizz AI,"<jats:p>The incorporation of Generative AI and platforms like Quizizz holds immense potential to deliver substantial benefits to both English students and teachers. Students can readily create their interactive self-assessment tools, allowing them to monitor their learning progress independently. Meanwhile, teachers can facilitate and enhance students’ independent learning, easing their workload, offering personalized insights, and boosting student engagement. Additionally, Quizizz AI can serve as a valuable resource for teachers looking to advance their professional development. Importantly, both students and teachers can explore new concepts at their own pace without the fear of judgment from others. While the platform offers numerous advantages for both educators and learners, several enhancements could render it an even more potent tool. By diversifying the types of AI-generated questions, the platform could create a wider range of activities targeting various English language skills. Furthermore, expanding the variety of AI-generated responses, such as voice or video, could facilitate the practice of productive skills like speaking. Additionally, written-text responses could promote writing skills. To further enrich the learning experience, the integration of AI for checking and providing personalized feedback on these responses might be considered.</jats:p>"
10.4108/ew.4825,"Transforming the Energy Sector: Addressing Key Challenges through Generative AI, Digital Twins, AI, Data Science and Analysis","<jats:p>The energy sector, both in the UK and globally, faces significant challenges in the pursuit of sustainability and efficient resource utilization. Climate change, resource depletion, and the need for decarbonization demand innovative solutions. This analytical research paper examines the key challenges in the energy sector and explores how generative AI, digital twins, AI, and data science can play a transformative role in addressing these challenges. By leveraging advanced technologies and data-driven approaches, the energy sector can achieve greater efficiency, optimize operations, and facilitate informed decision-making. Artificial Intelligence (AI) involves replicating human-like intelligence in machines, enabling them to execute tasks that typically demand human cognitive capabilities like perception, reasoning, learning, and problem[1]solving. AI encompasses various methodologies and technologies, such as machine learning, natural language processing, computer vision, and robotics. Its adoption in the energy sector carries significant promise for addressing critical concerns and revolutionizing the industry. An overarching challenge in the energy sector revolves around enhancing energy efficiency, and AI emerges as a pivotal tool for optimizing energy utilization and curbing wastage. By analyzing vast amounts of data from various sources such as sensors, smart meters, and historical energy consumption patterns, AI algorithms can identify patterns and anomalies that humans may not detect. This enables the development of predictive models and algorithms that optimize energy consumption, leading to significant energy savings.</jats:p>"
10.35373/kmes.29.2.5,Analysis on Users' Evaluation and Satisfaction toward Generative AI Service,N/A
10.1515/9783111323749-015,15 Utilizing AI technologies to enhance e-commerce business operations,N/A
10.1145/3633083.3633094,Assessing User Perceptions of Bias in Generative AI Models: Promoting Social Awareness for Trustworthy AI,N/A
10.1145/3644815.3644969,Innovating Translation: Lessons Learned from BWX Generative Language Engine,N/A
10.1007/s43681-022-00155-7,"Adopting smart glasses responsibly: potential benefits, ethical, and privacy concerns with Ray-Ban stories","<jats:title>Abstract</jats:title><jats:p>The adoption of innovative wearable technologies is potentially increasing as a new trend. Jumping into the augmented reality (AR) and Metaverse, Facebook (now known as Meta) launched smart glasses partnering with Ray-Ban sunglasses brand’s parent company EssilorLuxottica. Ray-Ban stories has several technical features for entertainment and socializing; more importantly, these features can be adopted in the future for more advanced wearable. However, these smart glasses also came with many ethical and privacy concerns along with their potential benefits. Furthermore, the unbridled deployment of these smart glasses brought several challenging questions for public social interaction when we will have more such devices in our lives. This short article has discussed the Ray-Ban stories’ ethical and privacy issues for social interaction and public places.</jats:p>"
10.1016/bs.dnb.2024.02.010,What the embedded ethics approach brings to AI-enhanced neuroscience,N/A
10.55574/erqq4677,CAN AI MAKE A CASE?AI VS. LAWYER IN THE DUTCH LEGAL CONTEXT,"<jats:p>The integration of AI, specifically GPT-4, into the legal field is a subject of both potential promise and intricate challenges. This thesis delves into the transformational possibilities of AI within the Dutch legal context, examining not only the quality and persuasiveness of AI-generated legal argumentation but also its competence in information retrieval, as measured by models’ ability to spot relevant legal issues. An experiment was conducted with 25 legal professionals, using a real-world Dutch case with the purpose to assess GPT-4’s capabilities against that of a human lawyer. To enable GPT-4 to handle case documents, the author first performed so-called co-reference resolution to remove ambiguities. Given the token limitations of GPT-4, a so-called prompt reducer technique was used to compress the text, retaining essential information. The above methods produced a coherent and full case summary within GPT-4’s token constraints. This case summary, together with original lawyer’s letter (denoted as Text A) was fed to ChatGPT-4 to obtain its AI-written alternative (Text B). The study subjects were presented with a case summary as well as both texts and asked for their preferences. The outcome of the experiment is as follow: 80% of participants chose the AI’s composed legal document, demonstrating a strong preference for both its linguistic competencies as well its ability to spot relevant legal issues. This preference for GPT-4 writing is very consistent among genders, age groups and professionssurveyed. Contextualising these findings within the broader implications for legal practice, the thesis explores potential benefits including increased access to justice and transformation of certain legal procedures. Insights and recommendations are offered for legal professionals, considering the technological evolution and ethical considerations inherent in AI integration. Acknowledging the need for further exploration, the study recognises its own limitations and encourages replication to solidify the understanding of AI’s transformative role in the legal realm.</jats:p>"
10.1145/3461702.3462616,Are AI Ethics Conferences Different and More Diverse Compared to Traditional Computer Science Conferences?,N/A
10.24185/sswuhr.2023.08.48.211,Humanities Manifesto in the Age of Generative AI: Saving Humanities in the Posthuman Condition,N/A
10.1108/k-04-2024-0894,From general AI to custom AI: the effects of generative conversational AI’s cognitive and emotional conversational skills on user's guidance,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>Generative conversational artificial intelligence (AI) demonstrates powerful conversational skills for general tasks but requires customization for specific tasks. The quality of a custom generative conversational AI highly depends on users’ guidance, which has not been studied by previous research. This study uses social exchange theory to examine how generative conversational AI’s cognitive and emotional conversational skills affect users’ guidance through different types of user engagement, and how these effects are moderated by users’ relationship norm orientation.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>Based on data collected from 589 actual users using a two-wave survey, this study employed partial least squares structural equation modeling to analyze the proposed hypotheses. Additional analyses were performed to test the robustness of our research model and results.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>The results reveal that cognitive conversational skills (i.e. tailored and creative responses) positively affected cognitive and emotional engagement. However, understanding emotion influenced cognitive engagement but not emotional engagement, and empathic concern influenced emotional engagement but not cognitive engagement. In addition, cognitive and emotional engagement positively affected users’ guidance. Further, relationship norm orientation moderated some of these effects such that the impact of user engagement on user guidance was stronger for communal-oriented users than for exchange-oriented users.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>First, drawing on social exchange theory, this study empirically examined the drivers of users’ guidance in the context of generative conversational AI, which may enrich the user guidance literature. Second, this study revealed the moderating role of relationship norm orientation in influencing the effect of user engagement on users’ guidance. The findings will deepen our understanding of users’ guidance. Third, the findings provide practical guidelines for designing generative conversational AI from a general AI to a custom AI.</jats:p></jats:sec>"
10.4018/979-8-3693-2440-0.ch002,AI-Powered Language Translation for Multilingual Classrooms,"<jats:p>The revolutionary effects of AI-powered language translation technologies on multilingual classrooms in the modern educational environment are explored in this chapter proposal. It starts with a historical investigation and follows the development of AI translation, highlighting innovations in neural networks and machine learning models that improve efficiency and accuracy. After that, the chapter focuses on deploying AI translation tools in educational contexts. To support this study, real-world case studies are used to examine platforms and apps that are already in use thoroughly. The use of AI-powered translation to improve accessibility for non-native speakers and foster an equal learning environment for students with different linguistic origins is critically discussed. The chapter also looks at how AI may help teachers and students from different cultural backgrounds communicate with one another, which can promote an inclusive learning environment.</jats:p>"
10.1007/s00146-022-01617-6,Connecting ethics and epistemology of AI,"<jats:title>Abstract</jats:title><jats:p>The need for fair and just AI is often related to the possibility of understanding AI itself, in other words, of turning an opaque box into a glass box, as inspectable as possible. Transparency and explainability, however, pertain to the technical domain and to philosophy of science, thus leaving the ethics and epistemology of AI largely disconnected. To remedy this, we propose an integrated approach premised on the idea that a glass-box epistemology should explicitly consider how to incorporate values and other normative considerations, such as intersectoral vulnerabilities, at critical stages of the whole process from design and implementation to use and assessment. To connect ethics and epistemology of AI, we perform a double shift of focus. First, we move from trusting the output of an AI system to trusting the process that leads to the outcome. Second, we move from expert assessment to more inclusive assessment strategies, aiming to facilitate expert and non-expert assessment. Together, these two moves yield a framework usable for experts and non-experts when they inquire into relevant epistemological and ethical aspects of AI systems. We dub our framework ‘epistemology-cum-ethics’ to signal the equal importance of both aspects. We develop it from the vantage point of the designers: how to create the conditions to internalize values into the whole process of design, implementation, use, and assessment of an AI system, in which values (epistemic and non-epistemic) are explicitly considered at each stage and inspectable by every salient actor involved at any moment.</jats:p>"
10.1355/9789815203684-002,From Paper to Practice: Utilizing the ASEAN Guide on Artificial Intelligence (AI) Governance and Ethics,N/A
10.21428/e4baedd9.bff4d0f2,"Artificial Eloquence: Style, Citation, and the Right to One’s Own Voice in the Age of AI, or, A Drama in Three Acts",N/A
10.4018/979-8-3693-1565-1.ch001,AI and Equity in Higher Education,"<jats:p>This thought-provoking chapter is systematic research based on theories from literature review and empirical data exploring AI's potential impact on educational equity in higher learning. The atuhors examine how AI-powered systems might inadvertently perpetuate biases, affecting marginalized students disproportionately. This chapter discusses institutions' responsibility to implement AI in ways that support inclusivity and diversity. In addition, it highlights initiatives that prioritize fairness and transparency in AI algorithms. AI systems can perpetuate existing biases if not designed with the right in mind. Additionally, if an AI system is not transparent about its decision-making processes, identifying or rectifying potential biases will be nearly impossible. Readers will gain a deeper understanding of AI's challenges and opportunities in reshaping education, focusing on ensuring that no student is left behind.</jats:p>"
10.1088/1674-4926/45/4/040204,Towards efficient generative AI and beyond-AI computing: New trends on ISSCC 2024 machine learning accelerators,N/A
10.1016/j.egyai.2022.100216,Conditional Generative Adversarial Networks for modelling fuel sprays,N/A
10.4018/979-8-3693-5578-7.ch012,The Improvements in Significance of AI in Experiential and Medical Tourism,"<jats:p>By enhancing specific experiences, simplifying healthcare services, and guaranteeing patient and travel safety, AI integration has completely changed these industries. Artificial intelligence (AI)-powered virtual assistants and recommendation systems in experiential tourism respond to individual preferences by providing personalized recommendations and seamless support during the trip. In the meantime, artificial intelligence (AI) in medical tourism makes it easier to obtain high-quality remote healthcare services by enabling precise diagnosis, customized treatment programs, and virtual healthcare platforms. AI also helps with risk assessment and preventative safety measures, which improves patients' and tourists' overall pleasure and peace of mind. This chapter promises more developments and innovation in the future by highlighting the critical role AI is playing in changing the face of medical and experience tourism.</jats:p>"
10.1007/s43681-021-00081-0,Bias and comparison framework for abusive language datasets,"<jats:title>Abstract</jats:title><jats:p>Recently, numerous datasets have been produced as research activities in the field of automatic detection of abusive language or hate speech have increased. A problem with this diversity is that they often differ, among other things, in context, platform, sampling process, collection strategy, and labeling schema. There have been surveys on these datasets, but they compare the datasets only superficially. Therefore, we developed a bias and comparison framework for abusive language datasets for their in-depth analysis and to provide a comparison of five English and six Arabic datasets. We make this framework available to researchers and data scientists who work with such datasets to be aware of the properties of the datasets and consider them in their work.</jats:p>"
10.1007/s43681-021-00064-1,Workplace automation without achievement gaps: a reply to Danaher and Nyholm,"<jats:title>Abstract</jats:title><jats:p>In a recent article in this journal, John Danaher and Sven Nyholm raise well-founded concerns that the advances in AI-based automation will threaten the values of meaningful work. In particular, they present a strong case for thinking that automation will undermine our achievements, thereby rendering our work less meaningful. It is also claimed that the threat to achievements in the workplace will open up ‘achievement gaps’—the flipside of the ‘responsibility gaps’ now commonly discussed in technology ethics. This claim, however, is far less worrisome than the general concerns for widespread automation, namely because it rests on several conceptual ambiguities. With this paper, I argue that although the threat to achievements in the workplace is problematic and calls for policy responses of the sort Danaher and Nyholm outline, when framed in terms of responsibility, there are no ‘achievement gaps’.</jats:p>"
10.1007/s43681-020-00005-4,Reaching consensus with human beings through blockchain as an ethical rule of strong artificial intelligence,N/A
10.4018/979-8-3693-1565-1.ch001,AI and Equity in Higher Education,"<jats:p>This thought-provoking chapter is systematic research based on theories from literature review and empirical data exploring AI's potential impact on educational equity in higher learning. The atuhors examine how AI-powered systems might inadvertently perpetuate biases, affecting marginalized students disproportionately. This chapter discusses institutions' responsibility to implement AI in ways that support inclusivity and diversity. In addition, it highlights initiatives that prioritize fairness and transparency in AI algorithms. AI systems can perpetuate existing biases if not designed with the right in mind. Additionally, if an AI system is not transparent about its decision-making processes, identifying or rectifying potential biases will be nearly impossible. Readers will gain a deeper understanding of AI's challenges and opportunities in reshaping education, focusing on ensuring that no student is left behind.</jats:p>"
10.1088/1674-4926/45/4/040204,Towards efficient generative AI and beyond-AI computing: New trends on ISSCC 2024 machine learning accelerators,N/A
10.1016/j.egyai.2022.100216,Conditional Generative Adversarial Networks for modelling fuel sprays,N/A
10.4018/979-8-3693-5578-7.ch012,The Improvements in Significance of AI in Experiential and Medical Tourism,"<jats:p>By enhancing specific experiences, simplifying healthcare services, and guaranteeing patient and travel safety, AI integration has completely changed these industries. Artificial intelligence (AI)-powered virtual assistants and recommendation systems in experiential tourism respond to individual preferences by providing personalized recommendations and seamless support during the trip. In the meantime, artificial intelligence (AI) in medical tourism makes it easier to obtain high-quality remote healthcare services by enabling precise diagnosis, customized treatment programs, and virtual healthcare platforms. AI also helps with risk assessment and preventative safety measures, which improves patients' and tourists' overall pleasure and peace of mind. This chapter promises more developments and innovation in the future by highlighting the critical role AI is playing in changing the face of medical and experience tourism.</jats:p>"
10.1007/s43681-023-00276-7,The rapid competitive economy of machine learning development: a discussion on the social risks and benefits,"<jats:title>Abstract</jats:title><jats:p>Research in artificial intelligence (AI) has started in the twentieth century but it was not until 2012 that modern models of artificial neural networks aided the machine learning process considerably so that in the past ten years, both computer vision as well as natural language processing have become increasingly better. AI developments have accelerated rapidly, leaving open questions about the potential benefits and risks of these dynamics and how the latter might be managed. This paper discusses three major risks, all lying in the domain of AI safety engineering: the problem of AI alignment, the problem of AI abuse, and the problem of information control. The discussion goes through a short history of AI development, briefly touching on the benefits and risks, and eventually making the case that the risks might potentially be mitigated through strong collaborations and awareness concerning trustworthy AI. Implications for the (digital) humanities are discussed.
</jats:p>"
10.21275/sr24529122811,The Ethics of Understanding: Exploring Moral Implications of Explainable AI,N/A
10.2139/ssrn.4689770,Proposing Central Asian AI Ethics Principles: A Multilevel Approach for Responsible AI,N/A
10.12681/jpentai.33299,A Public Sphere for AI,"<jats:p>The present article addresses key elements of the unique ontology of AI and argues that these require the expansion of the public sphere, in order to successfully manage the entry of new intelligent actors in legally regulated relationships which are based on the identification of causal connections. In this sense it attempts to link law and political science, given that the governance of any phenomenon or field includes law and in particular the detection, of legally interesting, causal relationships. Regulating such relationships effectively offers legal certainty, which in turn is a fundamental element of effective governance. In our self- evidently, human- centered world, whether we are talking about natural persons, or for legal persons, it is self- evident that there is, in the end, a human hand behind the causal relations with which law is involved. Once other, non- human, intelligent actors gradually enter the forefront, these causal relations become further complicated. It is on these complications and their impact that we focus.</jats:p>"
10.2307/jj.14491770.11,Incorporating Ethics into the AI Clinical Decision Support System Life Cycle,N/A
10.1016/j.egyai.2020.100019,Ethics in Engineering and the Role of Responsible Technology,N/A
10.1007/s00146-023-01630-3,Beta-testing the ethics plugin,N/A
10.1007/978-3-030-54173-6_15,Friendship Between Human Beings and AI Robots?,"<jats:title>Abstract</jats:title><jats:p>In this chapter the case for potential Robophilia is based upon the positive properties and powers deriving from humans and AI co-working together in synergy. Hence, Archer asks ‘Can Human Beings and AI Robots be Friends?’ The need to foreground social change for structure culture and agency is being stressed. Human enhancement speeded up with medical advances with artificial insertions in the body, transplants, and genetic modification. In consequence, the definition of ‘being human’ is carried further away from naturalism and human essentialism. With the growing capacities of AI robots the tables are turned and implicitly pose the question, ‘so are they not persons too?’ Robophobia dominates Robophilia, in popular imagination and academia. With AI capacities now including ‘error-detection’, ‘self-elaboration of their pre-programming’ and ‘adaptation to their environment’, they have the potential for <jats:italic>active collaboration</jats:italic> with humankind, in research, therapy and care. This would entail <jats:italic>synergy or co-working</jats:italic> between humans and AI beings.</jats:p>"
10.1007/978-3-030-54173-6_7,AI/Robotics and the Poor,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence and robotics (AI/R) have the potential to greatly change livelihoods. Information on how AI/R may affect the poor is scarce. This chapter aims to address this gap in research. A framework is established that depicts poverty and marginality conditions of health, education, public services, work, small businesses, including farming, as well as the voice and empowerment of the poor. This framework identifies points of entry of AI/R, and is complemented by a more detailed discussion of the way in which changes through AI/R in these areas may relate positively or negatively to the livelihood of the poor. Context will play an important role determining the AI/R consequences for the diverse populations in poverty and marginalized populations at risk. This chapter calls for empirical scenarios and modelling analyses to better understand the different components in the emerging technological and institutional AI/R innovations and to identify how they will shape the livelihoods of poor households and communities.</jats:p>"
10.47289/aiej20210531,"AI &amp; Democracy, and The Importance of Asking the Right Questions","<jats:p>Democracy is widely praised as a great achievement of humanity. However, in recent years there has been an increasing amount of concern that its functioning across the world may be eroding. In response, efforts to combat such change are emerging. Considering the pervasiveness of technology and its increasing capabilities, it is no surprise that there has been much focus on the use of artificial intelligence (AI) to this end. Questions as to how AI can be best utilized to extend the reach of democracy to currently non-democratic countries, how the involvement in the democratic process of certain demographic groups (e.g. ethnic minorities, women, and young people) can be increased, etc. are frequent topics of discussion. In this article, I would like not merely to question whether this is desirable but rather argue that we should be trying to envisage ways of using AI for the exact opposite purpose: that of replacing democratic systems with better alternatives.</jats:p>"
10.2139/ssrn.4595723,Confucian ‘Trustworthy AI’: Diversifying a Keyword in the Ethics of AI and Governance,N/A
10.2139/ssrn.4207128,What Firms Must Know Before Adopting AI: The Ethics of AI Transparency,N/A
10.1089/aipo.2024.0026,Practical Ways to Use Generative Artificial Intelligence Tools in Oncology,N/A
10.4018/979-8-3693-3731-8.ch004,Generative AI in Healthcare,"<jats:p>In recent years, the rapid development of AI technology Generative AI, has restructured the healthcare industry. Generative AI is a collection of algorithms that uses a large volume of medical data to generate new data in various formats, including medical images, data augmentation, and medicine development. A variety of techniques are employed in Generative AI in the healthcare industry, which includes Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), AutoRegressive Models, Flow-Based Models, and Probabilistic Graphical Models. Generative AI can applied in various domains in the healthcare sector including drug discovery, medical imaging enhancement, data augmentation, anomaly detection, simulation and training, and predictive modelling. The integration of Generative AI faces some challenges, such as addressing ethical and legal issues related to the use of Artificial Intelligence (AI) in healthcare and synthetic data in clinical decision-making, and ensuring the reliability and interpretability of AI-generated outputs.</jats:p>"
10.1007/s00146-023-01657-6,Editorial: Beyond regulatory ethics,N/A
10.1007/s10551-024-05753-5,Scoring the Ethics of AI Robo-Advice: Why We Need Gateways and Ratings,"<jats:title>Abstract</jats:title><jats:p>Unlike the many services already transformed by artificial intelligence (<jats:italic>AI</jats:italic>), the financial advice sector remains committed to a human interface. That is surprising as an AI-powered financial advisor (a <jats:italic>robo-advisor</jats:italic>) can offer personalised financial advice at much lower cost than traditional human advice. This is particularly important for those who need but cannot afford or access traditional financial advice. Robo-advice is easily accessible, available on-demand, and pools all relevant information in finding and implementing an optimal financial plan. In a perfectly competitive market for financial advice, robo-advice should prevail. Unfortunately, this market is imperfect with asymmetric information causing generalised advice aversion with a disproportionate lack of trust in robo-advice. Initial distrust makes advice clients reluctant to use, or switch to, robo-advice. This paper investigates the ethical concerns specific to robo-advice underpinning this lack of trust. We propose a regulatory framework addressing these concerns to ensure robo-advice can be an ethical resource for good, resolving the increasing complexity of financial decision-making. Fit for purpose regulation augments initial trust in robo-advice and supports advice clients in discriminating between high-trust and low-trust robo-advisors. Aspiring robo-advisors need to clear four licensing gateways to qualify for an AI Robo-Advice License (AIRAL). Licensed robo-advisors should then be monitored for ethical compliance. Using a balanced score card for ethical performance generates an ethics rating. This <jats:italic>gateways-and-ratings</jats:italic> methodology builds trust in the robo-advisory market through improved transparency, reduced information asymmetry, and lower risk of adverse selection.</jats:p>"
10.1093/oso/9780198883098.003.0006,Soft Ethics and the Governance of AI,"<jats:title>Abstract</jats:title>
               <jats:p>Previously, in Chapters 4 and 5, a unified framework for the ethical principles of AI was suggested and some main ethical risks that arise when translating principles into practices were identified. This chapter discusses the governance of AI, and more generally of digital technologies, as the new challenge posed by technological innovation. A new distinction between soft and hard ethics is introduced. Hard ethics first precede and then further contribute to shaping legislation. In contrast, soft ethics apply after legal compliance with legislation (that is, post-compliance ethics), such as the GDPR in the EU. The chapter concludes by developing an analysis of the role of digital ethics with respect to digital regulation and digital governance, thus preparing for the next chapter on the fundamental ethical principles for an ethics of AI.</jats:p>"
10.1109/ethics57328.2023.10154970,"ETHICS-2023 Session E4 - Tutorial: AI Safety, governance, and alignment tutorial",N/A
10.1007/s10676-020-09570-y,Critically engaging the ethics of AI for a global audience,N/A
10.55041/ijsrem25320,Text-to-Image Generation using Generative AI,"<jats:p>Abstract—This survey reviews text-to-image generation by using different approaches. One of the approaches identified in this study is Cross-modal Semantic Matching Generative Adversarial Networks (CSM-GAN), which is used to increase semantic consistency between text  descriptions and synthesised pictures for fine-grained text- to-image creation. This includes other two modules, Text  Encoder Module and Textual-Visual Semantic Matching  Module. We further discussed about Imagen which is a text- to-image diffusion model with photorealism and deep  language understanding, which is used on the COCO dataset. Lastly, we discussed about Text to image synthesis used to automates image generation using conditional generative models and GAN, enhancing artificial intelligence and deep learning. Based on these approaches we present a review of text to image generation using generative AI.  Keywords— Generative AI, Diffusion model, Text-to- image, Imagen, CSM-GAN</jats:p>"
10.4324/9781003507949-5,Conclusion,N/A
10.2139/ssrn.4632928,Toward Ethical Use of Generative AI in AP Courses,N/A
10.2139/ssrn.4358789,"AI as Agency Without Intelligence: On ChatGPT, Large Language Models, and Other Generative Models",N/A
10.47852/bonviewjdsis42022964,The Value of Generative AI  for Qualitative Research:  A Pilot Study,"<jats:p>This mixed methods approach study investigates the potential of introducing Generative AI (ChatGPT 4 and Bard) as part of a deductive qualitative research design that requires coding, focusing on possible gains in cost-effectiveness, coding throughput time and inter-coder reliability (Cohen’s Kappa). This study involved semi-structured interviews with five domain experts and analyzed a dataset of 122 respondents that required categorization into six pre-defined categories. The results from using Generative AI coders were compared with those from a previous study where human coders carried out the same task. In this comparison, we evaluated the performance of AI-based coders against two groups of human coders, comprising three experts and three non-experts. Our findings support the replacement of human coders with Generative AI ones, specifically ChatGPT for deductive qualitative research methods of limited scope. The experimental group, consisting of three independent Generative AI coders, outperformed both control groups in coding effort, with a fourfold (4x) efficiency and throughput time (15x) advantage. The latter could be explained by leveraging parallel processing. Concerning expert vs. non-expert coders, minimal evidence suggests a preference for experts. Although experts code slightly faster (17%), their inter-coder reliability showed no substantial advantage. A hybrid approach, combing ChatGPT and domain experts shows the most promise. This approach reduces costs, shortens project timelines, and enhances inter-coder reliability, as indicated by higher Cohen's Kappa values. In conclusion, Generative AI, exemplified by ChatGPT, offers a viable alternative to human coders, in combination with human research involvement, delivering cost savings and faster research completion without sacrificing notably reliability. These insights, while limited in scope, show potential for further studies with lager datasets, more inductive qualitative research designs and other research domains.</jats:p>"
10.21203/rs.3.rs-4826541/v1,An investigation of using Spark generative AI in solving physics concept inventories in English and Chinese: Performance and issues,"<title>Abstract</title>
        <p>Generative artificial intelligence (GenAI) has garnered considerable attention across various disciplines, including physics education. Numerous studies have explored the potential of using these tools in physics education by assessing their understanding of physics concepts. However, ChatGPT is the only model whose performance and integration into physics education have been extensively studied. Furthermore, previous research has primarily focused on English as the input language, leaving a gap in our understanding of other models and languages. This study aims to address this gap by examining the performance of Spark, another GenAI developed in China, in solving physics concept inventories. Four conditions were investigated: English input without explanation, English input with explanation, Chinese input without explanation, and Chinese input with explanation. The results showed that Spark's performance with English input was comparable to ChatGPT3.5 for the Force Concept Inventory but significantly lagged behind ChatGPT4. Notably, Chinese input with explanation significantly outperformed the other three conditions. This study also discussed concerns and issues related to Spark's physics conceptual understanding and language inequality. Finally, guidelines for incorporating GenAI into physics education were proposed.</p>"
10.22329/il.v44i1.8258,Generative AI and Argument Creativity,"<jats:p>Generative AI appears to threaten argument creativity. Because of its capacity to generate coherent texts, individuals are likely to integrate its ideas, and not their own, into arguments, thereby reducing their creative contribution. This article argues that this view is mistaken—it rests on a misunderstanding of the nature of creativity. Within arguments, creative and critical thinking cannot be separated. Because creativity is enmeshed with skills such as analysis and evaluation, the use of generative AI in the construction of arguments, especially in the role as universal audience, has the potential to heighten, not diminish argument creativity.</jats:p>"
10.2514/6.2024-1054,Transforming System Modeling with Declarative Methods and Generative AI,N/A
10.2139/ssrn.4494272,Gracenote.ai: Legal Generative AI for Regulatory Compliance,N/A
10.5194/egusphere-egu24-10627,Enhancing Geoscience Analysis: AI-Driven Imputation of Missing Data in Well Logging Using Generative Models,"<jats:p>The integrity of well logging data is paramount in geophysical explorations for accurate subsurface analysis, notably in the North Sea Dutch region known for its extensive hydrocarbon exploration. Addressing the common challenge of missing data in well logs, our study introduces an AI-driven methodology employing generative models. These models utilize machine learning to analyze existing data patterns and generate realistic imputations for missing values. The approach has shown to not only enhance the quality of geological interpretations but also to streamline the workflow in hydrocarbon exploration. This integration of AI signifies a substantial move towards more precise and efficient geoscience data analysis. A qualitative comparison using Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) was conducted to evaluate the results. The PCA comparison demonstrates the synthetic data&amp;#8217;s alignment with real data in principal component space, effectively capturing the variance. The t-SNE analysis further validates the model's fidelity, with the synthetic data exhibiting clustering behaviors analogous to real data. Together, these results showcase the transformative potential of machine learning in geosciences, providing a robust framework for enhancing data reliability in geophysical studies.</jats:p>"
10.2139/ssrn.4904876,On the Antitrust Implications of Embedding Generative AI in Core Platform Services,N/A
10.1201/9781032654829-9,Stochastic Parroting: Leaders Versus the Artificial Intelligence Echo Chamber,N/A
10.1201/9781003312338-20,"Ars Autopoetica: On Authorial Intelligence, Generative Literature, and the Future of Language",N/A
10.1109/iscas58744.2024.10558391,Exploration of Generative AI tools for an Electric Circuits Course,N/A
10.2118/216267-ms,Generative AI Enabled Conversational Chatbot for Drilling and Production Analytics,"<jats:title>Abstract</jats:title>
               <jats:p>Getting intelligent insight from large amount of dataset is critical for Energy companies to optimize their operations across various business segments such as drilling, production and completion etc. The paper proposes end-to-end workflow to 1) extract data form rig and production reports and store dataset into databases 2) build a conversational generative AI enabled chatbot which is trained to answer questions related to drilling and production monitoring, queries dataset, frequently performed diagnostic analysis and can generate recommendations to improve operations. The chatbot is integrated with large language models (LLM) and machine learning models (ML) on the cloud and based on questions asked by user it provides answers in conversational settings.</jats:p>
               <jats:p>Chatbot is hosted in cloud and is integrated with various databases, document repositories and several machine learning model. The machine learning models are built to enable chatbot's capability to answer questions related to drilling and production analytics. Chatbot is integrated with user interface where user can type or ask questions. Using natural language process (NLP) and artificial intelligence (Al), chatbot understands intent of question and if needed asks relevant follow-up questions to provide the answer. Chatbot can also perform statistical analysis, generate SQL queries on datasets and can use those statistics to answer questions. Further if enabled, chatbot can also search information from drilling and production reports and scientific articles. Three case studies are presented. In case study#1, chatbot was integrated with operator's historical PDF drilling reports (Volve dataset), which traditionally are not easy to extract and analyze at scale. Several thousand drilling reports were extracted and stored in database. Various capabilities were added to chatbot such has Cross-documents insights and trend, for example, well progression, operation history, can be generated and displayed on user interface and further analysis can be performed in conversational manner. The dataset created was used to perform comparative analysis identifying wells having significant higher non production time (NPT) due to repair or fishing events. In this manner, chatbot can compare one well's operational statistics with other well and generate various visuals which helps identifying possible ways to improve drilling operations. Similarly, chatbot was also trained to provide answers for production diagnostics such as comparing well's relative performances and root cause identification for poor performing wells. When analyzed on test dataset chatbot was able to identify 20% uplift in production for wells supported on plunger lift. Finally, chatbot was enabled to support NLP based searches. Engineers can ask specific questions such as ""provide operational log for well F4 when fishing happened and sort the result by reporting date in ascending order. Show me both SQL query and the resulted table"" and chatbot will generate SQL query and resulted table. The work demonstrates that generative AI has great potential to transform the Energy industry.</jats:p>"
10.4018/979-8-3693-1565-1,Exploring the Ethical Implications of Generative AI,N/A
10.1109/ms.2023.3346069,Generative AI Is Changing How and What We Learn,N/A
10.15530/urtec-2024-4044635,Innovating Oil and Gas Forecasting: Developing a Trailblazing Generative AI Model,N/A
10.15530/urtec-2024-4043583,Transforming Early-Stage Oil and Gas Production Forecasting with Generative AI,N/A
10.2139/ssrn.4645865,Economic Footprints of Tax Audits: A Generative AI-Driven Approach,N/A
10.1109/mc.2024.3401085,Certifying Generative AI: Retrieval-Augmented Generation Chatbots in High-Stakes Environments,N/A
10.1016/j.ijis.2024.04.004,The Metaverse: innovations and generative AI,N/A
10.1787/657a185a-en,Generative AI for anti-corruption and integrity in government,N/A
10.32388/c2gsi2,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.2172/2345015,Using the power of secure Generative AI to eliminate data silos,N/A
10.4018/979-8-3693-5578-7,Generative AI for Transformational Management,N/A
10.47941/ijf.2210,The Advent of Generative AI and Financial Industry,"<jats:p>Purpose: This paper explores the recent literature on Generative AI applications in the financial industry and delineates its role in the future.
Methodology: Our paper follows secondary research analyzing current literature on Generative AI in finance. It is one of the essential tools for understanding background information, identifying research problems, and filling the literature gaps. This paper studies how Generative AI has potential financial benefits and risks, providing unique insights into the financial landscape in the coming years.
Findings: The findings unveil that Generative AI can become a strategic tool to redefine financial services and operational effectiveness. It can substantially improve the services by reducing costs, bringing efficiency, and enhancing corporate performance. It has the enormous transformative power to revolutionize client product and service offerings, improving risk management assessments and bringing efficiency to operations. However, our study indicates that the financial service industry can get into practices and decisions that are potentially unethical and financial exclusion due to an embedded bias in its algorithm and design of Generative AI technologies. Since Generative AI continues to evolve, its role and effectiveness in decision-making are expected to shape the financial services landscape significantly.
Unique Contribution to Theory, Practice, and Policy: Generative AI can be a game changer for the financial industry, fueling digital transformation across industries. The transformative potential of generative AI can optimize operations, revolutionize customer experiences, and drive innovation seamlessly in finance. Our paper suggests how policymakers can foresee the challenges ahead due to the Generative AI in finance services, which is challenging the existing regulatory landscape. To stay ahead in the competition, financial firms must balance data privacy and algorithmic bias and ensure the responsible use of AI.</jats:p>"
10.1148/radiol.240935,Seeing the Unseen: Advancing Generative AI Research in Radiology,N/A
10.1109/ms.2023.3306641,Can Architecture Knowledge Guide Software Development With Generative AI?,N/A
10.1016/j.eng.2024.07.008,Generative AI for Materials Discovery: Design Without Understanding,N/A
10.1002/9781394308286.ref,References,N/A
10.1097/ncm.0000000000000681,AI and Case Management: From Artificial Intelligence to Generative Intelligence,<jats:p>Conversations about artificial intelligence (AI) are impossible to escape since the inception of ChatGPT in November 2022—whether it is about the end of our jobs or the end of the world! This Editorial talks about AI and its subsets and how this may relate to health care.</jats:p>
10.24132/csrn.3401.1,A Synergy of Computer Graphics and Generative AI: Advancements and Challenges,"<jats:p>A traditional computer graphics domain has received an unprecedented boost from the newest developments in generative Artificial Intelligence (GenAI). It affects all areas: from image generation, to face recognition, to object detection, to aerial surveillance, to autonomous car vision systems. The newest deep learning architectures make it possible to generate new images from texts, to apply styles to portraits, to de-identify facial images, and to recognize human and objects in videos. This keynote will delve into some of the most exciting applications in medical AI diagnostics, human face recognition and aesthetics domains, while making a strong case for resulting image authenticity, bias mitigation, and trust.</jats:p>"
10.34074/proc.240105,Assessment Validity in the Era of Generative AI Tools,"<jats:p>Generative AI tools, a recent disruptive educational technology, are expected to change how education is delivered and administered. This study proposes a risk identification framework to support educators in identifying assessment integrity risks caused by generative AI tools. The framework also suggests possible actions to mitigate these risks. The proposed framework uses four factors (Assessment Type, AI Knowledge, Course Level, and Bloom’s Taxonomy Cognitive Domain Level) to identify the risks associated with an assessment resulting from the usage of generative AI tools. It is critical to have such a framework to ensure the integrity of assessments while the education industry adapts to the generative AI tools era.</jats:p>"
10.21203/rs.3.rs-4587150/v1,Generative Language Reconstruction from Brain Recordings,"<title>Abstract</title>
        <p>Language reconstruction from non-invasive brain recordings has been a long-standing challenge. Existing research has addressed this challenge with a classification setup, where a set of language candidates are pre-constructed and then matched with the representation decoded from brain recordings. Here, we propose a new method that addresses language reconstruction through auto-regressive generation, which directly uses the representation decoded from functional magnetic resonance imaging (fMRI) as the input for a large language model (LLM), removing the reliance on the accuracy of pre-constructed candidates. While an LLM can already generate high-quality content, our approach produces results more closely aligned with the visual or auditory language stimuli in response to which brain recordings are sampled, especially for content deemed “surprising” for the LLM. Furthermore, we show that the proposed approach can be used in an auto-regressive manner to reconstruct a 10-minute-long stimulus, which outperforms previous methods with a classification setup. Our findings demonstrate the effectiveness of employing brain language interfaces in a generative setup and delineate a promising path to investigating language formation in the human brain.</p>"
10.31219/osf.io/mfvu5,The Potential for Political Backlash Against Generative AI,"<p>In developing regulatory frameworks to address generative AI’s transformative potential, governments will be guided, in part, by how the public views these technologies. How do people reason about the economic effects of generative AI? We answer this question by examining people’s causal theories of generative AI’s effects on firms, consumers, and workers, using a survey of 6,056 Americans and Canadians. Through latent class analysis, we show that a significant portion of the public views AI as a threat—harming consumers and replacing rather than complementing workers’ skills. We further show that these views are consequential for policy preferences, predicting support for policies that would halt immediate job loss over those that help workers adapt. Finally, we show that voters are already polarized on the basis of their views of technology, particularly in the United States; Americans who see technology as harming consumers and substituting workers’ skills are significantly more likely to vote Republican, while voters who see technology as complementing workers’ skills and producing consumer benefits are significantly more likely to vote Democrat. We conclude that fissures in the public’s attitudes toward AI are emerging and are easily channeled toward politics.</p>"
10.4324/9781003514237-6,Leveraging Generative AI for Content Analysis,N/A
10.4324/9781003367451-4,"What is the problem to which AI chatbots are the solution? AI ethics through Don Ihde's embodiment, hermeneutic, alterity, and background relationships",N/A
10.18260/1-2--45552,Preparing for ChatGPT: Comparing Student Attitudes on Generative AI in Contrasting Class Instruction,N/A
10.12781/978-1-907549-44-1-6,Reflections on Our Generative Journalism Journey,N/A
10.2139/ssrn.4455916,A Rumsfeldian Framework for Understanding How to Employ Generative AI Models for Financial Analysis,N/A
10.2139/ssrn.4621982,Generative AI and User-Generated Content: Evidence from Online Reviews,N/A
10.1007/978-1-4842-3931-5_8,Generative Models as Fashion Designers,N/A
10.2139/ssrn.4867815,Advances in Generative AI and Platform Moderation: Implications for Online Knowledge Sharing,N/A
10.69931/mviv5530,Generative AI Towards Mega-Crisis Mitigation: A Case for Transforming Crisis Communication Education Through Global Alliances,N/A
10.30890/2567-5273.2023-29-01-052,GENERATIVE AI AND PROMPT ENGINEERING IN EDUCATION,"<jats:p>The development of generative AIs and the variability of their use are still at the level of research and active development simultaneously. However, it has already become clear that the emergence of generative AI significantly impacts many industries, in</jats:p>"
10.2139/ssrn.4770620,Unveiling the Pros and Cons of Generative Ai Services: A Mixed-Methods Approach,N/A
10.3389/978-2-8325-3507-3,Generative AI for brain imaging and brain network construction,N/A
10.21203/rs.3.rs-4517638/v1,Generative AI-based Style Recommendation Using Fashion ItemDetection and Classification ,"<title>Abstract</title>
        <p>This research work focuses on the creation of a cutting-edge style recommendation system that uses generative AI and deep learning approaches to analyse fashion photos. The system is intended to process input images, such as selfies or studio-quality photos, and output a text file with extensive feedback on the individual's style and suggestions for improvement. The system consists of two main components: the YOLOv8 convolutional neural network, which detects and crops clothing items, and the GPT-4.0 large language model, which generates informative style commentary and recommendations. YOLOv8 is briefly trained on a specific dataset to improve its performance in recognising 10 different types of clothes, while GPT-4.0, which is accessible via the OpenAI API, is charged with giving cohesive and short style suggestions. To evaluate the success of the suggested solution, real experimental trials were conducted at many events in Madrid and Tallinn. Three well-known AI models were used for comparison: OpenAI's GPT-4.0 Vision, Google's Gemini 1.5 Pro, and Anthropic's Claude 3  - Opus. Participants judged the quality of each model's fashion recommendations. The results showed that GPT-4.0 Vision and Gemini 1.5 Pro had comparable average ratings, indicating higher perceived quality than Claude 3 - Opus. This research work demonstrates how cutting-edge computer vision and natural language processing technology may transform personalised fashion advising services, improving accuracy and relevance of style recommendations.</p>"
10.1007/s00266-023-03805-1,Dr. GAI: Significance of Generative AI in Plastic Surgery,N/A
10.1016/j.compcom.2024.102829,Composing with generative AI on digital advertising platforms,N/A
10.2139/ssrn.4597674,Contribution of ChatGPT and Other Generative Artificial Intelligence (AI) in Renewable and Sustainable Energy,N/A
10.54254/2755-2721/36/20230455,Exploring techniques and overcoming hurdles in generative AI,"<jats:p>The realm of artificial intelligence has witnessed significant advancements, with generative models standing at the forefront of this progress. Generative Artificial Intelligence concerns the development of algorithms and models equipped to generate novel content - be it images, text, or music. This paper delves into the primary techniques underpinning generative AI, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and autoregressive models like Transformers. These methodologies have enabled a myriad of applications, from synthesizing images to facilitating data augmentation and style transfer. While the results from generative models have been profoundly impressive, they are not devoid of challenges. The surge in their capabilities has brought forth issues related to ethics, inherent biases, scalability, and the quest for more stable training methods. This paper aims to provide an insightful exploration of the pivotal methods defining generative AI while shedding light on the prevailing challenges and ethical implications intertwined with its growth.</jats:p>"
10.3030/101159214,N/A,N/A
10.26434/chemrxiv-2023-xt65x,REINVENT4: Modern AI–Driven Generative Molecule Design,"<jats:p>REINVENT4 is a modern open–source generative AI framework for the design of small molecules. The software utilizes recurrent neural networks and transformer architectures to drive molecule generation. These generators are seamlessly embedded within the general machine learning optimization algorithms transfer learning, reinforcement learning and curriculum learning. REINVENT4 enables and facilitates de novo design, R-group replacement, library design, linker design, scaffold hopping and molecule optimization. This contribution gives an overview of the software and describes its design. Algorithms and their applications are discussed in detail. REINVENT4 is a command line tool which reads a user configuration in either TOML or JSON format. The aim of this release is to provide reference implementations for some of the most common algorithms in AI based molecule generation. An additional goal with the release is to create a framework for education and future innovation in AI based molecular design. The software is available from https://github.com/ MolecularAI/REINVENT4 and released under the permissive Apache 2.0 license.</jats:p>"
10.2139/ssrn.4759930,How Generative AI Can Augment Human Creativity,N/A
10.2139/ssrn.4650157,Institutions to constrain chaotic robots: why generative AI needs blockchain,N/A
10.1109/mprv.2023.3310748,Generative AI and the Call for Brevity,N/A
10.2196/preprints.53008,Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations.</p>
                </sec>"
10.2139/ssrn.4872845,Plastic Deformation and Damage Modeling of AA7075 Synthetic 3D Microstructure Created Using Generative AI,N/A
10.1162/qss_a_00285/v2/review1,"Review for ""Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents""",N/A
10.20944/preprints202403.0919.v1,"Unlocking the Future of Drug Development: Generative AI, Digital Twins, and Beyond","<jats:p>This article delves into the intersection of generative AI and digital twins within drug discovery, exploring their synergistic potential to revolutionize pharmaceutical research and development. Through various instances and examples, we illuminate how generative AI algorithms, capable of simulating vast chemical spaces and predicting molecular properties, are increasingly integrated with digital twins of biological systems to expedite drug discovery. By harnessing the power of computational models and machine learning, researchers can design novel compounds tailored to specific targets, optimize drug candidates, and simulate their behavior within virtual biological environments. This paradigm shift offers unprecedented opportunities for accelerating drug development, reducing costs, and, ultimately, improving patient outcomes. As we navigate this rapidly evolving landscape, collaboration between interdisciplinary teams and continued innovation will be paramount in realizing the promise of generative AI and digital twins in advancing drug discovery.</jats:p>"
10.20944/preprints202306.0323.v1,Utilizing Text Mining for Labeling Training Models from Futures Corpus in Generative AI,"<jats:p>For highly time-constrained very short-term investors, reading and extracting valuable information from financial news poses significant challenges. The wide range of topics covered in these news articles further compounds the difficulties for investors. The diverse content adds complexity and uncertainty to the text, making it arduous for very short-term investors to swiftly and accurately extract valuable insights. Variations in authors, media sources, and cultural backgrounds also introduce additional complexities. Hence, performing a bull-bear semantic analysis of financial news using text mining technologies can alleviate the volume, time, and energy pressures on very short-term investors while enhancing the efficiency and accuracy of their investment decisions. This study proposes labeling bull-bear words from a futures corpus detection method that extracts valuable information from financial news, allowing investors to understand market trends quickly. Generative AI models are trained to provide real-time bull-bear advice, aiding investors in adapting to market changes and devising effective trading strategies. Experimental results show the effectiveness of various models, with Random Forest and SVMs achieving an impressive 80% accuracy rate. MLP and Deep learning models also perform well. By leveraging these models, the study reduces the time spent reading financial articles, enabling faster decision-making and increasing the likelihood of investment success. Future research can explore the application of this method in other domains and enhance model design for improved predictive capabilities and practicality.</jats:p>"
10.2139/ssrn.4624621,Comments on “Guide on the use of Generative AI”,N/A
10.2139/ssrn.4600536,The Market Value of Generative AI: Evidence from China Market,N/A
10.35542/osf.io/k95js,Advancing Mobile App Development and Generative AI Education through MIT App Inventor,"<p>This study assesses the MIT App Inventor's effectiveness in instilling Computational Thinking and Generative AI skills through a five-day workshop focused on mobile app development. With its block-based coding system, App Inventor proved to be user-friendly, significantly enhancing programming accessibility for beginners. Participants experienced a marked increase in confidence and expressed enthusiasm for applying these new skills in real-life contexts, exemplifying the principles of Computational Action. The results underscore the potential of App Inventor as a valuable educational resource, fostering technical aptitude and innovation among an international array of students.</p>"
10.22214/ijraset.2024.63324,Renaissance on Generative AI,"<jats:p>Abstract: Generative Artificial Intelligence (Generative AI) stands at the forefront of innovation, promising to revolutionize creative content generation across various domains. This paper delves into the multifaceted implications of Generative AI in reshaping the landscape of artistic expression. Through an extensive literature survey and analysis, we explore the applications, advancements, and challenges of Generative AI in text generation, visual arts, and music composition. From state-of-the-art models like OpenAI's GPT series to cutting-edge techniques such as Generative Adversarial Networks (GANs) and Transformer architectures, Generative AI enables the automated creation of diverse and high-quality content. However, ethical considerations regarding authenticity, bias, and ownership of AI-generated content remain paramount. By uncovering key findings and insights, this paper aims to guide future research, development, and responsible integration of Generative AI in fostering a renaissance of artistic innovation and collaboration</jats:p>"
10.3102/2106149,Reconceptualizing Self-Directed Language Learning in the Era of Generative AI: An Exploratory Analysis,N/A
10.33767/osf.io/w5a2n,"Not ‘what’, but ‘where is creativity?’: towards a relational-materialist approach to generative AI","<p>The recent emergence of generative AI software as viable tools for use in the cultural and creative industries has sparked debates about the potential for “creativity” to be automated and “augmented” by algorithmic machines. Such discussions, however, begin from an ontological position, attempting to define creativity by either falling prey to universalism (i.e. “creativity is X”) or reductionism (i.e. “only humans can be truly creative” or “human creativity will be fully replaced by creative machines”). Furthermore, such an approach evades addressing the real and material impacts of AI on creative labour in these industries. This article thus offers more expansive methodological and conceptual approaches to the recent hype on generative AI. By combining Mihaly Csikszentmihalyi’s (2014) systems view of creativity, in which we emphasise the shift from “what” to “where” is creativity, with Leah Lievrouw’s (2014) relational-materialist theory of “mediation”, we argue that the study of “creativity” in the context of generative AI must be attentive to the interactions between technologies, practices, and social arrangements. When exploring the relational space between these elements, three core concepts become pertinent: creative labour, automation, and distributed agency. Critiquing “creativity” through these conceptual lenses allows us to re-situate the use of generative AI within discourses of labour in post-industrial capitalism and brings us to a conceptualisation of creativity that privileges neither human user nor machine algorithm but instead emphasises a relational and distributed form of agency.</p>"
10.61137/ijsret.vol.10.issue4.203,Effective Techniques for Generative AI Precision,N/A
10.2139/ssrn.4933379,Generative Ai Adoption in Service Industries: A Systematic Literature Review and Future Research Directions,N/A
10.18260/1-2--48235,Using Generative AI for a Graduate Level Capstone Course Design—a Case Study,N/A
10.1109/rew57809.2023.00050,MAPE-K Loop-Based Goal Model Generation Using Generative AI,N/A
10.33774/apsa-2023-f1fkq,Ethics and Responsible AI Deployment,"<jats:p>On the recent Bletchley summit, some British officials had hoped other countries would agree to establish a UK based AI task force, to test new models from around the world before they are released to the public. But instead, Raimondo used the summit to announce a separate American AI Safety Institute within the country's National Institute of Standards and Technology. This article explores the need for international and collaborative effort, led by the new American AI Safety Institute, on regualting AI systems to be more ethical , and to safeguard individual privacy while complying with existing privacy standards. The study concludes that these algorithms effectively enhance privacy protection while balancing the utility of AI with the need to protect personal data. This requires a comprehensive approach that combines technological innovation with ethical and regulatory strategies to harness the power of AI in a way that respects and protects individual privacy.</jats:p>"
10.47191/ijcsrr/v7-i7-90,Factors Influencing Intention to Adopt Generative AI Tools in Indonesian Enterprise Users,<jats:p>Indonesian enterprises are in the early stage of adopting generative AI tools. A report from Forbes shows that AI companies around the world have raised around $354 billion for Generative AI technology. One of the key drivers of those funding is to capitalize on the growing market demand. Market demand in Indonesia for Generative AI technology is explored by studying the intention to adopt such tools among enterprise users. Quantitative study reveals that perceived ease of use is a factor influencing the user’s intention to adopt Generative AI Tools in Indonesian enterprises. Recommendations for further research includes exploring more predictive factors and reaching broader target audiences for the study.</jats:p>
10.4337/9781803926728.00019,Uses and Abuses of AI Ethics,N/A
10.1136/jme-2024-110074,AI and XAI second opinion: the danger of false confirmation in human–AI collaboration,"<jats:p>Can AI substitute a human physician’s second opinion? Recently the<jats:italic>Journal of Medical Ethics</jats:italic>published two contrasting views: Kempt and Nagel advocate for using artificial intelligence (AI) for a second opinion except when its conclusions significantly diverge from the initial physician’s while Jongsma and Sand argue for a second human opinion irrespective of AI’s concurrence or dissent. The crux of this debate hinges on the prevalence and impact of ‘false confirmation’—a scenario where AI erroneously validates an incorrect human decision. These errors seem exceedingly difficult to detect, reminiscent of heuristics akin to confirmation bias. However, this debate has yet to engage with the emergence of explainable AI (XAI), which elaborates on why the AI tool reaches its diagnosis. To progress this debate, we outline a framework for conceptualising decision-making errors in physician–AI collaborations. We then review emerging evidence on the magnitude of false confirmation errors. Our simulations show that they are likely to be pervasive in clinical practice, decreasing diagnostic accuracy to between 5% and 30%. We conclude with a pragmatic approach to employing AI as a second opinion, emphasising the need for physicians to make clinical decisions before consulting AI; employing nudges to increase awareness of false confirmations and critically engaging with XAI explanations. This approach underscores the necessity for a cautious, evidence-based methodology when integrating AI into clinical decision-making.</jats:p>"
10.3102/ip.24.2108011,Driving Educational Change in Ghana: The Potential Role of Generative AI in Promoting Practical-Based Teaching and Assessment Methods,N/A
10.2139/ssrn.4575257,MeloHarmony: Exploring Emotion in Crafting AI-Generated Music with Generative Adversarial Network Powered Harmony,N/A
10.2139/ssrn.4812513,Generative AI Usage and Academic Performance,N/A
10.18785/jetde.1602.01,"Generative AI, learning and new literacies","<jats:p>Launched in November 2022, OpenAI’s ChatGPT garnered over 100 million users within two months, sparking a surge in research and concern over potential risks of extensive AI experiments. The article, originating from a conference presentation by Tsinghua University and NTHU, Taiwan, provides a nuanced overview of Generative AI. It explores the classifications, applications, governance challenges, societal implications, and development trajectory of Generative AI, emphasizing its transformative role in employment and education. The piece highlights ChatGPT’s significant impact and the strategic adaptations required in various sectors, including medical education, engineering, information management, and distance education. Furthermore, it explores the opportunities and challenges associated with incorporating ChatGPT in educational settings, emphasizing its support in facilitating personalized learning, developing 21st-century competencies, fostering self-directed learning, and enhancing information accessibility. It also illustrates the integration of ChatGPT and text-to-image models in high school language courses through the lens of new literacies. The text uniquely integrates three layers of discourse: introductions to Generative AI by experts, scholarly debates on its merits and drawbacks, and practical classroom applications, offering a reflective snapshot of the current and potential states of Generative AI applications while emphasizing the interconnected discussions across various layers of discourse.</jats:p>"
10.21125/inted.2024.1611,INCREASING STUDENT CAPACITY THROUGH INCREMENTALLY COMPLEX PRACTICE USING GENERATIVE AI,N/A
10.21125/inted.2024.1924,TRANSFORMING EXECUTIVE AND PROFESSIONAL DEVELOPMENT BY HIGHER EDUCATION THROUGH GENERATIVE AI-BASED PEDAGOGY,N/A
10.21125/inted.2024.1148,MEANINGFUL USE OF GENERATIVE AI IN VISUAL ARTS EDUCATION,N/A
10.1007/979-8-8688-0205-8_2,LangChain: Your Swiss Army Knife,N/A
10.2139/ssrn.4744202,Who Owns Generative Ai Training Data? Mapping the Issue and a Way Forward,N/A
10.31219/osf.io/h74gw,Regulating Generative AI: Ethical Considerations and Explainability Benchmarks,"<p>This study looks into the critical discussion surrounding the ethical regulation and explainability of generative artificial intelligence (AI). Amidst the rapid advancement of generative AI technologies, this paper identifies and explores the multifaceted ethical concerns that arise, highlighting the paramount importance of transparency, accountability, and fairness. Through an examination of existing regulatory frameworks and the introduction of novel benchmarks for explainability, the study advocates for a balanced approach that fosters innovation while ensuring ethical oversight. Case studies illustrate the dual potential of generative AI to benefit society and pose significant ethical challenges, underscoring the complexity of its integration into various domains. The findings emphasize the necessity for dynamic regulatory mechanisms, interdisciplinary collaboration, and ongoing research to navigate the ethical landscape of generative AI, aiming to harness its capabilities responsibly for the betterment of humanity.</p>"
10.1515/9783839474723-004,Bibliographic information published by the Deutsche Nationalbibliothek,N/A
10.4324/9781003500629-23,Generative AI and the Roles of Business School Teachers,N/A
10.1007/978-3-031-55642-5_2,Comparing Proficiency of ChatGPT and Bard in Software Development,N/A
10.1002/9781394308286.index,Index,N/A
10.1002/9781394308286.part1,Fundamentals,N/A
10.1038/s44159-024-00339-4,How to use generative AI more responsibly,N/A
10.69554/ayhz2687,Recognising generative and autonomous AI as a ‘juridical person’,"<jats:p xml:lang=""en"">In the current era, there is a significant global effort to establish legal and regulatory frameworks for the responsible use of artificial intelligence (AI). The discussions surrounding autonomous AI highlight challenges related to its technological transparency and, often, opacity. Despite the widespread application of AI in various fields, debates persist on decision-making processes, the necessity for safe and fair outcomes and the need for regulatory platforms ensuring compliance and governance in AI implementation. Issues such as the ‘authorship’ and ‘inventorship’ of autonomously generated creations, particularly in cases like the ‘device for autonomous bootstrapping of unified sentience’, have sparked intense debates and legal proceedings in multiple locations, including the UK, USA, Australia, Germany, New Zealand, Taiwan and the EU. The Supreme Court of India judgment in 2019 presents a detailed analysis for the recognition of idols as juristic personality. The judgment provides sufficient basis for the creative recognition of ‘generative and autonomous AI’ as a ‘juridical person’. Such a recognition would entitle the AI system to a patent both as an inventor and an applicant satisfying all the essential requirements of the Indian Patents Act. Alternatively, an appropriate `sui generis` system will have to be developed in various jurisdictions based on some commonly accepted principles.</jats:p>"
10.1016/j.techsoc.2023.102372,"Generative AI: Here to stay, but for good?",N/A
10.1201/9781003369356-7,Ethics of Artificial Intelligence (AI-Ethics) in Science and Engineering,N/A
10.59350/t5gj9-1fe45,Ethics and AI: Confronting the Challenges Ahead,"<p>&lt;strong&gt; Exploring AI’s Ethical Terrain: Addressing Bias, Security, and&amp;nbsp;Beyond &lt;/strong&gt; Author: Vaibhav Khobragade ( &lt;strong&gt; ORCID: &lt;/strong&gt; 0009–0009–8807–5982) Large language models (LLMs) like OpenAI’s GPT-4, Meta’s LLaMA, and Google Gemini (previously called Bard) have showcased their vast capabilities, from passing bar exams and crafting articles to generating images and website code.</p>"
10.4324/9781003271284-17,AI Ethics,N/A
10.1007/s43681-022-00251-8,"Towards artificial virtuous agents: games, dilemmas and machine learning",N/A
10.1007/s43681-021-00132-6,Artificial intelligence and real-time predictive maintenance in industry 4.0: a bibliometric analysis,N/A
10.1080/15027570.2023.2175887,"The AI Commander Problem: Ethical, Political, and Psychological Dilemmas of Human-Machine Interactions in AI-enabled Warfare",N/A
10.1007/s43681-024-00529-z,Securing tomorrow: a comprehensive survey on the synergy of Artificial Intelligence and information security,"<jats:title>Abstract</jats:title><jats:p>This survey paper explores the transformative role of Artificial Intelligence (AI) in information security. Traditional methods, especially rule-based approaches, faced significant challenges in protecting sensitive data from ever-changing cyber threats, particularly with the rapid increase in data volume. This study thoroughly evaluates AI’s application in information security, discussing its strengths and weaknesses. It provides a detailed review of AI’s impact on information security, examining various AI algorithms used in this field, such as supervised, unsupervised, and reinforcement learning, and highlighting their respective strengths and limitations. The study identifies key areas for future AI research in information security, focusing on improving algorithms, strengthening information security, addressing ethical issues, and exploring safety and security-related concerns. It emphasizes significant security risks, including vulnerability to adversarial attacks, and aims to enhance the robustness and reliability of AI systems in protecting sensitive information by proposing solutions for potential threats. The findings aim to benefit cybersecurity professionals and researchers by offering insights into the intricate relationship between AI, information security, and emerging technologies.</jats:p>"
10.1145/3514094.3534168,Human Autonomy in Algorithmic Management,N/A
10.1145/3375627.3375868,"More Than ""If Time Allows""",N/A
10.4324/9781003074991-26,Particularism and Generalism: How AI can Help us to Better Understand Moral Cognition,N/A
10.1007/978-3-319-32103-5_16,Algorithms and Media Ethics in the AI Age,N/A
10.1007/s00146-022-01545-5,AI and society: a virtue ethics approach,N/A
10.1093/oxfordhb/9780190067397.013.1,The Artificial Intelligence of the Ethics of Artificial Intelligence,"<p>This chapter provides an overview of the nature and implications of artificial intelligence (AI), with particular attention to how they impinge on possible applications to and of law. Any artifact that transforms perception to more relevant information, including action, is AI. There is no question that AI, and digital technologies in general, are introducing massive transformations to society. Nevertheless, these impacts should be governable by less transformative legislative change. Indeed, the vast majority of AI—particularly where it has social impact—is and will remain a consequence of corporate commercial processes, and as such subject to existing regulations and regulating strategies. However, it is critical to remember that what is being held accountable is not machines themselves but the people who build, own, or operate them—including any who alter their operation through assault on their cybersecurity. It is thus important to govern the human application of technology—the human processes of development, testing, operation, and monitoring.</p>"
10.1007/s10551-022-05050-z,The Dawn of the AI Robots: Towards a New Framework of AI Robot Accountability,"<jats:title>Abstract</jats:title><jats:p>Business, management, and business ethics literature pay little attention to the topic of AI robots. The broad spectrum of potential ethical issues pertains to using driverless cars, AI robots in care homes, and in the military, such as Lethal Autonomous Weapon Systems. However, there is a scarcity of in-depth theoretical, methodological, or empirical studies that address these ethical issues, for instance, the impact of morality and where accountability resides in AI robots’ use. To address this dearth, this study offers a conceptual framework that interpretively develops the ethical implications of AI robot applications, drawing on descriptive and normative ethical theory. The new framework elaborates on how the locus of morality (human to AI agency) and moral intensity combine within context-specific AI robot applications, and how this might influence accountability thinking. Our theorization indicates that in situations of escalating AI agency and situational moral intensity, accountability is widely dispersed between actors and institutions. ‘Accountability clusters’ are outlined to illustrate interrelationships between the locus of morality, moral intensity, and accountability and how these invoke different categorical responses: (i) illegal, (ii) immoral, (iii) permissible, and (iv) supererogatory pertaining to using AI robots. These enable discussion of the ethical implications of using AI robots, and associated accountability challenges for a constellation of actors—from designer, individual/organizational users to the normative and regulative approaches of industrial/governmental bodies and intergovernmental regimes.</jats:p>"
10.1007/s43681-021-00131-7,The ethical issues of the application of artificial intelligence in healthcare: a systematic scoping review,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) is being increasingly applied in healthcare. The expansion of AI in healthcare necessitates AI-related ethical issues to be studied and addressed. This systematic scoping review was conducted to identify the ethical issues of AI application in healthcare, to highlight gaps, and to propose steps to move towards an evidence-informed approach for addressing them. A systematic search was conducted to retrieve all articles examining the ethical aspects of AI application in healthcare from Medline (PubMed) and Embase (OVID), published between 2010 and July 21, 2020. The search terms were “artificial intelligence” or “machine learning” or “deep learning” in combination with “ethics” or “bioethics”. The studies were selected utilizing a PRISMA flowchart and predefined inclusion criteria. Ethical principles of respect for human autonomy, prevention of harm, fairness, explicability, and privacy were charted. The search yielded 2166 articles, of which 18 articles were selected for data charting on the basis of the predefined inclusion criteria. The focus of many articles was a general discussion about ethics and AI. Nevertheless, there was limited examination of ethical principles in terms of consideration for design or deployment of AI in most retrieved studies. In the few instances where ethical principles were considered, fairness, preservation of human autonomy, explicability and privacy were equally discussed. The principle of prevention of harm was the least explored topic. Practical tools for testing and upholding ethical requirements across the lifecycle of AI-based technologies are largely absent from the body of reported evidence. In addition, the perspective of different stakeholders is largely missing.</jats:p>"
10.1007/s43681-021-00120-w,The social dilemma in artificial intelligence development and why we have to solve it,"<jats:title>Abstract</jats:title><jats:p>While the demand for ethical artificial intelligence (AI) systems increases, the number of unethical uses of AI accelerates, even though there is no shortage of ethical guidelines. We argue that a possible underlying cause for this is that AI developers face a social dilemma in AI development ethics, preventing the widespread adaptation of ethical best practices. We define the social dilemma for AI development and describe why the current crisis in AI development ethics cannot be solved without relieving AI developers of their social dilemma. We argue that AI development must be professionalised to overcome the social dilemma, and discuss how medicine can be used as a template in this process.</jats:p>"
10.1007/s43681-023-00312-6,Ethical considerations in working with ChatGPT on a questionnaire about the future of work with ChatGPT,"<jats:title>Abstract</jats:title><jats:p>The prospect of the use of Large Language Models, like ChatGPT, in work environments raises important questions regarding both the potential for a dramatic change in the quality of jobs and the risk of unemployment. The answers to these questions, but, also, the posing of questions to be answered, may involve the use of ChatGPT. This, in turn, may give rise to a series of ethical considerations. The article seeks to identify such considerations by presenting a research on a questionnaire that was developed by means of ChatGPT before it was answered, first, by a group of humans (H) and, then, through the use of a machine (M), ChatGPT. The language model was actually used to respond to the questionnaire twice. First, based on its data (M1), and, second, based on it being asked to imitate a human (M2). Based on the significant differences between the H and M answers, and, further, on the noticeable differences occurring within the M answers (the differences between the M1 and M2 answers), the article concludes by registering a cluster of three ethical considerations.</jats:p>"
10.1007/s00146-023-01644-x,The future of ethics in AI: challenges and opportunities,N/A
10.3390/ai5030055,ZTCloudGuard: Zero Trust Context-Aware Access Management Framework to Avoid Medical Errors in the Era of Generative AI and Cloud-Based Health Information Ecosystems,"<jats:p>Managing access between large numbers of distributed medical devices has become a crucial aspect of modern healthcare systems, enabling the establishment of smart hospitals and telehealth infrastructure. However, as telehealth technology continues to evolve and Internet of Things (IoT) devices become more widely used, they are also increasingly exposed to various types of vulnerabilities and medical errors. In healthcare information systems, about 90% of vulnerabilities emerge from medical error and human error. As a result, there is a need for additional research and development of security tools to prevent such attacks. This article proposes a zero-trust-based context-aware framework for managing access to the main components of the cloud ecosystem, including users, devices, and output data. The main goal and benefit of the proposed framework is to build a scoring system to prevent or alleviate medical errors while using distributed medical devices in cloud-based healthcare information systems. The framework has two main scoring criteria to maintain the chain of trust. First, it proposes a critical trust score based on cloud-native microservices for authentication, encryption, logging, and authorizations. Second, a bond trust scoring system is created to assess the real-time semantic and syntactic analysis of attributes stored in a healthcare information system. The analysis is based on a pre-trained machine learning model that generates the semantic and syntactic scores. The framework also takes into account regulatory compliance and user consent in the creation of the scoring system. The advantage of this method is that it applies to any language and adapts to all attributes, as it relies on a language model, not just a set of predefined and limited attributes. The results show a high F1 score of 93.5%, which proves that it is valid for detecting medical errors.</jats:p>"
10.1007/s43681-024-00459-w,Ethical decision-making in human-automation collaboration: a case study of the nurse rostering problem,"<jats:title>Abstract</jats:title><jats:p>As artificial intelligence (AI) is increasingly present in different aspects of society and its harmful impacts are more visible, concrete methods to help design ethical AI systems and limit currently encountered risks must be developed. Taking the example of a well-known Operations Research problem, the Nurse Rostering Problem (NRP), this paper presents a way to help close the gap between abstract principles and on-the-ground applications with two different steps. We first propose a normative step that uses dedicated scientific knowledge to provide new rules for an NRP model, with the aim of improving nurses’ well-being. However, this step alone may be insufficient to comprehensively deal with all key ethical issues, particularly autonomy and explicability. Therefore, as a complementary second step, we introduce an interactive process that integrates a human decision-maker in the loop and allows practical ethics to be applied. Using input from stakeholders to enrich a mathematical model may help compensate for flaws in automated tools.</jats:p>"
10.1007/s43681-022-00149-5,Artificial intelligence and consumer manipulations: from consumer's counter algorithms to firm's self-regulation tools,N/A
10.52591/lxai201912083,Segmentation of skin lesions and their attributes using Generative Adversarial Networks,"<jats:p>This work is about the semantic segmentation of skin lesion boundary and their attributes using Image-to-Image Translation with Conditional Adversarial Nets. Melanoma is a type of skin cancer that can be cured if detected in time and the process of segmentation into dermoscopic images is an essential procedure for computer-assisted diagnosis due to its existing artifacts typical of skin images. To alleviate the image annotation process, we propose to use a modificated Pix2Pix network. The discriminator network learns the mapping from a dermal image as an input and a mask image of six channels as an output. Likewise, the output of the discriminative network called PatchGAN is varied for one channel and six output channels. The images used come from the 2018 ISIC Challenge where 500 images are used with their respective semantic map, divided into 75% for training and 35% for testing. Obtaining for 100 training epochs high jaccard indices for all attributes of the segmentation map.</jats:p>"
10.1007/s40962-023-01025-6,From the Editor,N/A
10.2139/ssrn.4746770,Experimenting with Generative AI: Does ChatGPT Really Increase Everyone’s Productivity?,N/A
10.2139/ssrn.4848956,"Improving Sales Forecasting: Leveraging Unstructured CRM Activity Logs, Large Language Models, and Generative AI",N/A
10.1002/9781394308286.ack,Acknowledgments,N/A
10.2139/ssrn.4594509,Grant Drafting Support with Guided Generative AI Software,N/A
10.5860/crl.84.6.836,The Evolution of Library Workplaces and Workflows via Generative AI,N/A
10.2139/ssrn.4615929,"The Copyright, Text &amp;amp; Data Mining and the Innovation dimension of Generative AI",N/A
10.21203/rs.3.rs-4107900/v1,Optimally Configured Generative Adversarial Networks to Distinguish Real and AI- Generated Human Faces,"<title>Abstract</title>
        <p>Artificial Intelligence (AI) has come a long way in the last several years, especially in terms of producing human-like faces with deep-fake technology. However, the challenge lies in accurately distinguishing between real and AI-generated human faces. As the applications of such technology continue to expand, the need for robust classification methods becomes crucial to ensure ethical and responsible use. Existing Generative Adversarial Networks (GANs) produce increasingly realistic synthetic faces, making it difficult for traditional methods to differentiate between real and generated faces. This poses potential risks in various domains, including security, identity verification, and misinformation. The primary objective of this research is to design an optimally configured GAN capable of distinguishing between real and generated faces and to develop a robust classifier that accurately classifies human faces as either real or generative. The results showcase the effectiveness of the optimally configured GAN model in achieving high accuracy, reaching 95%, in distinguishing between real and AI-generated faces across state-of-the-art techniques. The research contributes to the ethical deployment of AI technologies, safeguards security applications, strengthens identity verification systems, combats misinformation, and fosters public trust in the era of advanced AI.</p>"
10.1002/icd.2514/v1/review1,"Review for ""Perspectives on the impact of generative &lt;scp&gt;AI&lt;/scp&gt; on early‐childhood development and education""",N/A
10.2139/ssrn.4841318,Application of Generative Ai (Chatgpt as Example) in Risk Management,N/A
10.12781/978-1-907549-16-8-2,"The Generative Nature of SOAR: Applications, Results and the New SOAR Profile",N/A
10.1109/mmul.2024.3413395,Generative AI for 3-D Point Clouds,N/A
10.1145/3625468.3652912,Generative AI for HTTP Adaptive Streaming,N/A
10.32996/jcsts.2024.6.2.3,Generative AI: A New Challenge for Cybersecurity,"<jats:p>The rapid development of Generative Artificial Intelligence (GAI) technology has shown tremendous potential in various fields, such as image generation, text generation, and video generation, and it has been widely applied in various industries. However, GAI also brings new risks and challenges to cybersecurity. This paper analyzes the application status of GAI technology in the field of cybersecurity and discusses the risks and challenges it brings, including data security risks, scientific and technological ethics and moral challenges, Artificial Intelligence (AI) fraud, and threats from cyberattacks. On this basis, this paper proposes some countermeasures to maintain cybersecurity and address the threats posed by GAI, including: establishing and improving standards and specifications for AI technology to ensure its security and reliability; developing AI-based cybersecurity defense technologies to enhance cybersecurity defense capabilities; improving the AI literacy of the whole society to help the public understand and use AI technology correctly. From the perspective of GAI technology background, this paper systematically analyzes its impact on cybersecurity and proposes some targeted countermeasures and suggestions, possessing certain theoretical and practical significance.</jats:p>"
10.31234/osf.io/v4mfz,What label should be applied to content produced by generative AI?,"<p>The rise of generative AI has created pressure for content labeling. This paper investigates the public’s understanding of nine potential labels. Participants from the US (N=1056), Mexico (N=1060), Brazil (N=1065), India (N=1038), and China (N=1031) were shown twenty different types of content that varied in the extent to which they were AI-generated, and the extent to which they were misleading. Across countries and demographic subgroups, participants consistently associated “AI Generated,” “Generated with an AI tool,” and “AI manipulated” with AI-generated content, regardless of misleadingness; and associated “Deepfake” and “Manipulated” with mis- leading content, regardless of AI involvement. Interestingly, “Artificial” performed poorly in China due to translation nuances, but performed well on both alignment tasks in the other countries. Finally, we examined self-reported effects of the terms on belief in, and attitudes toward, labeled content. Our study underscores the need for deliberate decision-making regarding the objectives and implementation of generative AI disclosure.</p>"
10.2196/54482,Comparing the Efficacy and Efficiency of Human and Generative AI: Qualitative Thematic Analyses,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>Qualitative methods are incredibly beneficial to the dissemination and implementation of new digital health interventions; however, these methods can be time intensive and slow down dissemination when timely knowledge from the data sources is needed in ever-changing health systems. Recent advancements in generative artificial intelligence (GenAI) and their underlying large language models (LLMs) may provide a promising opportunity to expedite the qualitative analysis of textual data, but their efficacy and reliability remain unknown.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>The primary objectives of our study were to evaluate the consistency in themes, reliability of coding, and time needed for inductive and deductive thematic analyses between GenAI (ie, ChatGPT and Bard) and human coders.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>The qualitative data for this study consisted of 40 brief SMS text message reminder prompts used in a digital health intervention for promoting antiretroviral medication adherence among people with HIV who use methamphetamine. Inductive and deductive thematic analyses of these SMS text messages were conducted by 2 independent teams of human coders. An independent human analyst conducted analyses following both approaches using ChatGPT and Bard. The consistency in themes (or the extent to which the themes were the same) and reliability (or agreement in coding of themes) between methods were compared.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>The themes generated by GenAI (both ChatGPT and Bard) were consistent with 71% (5/7) of the themes identified by human analysts following inductive thematic analysis. The consistency in themes was lower between humans and GenAI following a deductive thematic analysis procedure (ChatGPT: 6/12, 50%; Bard: 7/12, 58%). The percentage agreement (or intercoder reliability) for these congruent themes between human coders and GenAI ranged from fair to moderate (ChatGPT, inductive: 31/66, 47%; ChatGPT, deductive: 22/59, 37%; Bard, inductive: 20/54, 37%; Bard, deductive: 21/58, 36%). In general, ChatGPT and Bard performed similarly to each other across both types of qualitative analyses in terms of consistency of themes (inductive: 6/6, 100%; deductive: 5/6, 83%) and reliability of coding (inductive: 23/62, 37%; deductive: 22/47, 47%). On average, GenAI required significantly less overall time than human coders when conducting qualitative analysis (20, SD 3.5 min vs 567, SD 106.5 min).</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>The promising consistency in the themes generated by human coders and GenAI suggests that these technologies hold promise in reducing the resource intensiveness of qualitative thematic analysis; however, the relatively lower reliability in coding between them suggests that hybrid approaches are necessary. Human coders appeared to be better than GenAI at identifying nuanced and interpretative themes. Future studies should consider how these powerful technologies can be best used in collaboration with human coders to improve the efficiency of qualitative research in hybrid approaches while also mitigating potential ethical risks that they may pose.</jats:p>
          </jats:sec>"
10.1109/icetsis61505.2024.10459661,"The Impact of Generative Artificial Intelligence on Organizational Innovation Performance: Roles of AI Generated Content Quality, AI Experience, and AI Usage Environment",N/A
10.1093/oxfordhb/9780190067397.013.32,Automating Origination,"<p>This chapter highlights historic and contemporary efforts to engineer artificial intelligence (AI) capable of producing artifacts previously associated with the creative arts. While creativity and artistic origination have historically been tied to art’s social value, AI art has recently begun to sell for high prices, and the promise of automating artistic production heralds new sectors of economic profit. Yet AI creativity also highlights the changing status of human innovation, origination, and newness in ways that deserve careful thought. The chapter then explores how future research in the field of computational creativity might benefit from a more robust appreciation for the uniquely nonhuman qualities of AI’s creativity, rather than its ability to imitate the human. AI’s exponentially intensifying capabilities raise meaningful and urgent questions of how to relate to a technology that, in many meaningful senses, invents itself. As automation, origination, and creativity cross-pollinate in unknowable ways, an intelligence both truly other and yet still conversant with human categories emerges. Thinking from the perspective of the humanities can help one negotiate this unprecedented challenge.</p>"
10.1007/s43681-023-00292-7,"They shall be fair, transparent, and robust: auditing learning analytics systems","<jats:title>Abstract</jats:title><jats:p>In the near future, systems, that use Artificial Intelligence (AI) methods, such as machine learning, are required to be certified or audited for fairness if used in ethically sensitive fields such as education. One example of those upcoming regulatory initiatives is the European Artificial Intelligence Act. Interconnected with fairness are the notions of system transparency (i.e. how understandable is the system) and system robustness (i.e. will similar inputs lead to similar results). Ensuring fairness, transparency, and robustness requires looking at data, models, system processes, and the use of systems as the ethical implications arise at the intersection between those. The potential societal consequences are domain specific, it is, therefore, necessary to discuss specifically for Learning Analytics (LA) what fairness, transparency, and robustness mean and how they can be certified. Approaches to certifying and auditing fairness in LA include assessing datasets, machine learning models, and the end-to-end LA process for fairness, transparency, and robustness. Based on Slade and Prinsloo’s six principals for ethical LA, relevant audit approaches will be deduced. Auditing AI applications in LA is a complex process that requires technical capabilities and needs to consider the perspectives of all stakeholders. This paper proposes a comprehensive framework for auditing AI applications in LA systems from the perspective of learners' autonomy, provides insights into different auditing methodologies, and emphasizes the importance of reflection and dialogue among providers, buyers, and users of these systems to ensure their ethical and responsible use.</jats:p>"
10.1007/s43681-021-00096-7,Artificial intelligence in education: Addressing ethical challenges in K-12 settings,N/A
10.1145/3640544.3645241,Advancing GUI for Generative AI: Charting the Design Space of Human-AI Interactions through Task Creativity and Complexity,N/A
10.2139/ssrn.4738829,When Advanced AI Isn't Enough: Human Factors as Drivers of Success in Generative AI-Human Collaborations,N/A
10.11591/ijai.v10.i2.pp374-381,Implementation of generative adversarial networks in HPCC systems using GNN bundle,"<jats:p>&lt;span id=""docs-internal-guid-8df40bfb-7fff-fcb8-b52f-54f39570d649""&gt;&lt;span&gt;HPCC systems, an open source cluster computing platform for big data analytics consists of generalized neural network bundle with a wide variety of features which can be used for various neural network applications. To enhance the functionality of the bundle, this paper proposes the design and development of generative adversarial networks (GANs) on HPCC systems platform using ECL, a declarative language on which HPCC systems works. GANs have been developed on the HPCC platform by defining the generator and discriminator models separately, and training them by batches in the same epoch. In order to make sure that they train as adversaries, a certain weights transfer methodology was implemented. MNIST dataset which has been used to test the proposed approach has provided satisfactory results. The results obtained were unique images very similar to the MNIST dataset, as it were expected.&lt;/span&gt;&lt;/span&gt;</jats:p>"
10.2139/ssrn.3193693,"AI, Machine Learning &amp; Deep Learning Risk Management &amp; Controls: Beyond Deep Learning and Generative Adversarial Networks: Model Risk Management in AI, Machine Learning &amp; Deep Learning",N/A
10.3390/su151813595,"Fostering AI Literacy in Elementary Science, Technology, Engineering, Art, and Mathematics (STEAM) Education in the Age of Generative AI","<jats:p>The advancement of generative AI technologies underscores the need for AI literacy, particularly in Southeast Asia’s elementary Science, Technology, Engineering, Art, and Mathematics (STEAM) education. This study explores the development of AI literacy principles for elementary students. Utilizing existing AI literacy models, a three-session classroom intervention was implemented in an Indonesian school, grounded in constructivist, constructionist, and transformative learning theories. Through design-based research (DBR) and network analysis of reflection papers (n = 77), the intervention was evaluated and redesigned. Findings revealed clusters of interdependent elements of learner experiences, categorized into successes, struggles, and alignments with learning theories. These were translated into design moves for future intervention iterations, forming design principles for AI literacy development. The study contributes insights into optimizing the positive effects and minimizing the negative impacts of AI in education.</jats:p>"
10.21125/edulearn.2024.2216,CREATING ARTIFICIAL INTELLIGENCE (AI) BEST PRACTICES IN VIRTUAL LEARNING ENVIRONMENTS: A PRIMER APPROACH FOR INTRODUCING THE IMPLEMENTATION OF GENERATIVE-AI AS A TEACHING TOOL,N/A
10.4018/979-8-3693-3597-0.ch008,Data Guardians,"<jats:p>This chapter provides an in-depth exploration of generative adversarial networks (GANs) and their profound impact on the field of cybersecurity. GANs have evolved from their initial application in image generation to play a crucial role across a wide spectrum of cybersecurity domains, including synthetic data generation, anomaly detection, malware identification, cryptographic key generation, and biometric security enhancement. By detailing the architecture and types of GANs, alongside their application in generating synthetic data for robust security model training and simulating cyber threats, this analysis highlights the versatility and adaptability of GANs in addressing contemporary cybersecurity challenges. Additionally, the chapter confronts the technical challenges associated with GAN development, and navigates the ethical considerations surrounding their use, advocating for responsible deployment and the establishment of ethical guidelines. Through this comprehensive overview, GANs are positioned as indispensable tools in the development of secure digital infrastructures.</jats:p>"
10.4018/979-8-3693-1351-0.ch006,Future Possibilities and Challenges of AI in Education,"<jats:p>“Future Possibilities and Challenges of AI in Education” delves into the unprecedented intersection of artificial intelligence (AI) and the educational landscape. The chapter navigates the transformative possibilities and intricate challenges that arise as AI infiltrates learning management systems. In this exploration, the symbiotic relationship between AI and education is dissected, revealing a dynamic environment teeming with potential. The chapter unfolds in two movements: envisioning a future where personalized learning, AI-enhanced creativity, seamless assessment, global collaboration, and ethical education thrive, and concurrently, acknowledging the uncharted difficulties that could reshape the foundational paradigms of education. As the world hurtles toward an era of automation and interconnected virtuality, understanding the effects of AI in education becomes important. This chapter beckons readers on a journey through the limitless potential of AI's impact on education, inviting reflection on how to harness its power responsibly for the betterment of learning.</jats:p>"
10.4324/9781003482918-11,The potential of AI text-to-image generation in medical education,N/A
10.1145/3664824,Public-Private Powerplays in Generative AI Era: Balancing Big Tech Regulation Amidst Global AI Race,"<jats:p>The past decades have seen unbridled growth in the economic, social and political influence of large technology corporations (Big Tech) in the United States. The rising popularity of Generative Artificial Intelligence (GenAI) is likely to further consolidate the power of these companies. The rapid expansion of Big Tech in various domains has triggered a wide range of economic, ethical, and political concerns. However, the US Government is also engaged in a growing technology and AI race with China. As a result, the US government now faces the challenges of balancing the external goal of winning the AI race through close collaboration with the Big Tech and the internal objective of regulating the Big Tech. In this paper, we argue that this intersection of interest has been the primary motivator of US policy on the governance of Big Tech. By exploring the evolution of AI policy in the US, we highlight the role internal and external pressures have played in its approach to AI governance.</jats:p>"
10.1162/99608f92.a377379b,Toward International Cooperation on Foundational AI Models: An Expanded Role for Trade Agreements and International Economic Policy,N/A
10.1007/s43503-023-00017-z,A brief introductory review to deep generative models for civil structural health monitoring,"<jats:title>Abstract</jats:title><jats:p>The use of deep generative models (DGMs) such as variational autoencoders, autoregressive models, flow-based models, energy-based models, generative adversarial networks, and diffusion models has been advantageous in various disciplines due to their high data generative skills. Using DGMs has become one of the most trending research topics in Artificial Intelligence in recent years. On the other hand, the research and development endeavors in the civil structural health monitoring (SHM) area have also been very progressive owing to the increasing use of Machine Learning techniques. As such, some of the DGMs have also been used in the civil SHM field lately. This short review communication paper aims to assist researchers in the civil SHM field in understanding the fundamentals of DGMs and, consequently, to help initiate their use for current and possible future engineering applications. On this basis, this study briefly introduces the concept and mechanism of different DGMs in a comparative fashion. While preparing this short review communication, it was observed that some DGMs had not been utilized or exploited fully in the SHM area. Accordingly, some representative studies presented in the civil SHM field that use DGMs are briefly overviewed. The study also presents a short comparative discussion on DGMs, their link to the SHM, and research directions.</jats:p>"
10.1007/s43681-022-00135-x,Diffused responsibility: attributions of responsibility in the use of AI-driven clinical decision support systems,"<jats:title>Abstract</jats:title><jats:p>Good decision-making is a complex endeavor, and particularly so in a health context. The possibilities for day-to-day clinical practice opened up by AI-driven clinical decision support systems (AI-CDSS) give rise to fundamental questions around responsibility. In causal, moral and legal terms the application of AI-CDSS is challenging existing attributions of responsibility. In this context, responsibility gaps are often identified as main problem. Mapping out the changing dynamics and levels of attributing responsibility, we argue in this article that the application of AI-CDSS causes diffusions of responsibility with respect to a causal, moral, and legal dimension. Responsibility diffusion describes the situation where multiple options and several agents can be considered for attributing responsibility. Using the example of an AI-driven ‘digital tumor board’, we illustrate how clinical decision-making is changed and diffusions of responsibility take place. Not denying or attempting to bridge responsibility gaps, we argue that dynamics and ambivalences are inherent in responsibility, which is based on normative considerations such as avoiding experiences of disregard and vulnerability of human life, which are inherently accompanied by a moment of uncertainty, and is characterized by revision openness. Against this background and to avoid responsibility gaps, the article concludes with suggestions for managing responsibility diffusions in clinical decision-making with AI-CDSS.</jats:p>"
10.1007/s00146-010-0275-5,Erratum to: Ethics of calculation,N/A
10.1007/978-981-19-9382-4_8,Persons and AI,N/A
10.1201/9781003276135-3,AI ethics in and for Africa,N/A
10.1007/978-981-19-2531-3_1,New Problems in the Era of AI,N/A
10.1089/aipo.2023.0004,Designing Prompts for Generative Artificial Intelligence in Clinical Oncology Contexts,N/A
10.46645/inoutsesk.56.4,AI for UDL: Generative AI and the Imagination for Literacy Education,N/A
10.1007/978-3-031-55642-5_16,Toward Guiding Students: Exploring Effective Approaches for Utilizing AI Tools in Programming Courses,N/A
10.3389/frobt.2023.1290604,Emergent communication of multimodal deep generative models based on Metropolis-Hastings naming game,"<jats:p>Deep generative models (DGM) are increasingly employed in emergent communication systems. However, their application in multimodal data contexts is limited. This study proposes a novel model that combines multimodal DGM with the Metropolis-Hastings (MH) naming game, enabling two agents to focus jointly on a shared subject and develop common vocabularies. The model proves that it can handle multimodal data, even in cases of missing modalities. Integrating the MH naming game with multimodal variational autoencoders (VAE) allows agents to form perceptual categories and exchange signs within multimodal contexts. Moreover, fine-tuning the weight ratio to favor a modality that the model could learn and categorize more readily improved communication. Our evaluation of three multimodal approaches - mixture-of-experts (MoE), product-of-experts (PoE), and mixture-of-product-of-experts (MoPoE)–suggests an impact on the creation of latent spaces, the internal representations of agents. Our results from experiments with the MNIST + SVHN and Multimodal165 datasets indicate that combining the Gaussian mixture model (GMM), PoE multimodal VAE, and MH naming game substantially improved information sharing, knowledge formation, and data reconstruction.</jats:p>"
10.1007/978-1-4842-6537-6_8,Machine Learning and AI Ethics,N/A
10.1007/978-981-19-2531-3_4,"AI Medical Treatment: Epidemic, Death and Love",N/A
10.3389/fpsyg.2022.836650,AI and Ethics When Human Beings Collaborate With AI Agents,"<jats:p>The relationship between a human being and an AI system has to be considered as a collaborative process between two agents during the performance of an activity. When there is a collaboration between two people, a fundamental characteristic of that collaboration is that there is co-supervision, with each agent supervising the actions of the other. Such supervision ensures that the activity achieves its objectives, but it also means that responsibility for the consequences of the activity is shared. If there is no co-supervision, neither collaborator can be held co-responsible for the actions of the other. When the collaboration is between a person and an AI system, co-supervision is also necessary to ensure that the objectives of the activity are achieved, but this also means that there is co-responsibility for the consequences of the activities. Therefore, if each agent's responsibility for the consequences of the activity depends on the effectiveness and efficiency of the supervision that that agent performs over the other agent's actions, it will be necessary to take into account the way in which that supervision is carried out and the factors on which it depends. In the case of the human supervision of the actions of an AI system, there is a wealth of psychological research that can help us to establish cognitive and non-cognitive boundaries and their relationship to the responsibility of humans collaborating with AI systems. There is also psychological research on how an external observer supervises and evaluates human actions. This research can be used to programme AI systems in such a way that the boundaries of responsibility for AI systems can be established. In this article, we will describe some examples of how such research on the task of supervising the actions of another agent can be used to establish lines of shared responsibility between a human being and an AI system. The article will conclude by proposing that we should develop a methodology for assessing responsibility based on the results of the collaboration between a human being and an AI agent during the performance of one common activity.</jats:p>"
10.1145/3600211.3604714,Action Guidance and AI Alignment,N/A
10.1007/s11245-024-10078-z,Correction: Decolonizing AI Ethics: Relational Autonomy as a Means to Counter AI Harms,N/A
10.22876/kjbtr.2024..127.005,"A Study on Factors Influencing Users"" Intention to Accept and Reject Generative AI-based Content : Focusing on AI Cover Music",N/A
10.1007/bf01174477,"Ethics is fragile, goodness is not",N/A
10.1007/s00146-010-0265-7,Ethics and aesthetics of technologies,N/A
10.1007/978-981-19-2531-3_6,AI and Robot: Darwin and Rebellious Machine,N/A
10.1007/s00146-007-0147-9,Technology as excuse for questionable ethics,N/A
10.5772/intechopen.97478,An Ontology for Standardising Trustworthy AI,"<jats:p>Worldwide, there are a multiplicity of parallel activities being undertaken in developing international standards, regulations and individual organisational policies related to AI and its trustworthiness characteristics. The current lack of mappings between these activities presents the danger of a highly fragmented global landscape emerging in AI trustworthiness. This could present society, government and industry with competing standards, regulations and organisational practices that will then serve to undermine rather than build trust in AI. This chapter presents a simple ontology that can be used for checking the consistency and overlap of concepts from different standards, regulations and policies. The concepts in this ontology are grounded in an overview of AI standardisation currently being undertaken in ISO/IEC JTC 1/SC 42 and identifies its project to define an AI management system standard (AIMS or ISO/IEC WD 42001) as the starting point for establishing conceptual mapping between different initiatives. We propose a minimal, high level ontology for the support of conceptual mapping between different documents and show in the first instance how this can help map out the overlaps and gaps between and among SC 42 standards currently under development.</jats:p>"
10.1007/s43681-023-00309-1,Embedded Ethics for Responsible Artificial Intelligence Systems (EE-RAIS) in disaster management: a conceptual model and its deployment,N/A
10.3390/ai4040042,Ethics and Transparency Issues in Digital Platforms: An Overview,"<jats:p>There is an ever-increasing application of digital platforms that utilize artificial intelligence (AI) in our daily lives. In this context, the matters of transparency and accountability remain major concerns that are yet to be effectively addressed. The aim of this paper is to identify the zones of non-transparency in the context of digital platforms and provide recommendations for improving transparency issues on digital platforms. First, by surveying the literature and reflecting on the concept of platformization, choosing an AI definition that can be adopted by different stakeholders, and utilizing AI ethics, we will identify zones of non-transparency in the context of digital platforms. Second, after identifying the zones of non-transparency, we go beyond a mere summary of existing literature and provide our perspective on how to address the raised concerns. Based on our survey of the literature, we find that three major zones of non-transparency exist in digital platforms. These include a lack of transparency with regard to who contributes to platforms; lack of transparency with regard to who is working behind platforms, the contributions of those workers, and the working conditions of digital workers; and lack of transparency with regard to how algorithms are developed and governed. Considering the abundance of high-level principles in the literature that cannot be easily operationalized, this is an attempt to bridge the gap between principles and operationalization.</jats:p>"
10.1093/oxfordhb/9780197579329.013.68,The Role of Workers in AI Ethics and Governance,"<jats:title>Abstract</jats:title>
               <jats:p>While the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention. This chapter looks at the role that workers play in identifying and mitigating harms from AI technologies. Harms are the causally assessed “impacts” of technologies. They arise despite technical reliability and are not a result of technical negligence but rather of normative uncertainty around questions of safety and fairness in complex social systems. There is high consensus in the AI ethics community on the benefits of reducing harms but less consensus on mechanisms for determining or addressing harms. This lack of consensus has led to numerous collective actions by workers protesting how harms are identified and addressed in their workplace. This chapter theorizes the role of workers within AI governance and constructs a model of harm reporting processes in AI workplaces. The harm reporting process involves three steps: identification, the governance decision, and the response. Workers draw upon three types of claims to argue for jurisdiction over questions of AI governance: subjection, control over the product of one’s labor, and proximate knowledge of systems. Examining the past decade of AI-related worker activism allows us to understand how different types of workers are positioned within a workplace that produces AI systems, how their position informs their claims, and the place of collective action in staking their claims. This chapter argues that workers occupy a unique role in identifying and mitigating harms caused by AI systems.</jats:p>"
10.1007/978-3-030-51110-4_3,What Is Ethics?,N/A
10.29173/irie415,"Constructing AI: Examining how AI is shaped by data, models and people","<jats:p>Artificial Intelligence (AI) is a technology that is quickly becoming part of our digital infrastructure and woven into aspects of daily life. AI has the potential to impact society in many positive ways. However, there are numerous examples of AI systems that are operating in ways that are harmful, unjust and discriminatory. AI systems are constructs of the choices made in their design. They exist within a socio-cultural context that reflects the data used in their training, the design of their mathematical models and the values of their creators. If we want to build AI systems that benefit society, we need to change how we construct AI.</jats:p>"
10.1016/b978-0-443-18851-0.00001-9,Assessing and implementing trustworthy AI across multiple dimensions,N/A
10.1201/b23345-4,"AI, Ethics, and Coloniality: A Feminist Critique",N/A
10.1007/s00146-007-0091-8,Ethics and consciousness in artificial agents,N/A
10.1007/978-3-031-48135-2_11,"AI, Sustainability, and Environmental Ethics",N/A
10.3390/buildings14082533,Integrating Multimodal Generative AI and Blockchain for Enhancing Generative Design in the Early Phase of Architectural Design Process,"<jats:p>Multimodal generative AI and generative design empower architects to create better-performing, sustainable, and efficient design solutions and explore diverse design possibilities. Blockchain technology ensures secure data management and traceability. This study aims to design and evaluate a framework that integrates blockchain into generative AI-driven design drawing processes in architectural design to enhance authenticity and traceability. We employed a scenario as an example to integrate generative AI and blockchain into architectural designs by using a generative AI tool and leveraging multimodal generative AI to enhance design creativity by combining textual and visual inputs. These images were stored on blockchain systems, where metadata were attached to each image before being converted into NFT format, which ensured secure data ownership and management. This research exemplifies the pragmatic fusion of generative AI and blockchain technology applied in architectural design for more transparent, secure, and effective results in the early stages of the architectural design process.</jats:p>"
10.29173/irie381,(De)constructing ethics for autonomous cars: A case study of Ethics Pen-Testing towards “AI for the Common Good”,"<jats:p>Recently, many AI researchers and practitioners have embarked on research visions that involve doing AI for “Good”. This is part of a general drive towards infusing AI research and practice with ethical thinking. One frequent theme in current ethical guidelines is the requirement that AI be good for all, or: contribute to the Common Good. But what is the Common Good, and is it enough to want to be good? Via four lead questions, the concept of Ethics Pen-Testing (EPT) identifies challenges and pitfalls when determining, from an AI point of view, what the Common Good is and how it can be enhanced by AI.&#x0D;
The current paper reports on a first evaluation of EPT. EPT is applicable to various artefacts that have ethical impact, including designs for or implementations of specific AI technology, and requirements engineering methods for eliciting which ethical settings to build into AI. The current study focused on the latter type of artefact. In four independent sessions, participants with close but varying involvements in “AI and ethics” were asked to deconstruct a method that has been proposed for eliciting ethical values and choices in autonomous car technology, an online experiment modelled on the Trolley Problem.&#x0D;
The results suggest that EPT is well-suited to this task: the remarks made by participants lent themselves well to being structured by the four lead questions of EPT, in particular, regarding the question what the problem is and about which stakeholders define it. As part of the problem definition, the need became apparent for thorough technical domain knowledge in discussions of AI and ethics. Thus, participants questioned the framing and the presuppositions inherent in the experiment and the discourse on autonomous cars that underlies the experiment. They transitioned from discussing a specific AI artefact to discussing its role in wider socio-technical systems.&#x0D;
Results also illustrate to what extent and how the requirements engineering method forces us to not only have a discussion about which values to “build into” AI systems, the substantive building blocks of the Common Good, but also about how we want to have this discussion at all. Thus, it forces us to become explicit about how we conceive of democracy and the constitutional state and the procedural building blocks of the Common Good.</jats:p>"
10.1080/17511321.2023.2228118,Sport and AI,N/A
10.1007/978-3-031-55744-6_6,Relationships,N/A
10.1007/978-3-031-55744-6_1,Introduction,N/A
10.1017/9781009072168.029,"AI, Ethics, and Law",N/A
10.1007/978-3-031-55744-6_7,Environments,N/A
10.1007/s00146-023-01722-0,Correction: Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms,N/A
10.1007/s00146-019-00920-z,Decentered ethics in the machine era and guidance for AI regulation,N/A
10.1007/s00146-024-01993-1,The meaningfulness gap in AI ethics: a guide on how to think through a complex challenge,"<jats:title>Abstract</jats:title><jats:p>Technological outsourcing is increasingly prevalent, with AI systems taking over many tasks once performed by humans. This shift has led to various discussions within AI ethics. A question that was largely ignored until recently, but is now increasingly being discussed, concerns the meaningfulness of such a lifestyle. The literature largely features skeptical views, raising several challenges. Many of these challenges can be grouped under what I identify as the “meaningfulness gap”. Although this gap is widely acknowledged, there is a notable absence of systematic exploration in the literature. This paper aims to fill this void by offering a detailed, step-by-step guide for systematically exploring the different instances of the meaningfulness gap and aids in navigating their complexities. More specifically, it proposes differentiating the gaps according to their realms and objects, normative nature, scope, and severity. To make these areas manageable, the paper takes several taxonomies and distinctions on board. Finally, the guide is summarized, and some skeptical replies are anticipated and countered by clarificatory remarks.</jats:p>"
10.1145/3278721.3278792,Ethics in Norm Decision Making,N/A
10.1007/s10551-024-05754-4,"Deception, Discrimination, and Objectification: Ethical Issues of Female AI Agents",N/A
10.1145/3278721.3278801,AI Risk Mitigation Through Democratic Governance,N/A
10.4337/9781802203110.00012,Conclusion to Data Ethics of Power,N/A
10.1016/b978-0-443-18851-0.00020-2,Perspectives on the ethics of a VR-based empathy experience for educators,N/A
10.1007/978-3-031-55744-6_5,Practices,N/A
10.4018/979-8-3693-0074-9.ch014,Engaging Future-Minded Visualizations and Artful Aesthetics,"<jats:p>Artmaking generative AIs have been used to create visual works (still images, motion videos, and animated sequences, including simulations) that are photorealistic, high-fidelity, hyper-realistic, and emulative of the world; artful works are translated in style transfers derivative of known artist, artworks, art styles, schools of art practices, and defined visual styles. This work argues that artmaking generative AIs should not be about reproductions of the familiar but should be used for future-minded visualizations and future-minded aesthetics, through true collaborations between human and machine imaginations. Through generative AI, people should be “seeing different” to enable nouveau “thinking” and “doing” and “being” different… into the near- and far- futures. This early and exploratory work is based on research, year-long experimentation with multiple public artmaking generative AI tools, and abductive logic.</jats:p>"
10.3897/ese.2024.e132192,"Written by AI, reviewed by AI, and published by AI - the human editor as the ultimate gatekeeper in publication ethics",<jats:p>An exercise in AI driven publishing</jats:p>
10.4337/9781802203110.00006,Introduction to Data Ethics of Power,N/A
10.1177/17470161241267782,"Assessing dual use risks in AI research: necessity, challenges and mitigation strategies","<jats:p> This article argues that due to the difficulty in governing AI, it is essential to develop measures implemented early in the AI research process. The goal of dual use considerations is to create robust strategies that uphold AI’s integrity while protecting societal interests. The challenges of applying dual use frameworks to AI research are examined and dual use and dual use research of concern (DURC) are defined while highlighting the difficulties in balancing the technology’s benefits and risks. AI’s dual use potential is discussed, particularly in areas like NLP and LLMs, and the need for early consideration of dual use risks to ensure ethical and secure development is underscored. In the section on shared responsibilities in AI research and avenues for mitigation strategies the importance of early-stage risk assessments and ethical guidelines to mitigate misuse is emphasized, accentuating self-governance within scientific communities and structured measures like checklists and pre-registration to promote responsible research practices. The final section argues that research ethics committees play a crucial role in evaluating the dual use implications of AI technologies within the research pipeline. The need for tailored ethics review processes is articulated, drawing parallels with medical research ethics committees. </jats:p>"
10.4018/979-8-3693-1351-0.ch007,Generative Artificial Intelligence as Academic Assistant,"<jats:p>The study conducted a literature review focusing on the role of generative artificial intelligence (GAI) utilized as an academic assistant. Within the scope of the research, the opportunities and challenges presented by GAI-based academic assistants to researchers, along with a detailed examination of tools used in the research process, were intended to be explored. It was emphasized that artificial intelligence applications used in academic writing processes have the potential to expedite tasks such as performing repetitive functions, literature reviews, data analysis, and document organization effectively. This situation was noted to offer a range of advantages for students and researchers. However, it was underscored that considerations regarding accuracy, reliability, and ethical concerns must be considered. Factors limiting the effectiveness of GAI in academic writing processes, such as limitations in language understanding, concerns about ethical violations, and difficulties in selecting appropriate data visualization, were discussed.</jats:p>"
10.1177/20438869241266258,AI for learning unleashed: Pioneering generative AI in education at the University of Miami,"<jats:p> The University of Miami (UM) is at the forefront of integrating generative AI into education, spearheaded by Ann M. Olazábal, Interim Dean of the Miami Herbert Business School. This case study explores the dynamic landscape of AI adoption at UM, highlighting both the innovative potential and ethical challenges. The university has embraced AI tools like Adobe Firefly and Copilot, aiming to enhance learning experiences and internal processes. However, the misuse of AI for academic dishonesty poses significant dilemmas. The case contrasts two MBA students' approaches to AI: Alex, who uses AI to shortcut his studies, and Jordan, who leverages AI to deepen her understanding and enhance her learning. The narrative underscores the need for ethical AI use and the importance of fostering critical thinking and creativity. Ann’s strategic vision includes forming an AI task force to explore AI’s organizational impacts and potential use cases, such as a smart assistant for student curriculum planning and an AI-powered entrepreneurship lab. The case concludes with a discussion on balancing innovation with academic integrity and the challenges of centralized IT governance. The case serves as a foundation for classroom discussion including a comprehensive examination of the opportunities and risks associated with generative AI in higher education. </jats:p>"
10.18658/humancon.2023.12.197,Study on Copyright recognition and improvement measures related to generative AI creations in the publishing ecosystem,N/A
10.4018/979-8-3693-2964-1.ch011,Exploring the Ethical Implications of Generative AI in Healthcare,"<jats:p>This chapter critically evaluates the ethical challenges posed by the advent of generative artificial intelligence (GenAI) in healthcare. It investigates how GenAI's potential to revolutionize patient care and medical research is counterbalanced by significant ethical concerns, including privacy, security, and equity. An extensive literature review supports a deep dive into these issues, comparing GenAI's impact on traditional healthcare ethics. Through case studies and theoretical analysis, the chapter seeks to understand GenAI's ethical implications thoroughly, aiming to contribute to the development of nuanced ethical frameworks in this rapidly advancing area.</jats:p>"
10.1007/978-3-031-61221-3_3,Generative AI-Language Models in Didactics and Communication for Inclusiveness,N/A
10.54254/2754-1169/92/20231109,Generative AI Impact on Marketing Agency,"<jats:p>Background: By the end of 2022, a disruptive generative AI product, ChatGPT, was launched and impressed in both the B2C and B2B business field. Among the industries impacted, sales and marketing were identified as one of the most affected by McKinsey. This article focuses on marketing agencies and individual marketers, whose core business revolves around marketing campaigns. Problem Definition: In this paper, the fundamental capabilities of generative AI and its current applications in the marketing industry have been assessed. Furthermore, by considering both macro and micro-level factors and incorporating the influence of consumer perceptions on generative AI development, conclusions have been drawn regarding the impact of generative AI on marketing agencies. Methodology/results: First, this paper summarizes amount of literature and establishes that while the concept of AI has a long history, it is still in its early stages from a technical perspective. Its impact on the marketing industry is therefore worthy to take attention. This paper's analysis includes both qualitative and quantitative aspects. The macro-market analysis using the PEST framework indicates that the development environment is complex. Consumer-side surveys show that consumers' open attitudes are driving choices made by marketing agency clients. Ultimately, conclusions are drawn regarding the extent of the impact of generative AI products on marketers.</jats:p>"
10.3390/electronics13173509,A Systematic Review of Synthetic Data Generation Techniques Using Generative AI,"<jats:p>Synthetic data are increasingly being recognized for their potential to address serious real-world challenges in various domains. They provide innovative solutions to combat the data scarcity, privacy concerns, and algorithmic biases commonly used in machine learning applications. Synthetic data preserve all underlying patterns and behaviors of the original dataset while altering the actual content. The methods proposed in the literature to generate synthetic data vary from large language models (LLMs), which are pre-trained on gigantic datasets, to generative adversarial networks (GANs) and variational autoencoders (VAEs). This study provides a systematic review of the various techniques proposed in the literature that can be used to generate synthetic data to identify their limitations and suggest potential future research areas. The findings indicate that while these technologies generate synthetic data of specific data types, they still have some drawbacks, such as computational requirements, training stability, and privacy-preserving measures which limit their real-world usability. Addressing these issues will facilitate the broader adoption of synthetic data generation techniques across various disciplines, thereby advancing machine learning and data-driven solutions.</jats:p>"
10.1061/ciegag.0001711,Relying on Generative AI Has Its Pitfalls,N/A
10.1145/3604237.3626884,Generative Machine Learning for Multivariate Equity Returns,N/A
10.18260/1-2--48074,Team Dynamics And Conflict Resolution: Integrating Generative AI in Project-Based Learning to Support Student Performance,N/A
10.21100/msor.v22i3.1485,Using Generative AI to help with statistical test selection and analysis,"<jats:p>One of the most common questions that students ask statistics advisors is ‘What test should I do?’ This paper explores the use of generative AI chatbots, specifically ChatGPT, as a tool to assist students, in particular those with limited experience in statistics, in selecting appropriate statistical tests for their analyses. Traditional methods, such as flowcharts and online test selectors, require at least a basic understanding of measurement scales and research design, which can be an issue for many students who have limited exposure to statistics on their courses. This research focuses on developing and refining prompts to guide ChatGPT in providing accurate and relevant statistical test recommendations. A hypothetical scenario was used to test the effectiveness of various prompts, ranging from simple, naïve questions to more sophisticated ones utilising specific prompt patterns, such as the ‘context manager’ and ‘flipped interaction.’ These patterns were selected to enhance the chatbot’s responses and ensure the relevance and accuracy of the test suggestions. The findings suggest that while AI chatbots like ChatGPT can be a valuable resource for students, their effectiveness is highly dependent on the quality of the prompts used. The paper concludes with a discussion on the potential of these AI tools in educational settings, acknowledging the limitations of current technology and suggesting directions for future research and development.</jats:p>"
10.59936/stile.v1i1.135,Generative AI and its Potential Implications for EAP Practitioner Scholarship,N/A
10.14742/apubs.2023.565,The LMS in the age of Generative AI,"<jats:p>Whilst Covid-19 raised a range of questions about the utility of the Learning Management System (LMS) and other technologies for teaching and learning, the advent of Generative AI (GenAI) poses new and fundamental questions about the teaching and learning technology ecosystem of our universities. We argue that these questions extend beyond the impact of GenAI on student learning, the assessment of students and academic integrity. Rather, the impact of GenAI on higher education represents a threshold concept that, once grasped, will entail a paradigm shift in the sector’s self-understanding.</jats:p>"
10.5210/fm.v29i3.13266,Fostering children’s agency in their learning futures: Exploring the synergy of generative AI and sensory learning,"<jats:p>The discourse surrounding the potential educational transformation brought about by generative AI has largely neglected the sensory aspect of learning. In this position paper, I emphasize the significance of sensory studies and their theoretical foundations of embodiment and multimodality as catalysts for novel perspectives on the intersection of AI and the future of education. I delve into the question of whether generative AI serves as a precursor to a new literacy or merely arises as a consequence of ongoing theoretical advancements in contemporary literacy studies. I argue that the concept of agency, which includes both personal and social aspects, should be central to recognizing the importance of sensory learning as an emerging paradigm in reimagining learning futures.</jats:p>"
10.1002/icd.2514/v1/decision1,"Decision letter for ""Perspectives on the impact of generative &lt;scp&gt;AI&lt;/scp&gt; on early‐childhood development and education""",N/A
10.20944/preprints202409.0311.v1,Generative AI in Medicine and Healthcare: Moving Beyond the ‘Peak of Inflated Expectations’,"<jats:p>The rapid development of specific-purpose Large Language Models (LLMs), such as Med-PaLM, MEDITRON-70B, and Med-Gemini, has significantly impacted healthcare, offering unprecedented capabilities in clinical decision support, diagnostics, and personalized health monitoring. This paper reviews the advancements in medicine-specific LLMs, the integration of Retrieval-Augmented Generation (RAG) and prompt engineering, and their applications in improving diagnostic accuracy and educational utility. Despite the potential, these technologies present challenges, including bias, hallucinations, and the need for robust safety protocols. The paper also discusses the regulatory and ethical considerations necessary for integrating these models into mainstream healthcare. By examining current studies and developments, this paper aims to provide a comprehensive overview of the state of LLMs in medicine and highlight the future directions for research and application. The study concludes that while LLMs hold immense potential, their safe and effective integration into clinical practice requires rigorous testing, ongoing evaluation, and continuous collaboration among stakeholders.</jats:p>"
10.2139/ssrn.4426190,"Generative AI in Public Opinion Guidance during Emergency Public Events: Challenges, Opportunities, and Ethical Considerations",N/A
10.1162/qss_a_00285/v2/decision1,"Decision letter for ""Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents""",N/A
10.1007/s12262-024-04091-0,ChatGPT Enigma: Navigating the Jumble of Surgical Generative AI,N/A
10.1101/2024.04.25.24306183,Automating Responses to Patient Portal Messages Using Generative AI,"<jats:title>ABSTRACT</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>Patient portals serve as vital bridges between patients and providers, playing an increasing role in healthcare communication. The rising volume and complexity of these messages is exacerbating physician and nursing burnout. Recent studies have demonstrated that AI chatbots can generate message responses that are viewed favorably by healthcare professionals; however, these studies have not included the diverse range of messages typically found in patient portals. Our goal is to investigate the quality of GPT-generated message responses across the spectrum of message types within a patient portal.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We used novel prompt engineering techniques to craft synthetic responses tailored to adult primary care patients. We enrolled a sample of primary care providers in a cross-sectional study to compare authentic with synthetic patient portal message responses, generated by GPT-4. The survey assessed each message’s empathy, relevance, medical accuracy, and readability on a scale from 0 to 5. Respondents were asked to identify messages that were GPT-generated vs. provider-generated. Mean scores for all metrics were computed for subsequent analysis.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>A total of 49 health care providers participated in the survey (59% completion rate), comprising 16 physicians and 32 advanced practice providers (APPs). When presented with GPT vs. authentic message response pairs, participants correctly identified GPT-generated responses 73% of the time and correctly identified authentic responses 50% of the time. In comparison to messages generated by physicians, GPT-4 generated messages exhibited higher mean scores for empathy (3.57 vs. 3.07, p &lt; 0.001), relevance (3.94 vs. 3.81, p = 0.08) accuracy (4.05 vs. 3.95, p= 0.12) and readability (4.5 vs. 4.13, p &lt; 0.001).</jats:p></jats:sec><jats:sec><jats:title>Limitations</jats:title><jats:p>The study is a single site, single-specialty study, limited due to the use of synthetic data.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>Our findings affirm the potential of GPT-generated patient portal message responses to achieve comparable levels of empathy, relevance, and readability to those found in typical responses according to the health care providers and indicates promising prospects for their integration in the healthcare sector. Additional studies should be done within provider workflows and with careful evaluation of patient attitudes and concerns related to the ethics as well as the quality of generated patient portal message responses in all settings.</jats:p></jats:sec>"
10.58532/v3bhma12p1ch3,"CAPITALIZING ON SOCIAL, RELATIONSHIP &amp; HUMAN CAPITAL WITH GENERATIVE AI","<jats:p>Capitalizing on Social, Relationship &amp; Human Capital with Generative Al: in today's interconnected world, the importance of social, relationship &amp;human capital within organizations cannot be overstated. Organizations that harness the power of these intangible assets effectively obtain a competitive advantage, drive innovation, and foster collaboration. Our chapter explores the transformative power of generative Al solutions in unleashing the potential of social, relationship and human capital. It examines the distinctive characteristics and benefits of such solution and cast light on its potential to unlock new growth opportunities. The chapter also discusses a novel solution that revolutionizes the way organizations capitalize on social, relationship, and human capital. Built on the foundation of generative Al, it combines advanced algorithms with sophisticated data processing techniques to unlock the true potential of these valuable assets. This will assist organizations shift to a more collaborative and nurturing environment and will lead to better people management, decision-making, and brand building. Through community-building and stakeholder engagement it will enhance social sustainability of the organization, leading to improved ESG ratings.</jats:p>"
10.36948/ijfmr.2023.v05i05.7537,The Evolution of Generative AI: Implications for the Media and Film Industry,"<jats:p>This research paper scrutinizes and explores the substantial impact of Artificial Intelligence (AI) and Generative AI on the media and film industry. It delves into the continuously evolving applications of AI algorithms and advanced models, emphasizing their profound implications for content creation, production workflows, and distribution strategies. The paper offers comprehensive insights into the operational mechanics of key AI models, underscoring their direct relevance within the domain of media and film. This inquiry provides a timeless and academic perspective on the transformative influence of AI and Generative AI in these industries, facilitating a deeper understanding of their applications and implications.</jats:p>"
10.2139/ssrn.4621732,Analytic Hierarchy Process in the Field of Cybersecurity Using Generative AI,N/A
10.4018/979-8-3693-2418-9,Impacts of Generative AI on Creativity in Higher Education,N/A
10.31219/osf.io/hdjpk,The Uneven Impact of Generative AI on Entrepreneurial Performance,"<p>Scalable and low-cost AI assistance has the potential to improve firm decision-making and economic performance. However, running a business involves a myriad of open-ended problems, making it difficult to know whether recent AI advances can help business owners make better decisions in real-world markets. In a field experiment with Kenyan entrepreneurs, we assessed the impact of AI advice on small business revenues and profits by randomizing access to a GPT-4-powered AI business assistant via WhatsApp. While we are unable to reject the null hypothesis that there is no average treatment effect, we find the treatment effect for entrepreneurs who were high performing at baseline to be 0.27 standard deviations greater than for low performers. Sub-sample analyses show high performers benefited by just over 15% from the AI assistant, whereas low performers did about 8% worse. This increase in performance inequality does not stem from differences in the questions posed to or advice received from the AI, but from how entrepreneurs selected from and implemented the AI advice they received. More broadly, our findings demonstrate that generative AI is already capable of impacting—though in uneven and unexpected ways—real, open-ended, and unstructured business decisions.</p>"
10.36948/ijfmr.2024.v06i03.23271,Generative Ai in Software Development : an Overview and Evaluation of Modern Coding Tools,"<jats:p>Generative AI has significantly transformed software development by leveraging advanced machine learning models to automate coding tasks, generate code, and enhance productivity. This paper provides an overview and evaluation of modern AI-powered coding tools, including GitHub Copilot, OpenAI Codex, DeepCode, Amazon CodeGuru, TabNine, Kite, and IntelliCode, which use large language models (LLMs) to offer real-time code suggestions, automated error detection, and intelligent code completions. Despite their benefits, these tools face challenges related to accuracy, contextual understanding, security, privacy, and ethical considerations, necessitating thorough review and testing of AI-generated code by developers. The integration of AI in coding also raises concerns about proprietary information protection and ethical implications such as job displacement. This paper explores the capabilities, applications, and limitations of current generative AI tools, highlighting their impact on software development and discussing future directions. Emphasis is placed on the need for improved model training, enhanced contextual understanding, secure AI training methods, and ethical AI usage. By addressing these challenges, the industry can maximize the potential of generative AI, creating more accurate, reliable, and ethically sound tools that support a collaborative and innovative software development environment.</jats:p>"
10.4324/9781003274414-14,Using Generative AI and Automating Your Content,N/A
10.4324/9781003459026-7,The Future of Artificial Intelligence in Education,N/A
10.24059/olj.v28i3.4508,Integrating Generative AI in University Teaching and Learning:  A Model for Balanced Guidelines,"<jats:p>The study proposes a balanced approach and flexible guidelines for incorporating generative artificial intelligence (AI) into university-level teaching and learning processes at both the university-departmental level and within individual academic autonomy. Building on Chan’s (2023) AI Ecological Education Policy Framework, the guidelines offer a suggestive frame of reference for faculty and students to integrate generative AI into their coursework. Furthermore, feedback from 118 students and 14 academics at a teacher education institution in the Philippines underscores the guidelines' potential benefits, concerns, usefulness, and necessity in their academic undertakings. While the policy may not cover every detail exhaustively, it seeks to provide practical and context-sensitive recommendations for ethical, honest, responsible, and fair use of AI in course development, implementation, and student engagement. Consequently, other higher education institutions in general, and academics in particular, may adopt and/or modify the guidelines to suit their positions, goals, needs, and directions.</jats:p>"
10.21125/edulearn.2024.0959,MAKING SENSE OF GENERATIVE AI: SEEKING TO CATALYSE DIGITAL TRANSFORMATION IN EDUCATION,N/A
10.54941/ahfe1004581,The Convergent Validity of Computer Operating Systems’ Usability Evaluation by Popular Generative Artificial Intelligence (AI) Robots,"<jats:p>This article seeks to examine the convergent validity of (and thus the consistency between) computer operating systems’ (OSs’) usability evaluation by a number of popular generative artificial intelligence (AI) robots. Totally 18 popular OS versions were included in the study, they specifically being the various versions of the three leading OS families of Windows, macOS, and Linux. Usability was evaluated in eight major dimensions, namely, (1) effectiveness, (2) efficiency, (3) learnability, (4) memorability, (5) safety, (6) utility, (7) ergonomics, and (8) accessibility. Experimenting with a handful of generative AI robots, Microsoft’s Copilot, Google’s PaLM, and Meta’s Llama managed to individually accord rating scores to the aforementioned eight dimensions. For each robot of this trio, the minimum, the maximum, the range, and the standard deviation of the rating scores for each of the eight dimensions were computed across the OS versions. The rating score difference for each of the eight dimensions between each pair of these robots was calculated for each OS version. The mean of the absolute value, the minimum, the maximum, the range, and the standard deviation of the differences for each dimension between each robot pair were calculated across the OS versions. A paired sample t-test was then applied to each dimension for the rating score difference between each robot pair over the versions. Finally, Cronbach's coefficient alpha () of the rating scores was computed for each dimension between all the three robots across the versions. These computational outcomes were to affirm whether each robot awarded discrimination in evaluating each dimension across the OS versions, whether each robot vis-à-vis any other robots erratically and/or systematically overrate or underrate any dimension over the OS versions, and whether there was high convergent validity of (and thus consistency between) all the three robots in evaluating each dimension across the OS versions. Among other ancillary results, it was found that the convergent validity of the three robots in evaluating all the eight dimensions was high, and thus such evaluation is trustworthy at least to an extent.</jats:p>"
10.33778/kcsa.2024.24.2.009,Security Threats to Enterprise Generative AI Systems and Countermeasures,N/A
10.7230/koscas.2023.72.093,Study on Generative AI Comics - Focusing on the ‘Bestiary Chronicles’ -,N/A
10.4018/979-8-3693-5578-7.ch005,AI Learning and Work Attitude Mediation Between Reward and Organizational Support in Ethiopia,"<jats:p>The chapter's main focus was to analyze the mediator role of employee work attitude and AI-based re-enforcement learning in between employee psychological intrinsic reward system and organization perceived organizational support in context of Ethiopia. Study area was selected as textile industries working in industrial park of Ethiopia on the basis of their contribution to the GDP of the economy. Explanatory research design and quantitative research approach were used. A multistage sampling technique was implied. With AMOS software, exploratory confirmatory analysis was conducted to measure discriminate and convergent validity. It was found that in absence of AI-based re-enforcement learning and employee work attitude as a mediator relation between employee psychological intrinsic reward system and organization perceived organizational support was very weak. Therefore, employee work attitude and reinforcement learning (AI) behave like full mediators between psychological intrinsic reward and organization perceived organizational.</jats:p>"
10.58496/adsa/2023/009,The Considerations of Trustworthy AI Components in Generative AI; A Letter to Editor,"<jats:p>Dear Editor: In navigating the ever-expanding realms of Artificial Intelligence (AI) across diverse domains, particularly within the purviews of Generative AI, it becomes imperative to delve into the intricate considerations of trustworthy AI components [1]. This letter aims to underscore the salient aspects of this discussion with a specific focus on the thematic areas championed by the journal.</jats:p>"
10.47760/ijcsmc.2023.v12i09.002,GENERATIVE AI USE CASES FOR E-COMMERCE,<jats:p>GenAI (Generative AI) is overtaking the world of technology at a fast pace never seen before and continuous to disrupt the Technology and customer experience areas. One of the other advent of the Post COVID era have been the Omnichannel Commerce Experience and The COVID era presented challenges to the Retail giants to explore all their armors and bring the best user options and tie them together to create a unique experience of the technology boosted Digital Commerce and Fulfillment systems with a touch to the In Store Shopping satisfaction. The seamless experience of the Omnichannel Commerce can be enabled with powers of enormous data-based Models and intelligence driven by GenAI. Classic AI has been in use by the retailers till now and has shown significant growth and boost to a unified experience for the Retailers and their customers. It is worth to discuss the significance of the GenAI on different aspects and how it can shape the Omnichannel commerce and below are few uses cases.</jats:p>
10.2139/ssrn.4571867,Benchmarking Generative AI: A Comparative Evaluation and Practical Guidelines for Responsible Integration into Academic Research,N/A
10.47611/jsrhs.v12i3.4780,The Impact of Generative AI on Human Productivity in Creative Writing,"<jats:p>With the innovative field of generative artificial intelligence having advanced at an incredibly rapid rate, its applications are of the utmost priority to study. The purpose of this study is to determine whether or not generative AI can help increase human productivity, specifically in writing. We hypothesized that generative AI will have a positive impact on human productivity. In order to test this, we employed the use of participants to write short fictional stories, one with the help of AI and one without. They were provided with survey questions that helped assess any changes in productivity levels. The productivity of the participants was also analyzed in terms of grammar, spelling, and consistency while compared against time . With the results obtained, we hoped to assess how AI can impact productivity in creative tasks (i.e., writing, art). We also hoped to understand its broader applications for human use and potential benefits and caveats to using generative AI. Based on the results, we concluded that using generative AI did indeed improve writing productivity as it lowered the amount of errors and shortened the time taken. However, how productive the individual was in producing quality work of their own merit also depended on how much work they delegated to the generative AI as well as how they perceived it.    </jats:p>"
10.12781/978-1-907549-48-9-5,Generative Teams: Going Beyond High Performance,"<jats:p>Al cambiar nuestra atención de un enfoque tradicional centrado en el desempeño individual al de reconocer y apreciar la entidad única del equipo, el ‘nosotros’, aumentamos nuestra capacidad para descubrir y aprovechar su inherente inteligencia, fortalezas y todo su potencial generativo. Para lograr este cambio, el presente artículo propone utilizar como agente catalizador la integración de una mirada sistémica del equipo con un estado mental generativo y el seguimiento continuo del impacto de esta integración por medio de un modelo bidimensional que desarrolle su productividad y positividad. By shifting our attention from a traditional focus on individual performance to recognising and appreciating the unique entity of the team – the ‘we’ – we increase our ability to discover and harness its inherent intelligence, strengths and all of its generative potential. In order to achieve this shift, this article proposes using as a catalyst the integration of a systemic view of the team with a generative state of mind, and the continued monitoring of the impact of this integration through a two-dimensional model that enhances the team’s productivity and positivity.</jats:p>"
10.13052/jwe1540-9589.2322,Enhancing English Language Education Through Big Data Analytics and Generative AI,"<jats:p>This research paper provides a comprehensive examination of the significant impact of big data analytics and generative artificial intelligence (GAI) on the field of English language education. Utilizing a meticulous framework rooted in the evolutionary network influence of big data, our study critically analyzes several aspects of student engagement, learning motivation, self-efficacy, and the existing disparities among learners. Our primary objective is to enhance students’ active participation, intrinsic interest, and self-confidence in the context of English language learning, thus advancing their overall linguistic competence. To achieve these objectives, our study systematically integrates the concept of practice education with a multidisciplinary approach, leveraging the power of big data analysis and GAI, and reveals profound insights into student learning behaviors, preferences, and personalized educational needs. We employ advanced techniques for meticulous data processing and interpretation, empowering educators to make data-informed decisions and tailor pedagogical strategies to meet the unique requirements of each student. This data-driven pedagogical approach not only facilitates the implementation of effective teaching methodologies but also effectively addresses the disparities stemming from diverse student backgrounds, thereby fostering a more inclusive and personalized learning environment.</jats:p>"
10.31234/osf.io/hm54g,Generative AI are More Truth-Biased than Humans: A Replication and Extension of Core Truth-Default Theory Principles,"<p>Human communication requires cooperative partners for it to be effective and efficient. A result of this requirement is the truth-bias, defined as the perception that others are honest independent of actual message veracity. Does the truth-bias extend to technology like generative Artificial Intelligence (AI)? Drawing on truth-default theory (TDT), we had humans and three chatbots running different large language models — ChatGPT (GPT-3.5), Bard (LaMDA), ChatSonic (GPT-4) — make nearly 1,000 veracity judgments across three prompts. Consistent with TDT, human detection accuracies were near chance (50%-53%) with notable truth-biases (59%-64%). Critically, AI had a substantially greater truth-bias than humans (67%-99%), even after providing AI with a genuine lie-truth base-rate. GPT-4 was also truth-default, not suspecting any deception across samples when veracity assessments were unprompted. These data replicate the idea that people judge most information to be true, and such evidence also extends to artificial intelligence.</p>"
10.1007/s00345-023-04622-6,Generative AI: in rescue of healthcare reformation,N/A
10.14445/22312803/ijctt-v71i6p109,Impact of Generative AI on Data Integration,N/A
10.14445/22312803/ijctt-v72i7p103,Go-To-Market Transformation with Generative AI,N/A
10.4018/979-8-3693-3278-8,The Pioneering Applications of Generative AI,N/A
10.14361/9783839474723-001,INDEX,N/A
10.54941/ahfe1005099,Redefining Emergency Services with Generative AI: Insights from a preliminary literature review,"<jats:p>The rapid evolution of artificial intelligence (AI) technologies, particularly generative AI (GenAI), has opened new frontiers in various sectors including emergency services. We present the results of a preliminary literature review of the current research on generative AI in emergency departments with a focus on use cases and their implications. We systematically examine peer-reviewed articles, case studies, and practical implementations to identify key trends, challenges, and opportunities in this growing field. Our findings underscore the need for ongoing research, ethical considerations, and cross-sector collaboration to fully leverage AI's capabilities to enhance the effectiveness and efficiency of emergency response.</jats:p>"
10.32388/k9scnu,"Review of: ""Digital Persona: Reflection on the Power of Generative AI for Customer Profiling in Social Media Marketing""",N/A
10.1002/icd.2514/v2/decision1,"Decision letter for ""Perspectives on the impact of generative &lt;scp&gt;AI&lt;/scp&gt; on early‐childhood development and education""",N/A
10.1108/shr-05-2024-0039,Generative AI and training employees with special needs,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>The viewpoint paper aims to highlight the assistive role that Generative artificial intelligence (Gen AI) can play in the design of learning and development programs for employees with special needs. The article discusses the challenges, benefits and reasons why Gen AI should be used to manage diversity, equity and inclusion by creating personalized and customized training and development programs.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>The viewpoint paper is based on reviewing articles and videos on the application of Gen AI in learning and development.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>Gen AI offers immense opportunities to design personalized learning solutions for employees with special needs due to disability that can be physical or cognitive. The AI-based solutions support special learners by customizing assistive technology-based solutions and content based on the level of disability and need of the learner. This paper also highlights the importance of synergy between the training department, government and technology solution providers.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>The viewpoint paper fills in an important gap by discussing the role that Gen AI can play by facilitating the learning and development of employees with unique skills.</jats:p>
</jats:sec>"
10.2139/ssrn.4908364,Reinventing Intellectual Property Rights Protection in an Era of Generative AI: The Synergy of Polycentric Governance and Blockchain Technology,N/A
10.2139/ssrn.4921128,Generative Ai-Driven Edge Intelligence for Optimizing Hybrid Renewable Energy Production,N/A
10.31031/cojra.2024.04.000577,"""Generative AI: A Promising, but Cautious, Ally in Children’s Mental Health""",N/A
10.3997/2214-4609.202439086,Leveraging Generative AI to Simplify Spatial Analysis and Visualization of Digital Twins,N/A
10.1145/3632523,Generative AI and CS Education,<jats:p>Increasing knowledge sharing between industry and academia.</jats:p>
10.12781/978-1-907549-22-9-3,Appreciative and Generative Perspectives Points of Convergence in a Productive Dialogue,N/A
10.2139/ssrn.4547208,Let’s Ask the Authors! 'Did You Use Generative AI to Write Your Manuscript?',N/A
10.1109/educon54358.2023.10125121,Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education,N/A
10.2196/preprints.48949,Cocreating an Automated mHealth Apps Systematic Review Process With Generative AI: Design Science Research Approach (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>The use of mobile devices for delivering health-related services (mobile health [mHealth]) has rapidly increased, leading to a demand for summarizing the state of the art and practice through systematic reviews. However, the systematic review process is a resource-intensive and time-consuming process. Generative artificial intelligence (AI) has emerged as a potential solution to automate tedious tasks.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>This study aimed to explore the feasibility of using generative AI tools to automate time-consuming and resource-intensive tasks in a systematic review process and assess the scope and limitations of using such tools.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>We used the design science research methodology. The solution proposed is to use cocreation with a generative AI, such as ChatGPT, to produce software code that automates the process of conducting systematic reviews.</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>A triggering prompt was generated, and assistance from the generative AI was used to guide the steps toward developing, executing, and debugging a Python script. Errors in code were solved through conversational exchange with ChatGPT, and a tentative script was created. The code pulled the mHealth solutions from the Google Play Store and searched their descriptions for keywords that hinted toward evidence base. The results were exported to a CSV file, which was compared to the initial outputs of other similar systematic review processes.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>This study demonstrates the potential of using generative AI to automate the time-consuming process of conducting systematic reviews of mHealth apps. This approach could be particularly useful for researchers with limited coding skills. However, the study has limitations related to the design science research methodology, subjectivity bias, and the quality of the search results used to train the language model.</p>
                </sec>"
10.21428/2c646de5.20c92601,Integrals and Integrity: Generative AI Tries to Learn Cosmology,N/A
10.2139/ssrn.4366749,How ChatGPT and Generative AI Systems will Revolutionize Legal Services and the Legal Profession,N/A
10.4324/9781003459026-1,Introduction to Artificial Intelligence in Higher Education,N/A
10.25172/smustlr.26.2.4,Generative AI Art: Copyright Infringement and Fair Use,"<jats:p>The discussion of AI copyright infringement or fair use often skips over all the required steps of the infringement analysis in order to focus on the most intriguing question, “Could a visual generative AI generate a work that potentially infringes a preexisting copyrighted work?” and then the discussion skips further ahead to, “Would the AI have a fair use defense, most likely under the transformative test?” These are relevant questions, but without considering the actual steps of the copyright infringement analysis, the discussion is misleading or even irrelevant. This neglecting of topics and stages of the infringement analysis fails to direct our attention to a properly accused party or entity whose actions prompt the question. Making a sudden transition from a question of infringement in the creation of training datasets to the creation of foundation models that draw from the training data to the actual operation of the generative AI system to produce images makes a false equivalency regarding the processes themselves and the persons responsible for them. The questions ought to shift focus from the persons compiling the training dataset used to train the AI system and the designers and creators of the AI system itself to the end users of the AI system who conceive of and cause the creation of images.

The analysis of infringement or fair use in the generative AI context has suffered from widespread misunderstanding concerning the generative AI processes and the control and authorship of the end-user. Claimants, commentators, and regulators have made incorrect assumptions and inaccurate simplifications concerning the process, which I refer to as the Magic File Drawer theory, the Magic Copy Machine theory, and the Magic Box Artist theory. These theories, if they were true, would be much easier to envision and understand than the actual science and technology that goes into the creation and operation of a contemporary visual generative AI system. Throughout this Article, I will attempt to clarify and correct the understanding of the science and technology of the generative AI processes and explain the different roles of the training dataset designers, the generative AI system designers, and the end-users in the rendering of visual works by a generative AI system. Part II will discuss the requirements of a claim of copyright infringement including each step from the copyrightability of the claimant’s work, the doctrines that limit copyrightability, the requirement of an act of copying, and the infringement elements. Part III will summarize the copyright fair use test paying particular attention to the purpose and character of the use analysis, 17 U.S.C. § 107(1), and the current interpretation of the “transformative” test after Andy Warhol Foundation v. Goldsmith, particularly in circumstances relating to technology and the use of copyrighted or copyrightable data sources. Part IV will analyze potential infringement or fair use by the creators of generative AI training datasets. Part V will analyze potential infringement or fair use by the creators of visual generative AI systems. Part VI will analyze potential infringement or fair use by the end-users of visual generative AI systems.

For all their complexity, visual generative AI systems are tools that depend on an end-user who conceives of and designs the image and provides the system with a prompt to set the generative process in motion. The end-users are responsible for crafting the prompt or series of prompts used, for evaluating the outputs of the generative AI, for adjusting and editing the iterations of images offered by the AI system, and ultimately for selecting and adopting one of the images generated by the AI as the final image. The end-users then make further decisions about the actual use and its function and purpose for the images the end-users selected and adopted from the outputs of the AI. While working with the AI tool to try to produce a certain image, an end-user might steer the system to produce a work that could, under an infringement analysis, be regarded as potentially infringing, which would lead us again to the fair use analysis based on the end-user’s use of the image.</jats:p>"
10.1007/978-3-031-67991-9,Generative AI and Education,N/A
10.61737/xsgm9843,Interdisciplinary Dialogues: The Major Risks of Generative AI,"<jats:p>In an exciting series of Interdisciplinary Dialogues on the societal impacts of AI, we invite a guest speaker and panellists from the fields of science and engineering, health and humanities and social sciences to discuss the advances, challenges and opportunities raised by AI.  The first dialogue in this series began with Yoshua Bengio, who, concerned about developments in generative AI and the major risks they pose for society, initiated the organization of a conference on the subject. The event took place on August 14, 2023 in Montreal, and was aimed at initiating collective, interdisciplinary reflection on the issues and risks posed by recent developments in AI. The conference took the form of a panel, moderated by Juliette Powell, to which seven specialists were invited who cover a variety of disciplines, including: computer science (Yoshua Bengio and Golnoosh Farnadi), law (Caroline Lequesne and Claire Boine), philosophy (Jocelyn Maclure), communication (Sonja Solomun) and political science (Hugo Loiseau). This document is the result of this first interdisciplinary dialogue on the societal impacts of AI. The speakers were invited to respond concisely, in the language of their choice, to questions raised during the event.  Immerse yourself in reading these fascinating conversations, presented in a Q&amp;A format that transcends disciplinary boundaries. The aim of these dialogues is to offer a critical and diverse perspective on the impact of AI on our everchanging world.</jats:p>"
10.21125/edulearn.2024.1077,EMPOWERING EDUCATORS WITH GENERATIVE AI: THE GENAI EDUCATION FRONTIER INITIATIVE,N/A
10.34068/joe.61.02.13,Ensuring Responsible and Transparent Use of Generative AI in Extension,"<jats:p>Generative artificial intelligence (AI) systems capable of generating human-like text, images, and ideas from existing data based on user-defined prompts, will inevitably impact Extension, including increasing efficiency, productivity, and performing tasks previously exclusive to humans. There are ethical and risk-related considerations surrounding the use of generative AI, including concerns about bias and unintended consequences. It is important for Extension to consider these implications and take steps to ensure that generative AI is used in a responsible and transparent manner. Extension must ensure that educators and staff have the necessary knowledge and skills to effectively utilize and integrate this technology.</jats:p>"
10.1016/j.psychres.2024.115955,Generative AI for precision neuroimaging biomarker development in psychiatry,N/A
10.51191/issn.2637-1898.2024.7.12.12,Artificial Intelligence: Duality in Applications of Generative AI and Assistive AI in Music,"<jats:p>This paper explores the multifaceted role of Artificial Intelligence in the field of music, more specifically, examining the positives and negatives of generative and assistive capacities. Artificial Intelligence (AI) in music involves the application of computational techniques to various aspects of music creation, production and consumption. In the domain of assistive AI, the concentration is on how machine learning could potentially help musicians in the area of composition and performance to enhance their musical creativity. The paper will discuss an interesting collaborative effort between pure human creativity and computational assistance covering an explanation for a vast number of tools using generative as well as assistive artificial intelligence models.

In addition, the paper will address the concerns facing the music industry while this technology keeps on improving, the potential drawbacks and ethical considerations. It opens the question of authenticity and emotional depth, and when or if this new technology could be able to replicate it. Further explanation in the paper will consider music examples with a focus on music styles assisted and generated by the use of artificial intelligence, from pop to classical music.

With a thorough analysis of the aforementioned subject, the paper aims to provide a detailed perspective on the constant evolution of AI tools used in music with highlights on the need for a balanced approach. In providing a detailed perspective on the evolving landscape of AI tools in music, this study adopts a methodological approach that involves comprehensive analysis of both the benefits and challenges associated with these innovative gadgets. The paper contributes to the ongoing discussion on the intersection of technology and artistic expression. By examining the potential benefits and challenges with these innovative models, the paper signifies ongoing discourse on the impact of technology on artistic expression.</jats:p>"
10.1016/j.soilbio.2024.109510,Mitigating generative AI inaccuracies in soil biology,N/A
10.56726/irjmets60823,GENERATIVE AI AND SECURITY: TRANSFORMING CYBER SECURITY DEFENSE AND FRAUD PREVENTION,N/A
10.62441/nano-ntp.v20i4.5,The Role of Generative AI in Enhancing Social Media Engagement: A Comparative Study with Traditional Content Creation,N/A
10.2139/ssrn.4491342,Exploring Generative AI for Modeling the Dynamics of Asset Price Process,N/A
10.15444/gfmc2024.01.01.00,GENERATIVE AI FOR CREATIVE FASHION DESIGN,N/A
10.5040/9781509974979.ch-003,Under the Hood,N/A
10.14361/9783839474723-fm,Frontmatter,N/A
10.1016/j.jslw.2023.101067,Generative AI: Same same but different?,N/A
10.2139/ssrn.4527461,"Should We Ban Generative AI, Incentivise it or Make it a Medium for Inclusive Creativity?",N/A
10.1364/opn.34.10.000026,Generative AI Meets Scientific Publishing,"<jats:p>Will tools like ChatGPT help or impede the process of
				disseminating reliable, peer-reviewed scientific information? It’s
				complicated …</jats:p>"
10.2139/ssrn.4911710,Generative AI and Democracy: the synthetification of public opinion and its impacts,N/A
10.3030/101144749,N/A,N/A
10.1145/3593013.3593981,The Gradient of Generative AI Release: Methods and Considerations,N/A
10.36227/techrxiv.170775280.04186102/v1,At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence,N/A
10.1007/979-8-8688-0282-9_6,Exploring Chatbot Technologies,N/A
10.1038/s41415-023-6041-0,A brief tutorial on generative AI,N/A
10.2139/ssrn.4521754,The Consequences of Generative AI for UGC and Online Community Engagement,N/A
10.1109/te.2024.3435427/mm1,Q-Module-Bot: A Generative AI-Based Question and Answer Bot for Module Teaching Support_supp1-3435427.docx,N/A
10.1016/b978-0-443-28979-8.00007-3,Journal of Negative Data for the days of generative AI-assisted publication,N/A
10.1038/s41598-024-61221-0,The consequences of generative AI for online knowledge communities,"<jats:title>Abstract</jats:title><jats:p>Generative artificial intelligence technologies, especially large language models (LLMs) like ChatGPT, are revolutionizing information acquisition and content production across a variety of domains. These technologies have a significant potential to impact participation and content production in online knowledge communities. We provide initial evidence of this, analyzing data from Stack Overflow and Reddit developer communities between October 2021 and March 2023, documenting ChatGPT’s influence on user activity in the former. We observe significant declines in both website visits and question volumes at Stack Overflow, particularly around topics where ChatGPT excels. By contrast, activity in Reddit communities shows no evidence of decline, suggesting the importance of social fabric as a buffer against the community-degrading effects of LLMs. Finally, the decline in participation on Stack Overflow is found to be concentrated among newer users, indicating that more junior, less socially embedded users are particularly likely to exit.</jats:p>"
10.15308/sinteza-2024-45-50,Exploring the Application of Generative AI by YouTube Content Creators,N/A
10.21125/edulearn.2024.0382,EXPLORING THE ROLE OF GENERATIVE AI IN TRANSFORMING THE ANALYTICS EDUCATION IN BUSINESS SCHOOL,N/A
10.1007/s00146-024-01862-x,Blind search and flexible product visions: the sociotechnical shaping of generative music engines,"<jats:title>Abstract</jats:title><jats:p>Amidst the surge in AI-oriented commercial ventures, music is a site of intensive efforts to innovate. A number of companies are seeking to apply AI to music production and consumption, and amongst them several are seeking to reinvent the music listening experience as adaptive, interactive, functional and infinitely generative. These are bold objectives, having no clear roadmap for what designs, technologies and use cases, if any, will be successful. Thus each company relies on speculative product visions. Through four case studies of such companies, I consider how product visions must carefully provide a clear plan for developers and investors, whilst also remaining open to agile user-centred product development strategies, which I discuss in terms of the ‘blind search’ nature of innovation. I suggest that innovation in this area needs to be understood in terms of technological emergence, which is neither technologically determinist nor driven entirely by the visions of founders, but through a complex of interacting forces. I also consider, through these cases, how, through the accumulation of residual value, all such start-up work risks being exapted for more familiar extractive capitalist agendas under the general process that Doctorow calls “enshittification”. Lastly, I consider a number of other more specific ways in which these projects, if their growth is achieved, could influence music culture more broadly.</jats:p>"
10.4018/979-8-3693-1565-1.ch005,Ethical Navigations,"<jats:p>In an era driven by digital innovation, artificial intelligence (AI), or Generative AI, emerges as a transformative force reshaping the landscape of higher education. Its potential to personalize learning experiences, bolster research capacities, and streamline administrative operations is revolutionary. However, the integration of Open AI into academia raises complex ethical issues for faculty and learners. The need for comprehensive ethical guidelines is imperative to ensure that the integration and utilization of AI in higher education are aligned with the core values of academic integrity and social responsibility. This chapter examines the ethical frameworks essential for governing the use of generative AI technologies in academia and provides practical recommendations for stakeholders involved. Additionally, emerging AI technologies such as experimental NotebookLM and Gemini will be discussed as future directions for AI use in teaching, learning, and research.</jats:p>"
10.3126/eltp.v9i1-2.68716,Generative AI and AI Tools in English Language Teaching and Learning: An Exploratory Research,"<jats:p>Generative AI (GenAI) tools such as ChatGPT, Gemini and Copilot have created concerns in academia, particularly after the launch of ChatGPT. GenAI and AI have been the buzz words and academics are discussing about the possibilities of its positive and negative impacts on educations and research. Recently, studies have been conducted on the influence of GenAI tools in education and research. With the above concerns and the impact of GenAI, grounded on Vygotsky's Zone of Proximal Development (ZPD) as a theoretical lens, this study explores how English language teachers integrate GenAI tools to enhance teaching and learning. Particularly, this study explores the integration of GenAI tools in English language teaching and learning, focusing on teaching efficiency, student engagement, personalized learning, and writing skills, subscribing to exploratory research methods grounded on semi-structured interviews. The findings of the study affirmed the positive impact of GenAI tools on teaching efficiency, students’ engagement, and writing skills. The results indicated that GenAI positively influences teaching efficiency and student engagement in learning. The implications of this research highlighted the potential of GenAI tools to create a more intelligent and personalized learning environment for English language teaching that benefits both educators and learners. </jats:p>"
10.47363/jaicc/2023(2)245,The AI-powered PM Toolkit - A Project Manager’s Guide to Thrive the Generative AI Wave,"<jats:p>Project management stands at the precipice of a significant transformation driven by Generative AI (GenAI). This paper delves into the profound impact of GenAI on project managers, exploring how it reshapes roles, responsibilities, and the future of the field. We begin by outlining the evolution of AI, highlighting the emergence of GenAI and its ability to go beyond analysis and create novel content. GenAI will transform project management responsibilities from fully automated activities to a blend of assisted and augmented tasks, requiring collaboration and strategic application of this innovative technology. The paper explores the evolution of project management methodologies and the evolving skillset required of successful project managers in the age of AI and offers a ""Project Manager's Guide to Thriving in the Generative AI Tsunami"" by outlining the three pillars of the PMI Talent Triangle: Ways of Working, Power Skills, and Business Acumen. Each pillar is examined in detail, highlighting the role of GenAI in enhancing mastery and providing illustrative examples of AI-powered tools and frameworks. This paper equips project managers with the knowledge and tools to not only survive but thrive in the age of GenAI, ensuring their continued value and leadership in shaping successful projects.</jats:p>"
10.1007/979-8-8688-0447-2_3,A (Brief) Introduction to Responsible AI,N/A
10.1007/979-8-8688-0318-5_1,Introducing Brain Rush,N/A
10.36837/chapman.000611,Advancement in In-Silico Drug Discovery from Virtual Screening Molecular Dockings to De-Novo Drug Design Transformer-based Generative AI and Reinforcement Learning,N/A
10.2139/ssrn.4695119,A saviour or a dead end? Reservation of rights in the age of generative AI,N/A
10.56028/aemr.6.1.36.2023,Analysis of the Application of Generative AI in Business Management,"<jats:p>In the era of big data, many companies have already implemented RPA and AI as supporting tools for business management, which can effectively reduce the duplication of work by management and finance-related personnel, allowing them to devote more energy to high value-added work content. Since last year, generative AI, represented by ChatGPT and AI painting, has gradually come into the public's view, and even though it still has some shortcomings at this stage, its ability to ""think"" and ""create"" is different from traditional AI tools, and users are amazed. Users are amazed. The process of business management also needs to be creative and logical, and the emergence of generative AI is certainly a good fit for this need. In this paper, we will briefly discuss the future application of generative AI in business management and the issues that managers still need to address in order to fully utilize the capabilities of generative AI.</jats:p>"
10.1101/2023.11.01.23297938,Evaluating the Multimodal Capabilities of Generative AI in Complex Clinical Diagnostics,"<jats:title>Abstract</jats:title><jats:p>In the rapidly evolving landscape of artificial intelligence (AI) in healthcare, the study explores the diagnostic capabilities of Generative Pre-trained Transformer 4 Vision (GPT-4V) in complex clinical scenarios involving both medical imaging and textual patient data. Conducted over a week in October 2023, the study employed 93 cases from the New England Journal of Medicine’s image challenges. These cases were categorized into four types based on the nature of the imaging data, ranging from radiological scans to pathological slides. GPT-4V’s diagnostic performance was evaluated using multimodal inputs (text and image), text-only, and image-only prompts. The results indicate that GPT-4V’s diagnostic accuracy was highest when provided with multimodal inputs, aligning with the confirmed diagnoses in 80.6% of cases. In contrast, text-only and image-only inputs yielded lower accuracies of 66.7% and 45.2%, respectively (after correcting for random guessing: multimodal: 70.5 %, text only: 54.3 %, image only: 29.3 %). No significant variation was observed in the model’s performance across different types of images or medical specialties. The study substantiates the utility of multimodal AI models like GPT-4V as potential aids in clinical diagnostics. However, the proprietary nature of the model’s training data and architecture warrants further investigation to uncover biases and limitations. Future research should aim to corroborate these findings with real-world clinical data while considering ethical and privacy concerns.</jats:p>"
10.36227/techrxiv.170775280.04186102/v2,At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence,"<jats:p id=""p1"">As we transition from the 5G epoch, a new horizon beckons with the
advent of 6G, seeking a profound fusion with novel communication
paradigms and emerging technological trends, bringing once-futuristic
visions to life along with added technical intricacies. Although
analytical models lay the foundations and offer systematic insights, we
have recently witnessed a noticeable surge in research suggesting
machine learning (ML) and artificial intelligence (AI) can efficiently
deal with complex problems by complementing or replacing model-based
approaches. The majority of data-driven wireless research leans heavily
on discriminative AI (DAI) that requires vast real-world datasets.
Unlike the DAI, Generative AI (GenAI) pertains to generative models
(GMs) capable of discerning the underlying data distribution, patterns,
and features of the input data. This makes GenAI a crucial asset in
wireless domain wherein real-world data is often scarce, incomplete,
costly to acquire, and hard to model or comprehend. With these appealing
attributes, GenAI can replace or supplement DAI methods in various
capacities. Accordingly, this combined tutorial-survey paper commences
with preliminaries of 6G and wireless intelligence by outlining
candidate 6G applications and services, presenting a taxonomy of
state-of-the-art DAI models, exemplifying prominent DAI use cases, and
elucidating the multifaceted ways through which GenAI enhances DAI.
Subsequently, we present a tutorial on GMs by spotlighting seminal
examples such as generative adversarial networks, variational
autoencoders, flow-based GMs, diffusion-based GMs, generative
transformers, large language models, autoregressive GMs, to name a few.
Contrary to the prevailing belief that GenAI is a nascent trend, our
exhaustive review of approximately 120 technical papers demonstrates the
scope of research across core wireless research areas, including 1)
physical layer design; 2) network optimization, organization, and
management; 3) network traffic analytics; 4) cross-layer network
security; and 5) localization &amp; positioning. Furthermore, we outline
the central role of GMs in pioneering areas of 6G network research,
including semantic communications, integrated sensing and
communications, THz communications, extremely large antenna arrays,
near-field communications, digital twins, AI-generated content services,
mobile edge computing and edge AI, adversarial ML, and trustworthy AI.
Lastly, we shed light on the multifarious challenges ahead, suggesting
potential strategies and promising remedies. Given its depth and
breadth, we are confident that this tutorial-cum-survey will serve as a
pivotal reference for researchers and professionals delving into this
dynamic and promising domain.</jats:p>"
10.31274/itaa.17921,What is the Future of Fashion Retailing with Generative AI?  Understanding Consumer Response through Twitter Data,N/A
10.1109/candarw60564.2023.00059,Extraction of Subjective Information from Generative AI Models,N/A
10.14305/jn.29960819.2024.1.1.01,In a Digital World With Generative AI Detection Will Not be Enough,"<jats:p>Recent and dramatic improvements in AI driven large language models (LLMs), image generators, audio and video have fed an exponential growth in Generative AI applications and accessibility. The disruptive ripples of this rapid evolution have already begun to fundamentally impact how we create and consume content on a global scale. While the use of Generative AI has and will continue to enable massive increases in the speed and efficiency of content creation, it has come at the cost of uncomfortable conversations about transparency and the erosion of digital trust. To have any chance at actually diminishing the societal impact of digital disinformation in an age of generative AI, approaches strategically designed to assist human decision making must move past simple detection and provide more robust solutions.</jats:p>"
10.52214/stlr.v25i2.12762,Focusing On Fine-Tuning,"<jats:p>Those who design and deploy generative AI models, such as Large Language Models like GPT-4 or image diffusion models like Stable Diffusion, can shape model behavior in four distinct stages: pretraining, fine-tuning, in-context learning, and input and output filtering. The four stages differ among many dimensions, including cost, access, and persistence of change. Pretraining is always very expensive and in-context learning is nearly costless. Pretraining and fine-tuning change the model in a more persistent manner, while in-context learning and filters make less durable alterations. These are but two of many such distinctions reviewed in this Essay.
Legal scholars, policymakers, and judges need to understand the differences between the four stages as they try to shape and direct what these models do. Although legal and policy interventions can (and probably will) occur during all four stages, many will best be directed at the fine-tuning stage. Fine-tuning will often represent the best balance between power, precision, and disruption of the approaches.</jats:p>"
10.1515/9783111323749,Toward Artificial General Intelligence,N/A
10.2139/ssrn.4465446,Exploring the Multifaceted Nature of Generative AI in Journalism Studies: A Typology of Scholarly Definitions,N/A
10.1162/qss_a_00285/v1/review1,"Review for ""Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents""",N/A
10.1109/icapai61893.2024.10541170,Generative AI: Threatening Established Human Rights Instruments at Scale,N/A
10.1201/9781003458227-4,Integrating Generative AI and Blockchain Technologies to Create Musical Objects with Agency,N/A
10.3389/fpsyt.2024.1346059,Beyond Discrimination: Generative AI Applications and Ethical Challenges in Forensic Psychiatry,"<jats:p>The advent and growing popularity of generative artificial intelligence (GenAI) holds the potential to revolutionise AI applications in forensic psychiatry and criminal justice, which traditionally relied on discriminative AI algorithms. Generative AI models mark a significant shift from the previously prevailing paradigm through their ability to generate seemingly new realistic data and analyse and integrate a vast amount of unstructured content from different data formats. This potential extends beyond reshaping conventional practices, like risk assessment, diagnostic support, and treatment and rehabilitation plans, to creating new opportunities in previously underexplored areas, such as training and education. This paper examines the transformative impact of generative artificial intelligence on AI applications in forensic psychiatry and criminal justice. First, it introduces generative AI and its prevalent models. Following this, it reviews the current applications of discriminative AI in forensic psychiatry. Subsequently, it presents a thorough exploration of the potential of generative AI to transform established practices and introduce novel applications through multimodal generative models, data generation and data augmentation. Finally, it provides a comprehensive overview of ethical and legal issues associated with deploying generative AI models, focusing on their impact on individuals as well as their broader societal implications. In conclusion, this paper aims to contribute to the ongoing discourse concerning the dynamic challenges of generative AI applications in forensic contexts, highlighting potential opportunities, risks, and challenges. It advocates for interdisciplinary collaboration and emphasises the necessity for thorough, responsible evaluations of generative AI models before widespread adoption into domains where decisions with substantial life-altering consequences are routinely made.</jats:p>"
10.5539/ies.v17n5p22,Exploring Perspectives of Teacher Students Toward Generative AI Technologies,"<jats:p>The relevance of incorporating AI into educational settings is growing as the technology develops. This research delves into the perspective of pre-service teachers on generative AI technology. This study employed 45 pre-service teachers to raise their perspective towards AI technology and then explore its correlation about components of AI technology by their perspectives. The findings revealed that they had perspective towards artificial intelligence technology at slightly accepted level, perspective towards willingness to use AI technology at moderately accepted and very much accepted levels, and perspective towards concerns about artificial intelligence technology at moderately accepted and slightly accepted levels. These results have important things that it can be implied to pedagogy, curriculum development, and teacher training programs. </jats:p>"
10.7551/mitpress/8975.003.0007,Notes,N/A
10.1016/b978-0-443-18906-7.00004-0,AI + health ethics,N/A
10.1109/ethics57328.2023.10155081,The potential for co-operatives to mitigate AI ethics catastrophes: perspectives from media analysis,N/A
10.1007/s43681-022-00183-3,The statistical fairness field guide: perspectives from social and formal sciences,"<jats:title>Abstract</jats:title><jats:p>Over the past several years, a multitude of methods to measure the fairness of a machine learning model have been proposed. However, despite the growing number of publications and implementations, there is still a critical lack of literature that explains the interplay of fair machine learning with the social sciences of philosophy, sociology, and law. We hope to remedy this issue by accumulating and expounding upon the thoughts and discussions of fair machine learning produced by both social and formal (i.e., machine learning and statistics) sciences in this field guide. Specifically, in addition to giving the mathematical and algorithmic backgrounds of several popular statistics-based fair machine learning metrics used in fair machine learning, we explain the underlying philosophical and legal thoughts that support them. Furthermore, we explore several criticisms of the current approaches to fair machine learning from sociological, philosophical, and legal viewpoints. It is our hope that this field guide helps machine learning practitioners identify and remediate cases where algorithms violate human rights and values.</jats:p>"
10.1007/s43681-022-00140-0,How a non-conscious robot could be an agent with capacity for morally responsible behaviour,"<jats:title>Abstract</jats:title><jats:p>People have different opinions about which conditions robots would need to fulfil—and for what reasons—to be moral agents. Standardists hold that specific internal states (like rationality, free will or phenomenal consciousness) are necessary in artificial agents, and robots are thus not moral agents since they lack these internal states. Functionalists hold that what matters are certain behaviours and reactions—independent of what the internal states may be—implying that robots can be moral agents as long as the behaviour is adequate. This article defends a standardist view in the sense that the internal states are what matters for determining the moral agency of the robot, but it will be unique in being an internalist theory defending a large degree of robot responsibility, even though humans, but not robots, are taken to have phenomenal consciousness. This view is based on an event-causal libertarian theory of free will and a revisionist theory of responsibility, which combined explain how free will and responsibility can come in degrees. This is meant to be a middle position between typical compatibilist and libertarian views, securing the strengths of both sides. The theories are then applied to robots, making it possible to be quite precise about what it means that robots can have a certain degree of moral responsibility, and why. Defending this libertarian form of free will and responsibility then implies that non-conscious robots can have a stronger form of free will and responsibility than what is commonly defended in the literature on robot responsibility.</jats:p>"
10.5363/tits.29.1_40,How We Associate With Generative AI?,N/A
10.7551/mitpress/8975.003.0003,Introduction,N/A
10.7551/mitpress/8975.003.0002,Acknowledgments,N/A
10.1103/physics.13.107,"Physicists Must Engage with AI Ethics, Now",N/A
10.1007/s00146-017-0760-1,Social choice ethics in artificial intelligence,N/A
10.1145/3461702.3462520,Alienation in the AI-Driven Workplace,N/A
10.56297/buka4060/vrro1747,POTENTIAL AFFORDANCES OF GENERATIVE AI IN LANGUAGE EDUCATION:  DEMONSTRATIONS AND AN EVALUATIVE FRAMEWORK,N/A
10.1038/d41586-023-01693-8,Social media: generative AI could harm mental health,N/A
10.5772/intechopen.1005402,"Generative AI in Education: Technical Foundations, Applications, and Challenges","<jats:p>Generative artificial intelligence (AI) (GenAI) has emerged as a transformative force in various fields, and its potential impact on education is particularly profound. This chapter presents the development trends of “GenAI in Education” by exploring the technical background, diverse applications, and multifaceted challenges associated with its adoption in education. The chapter briefly introduces the technical background of GenAI, particularly the development of large language models (LLMs) such as ChatGPT &amp; Co. It provides key concepts, models, and recent technological advances. The chapter then navigates through the various applications of GenAI or LLMs in education, examining their impact on different levels of education, including school, university, and vocational training. The chapter will highlight how GenAI is reshaping the educational landscape through real-world examples and case studies, from personalized learning experiences to content creation and assessment. It also discusses various technical, ethical, and organizational/educational challenges to using technology in education.</jats:p>"
10.1007/979-8-8688-0282-9_2,Core Technical Concepts,N/A
10.1007/979-8-8688-0282-9_14,Summarizing Key Insights,N/A
10.2139/ssrn.4792315,Search Generative Experience (SGE): A User-Centric Approach to SEO in the Age of AI,N/A
10.5594/jmi.2023.3297238,Applications of Generative AI to Media,N/A
10.2139/ssrn.4453658,Large Generative AI Models vs Smaller Parameter Models with More Data: A Comprehensive Literature Review,N/A
10.5210/fm.v29i1.13541,Why do people use ChatGPT? Exploring user motivations for generative conversational AI,"<jats:p>Generative conversational artificial intelligence (AI), such as ChatGPT, has attracted substantial attention since November 2022. The advent of this technology showcases the vast potential of such AI for generating and processing text and raises compelling questions regarding its potential usage. To obtain the requisite knowledge of users’ motivations in adopting this technology, we surveyed early adopters of ChatGPT (n = 197). Analysis of free text responses within the uses and gratifications (U&amp;G) theoretical framework shows six primary motivations for using generative conversational AI: productivity, novelty, creative work, learning and development, entertainment, and social interaction and support. Our study illustrates how generative conversational AI can fulfill diverse user needs, surpassing the capabilities of traditional conversational technologies, for example, by outsourcing cognitive or creative works to technology.</jats:p>"
10.1007/979-8-8688-0473-1,Empowering the Public Sector with Generative AI,N/A
10.2139/ssrn.4891490,Mltogai: Semantic Web Based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations Using Generative Ai,N/A
10.18260/1-2--47819,Optimizing Database Query Learning: A Generative AI Approach for Semantic Error Feedback,N/A
10.1162/qss_a_00285/v1/decision1,"Decision letter for ""Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents""",N/A
10.47760/ijcsmc.2024.v13i05.010,Mitigating SAP Duplicate Payments Using Generative AI,"<jats:p>This document presents a method to mitigate duplicate payments in SAP systems using Generative AI. The integration of advanced AI techniques offers a significant improvement in the accuracy and efficiency of financial operations. This paper outlines the methodology, implementation steps, and expected outcomes, demonstrating the potential of AI to revolutionize financial management in ERP systems.</jats:p>"
10.1109/tts.2024.3413591,"Generative AI, Ingenuity, and Law",N/A
10.21203/rs.3.rs-4882673/v1,Usage and Knowledge of Online Tools and Generative AI: A Survey of Students,"<title>Abstract</title>
        <p>Artificial Intelligence (AI) tools like ChatGPT are poised to transform student and educator workflows in higher education. However, there is less documentation on the range of tools students in higher education use,  how they use them and in coordination with other online tools for learning, and their expertise using AI tools. We present a mixed-method analysis of a survey conducted at a doctoral-granting university in the United States investigating the adoption of AI tools in the context of other technologies. The findings include how the students used GenAI tools in light of other on-line technologies, their perception of expertise on the topic, and how they gain expertise in using AI for educational work.</p>"
10.1093/oso/9780198876434.003.0003,Cyber-Risks and Medical Ethics,"<jats:title>Abstract</jats:title>
               <jats:p>With digital AI-powered medicine being on the rise, the risk of cyber-attacks has become a reality. ‘You have been hacked. Pay or we will shut down your system!’ AI systems are not just subject to cyber-risks like any other computer-based system, but to new risks and new types of attacks which have previously existed. This chapter argues that inevitably there will have to be a change in medical practice.</jats:p>"
10.31235/osf.io/3xzaf,Onward for the freedom of others: Marching beyond the AI Ethics,"<p>The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers’ representatives, lobbyists, journalists, and ‘moral machines’. Prominent political institutions crafted principles for the ‘ethical being’ of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and JeanPaul Sartre, this paper explores the philosophical antinomies of the ‘AI Ethics’ debate as well as the conceptual disorientation of the ‘fairness discussion’. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is ‘the AI Ethics guidelines’ a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smokegas of the ‘AI Ethics’ discussion? And eventually, what’s the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.</p>"
10.7551/mitpress/8975.003.0008,References,N/A
10.7551/mitpress/8975.003.0009,Index,N/A
10.31219/osf.io/9ad4u,The Ethics of Emotion in AI Systems,"<p>In this paper, we develop a taxonomy of relevant models and proxy data for emotional expression and outline how the combinations and permutations of these models and data impact artificial intelligence (AI) systems deploying them. We should not take computer scientists at their word that the paradigms for human emotions they have developed internally and adapted from other fields are ground truth; instead, we ask how different conceptualizations of what emotions are, and how they can be sensed, measured and transformed into data, shape the way humans interact with and respond to these AI systems.</p>"
10.1007/s43681-024-00470-1,Using ChatGPT-3 as a writing tool: an educational assistant or a moral hazard? Current ChatGPT-3 media representations compared to Plato’s critical stance on writing in Phaedrus,"<jats:title>Abstract</jats:title><jats:p>ChatGPT-3, based on a large language model created by OpenAI, capable of generating human-like text, has been open to the public since November 2022. Since 2023, ChatGPT-3 has become a much-discussed educational writing tool. We elaborate on what we mean by referring to ChatGPT-3 as an educational assistant and define moral hazard. Then, we put this writing tool, as an extension of human capabilities, in a historical perspective with an analysis of Plato’s critical stance on writing in <jats:italic>Phaedrus</jats:italic>. After having analysed ChatGPT-3’s current representations in the media and academia, we ask ChatGPT-3 whether its writing tool should be considered an educational assistant or a moral hazard and if it could reflect on the similarity, if any, between Plato’s critical stance and ChatGPT-3 as a writing tool and comment on ChatGPT-3’s answers. Finally, we compare these to Plato’s main arguments with regard to writing and draw a number of conclusions.</jats:p>"
10.1007/s00146-018-0867-z,Organic and dynamic tool for use with knowledge base of AI ethics for promoting engineers’ practice of ethical AI design,N/A
10.1007/s00146-021-01285-y,"Integrating AI ethics in wildlife conservation AI systems in South Africa: a review, challenges, and future research agenda",N/A
10.7551/mitpress/8975.003.0001,Preface,N/A
10.2139/ssrn.4908338,"Descriptive, Normative, and Action AI ethics: The Case of X",N/A
10.1017/s0963180119000847,The Ethics of Medical AI and the Physician-Patient Relationship,"<jats:title>Abstract:</jats:title><jats:p>This article considers recent ethical topics relating to medical AI. After a general discussion of recent medical AI innovations, and a more analytic look at related ethical issues such as data privacy, physician dependency on poorly understood AI helpware, bias in data used to create algorithms post-GDPR, and changes to the patient–physician relationship, the article examines the issue of so-called robot doctors. Whereas the so-called democratization of healthcare due to health wearables and increased access to medical information might suggest a positive shift in the patient-physician relationship, the physician’s ‘need to care’ might be irreplaceable, and robot healthcare workers (‘robot carers’) might be seen as contributing to dehumanized healthcare practices.</jats:p>"
10.2196/preprints.51204,Role of Ethics in Developing AI-Based Applications in Medicine: Insights From Expert Interviews and Discussion of Implications (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>The integration of artificial intelligence (AI)–based applications in the medical field has increased significantly, offering potential improvements in patient care and diagnostics. However, alongside these advancements, there is growing concern about ethical considerations, such as bias, informed consent, and trust in the development of these technologies.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>This study aims to assess the role of ethics in the development of AI-based applications in medicine. Furthermore, this study focuses on the potential consequences of neglecting ethical considerations in AI development, particularly their impact on patients and physicians.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>Qualitative content analysis was used to analyze the responses from expert interviews. Experts were selected based on their involvement in the research or practical development of AI-based applications in medicine for at least 5 years, leading to the inclusion of 7 experts in the study.</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>The analysis revealed 3 main categories and 7 subcategories reflecting a wide range of views on the role of ethics in AI development. This variance underscores the subjectivity and complexity of integrating ethics into the development of AI in medicine. Although some experts view ethics as fundamental, others prioritize performance and efficiency, with some perceiving ethics as potential obstacles to technological progress. This dichotomy of perspectives clearly emphasizes the subjectivity and complexity surrounding the role of ethics in AI development, reflecting the inherent multifaceted nature of this issue.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>Despite the methodological limitations impacting the generalizability of the results, this study underscores the critical importance of consistent and integrated ethical considerations in AI development for medical applications. It advocates further research into effective strategies for ethical AI development, emphasizing the need for transparent and responsible practices, consideration of diverse data sources, physician training, and the establishment of comprehensive ethical and legal frameworks.</p>
                </sec>"
10.1007/s00146-003-0289-3,Global networking and universal ethics,N/A
10.1145/3514094.3539564,Ethical Design for AI in Medicine,N/A
10.13180/icres.2019.29-30.07.p04,What does it mean to trust AI systems?,N/A
10.1117/12.3012385.793bf7e3-c15b-ee11-a99c-00505691c5e1,N/A,N/A
10.1007/s43681-023-00399-x,Using ScrutinAI for visual inspection of DNN performance in a medical use case,"<jats:title>Abstract</jats:title><jats:p>Our Visual Analytics (VA) tool ScrutinAI supports human analysts to investigate interactively model performance and data sets. Model performance depends on labeling quality to a large extent. In particular in medical settings, generation of high quality labels requires in depth expert knowledge and is very costly. Often, data sets are labeled by collecting opinions of groups of experts. We use our VA tool to analyze the influence of label variations between different experts on the model performance. ScrutinAI facilitates to perform a root cause analysis that distinguishes weaknesses of deep neural network (DNN) models caused by varying or missing labeling quality from true weaknesses. We scrutinize the overall detection of intracranial hemorrhages and the more subtle differentiation between subtypes in a publicly available data set.</jats:p>"
10.1007/s43681-022-00168-2,"Meaningful human control of drones: exploring human–machine teaming, informed by four different ethical perspectives","<jats:title>Abstract</jats:title><jats:p>A human-centric approach to the design and deployment of AI systems aims to support and augment human capabilities. This sounds worthwhile indeed. But what could this look like in a military context? We explored a human-centric approach to the design and deployment of highly autonomous, unarmed Unmanned Aerial Vehicle (UAV), or drone, and an associated Decision Support System (DSS), for the drone’s operator. We explore how Human–Machine Teaming, through such a DSS, can promote Meaningful Human Control of the drone. We use four different ethical perspectives—utilitarianism, deontology, relational ethics and virtue ethics—to discuss different ways to design and deploy the drones and the DSS. Our aim is to explore ways to support and augment the operators’ capabilities.</jats:p>"
10.1007/s43681-022-00157-5,Autonomy and the social dilemma of online manipulative behavior,"<jats:title>Abstract</jats:title><jats:p>Persuasive online technologies were initially designed and used to gain insights into the online behavior of individuals to personalize advertising campaigns in an effort to influence people and convince them to buy certain products. But recently, these technologies have blurred the lines and morphed into technologies that covertly and gradually manipulate people into attaining a goal that is predetermined by the algorithm and disregards the decision-making rights of the individual. This may lead to people exercising decisions that do not align with their personal values and beliefs, and rob them of their autonomy—an ethical principle, in the absence of which the application of these technologies may be unethical. However, not all technologies that are persuasive are necessarily manipulative which require the careful consideration of a couple of elements to determine whether or not technologies are manipulative and ultimately whether their application is ethical or not. In this article, we analyze the ethical principle of autonomy and unpack the underlying elements of this ethical principle which must be considered to determine whether the application of a technology is ethical or not in the context of it being persuasive or manipulative.</jats:p>"
10.1186/s12910-023-00990-1,AI-driven decision support systems and epistemic reliance: a qualitative study on obstetricians’ and midwives’ perspectives on integrating AI-driven CTG into clinical decision making,"<jats:title>Abstract</jats:title><jats:sec>
                <jats:title>Background</jats:title>
                <jats:p>Given that AI-driven decision support systems (AI-DSS) are intended to assist in medical decision making, it is essential that clinicians are willing to incorporate AI-DSS into their practice. This study takes as a case study the use of AI-driven cardiotography (CTG), a type of AI-DSS, in the context of intrapartum care. Focusing on the perspectives of obstetricians and midwives regarding the ethical and trust-related issues of incorporating AI-driven tools in their practice, this paper explores the conditions that AI-driven CTG must fulfill for clinicians to feel justified in incorporating this assistive technology into their decision-making processes regarding interventions in labor.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Methods</jats:title>
                <jats:p>This study is based on semi-structured interviews conducted online with eight obstetricians and five midwives based in England. Participants were asked about their current decision-making processes about when to intervene in labor, how AI-driven CTG might enhance or disrupt this process, and what it would take for them to trust this kind of technology. Interviews were transcribed verbatim and analyzed with thematic analysis. NVivo software was used to organize thematic codes that recurred in interviews to identify the issues that mattered most to participants. Topics and themes that were repeated across interviews were identified to form the basis of the analysis and conclusions of this paper.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Results</jats:title>
                <jats:p>There were four major themes that emerged from our interviews with obstetricians and midwives regarding the conditions that AI-driven CTG must fulfill: (1) the importance of accurate and efficient risk assessments; (2) the capacity for personalization and individualized medicine; (3) the lack of significance regarding the type of institution that develops technology; and (4) the need for transparency in the development process.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Conclusions</jats:title>
                <jats:p>Accuracy, efficiency, personalization abilities, transparency, and clear evidence that it can improve outcomes are conditions that clinicians deem necessary for AI-DSS to meet in order to be considered reliable and therefore worthy of being incorporated into the decision-making process. Importantly, healthcare professionals considered themselves as the epistemic authorities in the clinical context and the bearers of responsibility for delivering appropriate care. Therefore, what mattered to them was being able to evaluate the reliability of AI-DSS on their own terms, and have confidence in implementing them in their practice.</jats:p>
              </jats:sec>"
10.1145/3306618.3314289,The Role and Limits of Principles in AI Ethics,N/A
10.1145/3514094.3534195,The Cost of Ethical AI Development for AI Startups,N/A
10.1145/3514094.3539522,Dialogue Explanation With Reasoning for AI,N/A
10.1007/s43681-023-00279-4,Personality and demographic correlates of support for regulating artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>The arrival of artificial intelligence (AI) in our society has sparked many hopes and fears, with people having diverging views on the need to strictly regulate AI. The current study investigates how demographic and personality traits are associated with a desire to strictly regulate AI using a representative sample of adults from New Zealand (<jats:italic>N</jats:italic> = 47,951 participants). Data revealed that support for strict regulation of AI is positively related with agreeableness, neuroticism, and honesty–humility. However, it is negatively related to openness to experiences. A wide range of demographic factors including gender, age, ethnicity, religiosity, neighbourhood level economic deprivation, living rural, relationship status, and parental status were additionally related to support for regulation of AI. However, all these effects were fairly small suggesting that both personality and socio-demographic factors contribute to support for regulating AI, but other factors beyond these characteristics should also be considered for understanding people’s support for regulating AI.</jats:p>"
10.1007/s43681-021-00108-6,"Bias, awareness, and ignorance in deep-learning-based face recognition","<jats:title>Abstract</jats:title><jats:p>Face Recognition (FR) is increasingly influencing our lives: we use it to unlock our phones; police uses it to identify suspects. Two main concerns are associated with this increase in facial recognition: (1) the fact that these systems are typically less accurate for marginalized groups, which can be described as “bias”, and (2) the increased surveillance through these systems. Our paper is concerned with the first issue. Specifically, we explore an intuitive technique for reducing this bias, namely “blinding” models to sensitive features, such as gender or race, and show why this cannot be equated with reducing bias. Even when not designed for this task, facial recognition models can deduce sensitive features, such as gender or race, from pictures of faces—simply because they are trained to determine the “similarity” of pictures. This means that people with similar skin tones, similar hair length, etc. will be seen as similar by facial recognition models. When confronted with biased decision-making by humans, one approach taken in job application screening is to “blind” the human decision-makers to sensitive attributes such as gender and race by not showing pictures of the applicants. Based on a similar idea, one might think that if facial recognition models were less aware of these sensitive features, the difference in accuracy between groups would decrease. We evaluate this assumption—which has already penetrated into the scientific literature as a valid de-biasing method—by measuring how “aware” models are of sensitive features and correlating this with differences in accuracy. In particular, we blind pre-trained models to make them less aware of sensitive attributes. We find that awareness and accuracy do not positively correlate, i.e., that <jats:italic>bias</jats:italic><jats:inline-formula><jats:alternatives><jats:tex-math>$$\ne$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">
                  <mml:mo>≠</mml:mo>
                </mml:math></jats:alternatives></jats:inline-formula><jats:italic>awareness</jats:italic>. In fact, blinding barely affects accuracy in our experiments. The seemingly simple solution of decreasing bias in facial recognition rates by reducing awareness of sensitive features does thus not work in practice: trying to ignore sensitive attributes is <jats:italic>not</jats:italic> a viable concept for less biased FR.</jats:p>"
10.1007/s43681-020-00028-x,"Automation, work and the achievement gap","<jats:title>Abstract</jats:title><jats:p>Rapid advances in AI-based automation have led to a number of existential and economic concerns. In particular, as automating technologies develop enhanced competency, they seem to threaten the values associated with meaningful work. In this article, we focus on one such value: the value of achievement. We argue that achievement is a key part of what makes work meaningful and that advances in AI and automation give rise to a number achievement gaps in the workplace. This could limit people’s ability to participate in meaningful forms of work. Achievement gaps are interesting, in part, because they are the inverse of the (negative) responsibility gaps already widely discussed in the literature on AI ethics. Having described and explained the problem of achievement gaps, the article concludes by identifying four possible policy responses to the problem.</jats:p>"
10.3390/ai4010003,Ethics &amp; AI: A Systematic Review on Ethical Concerns and Related Strategies for Designing with AI in Healthcare,"<jats:p>In modern life, the application of artificial intelligence (AI) has promoted the implementation of data-driven algorithms in high-stakes domains, such as healthcare. However, it is becoming increasingly challenging for humans to understand the working and reasoning of these complex and opaque algorithms. For AI to support essential decisions in these domains, specific ethical issues need to be addressed to prevent the misinterpretation of AI, which may have severe consequences for humans. However, little research has been published on guidelines that systematically addresses ethical issues when AI techniques are applied in healthcare. In this systematic literature review, we aimed to provide an overview of ethical concerns and related strategies that are currently identified when applying AI in healthcare. The review, which followed the PRISMA guidelines, revealed 12 main ethical issues: justice and fairness, freedom and autonomy, privacy, transparency, patient safety and cyber security, trust, beneficence, responsibility, solidarity, sustainability, dignity, and conflicts. In addition to these 12 main ethical issues, we derived 19 ethical sub-issues and associated strategies from the literature.</jats:p>"
10.61838/kman.aitech.1.4.1,AI Ethics: A Call for Global Standards in Technology Development,"<jats:p>The development of global standards for ethical AI is not merely a technical issue but a moral imperative that demands immediate and concerted action from all stakeholders involved in AI development and deployment. To navigate the ethical complexities of AI and ensure its benefits are maximized while minimizing its risks, a comprehensive and inclusive approach to AI ethics is essential. This approach must prioritize transparency, accountability, safety, and fairness, and include diverse perspectives to create a truly global ethical framework for AI. As we stand at the crossroads of technological innovation and ethical responsibility, the call for global standards in AI development cannot be overstated. It is time for researchers, policymakers, industry leaders, and the global community to come together to forge a path that ensures AI technologies are developed and used in a manner that upholds the highest ethical standards, respects human rights, and promotes the well-being of society and the environment. In conclusion, the journey towards ethical AI is a collective endeavor that requires the wisdom, insight, and cooperation of the global community. By embracing the challenge of developing and implementing global ethical standards for AI, we can ensure that this transformative technology serves as a force for good, driving progress and innovation in ways that are responsible, ethical, and sustainable.</jats:p>"
10.31988/scitrends.3377,Google's DeepMind Launches AI Ethics Research Unit,N/A
10.1145/3278721.3278797,Giving AI a Theory of Mind,N/A
10.1007/s11948-021-00336-3,Implementing Ethics in Healthcare AI-Based Applications: A Scoping Review,N/A
10.1007/978-981-99-7184-8_4,No AI After Auschwitz? Bridging AI and Memory Ethics in the Context of Information Retrieval of Genocide-Related Information,N/A
10.1007/s43681-022-00233-w,AI’s fairness problem: understanding wrongful discrimination in the context of automated decision-making,"<jats:title>Abstract</jats:title><jats:p>The use of predictive machine learning algorithms is increasingly common to guide or even take decisions in both public and private settings. Their use is touted by some as a potentially useful method to avoid discriminatory decisions since they are, allegedly, neutral, objective, and can be evaluated in ways no human decisions can. By (fully or partly) outsourcing a decision process to an algorithm, it should allow human organizations to clearly define the parameters of the decision and to, in principle, remove human biases. Yet, in practice, the use of algorithms can still be the source of wrongful discriminatory decisions based on at least three of their features: the data-mining process and the categorizations they rely on can reconduct human biases, their automaticity and predictive design can lead them to rely on wrongful generalizations, and their opaque nature is at odds with democratic requirements. We highlight that the two latter aspects of algorithms and their significance for discrimination are too often overlooked in contemporary literature. Though these problems are not all insurmountable, we argue that it is necessary to clearly define the conditions under which a machine learning decision tool can be used. We identify and propose three main guidelines to properly constrain the deployment of machine learning algorithms in society: algorithms should be vetted to ensure that they do not unduly affect historically marginalized groups; they should not systematically override or replace human decision-making processes; and the decision reached using an algorithm should always be explainable and justifiable.</jats:p>"
10.1787/c8d5f900-en,"AI job postings mentioning keywords related to ethics, 2019-22",N/A
10.21552/aire/2024/2/5,Ethics Governance of AI for the Legal Sector:,N/A
10.2139/ssrn.3542524,The Ethics of AI: What Does AI Do When Humans Cannot Agree What Is ‘Right’?,N/A
10.1007/s00146-007-0095-4,Special issue on ethics and artificial agents,N/A
10.1145/3600211.3604762,Towards formalizing and assessing AI fairness,N/A
10.1007/978-3-030-75354-2_4,AI Code of Ethics for Cybersecurity,N/A
10.1007/s43681-023-00410-5,Gender mobility in the labor market with skills-based matching models,N/A
10.1007/s43681-023-00356-8,Adversarial learning with optimism for bias reduction in machine learning,N/A
10.1007/s43681-021-00114-8,Artificial intelligence in research and development for sustainability: the centrality of explicability and research data management,"<jats:title>Abstract</jats:title><jats:p>Sustainability constitutes a focal challenge and objective of our time and requires collaborative efforts. As artificial intelligence brings forth substantial opportunities for innovations across industry and social contexts, so it provides innovation potential for pursuing sustainability. We argue that (chemical) research and development driven by artificial intelligence can substantially contribute to sustainability if it is leveraged in an ethical way. Therefore, we propose that the ethical principle <jats:italic>explicability</jats:italic> combined with (open) research data management systems should accompany artificial intelligence in research and development to foster sustainability in an equitable and collaborative way.</jats:p>"
10.1145/3644815.3644952,Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective,N/A
10.1007/s00146-017-0703-x,Rethinking the I-You relation through dialogical philosophy in the Ethics of AI and robotics,N/A
10.1007/s10676-022-09633-2,"Disability, fairness, and algorithmic bias in AI recruitment",N/A
10.1007/978-3-030-66913-3_3,Business Ethics,N/A
10.1007/s10506-024-09412-y,"An interdisciplinary account of the terminological choices by EU policymakers ahead of the final agreement on the AI Act: AI system, general purpose AI system, foundation model, and generative AI","<jats:title>Abstract</jats:title><jats:p>The European Union’s Artificial Intelligence Act (AI Act) is a groundbreaking regulatory framework that integrates technical concepts and terminology from the rapidly evolving ecosystems of AI research and innovation into the legal domain. Precise definitions accessible to both AI experts and lawyers are crucial for the legislation to be effective. This paper provides an interdisciplinary analysis of the concepts of <jats:italic>AI system</jats:italic>, <jats:italic>general purpose AI system</jats:italic>, <jats:italic>foundation model</jats:italic> and <jats:italic>generative AI</jats:italic> across the different versions of the legal text (Commission proposal, Parliament position and Council General Approach) before the final political agreement. The goal is to help bridge the understanding of these key terms between the technical and legal communities and contribute to a proper implementation of the AI Act. We provide an analysis of the concept of <jats:italic>AI system</jats:italic> considering its scientific foundation and the crucial role that it plays in the regulation, which requires a sound definition both from legal and technical standpoints. We connect the outcomes of this discussion with the analysis of the concept of <jats:italic>general purpose AI system</jats:italic> and its evolution during the negotiations. We also address the distinct conceptual meanings of <jats:italic>AI system</jats:italic> vs <jats:italic>AI model</jats:italic> and explore the technical nuances of the term <jats:italic>foundation model</jats:italic>. We conclude that rooting the definition of <jats:italic>foundation model</jats:italic> to its general purpose capabilities following standardised evaluation methodologies appears to be most appropriate approach. Lastly, we tackle the concept of <jats:italic>generative AI</jats:italic>, arguing that definitions of <jats:italic>AI system</jats:italic> that include “content” as one of the system’s outputs already captures it, and concluding that not all <jats:italic>generative AI</jats:italic> is based on <jats:italic>foundation models</jats:italic>.</jats:p>"
10.1016/j.ifacol.2021.08.084,From Ethics to Standards; an Overview of AI Ethics in CPPS,N/A
10.1007/s10676-024-09770-w,Transparency for AI systems: a value-based approach,"<jats:title>Abstract</jats:title><jats:p>With the widespread use of artificial intelligence, it becomes crucial to provide information about these systems and how they are used. Governments aim to disclose their use of algorithms to establish legitimacy and the EU AI Act mandates forms of transparency for all high-risk and limited-risk systems. Yet, what should the standards for transparency be? What information is needed to show to a wide public that a certain system can be used legitimately and responsibly? I argue that process-based approaches fail to satisfy, as knowledge about the development process is insufficient to predict the properties of the resulting system. Current outcome-based approaches [Mitchell et al., 2019; Loi et al., 2021] are also criticized for a lack of attention to the broader socio-technical system and failure to account for empirical results that show that people care about more than just the outcomes of a process [as reported by Meyerson et al. (Procedural justice and relational theory: Empirical, philosophical, and legal perspectives, Taylor &amp; Francis, 2021)]. Instead, I propose value-based transparency, on which the information we need to provide is what values have been considered in the design and how successful these have been realized in the final system. This can handle the objections to other frameworks, matches with current best practices on the design of responsible AI and provides the public with information on the crucial aspects of a system’s design.</jats:p>"
10.4337/9781803926728.00024,Feminist Ethics and AI: A Subfield of Feminist Philosophy of Technology,N/A
10.1007/s10676-023-09716-8,Calibrating machine behavior: a challenge for AI alignment,N/A
10.1016/b978-0-443-18851-0.00017-2,Beyond artificial intelligence ethics: exploring empathetic ethical outcomes for artificial intelligence,N/A
10.4018/979-8-3693-3597-0.ch013,Enhancing Cyber Security Through Generative Adversarial Networks,"<jats:p>GANs, or generative adversarial networks, are becoming a very useful tool in many fields, such as data generation, natural language processing, and computer vision. They still have untapped potential to strengthen cyber security protocols, nevertheless. The uses and ramifications of using GANs to improve cyber security are explored in this abstract. First, by using GANs for data augmentation, realistic and varied datasets that are essential for training malware classifiers, anomaly detection models, and intrusion detection systems can be created. Through the generation of synthetic data that closely mimics real-world cyber threats, GANs enable more thorough and efficient training, strengthening security mechanisms against emerging cyber-attacks in the process. GANs help with security testing and password cracking. This helps security experts assess how strong passwords are and strengthen authentication procedures to fend off possible intrusions.</jats:p>"
10.1109/ms.2024.3406333,Towards Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model based Systems,N/A
10.31310/hum.089.08,A New Transformation of Literacy Education in the Era of Generative AI,N/A
10.11591/ijai.v12.i4.pp1557-1568,Pedestrian detection under weather conditions using conditional generative adversarial network,"<jats:p>&lt;p&gt;Nowadays, many pedestrians are injured or killed in traffic accidents. As a result, several artificial vision solutions based on pedestrian detection have been developed to assist drivers and reduce the number of accidents. Most pedestrian detection techniques work well on sunny days and provide accurate traffic data. However, detection decreases dramatically in rainy conditions. In this paper, a new pedestrian detection system (PDS) based on generative adversarial network (GAN) module and the real-time object detector you only look once (YOLO) v3 is proposed to mitigate adversarial weather attacks. Experimental evaluations performed on the VOC2014 dataset show that our proposed system performs better than models based on existing noise reduction methods in terms of accuracy for weather situations.&lt;/p&gt;</jats:p>"
10.11591/ijai.v13.i3.pp2904-2911,Enhanced multi-ethnic speech recognition using pitch shifting generative adversarial networks,"<jats:p>&lt;p&gt;&lt;span lang=""EN-US""&gt;Research in the field of speech recognition is a challenging research area. Various approaches have been applied to build robust models. A problem faced in speech recognition research is overfitting, especially if there is insufficient data to train the model. A large enough amount of data can train the model well, resulting in high accuracy. Data augmentation is an approach often used to increase the quantity of dataset. This research uses a data augmentation approach, namely pitch shifting, to increase the quantity of speech dataset, which is then processed into spectrogram data and then classified using a generative adversarial network (GAN). Using the pitch shifting-generative adversarial network (PS-GAN) model, this research produces high accuracy performance in multi-ethnic speech recognition, namely 98.43%, better than several similar studies.&lt;/span&gt;&lt;/p&gt;</jats:p>"
10.4018/979-8-3693-3731-8.ch017,Generative AI Revolution,"<jats:p>A scarcity of data within the healthcare sector can present considerable obstacles for a range of applications, most notably in the implementation and advancement of machine learning models. Sometimes, inadequate data sets may also lead to the wrong interpretation of data with wider patient populations, forcing the models to be biased. Overfitting, where a model acquires knowledge about the features of the training data rather than underlying patterns. In cases of overfit, the models perform well on the training data, but they face difficulty with novel and unknown data. Generative adversarial networks (GANs) play a vital role in healthcare by accelerating medical research and diagnosis. Though GAN has evidenced, genuineness of the data they provide, need to adjust to the regulations to ensure the privacy and security of patient information. This chapter provides an overview of current research and mutual efforts between the medical and artificial intelligence (AI) communities to maximize the potential of GANs to address healthcare challenges.</jats:p>"
10.4018/979-8-3693-2440-0.ch007,Exploring AI-Assisted Students Online Proctoring System,"<jats:p>Online proctoring tools and techniques have gained significant attention in recent years due to the increasing popularity of online learning platforms and online assessments. This chapter provides an in-depth analysis of different online proctoring techniques and tools that are commonly used to maintain the integrity of online exams. The chapter begins by highlighting the importance of online proctoring in ensuring fair evaluation and preventing cheating. It then explores various types of online proctoring techniques such as live proctoring, automated proctoring, and recorded proctoring. The advantages and limitations of each technique are discussed, providing a comprehensive understanding of when and how to use them effectively. Overall, this chapter aims to empower educators and administrators with the knowledge and information needed to implement effective online proctoring methods. By understanding the various techniques and tools available, they can enhance the credibility and security of their online assessments, ensuring a fair and reliable evaluation process.</jats:p>"
10.1007/978-981-19-9382-4_4,Methods in Applied Ethics,N/A
10.4337/9781803926728.00013,"From Ethics to Law: Why, When, and How to Regulate AI",N/A
10.1017/jdm.2023.37,"Human favoritism, not AI aversion: People’s perceptions (and bias) toward generative AI, human experts, and human–GAI collaboration in persuasive content generation","<jats:title>Abstract</jats:title>
	  <jats:p>With the wide availability of large language models and generative AI, there are four primary paradigms for human–AI collaboration: human-only, AI-only (ChatGPT-4), augmented human (where a human makes the final decision with AI output as a reference), or augmented AI (where the AI makes the final decision with human output as a reference). In partnership with one of the world’s leading consulting firms, we enlisted professional content creators and ChatGPT-4 to create advertising content for products and persuasive content for campaigns following the aforementioned paradigms. First, we find that, contrary to the expectations of some of the existing algorithm aversion literature on conventional predictive AI, the content generated by generative AI and augmented AI is perceived as of higher quality than that produced by human experts and augmented human experts. Second, revealing the source of content production reduces—but does not reverse—the perceived quality gap between human- and AI-generated content. This bias in evaluation is predominantly driven by human favoritism rather than AI aversion: Knowing that the same content is created by a human expert increases its (reported) perceived quality, but knowing that AI is involved in the creation process does not affect its perceived quality. Further analysis suggests this bias is not due to a ‘quality prime’ as knowing the content they are about to evaluate comes from competent creators (e.g., industry professionals and state-of-the-art AI) without knowing exactly that the creator of each piece of content does not increase participants’ perceived quality.</jats:p>"
10.1007/978-981-99-9379-6_17,Where Generative AI Fits Within and in Addition to Existing AI K12 Education Interactions: Industry and Research Perspectives,N/A
10.7577/njcie.5736,High School Teachers’ Adoption of Generative AI,"<jats:p>In 2023, the breakthrough of generative artificial intelligence (AI) led to its adoption. While some teachers expressed frustration over pupil misuse of generative AI, others advocated for the availability of a school-relevant chatbot for pupil use. In October 2023, a local chatbot intended to meet that goal was launched by Oslo Municipality. After six weeks, an investigation was conducted to examine how 246 teachers perceived the opportunities and limitations of this new technology. The examination used structural equation modelling to explore antecedents of instructional AI utility. The analysis shows that the pathway between instructional self-efficacy and AI utility has the highest positively charged value, while the pathways between management and AI utility have low numerical value. This last finding can be interpreted as the influence of untapped management potential and must be seen in the context of the fact that no guidelines for the use of AI in schools existed when the survey was conducted. In addition, the pathway between colleague discussion and AI utility has relatively low numerical values. The potential for learning through discussion among colleagues can be utilized to an even greater degree. The pathway between management and colleague discussion is remarkable. Implications are discussed.</jats:p>"
10.1002/ail2.18,Practical notes on building molecular graph generative models,"<jats:title>Abstract</jats:title><jats:p>Here are presented technical notes and tips on developing graph generative models for molecular design. Although this work stems from the development of GraphINVENT, a Python platform for iterative molecular generation using graph neural networks, this work is relevant to researchers studying other architectures for graph‐based molecular design. In this work, technical details that could be of interest to researchers developing their own molecular generative models are discussed, including an overview of previous work in graph‐based molecular design and strategies for designing new models. Advice on development and debugging tools which are helpful during code development is also provided. Finally, methods that were tested but which ultimately did not lead to promising results in the development of GraphINVENT are described here in the hope that this will help other researchers avoid pitfalls in development and instead focus their efforts on more promising strategies for graph‐based molecular generation.</jats:p>"
10.2139/ssrn.4887933,&lt;span&gt;How to Harness AI for Justice: A Preliminary Agenda for Using Generative AI to Improve Access to Justice&lt;/span&gt;,N/A
10.1007/978-3-031-39619-9_2,Use Cases of Generative AI in Asset Management of Railways,N/A
10.2501/jar-2023-019,"Ready or Not, Generative AI Is Here to Stay",N/A
10.1007/s10676-022-09666-7,The ethics and epistemology of explanatory AI in medicine and healthcare,N/A
10.1007/978-3-030-66913-3_1,Ethics for Non-philosophers: Basis of Ethics and Ethical Perspectives,N/A
10.14445/22312803/ijctt-v72i6p111,Applications of Generative AI for Scaling Security Teams,N/A
10.2139/ssrn.4499768,Revolutionizing Translation with AI: Unravelling Neural Machine Translation and Generative Pre-Trained Large Language Models,N/A
10.5121/csit.2024.141305,Beyond Borders: Empowering Multilingual Forms with Generative AI using MarianMTModel and T5 Model,"<jats:p>In a world where connecting and working with people from different countries is more and more important, the language barriers are often the main reasons why the cross border communication and collaboration is not successful. This research paper is about the use of Generative AI models, most notably the MarianMT Model and T5 Model, that enable to go through the linguistic boundaries and create the multilingual forms. The paper,on the other hand, explores the real-life application of these models in a Python environment through the Hugging Face Transformers Library. The paper goes into detailed code sample to show how these models can be used to brightly transfer textual data from one language to another apart from currently utilized models . The experimental design concerns with the translation of different sample data, this data contains individual attributes like name, age, height, weight, and the medical problems, into a number of target languages. Besides, this study not only shows the technical difficulties of model initialization and translation but also it emphasizes the wider meaning of such technology for developing cross-cultural understanding and making the world communication easier. The results underline Generative AI's potentiality to overcome language obstacles, thus enabling the worldwide cooperation, knowledge spread, and cultural exchange.</jats:p>"
10.2174/0127724328311400240823062829,Leveraging Generative AI for Drug Safety and Pharmacovigilance,"<jats:sec>
<jats:title/>
<jats:p>Predictions are made by artificial intelligence, especially through machine learning,
which uses algorithms and past knowledge. Notably, there has been an increase in interest in using
artificial intelligence, particularly generative AI, in the pharmacovigilance of pharmaceuticals under
development, as well as those already in the market. This review was conducted to understand
how generative AI can play an important role in pharmacovigilance and improving drug safety
monitoring. Data from previously published articles and news items were reviewed in order to obtain
information. We used PubMed and Google Scholar as our search engines, and keywords
(pharmacovigilance, artificial intelligence, machine learning, drug safety, and patient safety) were
used. In toto, we reviewed 109 articles published till 31 January 2024, and the obtained information
was interpreted, compiled, evaluated, and conclusions were reached. Generative AI has transformative
potential in pharmacovigilance, showcasing benefits, such as enhanced adverse event detection,
data-driven risk prediction, and optimized drug development. By making it easier to process
and analyze big datasets, generative artificial intelligence has applications across a variety of
disease states. Machine learning and automation in this field can streamline pharmacovigilance
procedures and provide a more efficient way to assess safety-related data. Nevertheless, more investigation
is required to determine how this optimization affects the caliber of safety analyses. In
the near future, the increased utilization of artificial intelligence is anticipated, especially in predicting
side effects and Adverse Drug Reactions (ADRs).</jats:p>
</jats:sec>"
10.18260/1-2--47925,Redefining Engineering Literacy with Generative AI: Impacts and Implications for Diverse Languages and Expertise in Engineering Education,N/A
10.2139/ssrn.4616179,Generative AI in the Attorney-Client Relationship: An Exercise in Critical Revision and Client Management,N/A
10.4018/979-8-3693-1198-1,Generative AI and Multifactor Productivity in Business,N/A
10.1007/978-1-4842-9994-4,Applied Generative AI for Beginners,N/A
10.2139/ssrn.4541938,Leveraging Generative AI for Optimal Release Timing and Star Power Impact Analysis in Film and Television,N/A
10.21203/rs.3.rs-3652246/v1,Generative AI and digital twin integrated intelligent process planning：A conceptual framework,"<jats:title>Abstract</jats:title>
        <jats:p>Process planning serves as a critical link between design and manufacturing, exerting a pivotal influence on the quality and efficiency of production. However, current intelligent process planning systems, like computer-aided process planning (CAPP), still contend with the challenge of realizing comprehensive automation in process decision-making. These obstacles chiefly involve, though are not confined to, issues like limited intelligence, poor flexibility, low reliability, and high usage thresholds. Generative artificial intelligence (AI) has attained noteworthy accomplishments in natural language processing (NLP), offering new perspectives to address these challenges. This paper summarizes the limitations of current intelligent process planning methods and explores the potential of integrating generative AI into process planning. With synergistically incorporating digital twins, this paper introduces a conceptual framework termed generative AI and digital twin-enabling intelligent process planning (GIPP). The paper elaborates on two supporting methodologies: process generative pre-trained transformer (ProcessGPT) modelling and digital twin-based process verification method. Moreover, a prototype system is established to introduce the implementation and machining execution mechanism of GIPP for milling a specific thin-walled component. Three potential application scenarios and a comparative analysis are employed to elucidate the practicality of GIPP, providing new insights for intelligent process planning.</jats:p>"
10.1093/pubmed/fdae078,Generative AI in public health: pathways to well-being and positive health outcome,N/A
10.1007/s12596-024-02006-6,A new generative AI based for modelling free space optical links,N/A
10.22214/ijraset.2024.62488,Question Paper Checking Using Generative Ai,"<jats:p>Abstract: In the present era, the world is moving towards computerization. Everything is made easy so, automatic answer sheet checker is required. Checking the answer sheet manually takes a lot of time and energy. The application in this project is based on verification or evaluation of answer sheet using ML and AI. The main objective of this project will be to save time and manpower. An automated answer checker app that checks and marks answers just like a human. This software application is designed to check the answers in the exam and allocate marks to the students after verifying the answers. The system requires teachers to store the original answer for the system. This facility is provided to teachers. Teachers can enter questions and related subjective answers into the system. These responses are stored as database files. The first answer is captured in PDF form and then compares this answer with the original answer written in the database and allocates marks accordingly. The total marks are calculated and the result is finally shown. The system allocates marks according to how good a student is</jats:p>"
10.1016/j.mex.2023.102531,Smart grading: A generative AI-based tool for knowledge-grounded answer evaluation in educational assessments,N/A
10.1162/grey_a_00389,The Work of Copyright Law in the Age of Generative AI,N/A
10.35542/osf.io/x3dct,The Future of Feedback: Integrating Peer and Generative AI Reviews to Support Student Work,"<p>This paper explores the integration of generative artificial intelligence (AI) in education to enhance feedback processes and improve learning experiences. The main goal of the study is to investigate the potential of generative AI for feedback, specifically in complementing peer feedback practices among graduate students enrolled at a US-based university during the 2023 academic term. Drawing on existing literature, the study examines the application of generative AI and its implications for feedback mechanisms. Employing an exploratory research design, the study gathers both quantitative and qualitative data through post-course surveys to address key research questions regarding the quality, usefulness, and actionability of peer and AI reviews, as well as their respective advantages and disadvantages. Findings indicate that peer reviews were consistently perceived slightly higher across all three dimensions compared to AI reviews, with thematic analysis revealing the unique strengths and limitations of each review type. This research underscores the importance of integrating human expertise with AI technology in feedback mechanisms, offering practical insights for educators, instructional designers, and policymakers seeking to enhance feedback experiences through emerging digital technologies.</p>"
10.1007/979-8-8688-0282-9_5,Unpacking Transformer-Based NLP,N/A
10.2139/ssrn.4833954,"Old Moats for New Models: Openness, Control, and Competition in Generative Ai",N/A
10.1002/9781394308286.ch2,Understanding Artificial Intelligence,N/A
10.1002/9781394308286.about,About the Author,N/A
10.14361/9783839474723-003,DETAILS AND THANKS,N/A
10.1109/lt58159.2023.10092345,AI-Based Use-Pattern Generative Hybrid Spaces for Indoor and Outdoor Activities,N/A
10.21125/edulearn.2024.1013,SO MUCH TALK ABOUT GENERATIVE AI IN HIGHER EDUCATION. IS IT WORTH IT?,N/A
10.1145/3638529.3663650,Generative AI: why all the fuss?,N/A
10.31235/osf.io/59qna,"Sandboxes as “trading zones"" for engaging with AI regulation, ethics, and the EU AI Act: How to Reclaim Agency over the Future?","<p>The study uses the concept of ""quasi-public"" spaces and ""trading zones"" to explore the temporal dimension of politics and innovation—agency over the future. It argues that prototyping, often dismissed as apolitical, can be a potent form of public engagement. By focusing on emergent technologies like Large Language Models (LLMs) and Generative Artificial Intelligence (GAIs), the study examines how these spaces can influence regulatory frameworks, such as the EU AI Act . Through a series of interventions, including art installations and mock trials over synthetic agents generates via LLMs, the research reveals that these spaces can serve as platforms for collective action, challenging the technocratic narratives that often dominate discussions around disruptive technologies. The study posits that the act of prototyping can be a form of ""public time,"" a new arena for democratic participation, where the future itself becomes a contested, communal resource.</p>"
10.1007/978-1-4842-8088-1_7,AI Ethics,N/A
10.1007/s00146-019-00923-w,Dance of the artificial alignment and ethics,N/A
10.5040/9781350374430.0005,Conclusion,N/A
10.4324/9781003218326-4,AI Ethics and Governance in Defence Innovation,N/A
10.1007/978-981-19-2531-3_7,Virtual World Under AI: Augmented Reality and Deep Synthesis,N/A
10.1145/3306618.3314281,(When) Can AI Bots Lie?,N/A
10.1007/978-3-030-54173-6_18,Impact of AI/Robotics on Human Relations: Co-evolution Through Hybridisation,"<jats:title>Abstract</jats:title><jats:p>This chapter examines how the processes of human enhancement that have been brought about by the digital revolution (including AI and robotics, besides ICTs) have given rise to new social identities and relationships. The central question consists in asking how the Digital Technological Matrix, understood as a cultural code that supports artificial intelligence and related technologies, causes a hybridisation between the human and the non-human, and to what extent such hybridisation promotes or puts human dignity at risk. Hybridisation is defined here as entanglements and interchanges between digital machines, their ways of operating, and human elements in social practices. The issue is not whether AI or robots can assume human-like characteristics, but how they interact with humans and affect their social identities and relationships, thereby generating a new kind of society.</jats:p>"
10.31891/mdes/2024-11-29,LEVERAGING GENERATIVE AI: STRATEGIC ADOPTION PATTERNS FOR ENTERPRISES,"<jats:p>Generative Artificial Intelligence (AI) is a rapidly evolving subset of AI technologies that involves creating new content, such as text, images, and audio, using algorithms trained on large datasets. Well-known examples of generative AI technologies include Generative Adversarial Networks (GANs) and Generative Pre-trained Transformers (GPT). These innovations are increasingly being integrated into various business applications, from automating content creation and enhancing customer interactions to driving product development and innovation. The importance of generative AI in a business context lies in its potential to augment human creativity, improve operational efficiency, and unlock new business opportunities, making it a critical tool for enterprises aiming to maintain a competitive edge in the digital age. 
Despite its transformational potential, the implementation of generative AI in businesses faces significant challenges. High implementation costs, the complexity of integrating AI systems into existing infrastructures, a shortage of skilled specialists, and ethical issues related to data privacy and AI-generated content are among the primary obstacles. Additionally, businesses often struggle to align AI initiatives with their strategic goals and ensure that AI outputs meet high standards of quality and compliance. These barriers complicate the large-scale and effective adoption of generative AI, limiting its potential to revolutionize business operations and innovation.
The research encompasses a comprehensive review of existing literature, analysis of real-world cases, and synthesis of best practices in the strategic implementation of generative AI. It explores strategic models for the deployment of generative AI in enterprises, identifies key drivers and barriers to its adoption in business environments, and examines the strategic approaches businesses use to integrate generative AI into their operations. Insights and recommendations are provided for enterprises considering the adoption of generative AI technologies. The importance of the research lies in its potential to help businesses overcome implementation challenges and maximize the benefits of generative AI. Understanding strategic models and approaches to AI integration will enable businesses to better navigate the complexities of AI deployment, enhance their innovative capabilities, and ensure sustainable growth in an increasingly competitive market. The conclusions drawn from the research aim to bridge the gap between theoretical insights and practical applications of generative AI, providing a valuable resource for business leaders, technology strategists, and policymakers seeking to leverage AI for competitive advantage.</jats:p>"
10.37514/dbh-j.2023.11.1.05,Generative AI: The Voice of the Other,N/A
10.36948/ijfmr.2024.v06i02.14528,Programming Skills Recommendation System with Generative AI,"<jats:p>In today’s rapidly evolving technological landscape, the demand for proficient programmers across various domains continues to surge. However, identifying and cultivating the right programming skills remains a daunting task for individuals seeking to enter or advance in the field. To address this challenge, our team has embarked on developing a Programming Skill Recommendation System (PSRS). The PSRS is envisioned as an intelligent platform leveraging machine learning algorithms and data analytics to provide personalized programming skill recommendations. By analysing diverse data sources such as user profiles, project requirements, industry trends, and skill proficiency.</jats:p>"
10.1109/mitp.2023.3338026,StyleGAN-Based Advanced Semantic Segment Encoder for Generative AI,N/A
10.62802/gt1b4t58,Generative Artificial Intelligence in Education: Cheating Threat or Enhanced Learning Tool?,"<jats:p>Integrating generative AI into education has sparked a debate regarding its ethical and practical implications. This paper explores the transformative potential of generative AI in various educational contexts, highlighting its ability to generate new data and aid personalized learning. It also addresses concerns about academic integrity and learning quality. This paper advocates for a balanced approach to AI in education by examining different perspectives, including educators and technologists. It suggests that AI can be a valuable educational tool by enhancing student learning and teacher productivity. The discussion extends to AI implementation's potential economic and social challenges and proposes solutions to ensure equal access. Ultimately, the paper concludes that with responsible use, AI can significantly improve educational outcomes and efficiency and offer innovative support for students and educators.</jats:p>"
10.1002/npcr.32365,Practical (and Ethical) Uses For Generative AI,N/A
10.36227/techrxiv.24153183.v1,"Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol and the NIST-approved Quantum-Resistant Cryptographic Algorithms","<jats:p>&lt;p&gt;In the contemporary digital age, Quantum Computing and Artificial Intelligence (AI) convergence is reshaping the cyber landscape, introducing both unprecedented opportunities and potential vulnerabilities.&lt;/p&gt;
&lt;p&gt;This research, conducted over five years, delves into the cybersecurity implications of this convergence, with a particular focus on AI/Natural Language Processing (NLP) models and quantum cryptographic protocols, notably the BB84 method and specific NIST-approved algorithms. Utilising Python and C++ as primary computational tools, the study employs a ""red teaming"" approach, simulating potential cyber-attacks to assess the robustness of quantum security measures. Preliminary research over 12 months laid the groundwork, which this study seeks to expand upon, aiming to translate theoretical insights into actionable, real-world cybersecurity solutions. Located at the University of Oxford's technology precinct, the research benefits from state-of-the-art infrastructure and a rich collaborative environment. The study's overarching goal is to ensure that as the digital world transitions to quantum-enhanced operations, it remains resilient against AI-driven cyber threats. The research aims to foster a safer, quantum-ready digital future through iterative testing, feedback integration, and continuous improvement. The findings are intended for broad dissemination, ensuring that the knowledge benefits academia and the global community, emphasising the responsible and secure harnessing of quantum technology.&lt;/p&gt;</jats:p>"
10.18559/ebr.2023.2.744,Judgements of research co-created by generative AI: experimental evidence,"<jats:p>The introduction of ChatGPT has fuelled a public debate on the appropriateness of using generative AI (large language models; LLMs) in work, including a debate on how they might be used (and abused) by researchers. In the current work, we test whether delegating parts of the research process to LLMs leads people to distrust researchers and devalues their scientific work. Participants (N = 402) considered a researcher who delegates elements of the research process to a PhD student or LLM and rated three aspects of such delegation. First, they rated whether it is morally appropriate to do so. Secondly, they judged whether – after deciding to delegate the research process – they would trust the scientist (that decided to delegate) to oversee future projects. Thirdly, they rated the expected accuracy and quality of the output from the delegated research process. Our results show that people judged delegating to an LLM as less morally acceptable than delegating to a human (d = -0.78). Delegation to an LLM also decreased trust to oversee future research projects (d = -0.80), and people thought the results would be less accurate and of lower quality (d = -0.85). We discuss how this devaluation might transfer into the underreporting of generative AI use.</jats:p>"
10.2139/ssrn.4756641,"Regulating Competition in Generative AI: A Matter of Trajectory, Timing and Tools",N/A
10.31234/osf.io/rn97c,The Potential of Generative AI for Personalized Persuasion at Scale,"<p>Matching the language or content of a message to the psychological profile of its recipient (known as “personalized persuasion”) is widely considered to be one of the most effective messaging strategies. We demonstrate that the rapid advances in large language models (LLMs), like ChatGPT, could accelerate this influence by making personalized persuasion scalable. Across four studies (consisting of seven sub-studies; total N = 1,788), we show that personalized messages crafted by ChatGPT exhibit significantly more influence than non-personalized messages. This was true across different domains of persuasion (e.g., marketing of consumer products, political appeals for climate action), psychological profiles (e.g., personality traits, political ideology, moral foundations), and when only providing the LLM with a single, short prompt naming or describing the targeted psychological dimension. Thus, our findings are among the first to demonstrate the potential for LLMs to automate, and thereby scale, the use of personalized persuasion in ways that enhance its effectiveness and efficiency. We discuss the implications for researchers, practitioners, and the general public.</p>"
10.24818/mer/2024.02-11,Current and Future Trends in Strategic Procurement with Generative AI,"<jats:p>It is difficult to predict with certainty how the outlook for strategic procurement will be in the next decade, but the current trends of Generative AI have already had a significant impact. This study aims to analyse the recent research focused on Generative AI strategic procurement and global supply chains with structured pros and cons of the way they could impact the strategies. For this purpose, a literature review based on 19 research articles and studies was performed to identify the areas of strategic procurement where the Generative AI could effectively perform specific tasks and to formulate shared perspectives for greater impacts. As with any new technology, the use of Generative AI is not without effort and setbacks. There are several outstanding topics, and it is important to consider the highlights and the lowlights when integrating Generative AI into strategic procurement processes. Based on this analysis, the following areas of the strategic procurement activities, with most of the value added by the implementation of the new AI technologies, were identified, in this priority order: 1. Ethical considerations, 2. Market intelligence, 3. Risk management, 4. Supplier management, 5. Contract management. The results of this research may be of interest to category procurement professionals. Further research is needed to investigate cost-effective methods for further development of the Generative AI digital transformation.</jats:p>"
10.21125/edulearn.2024.0990,NEW TECHNOLOGIES IN EDUCATION: GENERATIVE AI AS A POSTPHENOMENOLOGICAL PHENOMENON,N/A
10.69554/qjnc6054,Generative AI: A master or servant of market research analysis?,"<jats:p xml:lang=""en"">This paper explores generative AI's potential impact on the analytics element of the market research process, examining whether AI is destined to become an analysis master (which reduces humans to a minor role), or whether it will play the role of a faithful, trusted and tireless servant to human researchers. Version 4.0 of ChatGPT was used to conduct a series of tasks ranging from the analysis of desk research to primary research qualitative transcripts, quantitative survey open-ended comments and numerical data. The paper concludes that the hype around generative AI is indeed justified. In its current state of evolution, ChatGPT is an extraordinarily efficient extractor, organiser, processor and summariser of qualitative, quantitative and secondary research data. However, its capability is more akin to that of a competent junior consultant; for projects which require a greater experience and understanding of the human condition (empathy, intuition, creative and abstract thinking), humans remain as, if not more, important than ever in helping brands to remain relevant and grow in an increasingly fast-moving and complex world. The paper concludes that a generative AI like ChatGPT 4.0 is an extremely smart, tireless, diligent collaborator which frees (or perhaps forces) human researchers to up their game so they can apply their uniquely human skills and value to the research process.</jats:p>"
10.1109/uemcon59035.2023.10316128,HarmonyVisage: Ethical Facial Emotion Dataset Using Advanced Generative AI,N/A
10.37868/sei.v5i2.id196,Generative AI: Challenges to higher education,"<jats:p>Generative Artificial Intelligence has rapidly expanded its footprint of use in educational institutions. It has been embraced by students, faculty, and staff alike. The technology is capable of carrying out a sustained sequence of interactive dialogs and creating reasonably meaningful text. Not surprisingly it seems to be routinely used by faculty to generate questions and assignments, by students to submit assignments and aid in self-learning, and administration to create manuals, memoranda, and policy documents. With its potential to lead to significant social innovation, tethering on the verge of becoming a disruptive technology, it seems most unlikely that it will fade away without being fully enfolded into almost all aspects of academic and pedagogical activity. While it is early to predict the exact place of this technology in education, we present thoughts to aid deliberations and give a brief review of the opportunities and challenges.</jats:p>"
10.54097/czptrz11,"Generative AI: An In-depth Exploration of Methods, Uses, and Challenges","<jats:p>Recently, artificial intelligence has surged to the forefront of computer science, with generative AI emerging as the most sought-after research area. The success of generative AI hinges on advancements in algorithms, training frameworks, and data. Among these, training algorithms are of paramount importance. This article aims to shed light on several leading training algorithms in the domain. Generative AI has left a profound imprint on a myriad of industries. Intelligent publishing, advertising content creation, and finance are just a few sectors that have been revolutionized by this technology. As with all significant technological shifts, generative AI is not without its challenges. The implications it holds for employment, intellectual property rights, as well as security and privacy concerns, are profound. It's vital for stakeholders in the AI domain and beyond to consider and address these challenges. As generative AI continues to integrate more deeply into industries and our daily lives, proactive steps need to be taken to ensure ethical, secure, and equitable use. This not only guarantees the continued growth and trust in the technology but also safeguards society's values and norms in the face of rapid innovation.</jats:p>"
10.1016/j.nxmate.2024.100275,Enhancing mechanical and bioinspired materials through generative AI approaches,N/A
10.30862/jri.v4i1.392,Bibliometric analysis: Learning using generative AI,"<jats:p>This research aims to 1) analyze the number of articles per year and their types, 2) analyze the five articles with the most citations, 3) analyze keywords in the data collected, and 4) analyze the relationship between authors. Bibliometric analysis was used to help researchers study bibliographic content and citation analysis of each article taken from Harzing's Publish or Perish (PoP) 8 database. The data was taken from Scopus January 2018-2024 entitled, “learning using generative AI” in English. The maximum number of articles accessed is 500 articles. The collected articles are stored in CSV and RIS format. Articles were filtered according to analysis needs using MS Excel and VOSViewer. Twenty-nine documents from the Scopus database have been used in this research. The results show that 1) in 2022, there was one manuscript; in 2023, there were 27 manuscripts; and in 2024, there was one manuscript published. There are ten manuscripts published in conference proceedings, 14 articles in journals, and five manuscripts in edited books; 2) Suh, Popenici, Crosthwaite, Kim, and Chan wrote the 5 most cited articles; 3) Future research topics include GPT, student agency, learning approaches, higher education, state-of-the-art, reinforcement learning, opportunities, and challenges. Potential future research (novelty) could use keywords that are not yet widely used, including pedagogy, simulations, Socratic tutors, teaching methods, neuro-inclusive learning, and metacognition; 4) a network of writers outside the group consisting of Han, Ariel; Leu, U; Leu, Eunso; Lim, Cheoil; Kim, Hyeoncheol; Lee, Jeongji; Kim, Jiwon is the cluster with the highest relationship between nodes.</jats:p>"
10.36227/techrxiv.24153183,"Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol and the NIST-approved Quantum-Resistant Cryptographic Algorithms","<jats:p>&lt;p&gt;In the contemporary digital age, Quantum Computing and Artificial Intelligence (AI) convergence is reshaping the cyber landscape, introducing both unprecedented opportunities and potential vulnerabilities.&lt;/p&gt;
&lt;p&gt;This research, conducted over five years, delves into the cybersecurity implications of this convergence, with a particular focus on AI/Natural Language Processing (NLP) models and quantum cryptographic protocols, notably the BB84 method and specific NIST-approved algorithms. Utilising Python and C++ as primary computational tools, the study employs a ""red teaming"" approach, simulating potential cyber-attacks to assess the robustness of quantum security measures. Preliminary research over 12 months laid the groundwork, which this study seeks to expand upon, aiming to translate theoretical insights into actionable, real-world cybersecurity solutions. Located at the University of Oxford's technology precinct, the research benefits from state-of-the-art infrastructure and a rich collaborative environment. The study's overarching goal is to ensure that as the digital world transitions to quantum-enhanced operations, it remains resilient against AI-driven cyber threats. The research aims to foster a safer, quantum-ready digital future through iterative testing, feedback integration, and continuous improvement. The findings are intended for broad dissemination, ensuring that the knowledge benefits academia and the global community, emphasising the responsible and secure harnessing of quantum technology.&lt;/p&gt;</jats:p>"
10.36227/techrxiv.171078030.08340862/v1,A Learning Agreement for Generative AI Use in University Courses: A Pilot Study,"<jats:p id=""p1"">The rapidly evolving landscape of Generative AI (GenAI) tools
necessitate continuous vigilance and adaptation by educators. This
dynamism requires stakeholders to stay up to date with developments to
address emerging issues effectively, creating complexity in managing the
responsible and ethical use of GenAI. This paper presents a pilot study
involving the use of student learning agreements for governing GenAI use
in a first-year engineering degree course. The learning agreement
contains ethical and social considerations students agree to make if
they decide to use GenAI in the course. As part of the pilot study,
pre-post surveys and student artefacts – a group assignment adapted to
accommodate GenAI use – were analysed. Results show that the vast
majority of students were in favour of the learning agreement approach
both at the start and upon completion of the course. However, 7 of 17
groups did not use GenAI in their assignment. Of the 10 groups that did,
only 1 acknowledged GenAI limitations in adherence with the learning
agreement. A thematic analysis of student suggestions for improving the
learning agreement approach include suggestions for making the agreement
easier to understand and adhere to (e.g., providing specific examples,
reengaging with the agreement during the course). Overall, findings
suggest that learning agreements have the potential to offer an
interface through which student decision-making can be supported and
interactions among students, educators, researchers, and policy makers
related to the ethical and societal challenges of GenAI can take place.</jats:p>"
10.21125/inted.2024.1481,GENERATIVEREADER: EXPLORING THE USE OF GENERATIVE AI IN ENHANCING READING EXPERIENCES FOR CHILDREN WITH DYSLEXIA,N/A
10.4324/9781003482918,Using Generative AI Effectively in Higher Education,N/A
10.46743/2160-3715/2024.6637,"How Can Generative AI (GenAI) Enhance or Hinder Qualitative Studies? A Critical Appraisal from South Asia, Nepal","<jats:p>Qualitative researchers can benefit from using generative artificial intelligence (GenAI), such as different versions of ChatGPT—GPT-3.5 or GPT-4, Google Bard—now renamed as a Gemini, and Bing Chat—now renamed as a Copilot, in their studies. The scientific community has used artificial intelligence (AI) tools in various ways. However, using GenAI has generated concerns regarding potential research unreliability, bias, and unethical outcomes in GenAI-generated research results. Considering these concerns, the purpose of this commentary is to review the current use of GenAI in qualitative research, including its strengths, limitations, and ethical dilemmas from the perspective of critical appraisal from South Asia, Nepal. I explore the controversy surrounding the proper acknowledgment of GenAI or AI use in qualitative studies and how GenAI can support or challenge qualitative studies. First, I discuss what qualitative researchers need to know about GenAI in their research. Second, I examine how GenAI can be a valuable tool in qualitative research as a co-author, a conversational platform, and a research assistant for enhancing and hindering qualitative studies. Third, I address the ethical issues of using GenAI in qualitative studies. Fourth, I share my perspectives on the future of GenAI in qualitative research. I would like to recognize and record the utilization of GenAI and/or AI alongside my cognitive and evaluative abilities in constructing this critical appraisal. I offer ethical guidance on when and how to appropriately recognize the use of GenAI in qualitative studies. Finally, I offer some remarks on the implications of using GenAI in qualitative studies</jats:p>"
10.37074/jalt.2024.7.2.27,"Generative AI in higher education: Perspectives of students, educators and administrators",N/A
10.69554/qcbp4952,Editorial - Generative AI and its impact on marketing analytics: A special issue,N/A
10.1145/3587421.3595416,Generative AI for Concept Creation in Footwear Design,N/A
10.1145/3635636.3656190,Evolving Roles and Workflows of Creative Practitioners in the Age of Generative AI,N/A
10.1097/nne.0000000000001453,Using Generative AI to Produce Images for Nursing Education,N/A
10.2139/ssrn.4927611,Generative Ai on Innovation Performance of Construction Enterprises: A Knowledge-Based Dynamic Capabilities Perspective,N/A
10.31235/osf.io/b3ezy,Generative AI in the Workplace: Employee Perspectives of ChatGPT Benefits and Organizational Policies,"<p>Research Questions•What attitudes do professionals hold towards the impact of AI on society and their jobs? How do early adopters of ChatGPT differ in their attitudes towards the impact of AI on society and their jobs? •In what ways are professionals using ChatGPT? How do these uses differ by managerial status?•What do professional perceive as benefits of generative AI? How do they differ in these views based on level of ChatGPT adoption and managerial status?•What do professionals perceive as the benefits of organizational policy for the use of generative AI? How do these views differ based on level of ChatGPT adoption and managerial status?Samples•Study 1 involved 148 working adults in the United States who had heard of ChatGPT•Study 2 involved 395 working adults in the United States who had heard of ChatGPTResearch Questions•What attitudes do professionals hold towards the impact of AI on society and their jobs? How do early adopters of ChatGPT differ from non-adopters in their attitudes towards the impact of AI on society and their jobs? •In what ways are professionals using ChatGPT? How do these uses differ by managerial status?•What do professionals perceive as benefits of generative AI? How do they differ in these views based on level of ChatGPT adoption and managerial status?•What do professionals perceive as the benefits of organizational policy for the use of generative AI? How do these views differ based on level of ChatGPT adoption and managerial status?Samples•Study 1 involved 148 working adults in the United States who had heard of ChatGPT•Study 2 involved 395 working adults in the United States who had heard of ChatGPTKey Findings and Conclusions•Many US workers in this sample are using ChatGPT for professional purposes. Roughly the following percentages have already used ChatGPT in the following ways:o42% for researching a topic or generating ideaso32% for drafting messageso26% for drafting longer documents, such as reportso22% for editing text•Many US workers in this sample believe ChatGPT can help them become better communicators. This is particularly the case for executives and managers. Roughly two thirds of executives (67%) and managers (64%) believe generative AI can help them communicate more effectively. •Early adopters of ChatGPT in this sample hold much different views of generative AI than do non-users of ChatGPT. Early adopters hold the following distinctive views:oThey are much more likely to think AI is good for society than non-users (64% to 22%) and believe it will make them more productive (82% for early adopters; 26% for non-users); however, they are also more likely to worry about the ethical implications of AI (68% to 55%) in the workplace and worry that their own job will be replaced by AI (41% to 20%).oThey are much more likely to think generative AI will support them in their work. About 85% of early adopters say that ChatGPT can help them generate ideas for work compared to about 50% of non-users. About 73% of early adopters say it can improve the quality of their work compared to 42% of non-users. About 74% of early adopters say it can help them communicate more effectively compared to 41% of non-users. Executives and managers are slightly more likely to be enthusiastic about the benefits.•Employees in organizations with generative AI policies view these policies positively. Those who are aware of an organizational policy about generative AI generally believe it has supported more comfort in using ChatGPT for work, has improved trust, has improved efficiency, and has provided legal protections. Those who are early adopters are generally more positive about each of these benefits of organizational policy than those who are non-users of ChatGPT.•Most early adopters of generative AI in organizations without generative AI policies want more guidance about ChatGPT use. Most early adopters believe an organizational policy would make them more comfortable using ChatGPT (61%), that it would increase trust (56%), and that it would improve efficiency (66%).Key Recommendations•Develop generative AI policies that support innovation and efficiency while putting into place legal safeguards for organizations and their employees.•Use a social contracts approach to develop generative AI policies.</p>"
10.14445/22312803/ijctt-v72i5p106,Generative AI in Digital Advertising Campaigns,N/A
10.31274/td-20240617-32,AI and fashion: Student perspectives on the application and ethical use of various forms of Generative Artificial Intelligence (GAI) in a fashion context,N/A
10.3410/f.737286584.793570630,Faculty Opinions recommendation of Assessing the impact of generative AI on medicinal chemistry.,N/A
10.18608/jla.2023.8333,Generative AI and Learning Analytics,"<jats:p>This editorial looks back at the Journal of Learning Analytics (JLA) in 2023 and forward to 2024. Considering the recent proliferation of large language models such as GPT4 and Bard, the first section of this editorial points to the need for robust Generative AI (GenAI) analytics, calling for consideration of how GenAI may impact learning analytics research and practice. The second section looks back over the past year, providing statistics on submissions and considering the cost of publication in an open-access journal.</jats:p>"
10.3390/rel15091059,Attention (to Virtuosity) Is All You Need: Religious Studies Pedagogy and Generative AI,"<jats:p>The launch of ChatGPT in November of 2022 provides the rare opportunity to consider both what artificial intelligence (AI) is and what human experts are. In the spirit of making the most of this opportunity, we invite the reader to follow a suggestive series of “what if” questions that lead to a plausible settlement in which the human expert and the generative AI system collaborate pedagogically to shape the (human) religious studies student. (1) What if, contrary to the Baconian frame, humans reason primarily by exercising intellectual virtuosity, and only secondarily by means of rules-based inference? (2) What if, even though we train AI models on human-generated data by means of rules-based algorithms, the resulting systems demonstrate the potential for exercising intellectual virtuosity? (3) What if, by deprioritizing mechanistic and algorithmic models of human cognition while being open to the possibility that AI represents a different species of cognition, we open a future in which human and AI virtuosos mutually inspire, enrich, and even catechize one another?</jats:p>"
10.54254/2755-2721/48/20241095,The power of generative AI in cybersecurity: Opportunities and challenges,"<jats:p>This paper undertakes a comprehensive exploration of the potential and challenges presented by Generative Artificial Intelligence, with particular emphasis on the GPT models, in the field of cybersecurity. Through a meticulous examination of existing literature and pertinent case studies, the paper evaluates the capabilities of these models in the detection and rectification of vulnerabilities, as well as in identifying malicious code. It also highlights the pivotal role of generative AI in enhancing honeypot technology, which has shown promising results in proactive threat detection. While underscoring the significant advantages of utilizing generative AI in bolstering cybersecurity measures, the paper does not shy away from shedding light on the accompanying security exposures. These range from traditional threats like vulnerabilities and privacy breaches to novel dangers such as jailbreaking, prompt injection, and prompt leakage that are associated with the deployment of these AI models. The overarching objective of this paper is to contribute to the ongoing dialogue about the integration of advanced AI technologies into cybersecurity strategies while emphasizing the importance of vigilance against potential misuse. The paper concludes with a call for continued research and development to ensure a safer and more secure cyberspace for all.</jats:p>"
10.2139/ssrn.4628126,Professional Development Needs of Teacher Educators for Generative AI Literacy and Application in Ghana,N/A
10.1201/9781032720104-23,Generative Artificial Intelligence as the Killer App,N/A
10.1109/emr.2024.3452682,Data Conversion in ERP SaaS Implementation with Generative AI,N/A
10.3390/info15090542,Generative AI and Its Implications for Definitions of Trust,"<jats:p>In this paper, we undertake a critical analysis of how chatbots built on generative artificial intelligence impact assumptions underlying definitions of trust. We engage a particular definition of trust and the object-oriented model of trust that was built upon it and identify how at least four implicit assumptions may no longer hold. Those assumptions include that people generally provide others with a default level of trust, the ability to identify whether the trusted agent is human or artificial, that risk and trust can be readily quantified or categorized, and that there is no expectation of gain by agents engaged in trust relationships. Based on that analysis, we suggest modifications to the definition and model to accommodate the features of generative AI chatbots. Our changes re-emphasize developers’ responsibility for the impacts of their AI artifacts, no matter how sophisticated the artifact may be. The changes also reflect that trust relationships are more fraught when participants in such relationships are not confident in identifying the nature of a potential trust partner.</jats:p>"
10.29007/hvzc,A Case Study on the Generative AI Project Life Cycle Using Large Language Models,"<jats:p>Large Language Models represent a disruptive technology set to revolutionize the fu- ture of artificial intelligence. While numerous literature reviews and survey articles discuss their benefits and address security and compliance concerns, there remains a shortage of research exploring the implementation life cycle of generative AI systems. This paper addresses this gap by presenting the various phases of the generative AI life cycle and detailing the development of a chatbot designed to address inquiries from prospective stu- dents. Utilizing Google Flan LLM and a question-answering pipeline, we processed user prompts. In addition, we compiled an input file containing domain knowledge of the edu- cation program, which was preprocessed and condensed into vector embeddings using the HuggingFace library. Furthermore, we designed a chat interface for user interaction using Streamlit. The responses generated by the chatbot are both descriptive and contextu- ally pertinent to the prompts, with their quality improving in response to more detailed prompts. However, a significant constraint is the size limit of the input file, given the processing power limitations of CPUs.</jats:p>"
10.3997/2214-4609.202439056,Scaling Generative AI for E&amp;P,N/A
10.1109/educon60312.2024.10578894,Empowering Professionals: A Generative AI Approach to Personalized Cybersecurity Learning,N/A
10.46397/jaih.8.4,A Study on the Necessity and Content Composition of AI Ethics Education,N/A
10.22318/icls2024.259570,Learning About and Against Generative AI Through Mapping Generative AI’s Ecologies and Developing a Luddite Praxis,N/A
10.1093/oso/9780197644461.003.0007,"Ethics, Justice, and Health AI","<jats:title>Abstract</jats:title>
               <jats:p>This chapter explores the differences between Ethical AI’s focus on technical solutions and auditing frameworks, and Just AI’s focus on precaution in artificial intelligence (AI) development. With reference to specific case studies in AI-based pain measurement, the chapter investigates the competing ethical challenges of ostensibly bias-correcting AI and efforts to extend care to people who are incapable of speech. In so doing, it explores the fraught ethical and clinical landscapes created by efforts to use technology to address racism and ableism in medicine. A primary argument of this chapter is that Ethical AI and Just AI need to come together in conversation, as each area of inquiry has important insights to contribute. The chapter closes with specific recommendations for better community-centered AI development.</jats:p>"
10.1007/s00146-024-01922-2,An elemental ethics for artificial intelligence: water as resistance within AI’s value chain,"<jats:title>Abstract</jats:title><jats:p>Research and activism have increasingly denounced the problematic environmental record of the infrastructure and value chain underpinning artificial intelligence (AI). Water-intensive data centres, polluting mineral extraction and e-waste dumping are incontrovertibly part of AI’s footprint. In this article, I turn to areas affected by AI-fuelled environmental harm and identify an ethics of resistance emerging from local activists, which I term ‘elemental ethics’. Elemental ethics interrogates the AI value chain’s problematic relationship with the elements that make up the world, critiques the undermining of local and ancestral approaches to nature and reveals the vital and quotidian harms engendered by so-called intelligent systems. While this ethics is emerging from grassroots and Indigenous groups, it echoes recent calls from environmental philosophy to reconnect with the environment via the elements. In empirical terms, this article looks at groups in Chile resisting a Google data centre project in Santiago and lithium extraction (used for rechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert. As I show, elemental ethics can complement top-down, utilitarian and quantitative approaches to AI ethics and sustainable AI as well as interrogate whose lived experience and well-being counts in debates on AI extinction.</jats:p>"
10.1007/978-3-030-54173-6_12,Applying AI on the Battlefield: The Ethical Debates,"<jats:title>Abstract</jats:title><jats:p>Because lethal autonomous weapon systems (LAWS) are designed to make targeting decisions without the direct intervention of human agents (who are “out of the killing loop”), considerable debate has arisen on whether this mode of autonomous targeting should be deemed morally permissible. Surveying the contours of this debate, the authors first present a prominent ethical argument that has been advanced in favor of LAWS, namely, that AI-directed robotic combatants have an advantage over their human counterparts, insofar as the former operate solely on the basis of rational assessment, while the latter are often swayed by emotions that conduce to poor judgment. Several counter arguments are then presented, inter alia, (1) that emotions have a positive influence on moral judgment and are indispensable to it; (2) that it is a violation of human dignity to be killed by a machine, as opposed to being killed by a human being; and (3) that the honor of the military profession hinges on maintaining an equality of risk between combatants, an equality that would be removed if one side delegates its fighting to robots. The chapter concludes with a reflection on the moral challenges posed by human-AI teaming in battlefield settings, and how virtue ethics provides a valuable framework for addressing these challenges.</jats:p>"
10.1016/j.bpj.2023.11.1993,Generative AI efficiently enriches scHi-C datasets at high resolution,N/A
10.2139/ssrn.4736295,Corporate Responses to Generative AI: Early Evidence from Conference Calls,N/A
10.22323/2.23050602,#AISCICOMM24. Discussing the role of (generative) AI for science communication research and science communication practice,"<jats:p>
The annual conference of the Science Communication Division of the German Communication Association (DGPuK) was held in Zurich, Switzerland, from 5–7 June 2024. The conference attracted around 125 researchers and science communication practitioners from Europe and beyond. In this review, I provide an overview of the conference and discuss some of the challenges for researching AI in science communication as well as for science communication practice.</jats:p>"
10.2139/ssrn.4725206,Generative Ai as (Un)Welcome Agents in Medical Crowdfunding: The Trust Dilemma and Moral Hazard,N/A
10.37074/jalt.2024.7.1.28,Higher education assessment practice in the era of generative AI tools,N/A
10.1016/j.athoracsur.2024.05.021,Generative AI in Clinical Medicine: Opportunity for Improvement,N/A
10.21203/rs.3.rs-4497795/v1,Rihla: A Tourism Chatbot System Powered by Generative AI for Riyadh and AlUla,"<title>Abstract</title>
        <p>International tourism has increased by 121% since July 2022, driven by efforts to fulfil Saudi Vision 2030 and capitalize on an AI-driven economy. With AI revolutionizing the tourism and travel sectors, Chatbots have emerged as popular tools, leveraging AI and natural language processing (NLP) to automate responses and enhance user experiences. This study addresses Saudi Vision 2030’s objectives by introducing an intelligent Chatbot for improved tourism experiences in Saudi Arabia. Through integrating the text-divinci-002 model with Flask, the Chatbot delivers prompt and precise responses, catering to tourists’ inquiries. The findings demonstrate its high usability and utility in assisting travelers. This research aims to contribute the ongoing advancements in AI-driven tourism, aligning with the broader vision of transforming Saudi Arabia’s tourism industry.</p>"
10.2139/ssrn.4657253,Scaling Culture in Blockchain Gaming: Generative AI and Pseudonymous Engagement,N/A
10.3997/2214-4609.202439088,"Improve Geoscience Interpretation, Reporting, and Transparency with Generative AI",N/A
10.2139/ssrn.4638260,Redefining a normative framework for Meritocracy in the era of Generative AI: An Inter-disciplinary perspective,N/A
10.1515/9781501518317-019,Chapter 18: Generative AI and Consciousness,N/A
10.2139/ssrn.4670714,Generative AI and Content-Creator Economy: Evidence from Online Content Creation Platforms,N/A
10.58875/guyg6120,Generative AI Is a Crisis for Copyright Law,N/A
10.4018/979-8-3693-3719-6,Machine Learning and Generative AI in Smart Healthcare,N/A
10.1109/icse-fose59343.2023.00009,Software Testing of Generative AI Systems: Challenges and Opportunities,N/A
10.1109/cog60054.2024.10645549,Generative AI with GOAP for Fast-Paced Dynamic Decision-Making in Game Environments,N/A
10.47116/apjcri.2024.07.06,Generative AI: A Chronological Review,N/A
10.2139/ssrn.4593660,From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI,N/A
10.24294/jipd.v8i8.6253,Copyright and generative AI,"<jats:p>This research investigates the relationship between Generative Artificial Intelligence (GAI), media content, and copyright laws. As GAI technologies continue to evolve and permeate various aspects of the media landscape, questions regarding the creation and protection of intellectual property have become paramount. The study aims to highlight the impact of GAI generated content, and the challenge it poses to the traditional copyright framework. Furthermore, the research addresses the evolving role of copyright laws in adapting to the dynamic landscape shaped by artificial intelligence. It investigates whether existing legal frameworks are equipped to handle the complexities introduced by GAI, or if there is a need for legislative and policy reforms. Ultimately, this research contributes to the ongoing discourse on the intersection of GAI, media, and copyrights, providing insights that can guide policymakers, legal practitioners, and industry stakeholders in navigating the evolving landscape of intellectual property in the age of artificial intelligence.</jats:p>"
10.1080/00396338.2023.2261260,"The Consequences of Generative AI for Democracy, Governance and War",N/A
10.24839/2164-9812.eye29.1.6,Psychological Literacy Is Critical in a World of Generative AI,N/A
10.20944/preprints202408.1230.v1,Human-Robot Collaboration in Business Environments: Leveraging GPU-Accelerated Computer Vision and Generative AI for Enhanced Productivity and Safety,"<jats:p>As businesses increasingly adopt advanced technologies to enhance productivity and safety, the integration of human-robot collaboration (HRC) emerges as a transformative approach. This paper explores the role of GPU-accelerated computer vision and generative AI in optimizing HRC within business environments. By leveraging the computational power of GPUs, real-time data processing, and advanced machine learning algorithms, robots can better interpret complex visual cues and adapt to dynamic workspaces. Generative AI further enables the design of intelligent robotic systems capable of anticipating human actions, optimizing task allocation, and ensuring safety through predictive modeling. This research highlights the potential of these technologies to improve operational efficiency, reduce human error, and create safer, more adaptable workplaces. The implications of such advancements are discussed, providing insights into the future of HRC in various industries.</jats:p>"
10.1515/9781474421126-015,CHAPTER 12 Generative Futures: On Affirmative Ethics,N/A
10.1145/3461702.3462469,Examining Religion Bias in AI Text Generators,N/A
10.1007/s00146-018-0848-2,"Mental time-travel, semantic flexibility, and A.I. ethics",N/A
10.1007/978-3-030-66913-3_11,Ethical AI Implementation,N/A
10.1007/s00146-010-0277-3,Erratum to: Ethics and aesthetics of technologies,N/A
10.29173/irie482,Forgotten African AI Narratives and the future of AI in Africa,"<jats:p>


Ancient and contemporary imaginative thoughts, stories, literary works, and beliefs about intelligent machines or otherwise known as AI narratives influence the development, implementation and governance of AI. Responsible AI therefore requires the understanding of these narratives. However, in the global AI narratives discourse, narratives of AI from Africa are missing or are often forgotten. Potentially, this has implications for how AI is or will be designed, deployed and regulated in Africa. This paper presents insights into our understanding of the reasons why Africa’s AI narratives are often missing, the implications this has for the future of AI in Africa, how the situation can be improved and the path to take to achieve responsible AI in Africa. These insights emerged following a workshop organized at Mozilla Festival 2021 and demonstrates the growing need to explore uncovered AI narratives in Africa to ensure better AI outcomes.


</jats:p>"
10.4337/9781802209525.00007,Modern AI ethics is a field in the making,N/A
10.1007/978-981-19-2531-3_8,Start of the “Age of Exploration” of AI Governance,N/A
10.1525/9780520397545-009,Chapter Six Incorporating Ethics into the AI Clinical Decision Support System Life Cycle,N/A
10.1007/s00146-015-0642-3,Ethics of responsibilities distributions in a technological culture,N/A
10.69554/fppy5406,The AI ethics officer and responding to data standards and accountability,N/A
10.21203/rs.3.rs-4844649/v1,Learning about AI ethics from cases: A scoping review of AI incident repositories and cases,"<title>Abstract</title>
        <p><bold>Background</bold>
Cases provide a practical resource for learning regarding the uses and challenges of AI applications. Cases give insight into how principles and values are implicated in real contexts, the trade-offs and different perspectives held regarding these contexts, and the – sometimes hidden – relationships between cases, relationships that may support analogical reasoning across contexts.
<bold>Objective</bold>
We aim to (1) provide an approach for structuring ethics cases and (2) investigate how repositories of cases structure those cases and their content.
<bold>Approach</bold>
: We motivate a scoping review through a conceptual analysis of ethics case desiderata. The review sought to retrieve repositories, (sometimes known as observatories, catalogues, galleries, or incident databases), and their cases, for analysis of their expression of ethics concepts.
<bold>Results</bold>
We identify n = 14 repositories, extracting the case schema used in each, to identify how this metadata can express ethical concepts. We find that most repositories focus on harm-indicators, with some indicating positive impacts, but with little explicit reference to ethical concepts; a subset (n = 4) includes no structural elements addressing ethical concepts or impacts. We extract a subset of cases from the total cases (n = 2,000) across repositories, identifying and extracting the subset of cases addressing education (n = 100). These are grouped by topic, with a structured content analysis provided of ethical implications from one sub-theme, offering qualitative insights into the ethical coverage.
<bold>Implications</bold>
Our conceptual analysis and empirical review exemplify a model for ethics cases (shorthanded as Ethics-case-CPR), while highlighting gaps both in existing case repositories and specific examples of cases.</p>"
10.7717/peerj-cs.1879,Evaluating generative AI integration in Saudi Arabian education: a mixed-methods study,"<jats:p>Incorporating generative artificial intelligence (GAI) in education has become crucial in contemporary educational environments. This research article thoroughly investigates the ramifications of implementing GAI in the higher education context of Saudi Arabia, employing a blend of quantitative and qualitative research approaches. Survey-based quantitative data reveals a noteworthy correlation between educators’ awareness of GAI and the frequency of its application. Notably, around half of the surveyed educators are at stages characterized by understanding and familiarity with GAI integration, indicating a tangible readiness for its adoption. Moreover, the study’s quantitative findings underscore the perceived value and ease associated with integrating GAI, thus reinforcing the assumption that educators are motivated and inclined to integrate GAI tools like ChatGPT into their teaching methodologies. In addition to the quantitative analysis, qualitative insights from in-depth interviews with educators unveil a rich tapestry of perspectives. The qualitative data emphasizes GAI’s role as a catalyst for collaborative learning, contributing to professional development, and fostering innovative teaching practices.</jats:p>"
10.22541/au.171507303.31456510/v1,A Survey of Machine-Readable Code of Regulations and Its Applications on Machine Learning with a Focus on Generative AI,N/A
10.2139/ssrn.4825716,The Valuation Paradox of Generative AI: Evidence from Gig Workers,N/A
10.2218/newreal.9249,Artist’s Roundtable - The artists’ take on Generative AI,"<jats:p>When looking for actionable strategies, insights and recommendations, often we look to researchers, consultants and external experts. It’s crucial though, that we gain insight from those at the coal-face; and with respect to Generative AI and the creative industry, that’s the artists who are making the work and building the tools.
The New Real’s creative agent, Caroline Sinders, sat down with three other artists working with Generative AI – Lex Fefegha, Eryk Salvaggio and Amelia Winger-Bearskin – to explore future landscapes for Generative AI Arts, find out what co-creation between AI and artists can look like, and - simply - capture what artists want from both AI and those influential people in the broader ecosystem of funding, curation, museums and policy-making.</jats:p>"
10.2139/ssrn.4764418,Generative AI and Price Discrimination in the Housing Market,N/A
10.18260/1-2--48100,The Future of Learning: Harnessing Generative AI for Enhanced Engineering Technology Education,N/A
10.4324/9781032648033-8,Ways forward,N/A
10.53987/2178-5368-2023-12-03,On Creativity and Generative-AI Aesthetics: some thoughts and concerns,N/A
10.2139/ssrn.4768788,How Will Ai-Generative Content Empower Virtual Reality Classrooms? Implementations and Investigations,N/A
10.1145/3458553.3458565,Neural generative models and representation learning for information retrieval,"<jats:p>
            Information Retrieval (IR) concerns about the structure, analysis, organization, storage, and retrieval of information. Among different retrieval models proposed in the past decades, generative retrieval models, especially those under the statistical probabilistic framework, are one of the most popular techniques that have been widely applied to Information Retrieval problems. While they are famous for their well-grounded theory and good empirical performance in text retrieval, their applications in IR are often limited by their complexity and low extendability in the modeling of high-dimensional information. Recently, advances in deep learning techniques provide new opportunities for representation learning and generative models for information retrieval. In contrast to statistical models, neural models have much more flexibility because they model information and data correlation in latent spaces without explicitly relying on any prior knowledge. Previous studies on pattern recognition and natural language processing have shown that semantically meaningful representations of text, images, and many types of information can be acquired with neural models through supervised or unsupervised training. Nonetheless, the effectiveness of neural models for information retrieval is mostly unexplored. In this thesis, we study how to develop new generative models and representation learning frameworks with neural models for information retrieval. Specifically, our contributions include three main components: (1)
            <jats:italic>Theoretical Analysis</jats:italic>
            : We present the first theoretical analysis and adaptation of existing neural embedding models for ad-hoc retrieval tasks; (2)
            <jats:italic>Design Practice</jats:italic>
            : Based on our experience and knowledge, we show how to design an embedding-based neural generative model for practical information retrieval tasks such as personalized product search; And (3)
            <jats:italic>Generic Framework</jats:italic>
            : We further generalize our proposed neural generative framework for complicated heterogeneous information retrieval scenarios that concern text, images, knowledge entities, and their relationships. Empirical results show that the proposed neural generative framework can effectively learn information representations and construct retrieval models that outperform the state-of-the-art systems in a variety of IR tasks.
          </jats:p>"
10.2139/ssrn.4844976,Generative AI Enhances Team Performance and Reduces Need for Traditional Teams,N/A
10.1016/j.bushor.2024.04.014,Innovating by prompting: How to facilitate innovation in the age of generative AI,N/A
10.1097/js9.0000000000001689,Applications of generative AI in peer review process: friend or foe?,N/A
10.2139/ssrn.4885867,The Impact of Generative AI on Tenant-Driven Commercial Real Estate Valuation,N/A
10.14361/9783839474723-002,Art Intelligence,N/A
10.1109/ictc58733.2023.10392465,A Brief Survey of Watermarks in Generative AI,N/A
10.1007/978-981-13-2262-4_314-1,Generative AI as a Writing Technology: Challenges and Opportunities for School Writing,N/A
10.11606/issn.1982-8160.v18i2p7-18,Separate and Reassemble: Generative AI through the lens of art and media histories,"<jats:p>AI image generation represents a logical evolution from early digital media algorithms, starting with basic paint programs in the 1970s and advancing to sophisticated 3D graphics and media creation software by the 1990s. Early algorithms struggled to simulate materials and effects, but advances in the 1970s and 1980s led to realistic simulations of natural phenomena and artistic techniques. Generative AI continues this trend, using neural networks to combine and interpolate visual patterns from extensive datasets. This method of digital media creation underscores the modular and discrete nature of computer-generated imagery, distinguishing it from traditional optical media.</jats:p>"
10.1007/s43681-021-00058-z,Conceptual and normative approaches to AI governance for a global digital ecosystem supportive of the UN Sustainable Development Goals (SDGs),"<jats:title>Abstract</jats:title><jats:p>AI governance is like one of those mythical creatures that everyone speaks of but which no one has seen. Sometimes, it is reduced to a list of shared principles such as transparency, non-discrimination, and sustainability; at other times, it is conflated with specific mechanisms for certification of algorithmic solutions or ways to protect the privacy of personal data. We suggest a conceptual and normative approach to AI governance in the context of a global digital public goods ecosystem to enable progress on the UN Sustainable Development Goals (SDGs). Conceptually, we propose rooting this approach in the human capability concept—what people are able to do and to be, and in a layered governance framework connecting the local to the global. Normatively, we suggest the following six irreducibles: <jats:underline>a</jats:underline>. human rights first; <jats:underline>b</jats:underline>. multi-stakeholder smart regulation; <jats:underline>c</jats:underline>. privacy and protection of personal data; <jats:underline>d</jats:underline>. a holistic approach to data use captured by the 3Ms—misuse of data, missed use of data and missing data; <jats:underline>e</jats:underline>. global collaboration (‘digital cooperation’); <jats:underline>f</jats:underline>. basing governance more in practice, in particular, thinking separately and together about data and algorithms. Throughout the article, we use examples from the health domain particularly in the current context of the Covid-19 pandemic. We conclude by arguing that taking a distributed but coordinated global digital commons approach to the governance of AI is the best guarantee of citizen-centered and societally beneficial use of digital technologies for the SDGs.</jats:p>"
10.1007/s43681-022-00167-3,Meaningful human control: actionable properties for AI system development,"<jats:title>Abstract</jats:title><jats:p>How can humans remain in control of artificial intelligence (AI)-based systems designed to perform tasks autonomously? Such systems are increasingly ubiquitous, creating benefits - but also undesirable situations where moral responsibility for their actions cannot be properly attributed to any particular person or group. The concept of meaningful human control has been proposed to address responsibility gaps and mitigate them by establishing conditions that enable a proper attribution of responsibility for humans; however, clear requirements for researchers, designers, and engineers are yet inexistent, making the development of AI-based systems that remain under meaningful human control challenging. In this paper, we address the gap between philosophical theory and engineering practice by identifying, through an iterative process of abductive thinking, four actionable properties for AI-based systems under meaningful human control, which we discuss making use of two applications scenarios: automated vehicles and AI-based hiring. First, a system in which humans and AI algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate. Second, humans and AI agents within the system should have appropriate and mutually compatible representations. Third, responsibility attributed to a human should be commensurate with that human’s ability and authority to control the system. Fourth, there should be explicit links between the actions of the AI agents and actions of humans who are aware of their moral responsibility. We argue that these four properties will support practically minded professionals to take concrete steps toward designing and engineering for AI systems that facilitate meaningful human control.</jats:p>"
10.7551/mitpress/8975.003.0010,[ Front Matter ],N/A
10.2196/preprints.56665,Ethics and Governance of Neurotechnology in Africa: Lessons From AI (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>As a novel technology frontier, neurotechnology is revolutionizing our perceptions of the brain and nervous system. With growing private and public investments, a thriving ecosystem of direct-to-consumer neurotechnologies has also emerged. These technologies are increasingly being introduced in many parts of the world, including Africa. However, as the use of this technology expands, neuroethics and ethics of emerging technology scholars are bringing attention to the critical concerns it raises. These concerns are largely not new but are uniquely amplified by the novelty of technology. They include ethical and legal issues such as privacy, human rights, human identity, bias, autonomy, and safety, which are part of the artificial intelligence ethics discourse. Most importantly, there is an obvious lack of regulatory oversight and a dearth of literature on the consideration of contextual ethical principles in the design and application of neurotechnology in Africa. This paper highlights lessons African stakeholders need to learn from the ethics and governance of artificial intelligence to ensure the design of ethically responsible and socially acceptable neurotechnology in and for Africa.</p>
                </sec>"
10.1007/s43681-024-00479-6,A global scale comparison of risk aggregation in AI assessment frameworks,"<jats:title>Abstract</jats:title><jats:p>AI applications bear inherent risks in various risk dimensions, such as insufficient reliability, robustness, fairness or data protection. It is well-known that trade-offs between these dimensions can arise, for example, a highly accurate AI application may reflect unfairness and bias of the real-world data, or may provide hard-to-explain outcomes because of its internal complexity. AI risk assessment frameworks aim to provide systematic approaches to risk assessment in various dimensions. The overall trustworthiness assessment is then generated by some form of risk aggregation among the risk dimensions. This paper provides a systematic overview on risk aggregation schemes used in existing AI risk assessment frameworks, focusing on the question how potential trade-offs among the risk dimensions are incorporated. To this end, we examine how the general risk notion, the application context, the extent of risk quantification, and specific instructions for evaluation may influence overall risk aggregation. We discuss our findings in the current frameworks in terms of whether they provide meaningful and practicable guidance. Lastly, we derive recommendations for the further operationalization of risk aggregation both from horizontal and vertical perspectives.</jats:p>"
10.1145/3461702.3462617,Skilled and Mobile: Survey Evidence of AI Researchers' Immigration Preferences,N/A
10.18230/tjye.2024.32.1.29,The Nature of AI Ethics Education in the Posthuman Era,"<jats:p>In light of the development and widespread adoption of artificial intelligence (AI), ethical issues surrounding AI have garnered significant attention in both academia and society. Consequently, this paper explores the theoretical foundations of AI ethics based on posthumanism and seeks to outline directions for AI ethics education. The core research questions of this study are twofold: first, what are the theoretical foundations of posthuman ethics, and second, what are the practical considerations and educational directions for posthuman AI ethics? The key conclusions of this paper can be summarized as follows. Firstly, ethical challenges are increasingly important for AI developers, emphasizing the need to establish supervisory systems that highlight ethical elements and incorporate ethical considerations into algorithm design. Secondly, AI ethics education should encompass not only the ethical attitudes of human users but also the ethical roles of AI itself. Thirdly, fundamental ethical considerations related to societal changes in the posthuman era are essential now and in the future. This paper aims to stimulate discussions in academia and educational settings by providing essential directions for AI ethics and education in anticipation of a future posthuman society.</jats:p>"
10.1007/s43681-023-00261-0,Regulating lethal autonomous weapon systems: exploring the challenges of explainability and traceability,"<jats:title>Abstract</jats:title><jats:p>We explore existing political commitments by states regarding the development and use of lethal autonomous weapon systems. We carry out two background reviewing efforts, the first addressing ethical and legal framings and proposals from recent academic literature, the second addressing recent formal policy principles as endorsed by states, with a focus on the principles adopted by the United States Department of Defense and the North Atlantic Treaty Organization. We then develop two conceptual case studies. The first addresses the interrelated principles of explainability and traceability, leading to proposals for acceptable scope limitations to these principles. The second considers the topic of deception in warfare and how it may be viewed in the context of ethical principles for lethal autonomous weapon systems.</jats:p>"
10.55574/xwtd1709,ARTIFICIAL INTELLIGENCE (AI) IN SCIENCE: IS THE SCIENTIST OR AI LIABLE WHEN A BELIEVER’S HUMAN RIGHTS ARE VIOLATED?,"<jats:p>Artificial Intelligence (AI) may appear to be one of the newest and most talked about areas of science amidst the current 4th Industrial Revolution (4IR), but it has, in fact, been under development since the beginning of time, from Arabic Alchemy to (Jewish) Talmudic scholar Rabbi Judah Loew ben Bezalel’s 16th century interpretation of Golem. More recently discussed only in the realm of science fiction movies, AI has now comfortably and securely entered the highest circles of academia, industry, and government. However, experts have only just begun to look at the impact of AI on human rights violations and God. As AI and technology become integral parts of our working lives, this essay aims to answer the question of whether the scientist or AI will be held liable when a Believer’s human rights are violated (physical and psychological violations) and whether the European Union’s Directive 85/374/EEC legislation is adequate in tackling this currently very niche issue.</jats:p>"
10.3390/ai4040046,From Trustworthy Principles to a Trustworthy Development Process: The Need and Elements of Trusted Development of AI Systems,"<jats:p>The current endeavor of moving AI ethics from theory to practice can frequently be observed in academia and industry and indicates a major achievement in the theoretical understanding of responsible AI. Its practical application, however, currently poses challenges, as mechanisms for translating the proposed principles into easily feasible actions are often considered unclear and not ready for practice. In particular, a lack of uniform, standardized approaches that are aligned with regulatory provisions is often highlighted by practitioners as a major drawback to the practical realization of AI governance. To address these challenges, we propose a stronger shift in focus from solely the trustworthiness of AI products to the perceived trustworthiness of the development process by introducing a concept for a trustworthy development process for AI systems. We derive this process from a semi-systematic literature analysis of common AI governance documents to identify the most prominent measures for operationalizing responsible AI and compare them to implications for AI providers from EU-centered regulatory frameworks. Assessing the resulting process along derived characteristics of trustworthy processes shows that, while clarity is often mentioned as a major drawback, and many AI providers tend to wait for finalized regulations before reacting, the summarized landscape of proposed AI governance mechanisms can already cover many of the binding and non-binding demands circulating similar activities to address fundamental risks. Furthermore, while many factors of procedural trustworthiness are already fulfilled, limitations are seen particularly due to the vagueness of currently proposed measures, calling for a detailing of measures based on use cases and the system’s context.</jats:p>"
10.1007/978-3-030-51110-4,An Introduction to Ethics in Robotics and AI,N/A
10.7551/mitpress/8975.003.0004,Moral Agency,N/A
10.31219/osf.io/u58jy,Robot Doctors: The Cost and Ethics of AI in Healthcare,<p>Robot Doctors: The Cost and Ethics of AI in Healthcare</p>
10.2139/ssrn.4336465,Ethics in German AI Start-Ups,N/A
10.7551/mitpress/8975.003.0005,Moral Patiency,N/A
10.7551/mitpress/8975.001.0001,The Machine Question,"<jats:p>An investigation into the assignment of moral responsibilities and rights to intelligent and autonomous machines of our own making.</jats:p>
               <jats:p>One of the enduring concerns of moral philosophy is deciding who or what is deserving of ethical consideration. Much recent attention has been devoted to the ""animal question""—consideration of the moral status of nonhuman animals. In this book, David Gunkel takes up the ""machine question"": whether and to what extent intelligent and autonomous machines of our own making can be considered to have legitimate moral responsibilities and any legitimate claim to moral consideration.</jats:p>
               <jats:p>The machine question poses a fundamental challenge to moral thinking, questioning the traditional philosophical conceptualization of technology as a tool or instrument to be used by human agents. Gunkel begins by addressing the question of machine moral agency: whether a machine might be considered a legitimate moral agent that could be held responsible for decisions and actions. He then approaches the machine question from the other side, considering whether a machine might be a moral patient due legitimate moral consideration. Finally, Gunkel considers some recent innovations in moral philosophy and critical theory that complicate the machine question, deconstructing the binary agent–patient opposition itself.</jats:p>
               <jats:p>Technological advances may prompt us to wonder if the science fiction of computers and robots whose actions affect their human companions (think of HAL in 2001: A Space Odyssey) could become science fact. Gunkel's argument promises to influence future considerations of ethics, ourselves, and the other entities who inhabit this world.</jats:p>"
10.1002/9781119551966.ch48,Independent AI Ethics Committees and ESG Corporate Reporting on AI as Emerging Corporate and AI Governance Trends,N/A
10.1007/978-981-99-9836-4_26,"New Frontiers for AI, Eco-Art, and Public Space Intersection: Fostering Interactive Environmental Engagements",N/A
10.1007/s43681-023-00282-9,Public perceptions of autonomous lethal weapons systems,"<jats:title>Abstract</jats:title><jats:p>This study attempts to bridge the gap in empirical and philosophical research on lethal autonomous weapons systems (LAWS), through a survey of attitudes using experimental methods. “LAWS” refer to “fully autonomous weapons” that can set attack targets without human involvement and are lethal. Based on previous research, we conducted a randomized controlled experiment to create, present, and collect responses to scenarios describing military operations and outcomes that are likely to express awareness of the ethical issues raised by LAWS. First, our hypothesis that LAWS are less likely to be used was rejected, and the opposite trend was observed. Second, the hypothesis that civilian casualties rather than combatant casualties would influence LAWS use was strongly and significantly confirmed. Third, the hypothesis that remote weapons are more likely to be used than LAWS was rejected. Fourth, there was some support for the hypothesis that LAWS are more likely to be used in homeland defense. Fifth, the hypothesis that male and younger individuals are more willing to use LAWS was strongly and significantly confirmed for male, but not on the basis of age. This study highlights the need for further discussion based on these findings.</jats:p>"
10.1007/s43681-023-00274-9,The participatory value-sensitive design (VSD) of a mHealth app targeting citizens with dementia in a Danish municipality,N/A
10.54941/ahfe1004568,Integrating Generative AI into the Design Process: A Case Study on Space Engineering,"<jats:p>Developing a product that satisfies certain standards follows a methodical approach known as the design process. It involves multiple phases of requirement elicitation, analysis, conceptualization, and evaluation. Generative Artificial Intelligence (AI) tools like ChatGPT have impacted the design process in various engineering fields, but there is limited research on how to integrate AI tools into the design process effectively. In a case study, we used ChatGPT to design an innovative object - a space boot with haptic technology. We explored various prompt methods and found that the process is iterative, requiring multiple adjustments to the prompts. This research paper highlights the design process and its evaluation methods, discusses its limitations, and provides a plan for future improvements.</jats:p>"
10.4018/979-8-3693-2440-0.ch023,"The Role of AI in Skilling, Upskilling, and Reskilling the Workforce","<jats:p>Rapid technical breakthroughs and economic upheavals in the modern labour market require constantly evolving workforce capabilities. This change is sparked by AI, which introduces individualised learning paths and transforms conventional training methods. This study explores how artificial intelligence (AI) uses machine learning algorithms and natural language processing to create personalised training programmes and identify skill gaps.Analysing AI's capacity to deliver customised learning experiences, the abstract probes how AI systems adjust to different learners' preferences for speed, style, and degree of expertise. It explores intelligent tutoring systems, AI-powered recommendation engines, and adaptive learning systems, emphasising their function in selecting tailored information according to student performance and preferences. Case examples from the real world demonstrate how AI can improve worker training programmes in various sectors and how scalable and flexible it can be in huge organisations and small and medium-sized businesses.</jats:p>"
10.1152/physrev.00029.2023,The role of quality metrics in the evolution of AI in health care and implications for generative AI,N/A
10.1145/3599975.3599979,The Recent Wave of Generative AI Systems: What Does This Tell Us About What AI Can Do Now?,"<jats:p>No, this paper content is not generated by AI. Not sure if that question came across my editor's mind while reviewing submissions. It's a rightful question nowadays after the wave of generative AI apps that swamped our lives between chatGPT to King Charles coronation afterparty videos. Finally, AI is here in our hands that everyone of us can touch and integrate in daily life; after long being a fancy research topic. The question now is how this is going to shape our industry? Or more proactively, how generative AI can help us improve our craft and processes. This paper surveys key areas where generative AI can help us shape our industry and the way we work.</jats:p>"
10.1145/3313831.3376739,Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models,N/A
10.1088/978-0-7503-6116-3ch1,Preamble,N/A
10.2139/ssrn.4553185,Defining Organizational AI Governance and Ethics,N/A
10.56934/spicerian.v72i2.178,Ethics and AI,N/A
10.1515/9783111323749-011,11 A survey of AI in industry: from basic concepts to industrial and business applications,N/A
10.37736/kjlr.2023.10.14.5.10,Study on how to use ‘generative AI’ in Writing Education for College students,"<jats:p>This study introduces case studies conducted under the premise that college students with relatively poor writing and discussion skills can get help to prepare their writing or discussion with the help of an AI program. The results of the discussion are as follows. First, it was confirmed that AI can be a good guide for learners in all stages of writing, and that better results can be produced through cooperative work. Second, it was confirmed that the emphasis in writing education using AI, albeit in a limited way, should be focused on developing prompt skills. Third, writing education using AI has two limitations for learners. The problem is when it is used without accurate confirmation and verification of the possibility of plagiarism and inaccurate or false information that may be mixed in the result. Therefore, it is urgent to prepare guidelines in terms of writing ethics. In addition, in terms of writing education, learners are awakened to the urgent need for verification and confirmation training.</jats:p>"
10.47294/ksbda.25.2.9,A Feasibility Study on Relief Creation Methods Based on Generative AI,N/A
10.4018/979-8-3693-0074-9.ch009,Harnessing AI and Big Data for Ethical and Efficient Decisions,"<jats:p>Smart technologies like AI and big data have transformed decision-making. These tools are becoming increasingly popular in various businesses, but India's political sector has not used them. The previous two years have seen a pandemic that has impacted society. The chapter investigated whether AI and big data can be used in politics to make faster, more efficient, and morally acceptable decisions, and how politicians can ensure ethical use. Question: How can big data and AI help politicians make ethical decisions? Using relevant literature, the qualitative investigation found utilization, problems, ethical issues, and hazards. Semi-structured interviews with politicians and AI experts were also done to determine if the technologies may be used in politics. Before AI can support decision-making, much political data must be digitized. In conclusion, AI and big data can streamline Indian decision-making through document reading and analysis. There is no ethical framework for AI and big data utilization today, making ethical decisions impossible.</jats:p>"
10.17148/iarjset.2022.91020,AI-Driven Enhancements in Cloud Computing: Exploring the Synergies of Machine Learning and Generative AI,N/A
10.4018/979-8-3693-2440-0.ch014,Revolutionizing Pedagogy,"<jats:p>This study investigates how AI coaching systems might boost Malaysian teacher professionalism. The study analyses how AI coaching affects teachers' teaching, self-reflection, and career advancement. Mixed-method research uses qualitative interviews, quantitative assessments, and observational analysis. Malaysian educational institution AI coaching system and instructor case studies are examined. This technology customises classes and gives professors professional development feedback. AI coaching appears to deliver quick, personalised feedback to improve instructors. These studies show how AI plus human expertise can improve education and meet student demands. This study promotes AI coaching as an educational innovation engine. To demonstrate how AI enhances digital educators' professionalism, learning, and stakeholder and policymaker comprehension. AI can alter education through teacher professional development, according to this study. Results improve Malaysian teacher development and technology by changing attitudes, policies, and beliefs.</jats:p>"
10.2139/ssrn.3391293,AI Ethics – Too Principled to Fail?,N/A
10.1007/978-981-19-2531-3,AI Ethics and Governance,N/A
10.1007/s43681-022-00249-2,There is an elephant in the room: towards a critique on the use of fairness in biometrics,"<jats:title>Abstract</jats:title><jats:p>The proliferation of biometric systems in our societies is shaping public debates around its political, social and ethical implications. Yet, whilst concerns towards the racialised use of this technology have been on the rise, the field of biometrics remains unperturbed by these debates. Despite the lack of critical analysis, algorithmic fairness has recently been adopted by biometrics. Different studies have been published to understand and mitigate demographic bias in biometric systems, without analysing the political consequences. In this paper, we offer a critical reading of recent debates about biometric fairness and show its detachment from political debates. Building on previous fairness demonstrations, we prove that biometrics will be always biased. Yet, we claim algorithmic fairness cannot distribute justice in scenarios which are broken or whose intended purpose is to discriminate. By focusing on demographic biases rather than examine how these systems reproduce historical and political injustices, fairness has overshadowed the elephant in the room of biometrics.</jats:p>"
10.1007/978-3-031-15746-2_13,Artificial Intelligence (AI) in a Time of Pandemics: Developing Options for the Ethical Governance of COVID-19 AI Applications,"<jats:title>Abstract</jats:title><jats:p>This chapter analyses the various applications of artificial intelligence (AI) developed in the context of the COVID-19 pandemic and examines the range of ethical questions that their multi-level deployment may raise. Within this frame, the author sheds light on the challenges posed by the fast-tracking authorization of some of the AI systems and pays particular attention to the form and shape that ‘emergency response’ in the field of ethics has taken in order to cope with these extraordinary challenges and the ethical practices that have been developed thus far. The chapter will also provide a detailed set of policy suggestions to overcome these challenges with a special focus on the need to develop an emergency ethics framework that will allow policy-makers to authorize the deployment of AI-powered tools in a responsible and trustworthy manner.</jats:p>"
10.1007/s43681-020-00014-3,"Smile, you are being identified! Risks and measures for the use of facial recognition in (semi-)public spaces",N/A
10.1007/s43681-022-00194-0,Reconsidering the regulation of facial recognition in public spaces,N/A
10.1007/978-3-030-69978-9_3,Concepts of Ethics and Their Application to AI,"<jats:title>Abstract</jats:title><jats:p>Any discussion of the ethics of AI needs to be based on a sound understanding of the concept of ethics. This chapter therefore provides a brief overview of some of the key approaches to ethics with a particular emphasis on virtue ethics and the idea of human flourishing. The chapter reviews the purposes for which AI can be used, as these have a bearing on an ethical evaluation. Three main purposes are distinguished: AI forefficiency, optimisation and profit maximisation, AI forsocial control and AI for human flourishing. Given the focus on human flourishing in this book, several theoretical positions are introduced that provide insights into different aspects and ways of promoting human flourishing. The chapter concludes with a discussion of the currently widespread principle-based approach to AI ethics.</jats:p>"
10.1007/s43681-021-00101-z,Artificial intelligence and the judicial memory: the great misunderstanding,N/A
10.1093/oxfordhb/9780190067397.013.3,Ethical Issues in Our Relationship with Artificial Entities,"<p>This chapter examines the ethics of human relationships with artificial entities—bots, robots, and other computational systems created to interact with humans as if they were sentient and autonomous individuals. Sentience—the ability to have emotions, to feel pain and want to avoid it—is a core concept here. All currently existing artificial entities are nonsentient, but their interactions and design evoke the impression of a conscious entity with personality and emotion. This impression of consciousness is inherently deceptive. Some artificial entities are beneficial, while others are manipulative and harmful. The chapter then addresses ethical issues in the design and deployment of artificial entities, considering the ethical responsibilities of researchers and designers.</p>"
10.1093/oxfordhb/9780190067397.013.52,Algorithms and the Social Organization of Work,"<p>This chapter argues that the proliferation of automated algorithms in the workplace raises questions as to how they might be used in service of the control of workers. In particular, scholars have noted machine learning algorithms as prompting a data-centric reorganization of the workplace and a <italic>quantification</italic> of the worker. The chapter then considers ethical issues implicated by three emergent algorithmic-driven work technologies: automated hiring platforms (AHPs), wearable workplace technologies, and customer relationship management (CRM). AHPs are “digital intermediaries that invite submission of data from one party through preset interfaces and structured protocols, process that data via proprietary algorithms, and deliver the sorted data to a second party.” The use of AHPs involves every stage of the hiring process, from the initial sourcing of candidates to the eventual selection of candidates from the applicant pool. Meanwhile, wearable workplace technologies exist in a variety of forms that vary in terms of design and use, from wristbands used to track employee location and productivity to exoskeletons used to assist employees performing strenuous labor. Finally, CRM is an approach to managing current and potential customer interaction and experience with a company using technology. CRM practices typically involve the use of customer data to develop customer insight to build customer relationships.</p>"
10.1007/s43681-024-00508-4,Optimizing fairness and accuracy: a Pareto optimal approach for decision-making,"<jats:title>Abstract</jats:title><jats:p>In the era of data-driven decision-making, ensuring fairness and equality in machine learning models has become increasingly crucial. Multiple fairness definitions have been brought forward to evaluate and mitigate unintended fairness-related harms in real-world applications, with little research on addressing their interactions with each other. This paper explores the application of a Minimax Pareto-optimized solution to optimize individual and group fairness at individual and group levels on the Adult Census Income dataset as well as on the German Credit dataset. The objective of training a classification model with a multi-objective loss function is to achieve fair outcomes without compromising utility objectives. We investigate the interplay of different fairness definitions, including definitions of performance consistency and traditional group and individual fairness measures, amongst each other coupled with performance. The results presented in this paper highlight the feasibility of incorporating several fairness considerations into machine learning models, which can be applied to use cases with multiple sensitive features and attributes that characterize real-world applications. This research is a valuable step toward building responsible and transparent machine learning systems that can be incorporated into critical decision-making processes.</jats:p>"
10.1007/s43681-022-00177-1,Explainability as fig leaf? An exploration of experts’ ethical expectations towards machine learning in psychiatry,"<jats:title>Abstract</jats:title><jats:p>The increasing implementation of programs supported by machine learning in medical contexts will affect psychiatry. It is crucial to accompany this development with careful ethical considerations informed by empirical research involving experts from the field, to identify existing problems, and to address them with fine-grained ethical reflection. We conducted semi-structured qualitative interviews with 15 experts from Germany and Switzerland with training in medicine and neuroscience on the assistive use of machine learning in psychiatry. We used reflexive thematic analysis to identify key ethical expectations and attitudes towards machine learning systems. Experts’ ethical expectations towards machine learning in psychiatry partially challenge orthodoxies from the field. We relate these challenges to three themes, namely (1) ethical challenges of machine learning research, (2) the role of explainability in research and clinical application, and (3) the relation of patients, physicians, and machine learning system. Participants were divided regarding the value of explainability, as promoted by recent guidelines for ethical artificial intelligence, and highlighted that explainability may be used as an ethical fig leaf to cover shortfalls in data acquisition. Experts recommended increased attention to machine learning methodology, and the education of physicians as first steps towards a potential use of machine learning systems in psychiatry. Our findings stress the need for domain-specific ethical research, scrutinizing the use of machine learning in different medical specialties. Critical ethical research should further examine the value of explainability for an ethical development of machine learning systems and strive towards an appropriate framework to communicate ML-based medical predictions.</jats:p>"
10.4018/979-8-3693-1798-3.ch006,The Role of Generative AI-Assisted Literature Reviews in Transforming Academic Research,"<jats:p>The chapter explores the transformative impact of generative artificial intelligence (AI) technologies on the traditional process of conducting literature reviews in academic research. Initially, it traces the historical evolution of literature reviews, highlighting their significance in shaping research across various fields. Generative AI technologies have revolutionized literature reviews by automating and enhancing information-gathering and synthesis. This democratizes the literature review process, making it more inclusive and accessible to researchers with limited resources. However, this advancement also brings challenges and ethical considerations regarding academic integrity and the reliability of AI-generated content. The chapter also discusses different types of literature reviews, their methodologies, strengths, and limitations, providing a comprehensive understanding of how generative AI can be integrated into these review types.</jats:p>"
10.22251/jlcci.2024.24.14.171,Writing instruction method using generative artificial intelligence(AI),"<jats:p>Objectives  In this study, we explored practical teaching and learning approaches that utilize generative artificial intelligence, which already influences various aspects of our lives, to assist students in their writing endeavors.
Methods  I proposed concrete teaching and learning methods for utilizing the generative AI, Luton, in the stage of content generation in writing. I explored application methods for addressing writing issues that authors encounter in areas such as topic selection, generating ideas, and content creation. Additionally, I investigated the possibility of implementing a program for self-assessment, enabling authors to receive evaluations of their writing outputs and make self-revisions accordingly.
Results  Generative artificial intelligence (AI) can be used as a tool to provide inspiration during creative activities, offer solutions for content creation, and assist in generating ideas for simple outlines or storyboards. Additionally, it proved helpful in producing writing materials and facilitating checks and feedback on written work.
Conclusions  We hope to enhance writing instruction and improve students' writing abilities through the utilization of generative artificial intelligence (AI).</jats:p>"
10.1016/j.chb.2024.108429,“I feel AI is neither too good nor too bad”: Unveiling Chinese EFL teachers’ perceived emotions in generative AI-Mediated L2 classes,N/A
10.17148/imrjr.2024.010103,"The Future of Automotive Manufacturing: Integrating AI, ML, and Generative AI for Next-Gen Automatic Cars",N/A
10.1145/3641399.3641434,Tutorial Report on Legacy Software Modernization: A Journey From Non-AI to Generative AI Approaches,N/A
10.1145/3656156.3663691,Re.Dis.Cover Place with Generative AI: Exploring the Experience and Design of City Wandering with image-to-image AI,N/A
10.1007/s11948-024-00476-2,Artificial Intelligence and Agency: Tie-breaking in AI Decision-Making,"<jats:title>Abstract</jats:title><jats:p>Determining the agency-status of machines and AI has never been more pressing. As we progress into a future where humans and machines more closely co-exist, understanding hallmark features of agency affords us the ability to develop policy and narratives which cater to both humans and machines. This paper maintains that decision-making processes largely underpin agential action, and that in most instances, these processes yield good results in terms of making good choices. However, in some instances, when faced with two (or more) choices, an agent may find themselves with equal reasons to choose either - thus being presented with a tie. This paper argues that in the event of a tie, the ability to create a voluntarist reason is a hallmark feature of agency, and second, that AI, through current tie-breaking mechanisms does not have this ability, and thus fails at this particular feature of agency.</jats:p>"
10.26904/rf-135-1215426699,Human-Centred AI: Bridging the gap between ethics and practice,N/A
10.1145/3375627,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",N/A
10.32473/flairs.37.1.135275,Ethics of AI Explained,"<jats:p>This tutorial introduces participants to the main issues and themes pertaining to ethics of artificial intelligence (AI). Analyzing what ethics of AI is by reflecting on our understanding of both ethics and AI, the aim is to clarify and expose how ethics of AI can be conceived in different ways depending on the approach one adopts. Through the use of concrete examples, participants will be introduced to various issues in the ethics of AI literature, allowing them to reflect upon their own research and practice.</jats:p>"
10.36227/techrxiv.172373798.84756169/v1,AI Ethics: Who is Accountable for the Decisions Made by Machines?,N/A
10.7551/mitpress/8975.003.0006,Thinking Otherwise,N/A
10.3102/ip.24.2111055,Investigating Undergraduate Students’ Perspectives of AI Ethics About ChatGPT (Poster 29),N/A
10.4018/979-8-3693-0240-8.ch009,AI as an Accessibility Tool,"<jats:p>While generative AI is often discussed in terms of challenging notions of academic integrity, it is increasingly looked at as a potential tool to break down barriers in post-secondary education. In particular, this chapter considers how generative artificial intelligence technologies could be leveraged to provide multiple means of engagement, representation and expression (CAST, n.d.). In other words, how can the use of generative AI support social justice in higher education. An ethical approach to generative AI requires respect for the diversity among learners. An ethical approach to generative AI must reflect fairness, which requires recognizing the inherent biases built into the post-secondary learning environments. Using generative AI to support universal design for learning shifts this narrative, considering how technology acceptance could support social justice in post-secondary learning.</jats:p>"
10.1145/3643691.3648588,Do Generative AI Tools Ensure Green Code? An Investigative Study,N/A
10.9728/dcs.2024.25.4.981,Analysis of Korean AI Ethics Research: A Focus on Network Text Analysis,N/A
10.1007/s43681-022-00176-2,Review of the state of the art in autonomous artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>This article presents a new design for autonomous artificial intelligence (AI), based on the state-of-the-art algorithms, and describes a new autonomous AI system called ‘AutoAI’. The methodology is used to assemble the design founded on self-improved algorithms that use new and emerging sources of data (NEFD). The objective of the article is to conceptualise the design of a novel AutoAI algorithm. The conceptual approach is used to advance into building new and improved algorithms. The article integrates and consolidates the findings from existing literature and advances the AutoAI design into (1) using new and emerging sources of data for teaching and training AI algorithms and (2) enabling AI algorithms to use automated tools for training new and improved algorithms. This approach is going beyond the state-of-the-art in AI algorithms and suggests a design that enables autonomous algorithms to self-optimise and self-adapt, and on a higher level, be capable to self-procreate.</jats:p>"
10.1007/s43681-022-00227-8,Turing test-inspired method for analysis of biases prevalent in artificial intelligence-based medical imaging,N/A
10.1007/s43681-023-00392-4,Toward a safe MLOps process for the continuous development and safety assurance of ML-based systems in the railway domain,N/A
10.47636/gkca.2023.7.1.75,A Pilot Study on Teaching Chinese Classics in the Era of AI: Focusing on Ancient Chinese Poetry Education with Generative AI,"<jats:p>This essay started with concerns about the direction of Chinese classical literature education in the era of artificial intelligence. In particular, with the recent prolonged COVID-19 pandemic, the so-called “non-face-to-face” concept has penetrated deep into our lives, and it is very natural for many people to show high interest in ChatGPT, which emerged at a time when people’s dependence on Information Technology (IT) has increased.
Of course, it is true that we have mostly become accustomed to the term “artificial intelligence” since earlier, but the exact concept of AI, especially how AI and humanities can be fused, has not yet been clearly summarized. Therefore, through the process of examining the concept of AI from a humanities perspective, some opinions were presented on the question of what kind of relationship AI and humanities can have and how it can live together.
Among AIs, in the case of so-called “generative artificial intelligence,” the amount of knowledge and intelligence have exploded recently, and the number of service subscribers has exploded, and it has the characteristic of diversifying within a short period of time. Therefore, in this essay, as one of the ways to utilize Generative AI, the discussion focused on how to use Chinese classical poetry for education, and also tried to include concerns and alternatives on how to use AI in general classical education in the future with various examples.</jats:p>"
10.1007/s43681-020-00035-y,A set of distinct facial traits learned by machines is not predictive of appearance bias in the wild,"<jats:title>Abstract</jats:title><jats:p>Research in social psychology has shown that people’s biased, subjective judgments about another’s personality based solely on their appearance are not predictive of their actual personality traits. But researchers and companies often utilize computer vision models to predict similarly subjective personality attributes such as “employability”. We seek to determine whether state-of-the-art, black box face processing technology can learn human-like appearance biases. With features extracted with FaceNet, a widely used face recognition framework, we train a transfer learning model on human subjects’ first impressions of personality traits in other faces as measured by social psychologists. We find that features extracted with FaceNet can be used to predict human appearance bias scores for deliberately manipulated faces but not for randomly generated faces scored by humans. Additionally, in contrast to work with human biases in social psychology, the model does not find a significant signal correlating politicians’ vote shares with perceived competence bias. With Local Interpretable Model-Agnostic Explanations (LIME), we provide several explanations for this discrepancy. Our results suggest that some signals of appearance bias documented in social psychology are not embedded by the machine learning techniques we investigate. We shed light on the ways in which appearance bias could be embedded in face processing technology and cast further doubt on the practice of predicting subjective traits based on appearances.</jats:p>"
10.1136/jme-2023-108945,AI knows best? Avoiding the traps of paternalism and other pitfalls of AI-based patient preference prediction,N/A
10.30582/kdps.2024.38.2.5,An Examination of Copyrightability and Creativity in AI-generated Works: Focusing on the Instrumentality of Generative AI,"<jats:p>With burgeoning interest in generative artificial intelligence (AI), the legal status of outputs generated by such technologies, henceforth referred to as AI-generated works, has become a pivotal issue. This manuscript examines the characteristics of prevalent generative AI systems and the intricate process behind AI-generated works. It underscores the imperative for a copyright discourse surrounding AI-generated works, in light of the aims and perspectives of extant copyright laws, and the policy objectives that underpin industrial advancement. The discourse posits the necessity to view generative AI as an innovative instrument for human creativity, substantiating the ‘instrumentality’ of generative AI with a multitude of evidences. Additionally, considering the uniqueness of AI-generated works compared to traditional creative works, the manuscript proposes criteria for acknowledging the copyrightability of AI-generated works, focusing on ‘Predictability of Generative Works’ to demonstrate the instrumentality of generative AI and ‘Specificity of Prompts’ to assess the creativity of AI-generated works. By presenting these new perspectives on AI-generated works, the paper aims to promote the development of culture, related industries, and the generative AI industry.</jats:p>"
10.2196/preprints.54482,Comparing the Efficacy and Efficiency of Human and Generative AI: Qualitative Thematic Analyses (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>Qualitative methods are incredibly beneficial to the dissemination and implementation of new digital health interventions; however, these methods can be time intensive and slow down dissemination when timely knowledge from the data sources is needed in ever-changing health systems. Recent advancements in generative artificial intelligence (GenAI) and their underlying large language models (LLMs) may provide a promising opportunity to expedite the qualitative analysis of textual data, but their efficacy and reliability remain unknown.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>The primary objectives of our study were to evaluate the consistency in themes, reliability of coding, and time needed for inductive and deductive thematic analyses between GenAI (ie, ChatGPT and Bard) and human coders.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>The qualitative data for this study consisted of 40 brief SMS text message reminder prompts used in a digital health intervention for promoting antiretroviral medication adherence among people with HIV who use methamphetamine. Inductive and deductive thematic analyses of these SMS text messages were conducted by 2 independent teams of human coders. An independent human analyst conducted analyses following both approaches using ChatGPT and Bard. The consistency in themes (or the extent to which the themes were the same) and reliability (or agreement in coding of themes) between methods were compared.</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>The themes generated by GenAI (both ChatGPT and Bard) were consistent with 71% (5/7) of the themes identified by human analysts following inductive thematic analysis. The consistency in themes was lower between humans and GenAI following a deductive thematic analysis procedure (ChatGPT: 6/12, 50%; Bard: 7/12, 58%). The percentage agreement (or intercoder reliability) for these congruent themes between human coders and GenAI ranged from fair to moderate (ChatGPT, inductive: 31/66, 47%; ChatGPT, deductive: 22/59, 37%; Bard, inductive: 20/54, 37%; Bard, deductive: 21/58, 36%). In general, ChatGPT and Bard performed similarly to each other across both types of qualitative analyses in terms of consistency of themes (inductive: 6/6, 100%; deductive: 5/6, 83%) and reliability of coding (inductive: 23/62, 37%; deductive: 22/47, 47%). On average, GenAI required significantly less overall time than human coders when conducting qualitative analysis (20, SD 3.5 min vs 567, SD 106.5 min).</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>The promising consistency in the themes generated by human coders and GenAI suggests that these technologies hold promise in reducing the resource intensiveness of qualitative thematic analysis; however, the relatively lower reliability in coding between them suggests that hybrid approaches are necessary. Human coders appeared to be better than GenAI at identifying nuanced and interpretative themes. Future studies should consider how these powerful technologies can be best used in collaboration with human coders to improve the efficiency of qualitative research in hybrid approaches while also mitigating potential ethical risks that they may pose.</p>
                </sec>"
10.15400/mccs.2023.12.46.02,AI as a literary machine beyond the final piece : On the Object of Generative Language Criticism,N/A
10.1016/j.caeai.2024.100283,Teachers' and students' perceptions of AI-generated concept explanations: Implications for integrating generative AI in computer science education,N/A
10.1145/3411764.3445219,AI as Social Glue: Uncovering the Roles of Deep Generative AI during Social Music Composition,N/A
10.1145/3461702.3462591,A Framework for Understanding AI-Induced Field Change: How AI Technologies are Legitimized and Institutionalized,N/A
10.1007/978-3-031-08215-3_2,Epistemic Just and Dynamic AI Ethics in Africa,"<jats:title>Abstract</jats:title><jats:p>This chapter considers the potential for actualising the ideal for responsible AI on the African continent, focusing on the AI ethics policy environment in Africa. I consider the impact of context and culture on successful adoption of AI technologies in general and on trust in AI technology and openness to AI regulation in particular. It concludes that actionable AI ethics in Africa should be driven by dynamic and epistemic just ethical systems.</jats:p>"
10.1609/aimag.v40i1.2854,Moral Orthoses: A New Approach to Human and Machine Ethics,<jats:p><jats:italic>I argue that both human and machine actions are more opaque than is generally realized and that the actions of both require explanation that an ethical orthosis might provide as aspects of artificial Companions for both human and machine actors. These explanations might well be closer to ethical accounts based on moral sentiment or emotion in the tradition of the primacy of sentiment over reason in this area of human and machine action.</jats:italic></jats:p>
10.3389/frobt.2019.00028,Ethics Guidelines for Immersive Journalism,N/A
10.1007/s00146-009-0243-0,"Anticipation and the artificial: aesthetics, ethics, and synthetic life",N/A
10.1007/bf01174480,The ethics of NHS computing: A terminal case,N/A
10.1007/s00146-007-0145-y,Socio-ethics of interaction with intelligent interactive technologies,N/A
10.1145/3514094.3534197,Towards a Feminist Metaethics of AI,N/A
10.1007/978-981-19-2531-3_10,New Rules and New Order in the Era of AI,N/A
10.3390/su14095153,The Ethics of AI-Powered Climate Nudging—How Much AI Should We Use to Save the Planet?,"<jats:p>The number of areas in which artificial intelligence (AI) technology is being employed increases continually, and climate change is no exception. There are already growing efforts to encourage people to engage more actively in sustainable environmental behavior, so-called “green nudging”. Nudging in general is a widespread policymaking tool designed to influence people’s behavior while preserving their freedom of choice. Given the enormous challenges humanity is facing in fighting climate change, the question naturally arises: Why not combine the power of AI and the effectiveness of nudging to get people to behave in more climate-friendly ways? However, nudging has been highly controversial from the very beginning because critics fear it undermines autonomy and democracy. In this article I investigate the ethics of AI-powered climate nudging and address the question whether implementing corresponding policies may represent hidden and unacceptable costs of AI in the form of a substantive damage to autonomy and democracy. I will argue that, although there are perfectly legitimate concerns and objections against certain forms of nudging, AI-powered climate nudging can be ethically permissible under certain conditions, namely if the nudging practice takes the form of what I will call “self-governance”.</jats:p>"
10.1007/979-8-8688-0279-9_13,AI Ethics,N/A
10.1007/s00146-021-01274-1,From posthumanism to ethics of artificial intelligence,N/A
10.1007/978-981-19-9382-4_9,"Individuals, Society, and AI: Online Communication",N/A
10.1515/9781501518430-003,Chapter 1: Focusing on AI Ethics and Social Impact,N/A
10.29173/irie423,Tangible and Intangible Impact of AI Usage: AI for Information Accessibility,"<jats:p>Any technology opens enormous opportunities for individuals as well as societies. This undoubtedly will contribute to the welfare of the individuals and the growth of society. Since any technology is the creation of human intellect, it may also carry the issues of socio-cultural and environmental concerns, such as issues of acceptability, access, and equity. AI is no exception to this. This paper investigates some of the aspects related to the development and usage of AI, that includes accessibility relating to socio-cultural, economic, and ethical concerns. It argues for the need to evolve nation-specific policies and regulations addressing the issues of inequalities.</jats:p>"
10.1201/9781003293125-9,AI and Nonhuman Animals,N/A
10.1145/3375627.3377139,The AI-development Connection - A View from the South,N/A
10.1007/s00146-021-01228-7,Discourse analysis of academic debate of ethics for AGI,"<jats:title>Abstract</jats:title><jats:p>Artificial general intelligence is a greatly anticipated technology with non-trivial existential risks, defined as machine intelligence with competence as great/greater than humans. To date, social scientists have dedicated little effort to the ethics of AGI or AGI researchers. This paper employs inductive discourse analysis of the academic literature of two intellectual groups writing on the ethics of AGI—applied and/or ‘basic’ scientific disciplines henceforth referred to as technicians (e.g., computer science, electrical engineering, physics), and philosophy-adjacent disciplines henceforth referred to as PADs (e.g., philosophy, theology, anthropology). These groups agree that AGI ethics is fundamentally about mitigating existential risk. They highlight our moral obligation to future generations, demonstrate the ethical importance of better understanding consciousness, and endorse a hybrid of deontological/utilitarian normative ethics. Technicians favor technocratic AGI governance, embrace the project of ‘solving’ moral realism, and are more deontologically inclined than PADs. PADs support a democratic approach to AGI governance, are more skeptical of deontology, consider current AGI predictions as fundamentally imprecise, and are wary of using AGI for moral fact-finding.</jats:p>"
10.1007/s00146-013-0473-z,"Science, technology and values: promoting ethics and social responsibility",N/A
10.22471/ai.2021.6.1.29,Suggestion of Building the AI Code of ETHICS through Deep Learning and Big Data Based AI,N/A
10.1007/979-8-8688-0110-5_9,AI Safety and Ethics,N/A
10.1007/s00146-013-0526-3,Considerations about the relationship between animal and machine ethics,N/A
10.1007/s00146-003-0293-7,"Ethics, science, and the mechanisation of the world picture",N/A
10.1016/j.forsciint.2023.111807,Mapping AI-ethics' dilemmas in forensic case work: To trust AI or not?,N/A
10.53879/id.61.02.p0005,NEED/ROLE OF GENERATIVE AI IN PHARMACEUTICAL RESEARCH,"<jats:p>Dear Reader,  In current times, when countries like Singapore are relying mainly on ""Gen AI"" (General Artificial Intelligence) for hastening the growth and modernization of the pharma industry, pharma leaders like India need to incorporate the latest trends in digital to Gen AI in the curriculum of the pharmacy degree and post-graduate degrees.</jats:p>"
10.4324/9781003453901-13,Copyright Aspects of Generative AI in Germany and the EU,N/A
10.2139/ssrn.4671369,The Uneven Impact of Generative AI on Entrepreneurial Performance,N/A
10.1145/3625251,"Legal Challenges to Generative AI, Part II",<jats:p>Deliberating on inconclusive AI-generated policy questions.</jats:p>
10.54648/asab2023059,"Arbitrators, Decision Making, and Generative AI","<jats:p>
            <jats:italic/>
          </jats:p>"
10.37272/jiecr.2023.04.23.2.59,Opportunities and threats of generative AI technology,N/A
10.12968/s1353-4858(23)70045-7,Generative AI: security implications for business automation,<jats:p> AI tools such as ChatGPT certainly appear to offer significant benefits to organisations looking to save costs and improve efficiency. But the technology is being adopted with an enthusiasm that leaves little room for the careful consideration of potential security vulnerabilities. So how do you embrace this revolution without putting yourself at risk? </jats:p>
10.1017/s1351324923000554,A year’s a long time in generative AI,"<jats:title>Abstract</jats:title><jats:p>A lot has happened since OpenAI released ChatGPT to the public in November 2022. We review how things unfolded over the course of the year, tracking significant events and announcements from the tech giants leading the generative AI race and from other players of note; along the way we note the wider impacts of the technology’s progress.</jats:p>"
10.33168/jsms.2024.0627,Transforming the Learning Experience: Insights on Integrating  Generative AI in Higher Education,N/A
10.1016/s2589-7500(24)00157-2,Pathology in the era of generative AI,N/A
10.1007/978-3-031-46238-2_22,Generating Artistic Portrait Drawings from Images,N/A
10.1609/aaaiss.v3i1.31244,Generative AI Applications in Helping Children with Speech Language Issues,"<jats:p>This paper reports how generative AI can help children with specific language impairment (SLI) issues by developing an AI-assisted tool to support children with challenges in phonological development in English, especially children with English as the secondary language in the United States. Children from bilingual families often experience challenges in developing proficiency in English pronunciation and communication, which has been exacerbated by remote learning during the pandemic and led to learning loss. School-aged children with speech problems require timely intervention because children with language disorders find it difficult to communicate with others, leading to social isolation and academic difficulties. The needed intervention is often delayed due to the high cost of speech services and the shortage of Speech and Language Pathologists (SLPs). Individuals with a history of SLI have an increased risk of unemployment. An AI-assisted Phonological Development (AI-PD) tool was prototyped, aiming to alleviate these challenges by assisting caregivers in evaluating children's phonological development, assisting SLPs in lesson preparation, and mitigating the severe shortage of SLPs.</jats:p>"
10.2139/ssrn.4868555,When AI Remembers Too Much: Reinventing the Right to Be Forgotten for the Generative Age,N/A
10.55041/ijsrem30724,IMPACT OF GENERATIVE AI ON EDUCATIONAL SECTOR,"<jats:p>This study looks into how generative artificial intelligence (AI) is affecting the field of education. The study examines the possible advantages of using generative AI into instruction, such as more individualized learning opportunities, enhanced accessibility, and creative teaching strategies. The study also recognizes and examines the moral issues raised by the application of AI in education, highlighting the significance of upholding academic honesty and openness. It draws attention to the necessity of cautious application in order to guarantee that AI fosters fruitful learning results. The suggested research technique describes a mixed methods strategy that combines quantitative data analysis with qualitative information from surveys and interviews. The goal of the project is to provide best practices and standards for the appropriate integration of generative AI in education, while also identifying the potential and problems that this technology presents. The anticipated results include a thorough comprehension of how generative AI will affect education and how it might improve learning opportunities, ethical issues for responsible usage, and the identification of possible problems and solutions. In the end, the research hopes to promote a more inclusive and productive learning environment by aiding in the creation of useful frameworks for incorporating AI into educational settings. The education industry has drastically evolved in recent years, thanks to the remarkable advancements made by artificial intelligence. With the integration of generative AI technology, educators and students are now presented with a wealth of possibilities and opportunities.  As AI continues to advance further, it holds great potential for revolutionizing education as we know it through improved efficiency measures that personalize learning experiences while enhancing overall student outcomes. Harnessing this power-packed tool in teaching environments creates an opportunity for educational institutions across various sectors allowing them efficient intelligent assessment tools coupled with personalized tutoring provisions towards addressing classroom weaknesses resulting in custom-made lesson plans aimed at betterment of individual academic progress developed using predictive models creating ample time such that teachers can focus solely on providing tailored instruction &amp; support based exclusively around their Student's requirements&amp; specifications.  This groundbreaking technological development provided meaningfully leveraging Generative Intelligence efforts promises nothing short but will be instrumental when shaping how Education is structured from henceforth enabling automation capabilities within the Administrative tasks management system,&amp; content-building-The advantages come twofold-Enhancement not only resides generally concerning Upgrade Learning Experience situations; there exist tailor solutions projecting Speaker understanding regarding distinctive needs pupils have always had throughout all levels ultimately leading to Developmental Progression heights hitherto attainable anticipated sooner than later.</jats:p>"
10.2139/ssrn.4495320,Prompt-engineering testing ChatGPT4 and Bard for assessing Generative-AI efficacy to support decision-making,N/A
10.21203/rs.3.rs-4612612/v1,"""It just happened to be the perfect thing"": Real-life experiences of generative AI chatbots for mental health","<title>Abstract</title>
        <p>The global mental health crisis underscores a critical need for accessible and effective interventions. Generative artificial intelligence (AI) chatbots, such as ChatGPT, are emerging as a novel solution, but research into their real-life usage is limited. We interviewed nineteen individuals about their experiences of using generative AI chatbots to work on their mental health. Most participants reported high levels of engagement and positive impacts, including improved mood, reduced anxiety, healing from trauma and loss, and improved relationships. Our analysis resulted in four overarching themes: 1) the value of an ‘<italic>emotional sanctuary’</italic>, i.e., a safe, validating space that is always available, 2) the ‘<italic>insightful guidance’</italic> provided, particularly on the topic of relationships, 3) the ‘<italic>joy of connection</italic>’ experienced, and 4) comparisons between the ‘<italic>AI therapist</italic>’ and human therapy. Some of these themes echo previous research on rule-based chatbots, while others appear to be novel to generative AI. Participants highlighted the need for a better approach to safety guardrails, more human-like memory and the ability to lead the therapeutic process. Our findings suggest that generative AI chatbots may offer meaningful mental health support, but further research is needed to explore their safety and effectiveness.</p>"
10.1007/7854_2024_482,Leveraging Generative AI Models in Urban Science,N/A
10.21203/rs.3.rs-4627814/v1,WITHDRAWN: The role of students’ higher-order thinking skills in the relationship between academic achievements and machine learning using generative AI chatbots,"<title>Abstract</title>
        <p>Students' perspectives on using generative artificial intelligence (AI) chatbots and machine learning are crucial in shaping the design, development, and implementation of their learning projects across various disciplines. Cognitive thinking, a key aspect of AI-related machine learning, aims to replicate human intelligence and behavior. However, the relation between cognitive thinking and knowledge acquisition is not always clear. Therefore, it is essential for students to engage in higher-order thinking, which allows them to critically analyze diverse viewpoints, assess their relevance, and understand the complex relationship between cognitive thinking and knowledge acquisition. This empirical study investigates the role of higher-order thinking skills, such as problem-solving, critical thinking, and creativity, in the relationship between academic achievements and attitudes toward machine learning technologies using generative AI chatbots. Four hundred sixteen undergraduate students (<italic>n</italic> = 416) from diverse academic backgrounds voluntarily took part in a project, in which they designed and developed generative AI chatbots in media and information literacy courses. The findings indicate that creativity mediated the relationship between academic achievements and attitudes toward machine learning, but its moderating impact was not significant. Problem-solving and critical thinking did not show significant mediating effects on attitudes toward machine learning, while they showed significant moderating effects in the connection between academic performance and attitudes toward machine learning. This study contributes by elucidating the interrelationships between students’ higher-order thinking skills, academic performance, and attitudes on the use of AI and machine learning technologies. By highlighting the mediating role of creativity and the moderating effects of problem-solving and critical thinking, this study offers a deeper understanding of how these skills shape students' perceptions of AI. The findings have significant implications for educational practices, suggesting that fostering higher-order thinking skills is crucial in preparing students to embrace AI and machine learning technologies.</p>"
10.1109/infoteh60418.2024.10495995,Output Manipulation via LoRA for Generative AI,N/A
10.1007/979-8-8688-0461-8_2,Understanding AI and Ethics,N/A
10.20944/preprints202307.0563.v1,ChatGPT and the Generation of Digitally Born “Knowledge”: How Does a Generative AI Language Model Interpret Cultural Heritage Values?,"<jats:p>The public release of ChatGPT, a generative artificial intelligence language model, caused wide-spread public interest in its abilities but also concern about the implications of the application on academia, depending on whether it was deemed benevolent (e.g., supporting analysis and simplification of tasks) or malevolent (e.g., assignment writing and academic misconduct). While ChatGPT has been shown to provide answers of sufficient quality to pass some university exams, its capacity to write essays that require an exploration of value concepts is unknown. This paper presents the results of a study where ChatGPT 4 (release May 2023) was tasked with writing a 1500-word essay to discuss the nature of values used in the assessment of cultural heritage significance. Based on an analysis of 36 iterations, ChatGPT writes essays of limited length of about 50% of the stipulated word count which are primarily descriptive without any depth or complexity. The concepts, which are often flawed and suffer from inverted logic, are presented in an arbitrary sequence with limited coherence and without any defined line of argument. Given that it is a generative language model, ChatGPT often splits concepts and uses one or more words to develop tangential arguments. While ChatGPT provides references as tasked, many are fictitious, albeit with plausible authors and titles. At present ChatGPT has the ability to critique its own work but seems unable to incorporate that critique in a meaningful way to improve a previous draft. Setting aside conceptual flaws such as inverted logic, several of the essays could possibly pass as a junior school assignment, but fall far short of what would be expected in senior school, let alone at a college and university level.</jats:p>"
10.1148/radiol.230957,Open-Source Inline Generative AI for Fast Cardiac MRI,N/A
10.14236/ewic/eva2024.27,Hyperhumanism in the Age of Generative AI: The impact on human creativity and identity,N/A
10.1609/aaaiss.v3i1.31182,A Generative AI-Based Virtual Physician Assistant,"<jats:p>We describe ""Dr. A.I."", a virtual physician assistant that uses generative AI to conduct a pre-visit patient interview and to create a draft clinical note for the physician. We document the effectiveness of Dr. A.I. by measuring the concordance of the actual diagnosis made by the doctor with the generated differ-ential diagnosis (DDx) list. This application demonstrates the practical healthcare capabilities of a large language model to improve efficiency of doctor visits while also addressing safety concerns for the use of generative AI in the workflow of patient care.</jats:p>"
10.1007/979-8-8688-0282-9_10,The Auditory and Multisensory Experience,N/A
10.1002/9781394308286.ch1,Understanding the Stock Market,N/A
10.2139/ssrn.4874061,Employer and Employee Responses to Generative AI: Early Evidence,N/A
10.1109/radarconf2458775.2024.10549436,Multi-perspective SAR to 3D Translation using Generative AI,N/A
10.21203/rs.3.rs-4627814/v2,WITHDRAWN: The role of students’ higher-order thinking skills in the relationship between academic achievements and machine learning using generative AI chatbots,"<title>Abstract</title>
        <p>The full text of this preprint has been withdrawn by the authors due to author disagreement with the posting of the preprint. Therefore, the authors do not wish this work to be cited as a reference. Questions should be directed to the corresponding author.</p>"
10.54517/m.v4i1.2164,Generative AI tools in art education: Exploring prompt engineering and iterative processes for enhanced creativity,"<jats:p>&lt;p&gt;The rapid development and adoption of generative artificial intelligence (AI) tools in the art and design education landscape have introduced both opportunities and challenges. This timely study addresses the need to effectively integrate these tools into the classroom while considering ethical implications and the importance of prompt engineering. By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI’s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context. Students in the digital at case study were taught prompt engineering techniques and were tasked with crafting multiple prompts, focusing on refining their ideas over time. Participants demonstrated an increased understanding of the potential and limitations of generative AI tools and how to manipulate subject matter for more effective results. The iterative process encouraged students to explore and experiment with their creative ideas, leading to a deeper understanding of the possibilities offered by AI tools. Despite acknowledging the ethical concerns regarding copyright and the potential replacement of artists, students appreciated the value of generative AI tools for enhancing their sketchbooks and ideation process. Through prompt engineering and iterative processes, students developed a more detail-oriented approach to their work. The challenge of using AI-generated images as final products was conceptually intriguing, requiring further investigation and consideration of the prompts. This study highlights the potential benefits and challenges of integrating generative AI tools into art and design classrooms, emphasizing the importance of prompt engineering, iterative processes, and ethical considerations as these technologies continue to evolve.&lt;/p&gt;</jats:p>"
10.1201/9781003473633-13,Automated Journalism in   Turkish Media before the Rise of Generative AI,N/A
10.3390/fi15090286,"Generative AI in Medicine and Healthcare: Promises, Opportunities and Challenges","<jats:p>Generative AI (artificial intelligence) refers to algorithms and models, such as OpenAI’s ChatGPT, that can be prompted to generate various types of content. In this narrative review, we present a selection of representative examples of generative AI applications in medicine and healthcare. We then briefly discuss some associated issues, such as trust, veracity, clinical safety and reliability, privacy, copyrights, ownership, and opportunities, e.g., AI-driven conversational user interfaces for friendlier human-computer interaction. We conclude that generative AI will play an increasingly important role in medicine and healthcare as it further evolves and gets better tailored to the unique settings and requirements of the medical domain and as the laws, policies and regulatory frameworks surrounding its use start taking shape.</jats:p>"
10.4324/9781003365891-9,On creative practice and generative AI,N/A
10.1038/d41586-024-01003-w,"‘Without these tools, I’d be lost’: how generative AI aids in accessibility",N/A
10.57237/j.cst.2024.03.004,A Review of the Latest Research Achievements in the Basic Theory of Generative AI and Artificial General Intelligence (AGI),N/A
10.1201/9781032654829-5,Best Practices and Key Considerations for Artificial Intelligence Regulation,N/A
10.12781/978-1-907549-22-9-4,Appreciative-Generative Inquiry as a Tool for Learning,N/A
10.2196/preprints.56256,3Ps for ChatGPT: Best Practices for Generative AI Discharge Instructions (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>Patients may leave the emergency room unsatisfied since so much of their care occurs away from their view. Additionally, generic discharge instructions further contribute to this dissatisfaction, lacking detail on the care provided and specific home guidance and precautions. The patients, their families, or care teams may then be left with lingering questions and potential adverse outcomes due to misinterpretation. Research has shown a positive correlation between patient satisfaction and their assessment of the quality of discharge instructions, but providers in a fast-paced emergency room are limited by time. The expansion and availability of AI technologies in recent years offer a potential solution. This article details a multi-step approach to interact with AI-powered language learning models, specifically ChatGPT3.5, to compose targeted, specific, and clear discharge instructions. We propose that strategic implementation of these technologies can improve both efficiency and promote patient-centered care.</p>
                </sec>"
10.14742/apubs.2023.618,The implications of generative AI for creative composition in higher education and initial teacher education,"<jats:p>This position paper explores the impact of generative AI on creativity and creative production in higher education, with focus also on initial teacher education. Creative outputs are an essential part of the business of higher education across different disciplinary areas, along with pedagogical and assessment practices matched to these outputs. However, the advent of generative AI has sparked a reconsideration of how we think about creativity, creative output, and the hybridity of relationships between generative AI and humans in creative endeavours. This emergent technology also has significant implications for teacher education and for the develop of new teachers who need confidence with using AI in learning contexts. We suggest that new thinking and critical dialogue are essential in developing educational futures in which generative AI is understood for its affordances, limitations, and dangers.</jats:p>"
10.4337/9781035311293.00020,Generative AI writing technologies,N/A
10.55752/amwa.2024.335,Reimagining Clinical and Regulatory Medical Writing With Generative AI,"<jats:p>The emergence of groundbreaking clinical targets and treatment modalities is fueling a surge in the number of clinical trials. This increased volume, coupled with the growing complexity of trials, is putting immense pressure on medicine’s development process. One immediate area of opportunity for streamlining clinical trials is clinical and regulatory medical writing. Generative AI (GenAI) has the potential to revolutionize medical writing by automating routine tasks, improving consistency and quality, and enabling faster turnaround times. This frees medical writers from mundane work, allowing them to focus on strategic initiatives and exert greater influence within life sciences organizations. This article explores the current state of clinical and regulatory medical writing, the challenges posed by the increasing complexity of clinical trials, and how GenAI can be used to address these challenges. The article also discusses the ethical considerations of using GenAI in medical writing and the importance of human oversight and review. Overall, the article builds a compelling case for the use of GenAI in medical writing and outlines a roadmap for the future of this field.</jats:p>"
10.1145/3627217.3631585,Leveling Up Education: Harnessing Generative AI for Game-Based Learning,N/A
10.1007/978-3-031-52569-8_9,Misinformation and Generative AI: How Users Construe Their Sense of Diagnostic Misinformation,N/A
10.4324/9781003459026-3,Strengths and Weaknesses in Embracing ChatGPT in Curriculum Design,N/A
10.1609/aaaiss.v3i1.31232,How Can Generative AI Enhance the Well-being of Blind?,"<jats:p>This paper examines the question of how generative AI can improve the well-being of blind or visually impaired people. It refers to a current example, the Be My Eyes app, in which the Be My AI feature was integrated in 2023, which is based on GPT-4 from OpenAI. The author’s tests are described and evaluated. There is also an ethical and social discussion. The power of the tool, which can analyze still images in an amazing way, is demonstrated. Those affected gain a new independence and a new perception of their environment. At the same time, they are dependent on the world view and morality of the provider or developer, who prescribe or deny them certain descriptions. An outlook makes it clear that the analysis of moving images will mean a further leap forward. It is fair to say that generative AI can fundamentally improve the well-being of blind and visually impaired people and will change it in various ways.</jats:p>"
10.36745/ijca.604,"Unboxing Generative AI for the Legal Professions: Functions, Impacts and Governance",N/A
10.36315/2024v1end035,Generative Artificial Intelligence (AI) in education: A cross-national survey on university teachers’ perceptions on the use of ChatGPT,N/A
10.26689/ssr.v6i7.7430,Legal Regulation of Risk Iteration in ChatGPT-like Generative AI Technology,"<jats:p>The technical iteration of generative artificial intelligence represented by ChatGPT generates strong computing power and intelligence, which brings convenience to people and improves people’s production and living standards. Due to the extreme language logic and the ability to crawl data, the technology also has other deep problems. To solve such problems, this paper believes that the development of special laws can first be promoted from the following aspects. The second is to emphasize the strategic layout of national security to prevent countries from being blocked by other countries in terms of generation-like artificial intelligence. Otherwise, the productivity level of the affected country would fall. Finally, the paper discusses the division of responsibility for generative artificial intelligence of ChatGPT.</jats:p>"
10.1145/3592981,Can Generative AI Bots Be Trusted?,<jats:p>It will be a long road to learning how to use generative AI wisely.</jats:p>
10.21125/edulearn.2024.1051,GENERATIVE AI THROUGH THE TEACHER'S LENS: PERSPECTIVES ON ADOPTION IN SCHOOL EDUCATION,N/A
10.15823/p.2024.154.1,The Integration of Generative AI in Foreign Language Teacher Education: A Systematic Literature Review,"<jats:p>The current paper presents a systematic review that was conducted based on the Preferred Reporting Items for Systematic Reviews (PRISMA) and contains a selection of 12 articles published in the Web of Science and Scopus. The aim is to identify research trends and directions that would contribute to the body of knowledge about empirical research on GAI integration in foreign language instruction from November 2022 (the launch of OpenAi’s ChatGPT) to February 2024. The paper analyses the most novel practices, reveals teachers’ approaches, and evaluates opportunities and challenges.</jats:p>"
10.1177/17504813241271444,Productivity implications for generative AI role-based prompts as a networked hermeneutic,N/A
10.18559/ebr.2023.2.732,The rise of generative AI and possible effects on the economy,"<jats:p>The aim of the paper is to analyse the likely implications of Generative AI (GAI) on various aspects of business and the economy. Amid the rapid growth and maturing of Generative AI technologies such as Large Language Models (like ChatGPT by OpenAI) a rapid growth of both immediate and potential applications can be seen. The implications for the economy and industries of this technological shift will be discussed. The foreseeable scenarios for the level and types of adoption that GAI might achieve - from useful analytical tool, invaluable assistant to the white-collar workers of the world to being trusted with a wide array of business and life-critical decision making. Both disruptive and premium service opportunities are foreseen. For instance general purpose models may provide quality service – such as copywriting – to overserved customers leaving human writers as the premium option. In this context, overserved customers would be those who would be satisfied with a non-human, potentially less creative content. On the other hand highly specialized models – specifically trained in a given domain and with access to proprietary knowledge can possibly provide a premium service over that provided by human experts. It is expected that some jobs will be replaced by new AI applications. However new workplaces will emerge. Not only the obvious expert-level data scientist roles but also low grade, “model supervisors” – people training the models, assessing the quality of responses given and handling escalations. Lastly new cybercrime risks emerging from the rise of GAI are discussed.</jats:p>"
10.1186/s13012-024-01357-9,"Generative AI in healthcare: an implementation science informed translational path on application, integration and governance","<jats:title>Abstract</jats:title><jats:sec>
                <jats:title>Background</jats:title>
                <jats:p>Artificial intelligence (AI), particularly generative AI, has emerged as a transformative tool in healthcare, with the potential to revolutionize clinical decision-making and improve health outcomes. Generative AI, capable of generating new data such as text and images, holds promise in enhancing patient care, revolutionizing disease diagnosis and expanding treatment options. However, the utility and impact of generative AI in healthcare remain poorly understood, with concerns around ethical and medico-legal implications, integration into healthcare service delivery and workforce utilisation. Also, there is not a clear pathway to implement and integrate generative AI in healthcare delivery.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Methods</jats:title>
                <jats:p>This article aims to provide a comprehensive overview of the use of generative AI in healthcare, focusing on the utility of the technology in healthcare and its translational application highlighting the need for careful planning, execution and management of expectations in adopting generative AI in clinical medicine. Key considerations include factors such as data privacy, security and the irreplaceable role of clinicians’ expertise. Frameworks like the technology acceptance model (TAM) and the Non-Adoption, Abandonment, Scale-up, Spread and Sustainability (NASSS) model are considered to promote responsible integration. These frameworks allow anticipating and proactively addressing barriers to adoption, facilitating stakeholder participation and responsibly transitioning care systems to harness generative AI’s potential.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Results</jats:title>
                <jats:p>Generative AI has the potential to transform healthcare through automated systems, enhanced clinical decision-making and democratization of expertise with diagnostic support tools providing timely, personalized suggestions. Generative AI applications across billing, diagnosis, treatment and research can also make healthcare delivery more efficient, equitable and effective. However, integration of generative AI necessitates meticulous change management and risk mitigation strategies. Technological capabilities alone cannot shift complex care ecosystems overnight; rather, structured adoption programs grounded in implementation science are imperative.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Conclusions</jats:title>
                <jats:p>It is strongly argued in this article that generative AI can usher in tremendous healthcare progress, if introduced responsibly. Strategic adoption based on implementation science, incremental deployment and balanced messaging around opportunities versus limitations helps promote safe, ethical generative AI integration. Extensive real-world piloting and iteration aligned to clinical priorities should drive development. With conscientious governance centred on human wellbeing over technological novelty, generative AI can enhance accessibility, affordability and quality of care. As these models continue advancing rapidly, ongoing reassessment and transparent communication around their strengths and weaknesses remain vital to restoring trust, realizing positive potential and, most importantly, improving patient outcomes.</jats:p>
              </jats:sec>"
10.7146/nja.v33i67.148455,Style and Revenge: The Vagaries of the Artistic Class in Generative AI,N/A
10.47743/ejes-2023-0203,Legal issues concerning Generative AI technologies,N/A
10.1145/3638567,How Generative AI Fits into Knowledge Work,<jats:p>Seeking to influence how generative artificial intelligence affects various professions.</jats:p>
10.31219/osf.io/6tjpk,“ChatGPT Seems Too Good to be True”: College Students’ Use and Perceptions of Generative AI,"<p>This survey study investigates ChatGPT’s usage and perceptions among U.S. college students (N=1001), exploring its relationship with societal structures and values. Most participants reported using ChatGPT at least monthly for general tasks and writing, while its use for computer programming was minimal. Non-native English speakers utilized ChatGPT more frequently for writing, consistent with its perceived role as a leveling tool in academic language challenges. Natural Language Processing and thematic analysis revealed five main student concerns about using ChatGPT: ethical implications, skepticism towards AI, quality of ChatGPT’s output, potential loss of learning and job opportunities, and accessibility. While some students expressed no concerns, citing confidence in their appropriate use of ChatGPT, others hesitated to use it for writing, valuing their unique writing process and voice. Computer science majors expressed concerns related to job displacement due to the advent of generative AI. Students from low-income backgrounds highlighted constraints in resources as a barrier to effectively utilizing ChatGPT. Sentiment analysis revealed that students from higher-income groups generally held less negative views on ChatGPT compared to their lower-income counterparts. Our research underscores how technology can both empower and marginalize within educational settings; we advocate for equitable integration of AI in academic environments.</p>"
10.21818/001c.122147,Maximizing Generative AI Benefits with Task Creativity and Human Validation,"<jats:p>Much of the existing literature on generative AI applications is conflicting, with findings suggesting that investing in AI will lead to better organizational outcomes but also pointing out that incorporating AI may be a wasteful even counterproductive initiative. We develop a conceptual framework to characterize generative AI benefits based on the types of tasks that generative AI may be used for in management. Our work suggests that task creativity plays a key role in successful generative AI outcomes, but human validation - the extent to which a human engages in a supervisory role - is required to reap the benefits. Our conceptual framework is focused on white collar jobs and suggests that the management of generative AI is a strategic choice with important managerial implications.</jats:p>"
10.5121/ijci.2023.120508,Regulating Generative AI: A Pathway to Ethical and Responsible Implementation,"<jats:p>Artificial intelligence (AI) is becoming more and more prevalent in our daily lives, and its potential applications are practically limitless. However, as with any technology, there are concerns about how AI could be misused or abused. One of the most serious concerns is the potential for discrimination, particularly against women or minorities, when AI systems are used for tasks like job hiring. Additionally, there are concerns about privacy and security, as AI could be used to monitor people's movements or launch cyberattacks. To address these concerns, regulations must be developed to ensure that AI is developed and used ethically and responsibly. These regulations should address issues like safety, privacy, security, and discrimination. Finally, it is important to educate the public about AI and how to use it safely and responsibly. In this paper, I will examine the AI regulations and challenges that exist today, particularly in the United States. Two regulations I will focus on are the AI in Government Act of 2020 and the National Artificial Intelligence Initiative Act of 2020. Additionally, I will examine two Executive Orders that have addressed the issue of AI in the federal government. Finally, I will conclude with some policy considerations and recommendations for federal agencies.</jats:p>"
10.1007/bfb0018438,On generative capacity of the Lambek calculus,N/A
10.2139/ssrn.4793889,Blockchain Sharding Scheme Based on Generative AI and DRL: Applied to Building Internet of Things,N/A
10.2139/ssrn.4846676,Assessing the Efficacy of Generative Ai in Pragmatics: Chatgpt and Japanese Learners of Chinese in Written Apologies,N/A
10.2139/ssrn.4833030,A Systematic Review and Analysis of Ethical Challenges of Generative Ai: An Interdisciplinary Perspective,N/A
10.2139/ssrn.4860511,"Expediting Documentation for Promotion, Letter Writing, and Annual Review with Generative AI",N/A
10.1109/sose62659.2024.10620975,Accelerating Model-Based Systems Engineering by Harnessing Generative AI,N/A
10.21203/rs.3.rs-3710923/v1,A Generative AI Chatbot in High School Advising: A Qualitative Analysis of Domain-Specific Chatbot and ChatGPT,"<jats:title>Abstract</jats:title>
        <jats:p>Due to the variety of chatbot types and classifications, students and advisers may experience confusion when trying to select the right chatbot that can more trust it, however, the classification of chatbots depends on different factors including, the complexity of the task, the response-based approach and the type of the domain. Since selecting the most effective chatbot is crucial for high schools and students, a semi-structured interviews in qualitative research were conducted with eight high school students in order to investigate the students ‘perspectives on different seven responses of generative questions from the domain-specific chatbot named HSGAdviser, comparing it with the ChatGPT. All questions were related to students’ advising interests including university applications, admission tests, majors and more. The transcribed data were reviewed and examined by using the thematic analysis. However, the results reveal that most students found that HSGAdviser chatbot is easier, shorter, faster and more concise compared to ChatGPT, especially for Yes/No questions as students expect brief answers. However, some students found that certain crucial questions that can have a significance impact on their future, they would prefer the ChatGPT for more detailed information. The limitation of this study is the limited size of the participants. Nevertheless, in the future research, other high school students from different regions will participate in the study.</jats:p>"
10.18260/1-2--47045,Board 49: Work in Progress: Using Generative AI for Reducing Faculty Workload in Online Engineering Courses,N/A
10.1109/icaiccit60255.2023.10465941,Comparative Analysis of Generative AI Models,N/A
10.1038/d41586-023-00340-6,What ChatGPT and generative AI mean for science,N/A
10.1007/s10676-023-09721-x,Ethics framework for predictive clinical AI model updating,N/A
10.46936/intm.proj.2024.61373/60012570,N/A,N/A
10.1177/20539517241252131,Generative AI and the politics of visibility,"<jats:p> Proponents of generative AI tools claim they will supplement, even replace, the work of cultural production. This raises questions about the politics of visibility: what kinds of stories do these tools tend to generate, and what do they generally not? Do these tools match the kind of diversity of representation that marginalized populations and non-normative communities have fought to secure in publishing and broadcast media? I tested three widely available generative AI tools with prompts designed to reveal these normative assumptions; I prompted the tools multiple times with each, to track the diversity of the outputs to the same query. I demonstrate that, as currently designed and trained, generative AI tools tend to reproduce normative identities and narratives, rarely representing less common arrangements and perspectives. When they do generate variety, it is often narrow, maintaining deeper normative assumptions in what remains absent. </jats:p>"
10.2139/ssrn.4388437,"A Practical Introduction to Generative AI, Synthetic Media, and the Messages Found in the Latest Medium",N/A
10.4324/9781032648033-6,Who assesses what?,N/A
10.1038/d41586-023-01546-4,Why Nature will not allow the use of generative AI in images and video,N/A
10.1353/hpn.2023.a906568,Exploring the Potential of Generative AI (ChatGPT) for Foreign Language Instruction: Applications and Challenges,N/A
10.2139/ssrn.4628786,Generative AI and Human Knowledge Sharing: Evidence from A Natural Experiment,N/A
10.3102/2106114,Exploring Inventions in Self-Directed Language Learning With Generative AI: Implementations and Perspectives of YouTube Content Creators,N/A
10.5121/csit.2024.140510,An Intelligent Mobile Application to Recommend Clothing using Machine Learning and Generative AI,"<jats:p>This paper addresses the critical issue of sustainability within the fast-paced fashion industry, characterized by overconsumption and waste. We propose an innovative solution, the Slow Low Fashion app, which leverages advanced artificial intelligence (AI) technology to analyze users' body measurements, providing personalized, gender-inclusive clothing recommendations [4]. This approach aims to minimize overconsumption and reduce the environmental footprint by encouraging more thoughtful purchasing decisions. The core technologies of our program include AI for body ratio analysis, a secure and privacy-compliant data handling framework, and an intuitive user interface designed to enhance user experience. Despite challenges such as ensuring the accuracy of body measurements and maintaining data privacy, we addressed these through rigorous testing and implementing encryption protocols. Experimentation across diverse user scenarios demonstrated the app's effectiveness in reducing unnecessary purchases and promoting sustainability. The significant reduction in clothing waste and increased user satisfaction highlight the potential of Slow Low Fashion as a tool for promoting sustainable fashion consumption. Our project underscores the necessity and feasibility of integrating technology and personalization in addressing the environmental challenges posed by the fashion industry.</jats:p>"
10.2139/ssrn.4778017,Legal Education's Role in Combating Automation Bias and Complacency with Generative AI in the Practice of Law,N/A
10.2139/ssrn.4769557,Generative AI Capability Maturity Model for Online and Adult Learning: Introducing the EMERALD-GenAI-CMM-OAL Framework,N/A
10.31274/itaa.17640,Exploring the power of human-AI collaboration: The role of perceived mind and expertise in generative fashion design,N/A
10.1109/icsa-c63560.2024.00034,Leveraging Generative AI for Architecture Knowledge Management,N/A
10.1145/3637627,U.S. Copyright Office's Questions about Generative AI,<jats:p>Generating more questions than answers.</jats:p>
10.1007/978-3-031-55642-5_3,DAnTE: A Taxonomy for the Automation Degree of Software Engineering Tasks,N/A
10.21125/edulearn.2024.2546,"INTEGRATING GENERATIVE AI METHODS IN COMPUTER SCIENCE EDUCATION: PERSPECTIVES, STRATEGIES, AND OUTCOMES",N/A
10.1145/3630106.3659009,Constructing Capabilities: The Politics of Testing Infrastructures for Generative AI,N/A
10.14445/22312803/ijctt-v72i7p107,Generative AI and the Future of Workforce in Marketing,N/A
10.1109/icdcsw60045.2023.00022,Empowering the Metaverse with Generative AI: Survey and Future Directions,N/A
10.5334/uproc.157,Teaching with Generative AI: moving forward with content creation,"<jats:p>Generative AI is widely available and has raised the expectation that it will impact Education. Models, such as ChatGPT, can quickly produce plausible texts on a wide range of topics, and this capability may be of potential use in course content production. This paper selects several important course content production tasks, describes the prompts used, and assesses the quality of the automatically generated texts by a team of experts. Across all tasks, the Generative AI produced content that could help solve specific tasks by aiding with brainstorming, creating outlines, and adhering to particular writing guidance. In all cases, the content required adjustments and checking by human experts.</jats:p>"
10.25236/ijnde.2024.060205,Beyond Traditional Pathways: Leveraging Generative AI for Dynamic Career Planning in Vocational Education,N/A
10.4018/979-8-3693-3597-0.ch020,Innovative Approaches to Public Safety,"<jats:p>This book chapter examines cutting-edge tactics to improve public safety by using Generative Adversarial Networks (GANs) to improve cyber security in public areas. Conventional security solutions frequently fail to provide adequate protection against cyber-attacks in public settings in an era characterized by growing digital interconnection and changing security risks. Using GANs, a state-of-the-art machine learning method, offers a viable way to strengthen cyber security measures and reduce possible threats. The chapter explores the theoretical underpinnings of GANs and how they are used to identify and neutralize cyber threats in public areas. Through the utilisation of GANs to produce artificial intelligence-generated data and replicate cyber-attack scenarios, entities can anticipate weaknesses and develop resilient protection strategies in advance.</jats:p>"
10.15801/je.1.126.201909.73,A Study on the Ethics Certification Program Based on the Morality Types of AI Robots,N/A
10.1007/s10551-019-04180-1,From Diversity to Inclusion to Equity: A Theory of Generative Interactions,N/A
10.1515/9780857450067-018,14. Anthropology and the Generative Primacy of Moral Order,N/A
10.1080/0731129x.2021.1951459,Can AI Weapons Make Ethical Decisions?,N/A
10.1007/s10676-020-09556-w,Friendly AI,"<jats:title>Abstract</jats:title><jats:p>In this paper we discuss what we believe to be one of the most important features of near-future AIs, namely their capacity to behave in a friendly manner to humans. Our analysis of what it means for an AI to behave in a friendly manner does not presuppose that proper friendships between humans and AI systems could exist. That would require reciprocity, which is beyond the reach of near-future AI systems. Rather, we defend the claim that social AIs should be programmed to behave in a manner that <jats:italic>mimics</jats:italic> a sufficient number of aspects of proper friendship. We call this “as-if friendship”. The main reason for why we believe that ‘as if friendship’ is an improvement on the current, highly submissive behavior displayed by AIs is the negative effects the latter can have on humans. We defend this view partly on virtue ethical grounds and we argue that the virtue-based approach to AI ethics outlined in this paper, which we call “virtue alignment”, is an improvement on the traditional “value alignment” approach.</jats:p>"
10.2139/ssrn.4776480,From Lexicons to Generative AI: Benchmarking Data Annotation in Business Research,N/A
10.1386/adch_00088_1,Making the case for introducing generative artificial intelligence (AI) into design curricula,"<jats:p>The use of generative artificial intelligence (AI) in higher education design programmes is expanding, yet there is little formalized approach to its integration. Professionally, generative AI is starting to become an indispensable tool for ideation and prototyping, two fundamental skills taught in design’s studio pedagogy. Yet this digital leap into the future risks leaving design educators behind unless they take a proactive approach to its implementation and present its strengths and weaknesses. This study surveyed 74 design students from an Australian university, exploring their current utilization of generative AI and their projections for its future application in design practice. Findings confirm that generative AI is being used in an ad hoc way by students to speed up the ideation process tempered by a sceptical view of its creative output. A list of generative AI training for integration into the design curricula based on current research and survey results is proposed.</jats:p>"
10.20944/preprints202409.0677.v1,Leveraging Generative AI for Inclusive Online Friendship Education: Integrating Easy-to-Read Guidelines for Enhanced Accessibility,"<jats:p>This study examines the feasibility and efficacy of utilizing generative AI technology to create inclusive online dating educational materials, focusing on integrating easy-to-read principles to enhance accessibility while improving efficiency and reducing the cost of material production. As online dating becomes increasingly prevalent, providing easily comprehensible safety education for special populations, such as individuals with disabilities and adolescents, is of paramount importance. This research developed an innovative AI-generated educational framework that combines easy-to-read principles with generative AI technology, aiming to enhance both the accessibility and production efficiency of educational materials. 
The study compared three AI generation strategies: independent generation by ChatGPT, independent generation by Claude, and Claude utilizing materials generated by ChatGPT. We evaluated 36 educational units through a mixed-methods approach for readability and accessibility. Results indicate that ChatGPT performed optimally in generating easily readable content (average compliance rate of 93.00%), followed by the strategy Claude using ChatGPT-generated materials (91.67%). Statistical analysis revealed significant differences among these strategies (p &amp;lt; 0.05). The research also found that employing AI generation technology significantly reduced material production time and costs, providing educators with a more efficient tool. 
The findings of this study underscore the significant potential of generative AI technology in creating inclusive educational resources. This not only opens up new avenues for enhancing online safety awareness and educational accessibility for special populations but also provides material developers with more efficient, cost-effective production methods. The empirical evidence presented here for the application of AI in inclusive education is a significant step forward. The study also proposes recommendations for future research and practice, aiming to further improve the accessibility, effectiveness, and production efficiency of online dating education.</jats:p>"
10.2196/preprints.51414,Incorporating AI Generative Language Models into Medical Education: An Instructional Tool for Health Professionals and Students-A Case of ChatGPT (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>incorporating generative language models or AI into medical education</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>To investigate and examine the combination of AI technology; generative language models and instructional teaching tools to ensure information dissemination in the health sector
Assess ChatGPT' s ability as an instructional teaching tool in medical sector</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>A systematic review of related literature and examination of the ChatGPT model</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>The research found that despite the limitations of ChatGPT such as data hallucination, and improper referencing it remains the best teaching tool. However, there are mitigation plans in place such as AI content detectors, safeguarding of academic integrity, and similarity checkers recommended for assistance. It is for this reason that, in as much as we are unable to escape Artificial intelligence, ChatGPT can emerge as the best teaching tool for adoption if properly used and understood in medical health.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>Generative AI models are powerful tools for use in the medical sector. This conversational transformer-based chat allows medical professionals to generate content, teach, learn, and improve the general overview of health science. With its ability to revolutionize the medical sector. ChatGPT came at the right when needed, it is on this premise that we need to teach the world about its advantages, disadvantages, and operations without breaching copyright policies, privacy, academic rules, patient-doctor relationships as well as ethical considerations. When used properly it provides the best advantages, considering mitigation plans are kept in place to prevent any malicious intent. Plagiarism checkers, AI content detectors, proper academic referencing, real-time data supply, similarity checkers and generally ensuring the right ethics are followed through the use of ChatGPT are some of the best applicable practices. Finally, though we admit its drawbacks it is a powerful conversational chat for use in our daily lives.</p>
                </sec>"
10.1002/9781394308286.app,Check Your Understanding Answers,N/A
10.12781/978-1-907549-53-3-1,"Learning and Leveraging Generative Approaches to Intercultural, Diversity, Equity and Inclusion","<jats:p>The commitment to meaningfully and transfomatively address intercultural, diversity, equity and inclusion issues in organizations has grown exponentially. In this issue we invited a diverse group of people to tell the stories of what they were discovering and learning from this work. The articles also reflect the diversity of journeys and hard work within organizations.</jats:p>"
10.1093/jiplp/jpad081,"Generative AI and copyright: principles, priorities and practicalities",N/A
10.26434/chemrxiv-2023-wcrv3,Generative AI-Driven Molecular Design: Combining Predictive Models and Reinforcement Learning for Tailored Molecule Generation,"<jats:p>Molecular design is a critical aspect of various scientific and industrial fields, where the properties of molecules hold significant importance. In this study, a three-fold methodology design is presented that leverages the power of generative artificial intelligence (AI), predictive modeling, and reinforcement learning to create tailored molecules with desired properties. This model synergistically combines deep learning techniques with Self-Referencing Embedded Strings (SELFIES) molecular representation to build a generative model which generates valid molecules and a graphical neural network model that accurately forecasts molecular properties. The Variational Autoencoder (VAE) coupled with reinforcement learning, helps refine molecule generation based on targeted attributes. Data from an experimental study involving surfactants was used to test the framework. Saliency maps for the generated surfactants were produced to identify the features explaining the property values. The results showed that the proposed framework can effectively produce valid molecules within the set property threshold value. This approach not only streamlines molecular design for surfactant systems but also augurs transformative advancements across different scientific and industrial landscapes.</jats:p>"
10.55454/rcsas.3.09.2023.001,Architecting A Secure Future: Cybersecurity in the Era of Generative AI,N/A
10.2196/preprints.56891,Generative AI in Medicine: Pioneering Progress or Perpetuating Historical Inequities? (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>In a pilot experiment, we utilized AI to generate 100 photographs of physicians in 19 medical subspecialties and compared these photographs to the existing and incoming medical specialty workforce. Our work demonstrates that generative AI has a tendency to under represent women in common specialties, and thus the utilization of this tool in the future must be viewed with caution and a lens of known historical bias.</p>
                </sec>"
10.2139/ssrn.4611557,Creating Innovation Value from Generative Ai: A Property Rights Perspective,N/A
10.2196/58370,Authors’ Reply: A Use Case for Generative AI in Medical Education,N/A
10.1145/3630106.3658987,The Impact and Opportunities of Generative AI in Fact-Checking,N/A
10.1145/3597151,"Legal Challenges to Generative AI, Part I",<jats:p>Questioning the legality of using in-copyright works for training data and producing outputs derived from copyrighted training data.</jats:p>
10.1109/icdt61202.2024.10489759,Impact of Generative AI in Diagnosing Diseases in Agriculture,N/A
10.3390/educsci13111155,The Effects of Generative AI Platforms on Undergraduates’ Narrative Intelligence and Writing Self-Efficacy,"<jats:p>Digital storytelling and generative artificial intelligence (AI) platforms have emerged as transformative tools that empower individuals to write with confidence and share their stories effectively. However, a research gap exists in understanding the effects of using such web-based platforms on narrative intelligence and writing self-efficacy. This study aims to investigate whether digital story creation tasks on web-based platforms can influence the narrative intelligence and writing self-efficacy of undergraduate students. A pretest–posttest comparison study between two groups was conducted with sixty-four undergraduate students (n = 64), majoring in Primary Education. More specifically, it compares the effects of the most well-known conventional platforms, such as Storybird, Storyjumper, and ZooBurst (control condition), and generative AI platforms, such as Sudowrite, Jasper, and Shortly AI (experimental condition) on undergraduate students, with an equal distribution in each group. The findings indicate that the utilization of generative AI platforms in the context of story creation tasks can substantially enhance both narrative intelligence scores and writing self-efficacy when compared to conventional platforms. Nonetheless, there was no significant difference in the creative identity factor. Generative AI platforms have promising implications for supporting undergraduates’ narrative intelligence and writing self-efficacy in fostering their story creation design and development.</jats:p>"
10.1109/cste62025.2024.00032,Generative AI in Higher Art Education,N/A
10.29173/irie419,"Value Pluralism in the AI Ethics Debate – Different Actors, Different Priorities","<jats:p>In the current debate on the ethics of Artificial Intelligence (AI) much attention has been paid to find some “common ground” in the numerous AI ethics guidelines. The divergences, however, are equally important as they shed light on the conflicts and controversies that require further debate. This paper analyses the AI ethics landscape with a focus on divergences across actor types (public, expert, and private actors). It finds that the differences in actors’ priorities for ethical principles influence the overall outcome of the debate. It shows that determining “minimum requirements” or “primary principles” on the basis of frequency excludes many principles that are subject to controversy, but might still be ethically relevant. The results are discussed in the light of value pluralism, suggesting that the plurality of sets of principles must be acknowledged and can be used to further the debate.</jats:p>"
10.1007/s10676-007-9138-2,AI Armageddon and the Three Laws of Robotics,N/A
10.1163/18758185-bja10005,"Pragmatic Ethics for Generative Adversarial Networks: Coupling, Cyborgs, and Machine Learning","<jats:title>Abstract</jats:title>
<jats:p>This article addresses the need for adaptive ethical analysis within machine learning that accounts for emerging problems concerning social bias and generative adversarial networks (<jats:sc>gan</jats:sc> s). I use John Dewey’s criticisms of the reflex arc concept in psychology as a basis for understanding how these problems stem from human-<jats:sc>gan</jats:sc> interaction. By combining Dewey’s criticisms with Donna Haraway’s idea of cyborgs, Luciano Floridi’s concept of distributed morality, and Shaowen Bardzell’s recommendations for a feminist approach to human-computer interaction, I suggest a dynamic perspective from which to begin analyzing and solving issues of injustice evident in this particular domain of machine learning.</jats:p>"
10.1007/s10676-020-09535-1,The possibility of deliberate norm-adherence in AI,N/A
10.1007/978-3-030-54173-6_16,Robots and Rights: Reviewing Recent Positions in Legal Philosophy and Ethics,"<jats:title>Abstract</jats:title><jats:p>Controversies about the moral and legal status of robots and of humanoid robots in particular are among the top debates in recent practical philosophy and legal theory. As robots become increasingly sophisticated, and engineers make them combine properties of tools with seemingly psychological capacities that were thought to be reserved for humans, such considerations become pressing. While some are inclined to view humanoid robots as more than just tools, discussions are dominated by a clear divide: What some find appealing, others deem appalling, i.e. “robot rights” and “legal personhood” for AI systems. Obviously, we need to organize human–robot interactions according to ethical and juridical principles that optimize benefit and minimize mutual harm. Avoiding disrespectful treatment of robots can help to preserve a normative basic ethical continuum in the behaviour of humans. This insight can contribute to inspire an “overlapping consensus” as conceptualized by John Rawls in further discussions on responsibly coordinating human/robot interactions.</jats:p>"
10.1007/s00146-021-01308-8,"Operationalising AI ethics: barriers, enablers and next steps","<jats:title>Abstract</jats:title><jats:p>By mid-2019 there were more than 80 AI ethics guides available in the public domain. Despite this, 2020 saw numerous news stories break related to ethically questionable uses of AI. In part, this is because AI ethics theory remains highly abstract, and of limited practical applicability to those actually responsible for designing algorithms and AI systems. Our previous research sought to start closing this gap between the ‘what’ and the ‘how’ of AI ethics through the creation of a searchable typology of tools and methods designed to translate between the five most common AI ethics principles and implementable design practices. Whilst a useful starting point, that research rested on the assumption that <jats:italic>all</jats:italic> AI practitioners are aware of the ethical implications of AI, understand their importance, and are actively seeking to respond to them. In reality, it is unclear whether this is the case. It is this limitation that we seek to overcome here by conducting a mixed-methods qualitative analysis to answer the following four questions: what do AI practitioners understand about the need to translate ethical principles into practice? What motivates AI practitioners to embed ethical principles into design practices? What barriers do AI practitioners face when attempting to translate ethical principles into practice? And finally, what assistance do AI practitioners want and need when translating ethical principles into practice?</jats:p>"
10.1007/978-3-031-09846-8_12,Ethics Auditing Framework for Trustworthy AI: Lessons from the IT Audit Literature,N/A
10.1093/oxfordhb/9780190067397.013.9,We’re Missing a Moral Framework of Justice in Artificial Intelligence,"<p>This chapter assesses the concepts of fairness and bias in artificial intelligence research and interventions. In considering the explosive growth, emergence of, and investment in high-profile AI fairness and ethics interventions within both the academy and industry—alongside the mounting and proliferating calls for the interrogation, regulation, and, in some cases, dismantling and prohibition of AI—it contests and questions the extent to which such remedies can address the original concerns and problems they are designed to address. Indeed, many community organizations are organizing responses and challenging AI used in predictive technologies—facial-recognition software and biometrics technologies—with increasing success. Ultimately, the canon of AI ethics must interrogate and deeply engage with intersectional power structures that work to further consolidate capital in the hands of the elites and that will undergird digital informational systems of inequality: there is no neutral or objective state through which the flows and mechanics of data can be articulated as unbiased or fair.</p>"
10.1007/s00146-022-01455-6,Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms,"<jats:title>Abstract</jats:title><jats:p>The increasing implementation of and reliance on machine-learning (ML) algorithms to perform tasks, deliver services and make decisions in health and healthcare have made the need for fairness in ML, and more specifically in healthcare ML algorithms (HMLA), a very important and urgent task. However, while the debate on fairness in the ethics of artificial intelligence (AI) and in HMLA has grown significantly over the last decade, the very concept of fairness as an ethical value has not yet been sufficiently explored. Our paper aims to fill this gap and address the AI ethics principle of fairness from a conceptual standpoint, drawing insights from accounts of fairness elaborated in moral philosophy and using them to conceptualise fairness as an ethical value and to redefine fairness in HMLA accordingly. To achieve our goal, following a first section aimed at clarifying the background, methodology and structure of the paper, in the second section, we provide an overview of the discussion of the AI ethics principle of fairness in HMLA and show that the concept of fairness underlying this debate is framed in purely distributive terms and overlaps with non-discrimination, which is defined in turn as the absence of biases. After showing that this framing is inadequate, in the third section, we pursue an ethical inquiry into the concept of fairness and argue that fairness ought to be conceived of as an ethical value. Following a clarification of the relationship between fairness and non-discrimination, we show that the two do not overlap and that fairness requires much more than just non-discrimination. Moreover, we highlight that fairness not only has a distributive but also a socio-relational dimension. Finally, we pinpoint the constitutive components of fairness. In doing so, we base our arguments on a renewed reflection on the concept of respect, which goes beyond the idea of equal respect to include respect for individual persons. In the fourth section, we analyse the implications of our conceptual redefinition of fairness as an ethical value in the discussion of fairness in HMLA. Here, we claim that fairness requires more than non-discrimination and the absence of biases as well as more than just distribution; it needs to ensure that HMLA respects persons both as persons and as particular individuals. Finally, in the fifth section, we sketch some broader implications and show how our inquiry can contribute to making HMLA and, more generally, AI promote the social good and a fairer society.</jats:p>"
10.47809/ictmf.2023.02.09,The Impact of Generative AI in Music Composition / Impactul inteligenței artificiale generative în compoziția muzicală,N/A
10.1109/iotm.001.2300255,"From Generative AI to Generative Internet of Things: Fundamentals, Framework, and Outlooks",N/A
10.4324/9780429329067-13,Integrating AI ethics across the computing curriculum,N/A
10.1007/978-3-658-40232-7_6,AI Ethics and Neuroethics Promote Relational AI Discourse,N/A
10.1007/978-3-031-53966-4_7,Diversity in Deep Generative Models and Generative AI,N/A
10.52783/eel.v13i5.888,The Ethics of Ai and Ml: Balancing Innovation and Responsibility in Business Applications,N/A
10.48047/jcr.09.01.55,"ETHICS IN AI: BIAS, FAIRNESS, AND ACCOUNTABILITY (2020)",N/A
10.1007/s43681-024-00482-x,"Automated ethical decision, value-ladenness, and the moral prior problem","<jats:title>Abstract</jats:title><jats:p>Part of the literature on machine ethics and ethical artificial intelligence focuses on the idea of defining autonomous ethical agents able to make ethical choices and solve dilemmas. While ethical dilemmas often arise in situations characterized by uncertainty, the standard approach in artificial intelligence is to use rational choice theory and maximization of expected utility to model how algorithm should choose given uncertain outcomes. Motivated by the <jats:italic>moral proxy problem</jats:italic>, which proposes that the appraisal of ethical decisions varies depending on whether algorithms are considered to act as proxies for higher- or for lower-level agents, this paper introduces the <jats:italic>moral prior problem</jats:italic>, a limitation that, we believe, has been genuinely overlooked in the literature. In a nutshell, the moral prior problem amounts to the idea that, beyond the thesis of the value-ladenness of technologies and algorithms, automated ethical decisions are predetermined by moral priors during both conception and usage. As a result, automated decision procedures are insufficient to produce ethical choices or solve dilemmas, implying that we need to carefully evaluate what autonomous ethical agents are and can do, and what they aren’t and can’t.</jats:p>"
10.1007/s43681-021-00116-6,"The road to a human-centred digital society: opportunities, challenges and responsibilities for humans in the age of machines",N/A
10.1007/s43681-023-00359-5,Exploring the status of artificial intelligence for healthcare research in Africa: a bibliometric and thematic analysis,"<jats:title>Abstract</jats:title><jats:p>This paper explores the status of Artificial Intelligence (AI) for healthcare research in Africa. The aim was to use bibliometric and thematic analysis methods to determine the publication counts, leading authors, top journals and publishers, most active institutions and countries, most cited institutions, funding bodies, top subject areas, co-occurrence of keywords and co-authorship. Bibliographic data were collected on April 9 2022, through the Lens database, based on the critical areas of authorship studies, such as authorship pattern, number of authors, etc. The findings showed that several channels were used to disseminate the publications, including articles, conference papers, reviews, and others. Publications on computer science topped the list of documented subject categories. The Annals of Tropical Medicine and Public Health is the top journal, where articles on AI have been published. One of the top nations that published AI research was the United Kingdom. With 143 publications, Harvard University was the higher education institution that produced the most in terms of affiliation. It was discovered that the Medical Research Council was one of the funding organizations that supported research, resulting in the publication of articles in AI. By summarizing the current research themes and trends, this work serves as a valuable resource for researchers, practitioners, and funding organizations interested in Artificial intelligence for healthcare research in Africa.</jats:p>"
10.22215/etd/2023-15586,Developing a Medical AI Ethics Framework: Integrating Ethical Principles with Healthcare Applications Using Topic Modelling,N/A
10.31235/osf.io/x5nck,AI Ethics Training in Higher Education: Competency Framework,"<p>Since any training in AI ethics is first and foremost indebted to a conception of ethics training in general, we identify the specific requirements related to the ethical dimensions of this cutting-edge technological innovation. We show how a pragmatist approach inspired by the work of John Dewey allows us to clearly identify both the essential components of such training and the specific fields related to the development of AI systems. More precisely, by focusing on some central characteristics of such a pragmatist approach, namely anti-foundationalism, anti-dualism and anti-skepticism, characteristics shared by the philosophies of the main representatives of the pragmatist movement, we will see how the different components of the ethical competence - namely ethical sensitivity, reflexive capacities and dialogical capacities - can be conceived in a dynamic and interdependent way. We will then be able to examine the specific fields of training in AI ethics, insisting on the necessary complementarity between the specific moral dilemmas associated with this technology and the technical, social and normative (especially legislative) aspects in order to adequately grasp the ethical issues related to the design, development and deployment of AI systems. In doing so, we will be able to determine the requirements that should guide the implementation of an adequate training in AI ethics, by providing benchmarks for the teaching of these issues.</p>"
10.1201/9781003293125,Real World AI Ethics for Data Scientists,N/A
10.31219/osf.io/bhm3w,Everyday AI ethics: from the global to local through facial recognition,"<p>The focus of AI ethics discussions, and flows of funding, have been concentrated at national or international levels. This contribution seeks to investigate the more ‘everyday’ practices and applications of AI ethics in local settings where individuals and communities encounterAI applications.  I do this by looking at one, controversial, application of AI in the form of facial recognition technology,  with a focus on how this has played out in local settings in the UK. Through the lens of legal, policy and activist activities around this technology, including protest and social movement activities, bans and moratoriums on police use of facial recognition, and litigation – I will demonstrate how a grounded, localized, everyday approach is needed in addition to higher level approaches and initiatives to understand practical implementations and contestations of AI ethics, especially emanating from protest, social movements and legal mobilisations. We must look to local settings to understand how AI ethics are negotiated, implemented and where they have actual impact. However, this ‘impact’ may not align with what researchers in AI ethics find important or are incentivized to achieve, as it may not align with government and corporate goals in AI, and instead involve prohibitions or restrictions of problematic AI uses.</p>"
10.1007/s43681-023-00394-2,An overview of key trustworthiness attributes and KPIs for trusted ML-based systems engineering,N/A
10.1007/s43681-024-00467-w,Beyond phase-in: assessing impacts on disinformation of the EU Digital Services Act,"<jats:title>Abstract</jats:title><jats:p>This work proposes a comprehensive research agenda to empirically evaluate the real-world impacts of the European Union’s Digital Services Act (DSA) on combating online disinformation. It provides background on the DSA’s context, mechanisms, timeline, and expected effects on platforms to situate the need for rigorous impact assessment. A detailed legal, technical, psychological, behavioral and ethical critique reveals meaningful gaps in the DSA requiring ongoing regulatory refinement and oversight. Most critically, the paper puts forth an encompassing framework spanning computational analytics, interviews, ethnography, surveys, discourse analysis and mixed methods to rigorously assess the DSA’s multi-dimensional effects on complex factors enabling disinformation proliferation. Priorities include evaluating notice-and-takedown efficacy, advertising transparency improvements, risk assessment outcomes, oversight integration, and procedural shifts in platform governance. Coordinated efforts between researchers, regulators and platforms are needed to address methodological challenges around isolating DSA impacts amidst an evolving EU regulatory landscape, constrained data access from platforms, and difficulties generalizing findings across the sociotechnical diversity of platforms and national contexts in EU Member States.</jats:p>"
10.31219/osf.io/f4v8s,The Impact of Free Access to AI Ethics and Governance Standards,<p>The Impact of Free Access to AI Ethics and Governance Standards</p>
10.1145/3278721,"Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",N/A
10.1145/3676231,"Session details: Design, Ethics and AI Explorations",N/A
10.4324/9781032694283-21,International call for AI Ethics,N/A
10.1007/978-3-030-22767-8_1297,AI and Corporate Responsibility,N/A
10.1145/3600211.3604743,AI-driven Automation as a Pre-condition for Eudaimonia,N/A
10.1145/3306618.3314250,Incomplete Contracting and AI Alignment,N/A
10.1007/s43681-024-00573-9,"Right to be forgotten in the Era of large language models: implications, challenges, and solutions","<jats:title>Abstract</jats:title><jats:p>The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja González, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. It was a significant emergent right as the result of the evolution of technology. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of differential privacy, machine unlearning, model editing, and guardrails. With the rapid advancement of AI and the increasing need of regulating this powerful technology, learning from the case of RTBF can provide valuable lessons for technical practitioners, legal experts, organizations, and authorities.</jats:p>"
10.1145/3461702,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",N/A
10.1145/3306618,"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",N/A
10.1088/978-0-7503-6116-3ch4,Content governance,N/A
10.4018/979-8-3693-2643-5.ch013,AI-Driven Job Displacement and Economic Impacts,"<jats:p>This chapter discusses the ethical implications of AI-driven technologies, particularly job displacement and economic impact. It emphasizes the need for transparency in AI algorithms and decision-making processes to foster trust and accountability. The paper also advocates for proactive measures like reskilling and upskilling programs to empower workers for future jobs and mitigate adverse effects on employment and economic stability. The paper emphasizes the significance of stakeholder engagement and inclusive decision-making in aligning AI deployment with societal values. It also discusses the role of government policies in preventing discrimination and promoting equal opportunities. The paper also highlights the need for ongoing monitoring and evaluation to assess the ethical implications of AI-driven job displacement. By prioritizing ethical principles, organizations can navigate AI's transformative potential while minimizing socio-economic impacts.</jats:p>"
10.4135/9789354792861.n9,Tracing the Ethics and Legality,N/A
10.2139/ssrn.4942322,Ethics &amp;amp; Responsible AI in Healthcare,N/A
10.2196/preprints.58493,Regulating AI in Mental Health – The Ethics of Care Perspective (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>This article contends that the responsible AI approach—which is the dominant ethics approach ruling most regulatory and ethical guidance—falls short because it overlooks the impact of AI on human relationships. Focusing only on responsible AI principles reinforces a narrow concept of accountability and responsibility of companies developing AI. This article proposes that integrating the ethics of care approach can offer a more comprehensive regulatory and ethical framework that addresses AI’s impact on human relationships. This dual approach is essential for the effective regulation of AI in the domain of mental health care. 
The article delves into the emergence of the new “therapeutic” area facilitated by AI-based bots, which operate without a therapist. The article highlights the difficulties involved, mainly the absence of a defined duty of care towards users, and shows how implementing ethics of care can establish clear responsibilities for developers. It also sheds light on the potential for emotional manipulation and the risks involved. In conclusion, the article proposes a series of considerations grounded in the ethics of care for the developmental process of AI-powered therapeutic tools.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>To suggest a new framework for regulating AI in mental health care based on the ethics of care</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>theoretical analysis</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>Ethics of care principles formulated for regulating AI in mental healthcare and a suggested mechanism for ethical evaluation for implementing AI-bots (without a therapist) in mental healthcare</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>Responsible AI is not sufficient for regulating AI in mental healthcare as it overlooks emotions and relationships, and a dual approach (both responsible AI approach and ethics of care approach) is needed to regulate AI in mental healthcare in an effective way</p>
                </sec>"
10.5772/intechopen.92952,"Factoring Ethics in Technology, Policy Making, Regulation and AI",N/A
10.1145/3514094,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",N/A
10.1145/3600211,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",N/A
10.1002/9781119790686.ch40,‘Designing’ Ethics into AI,N/A
10.37123/th.2022.10.39,Two Fundamental Questions in AI Ethics : With a Reflection on AI Chatbot Iruda,N/A
10.1007/978-1-4842-7158-2_2,"AI Ethics, Biasness, and Reliability",N/A
10.4337/9781802205657.00020,"Idealism, realism, pragmatism: three modes of theorising within secular AI ethics",N/A
10.1145/3278721.3278744,Towards Composable Bias Rating of AI Services,N/A
10.1007/s00146-022-01563-3,"The ethics of conceptual, ontological, semantic and knowledge modeling",N/A
10.1145/3375627.3375817,Exploring AI Futures Through Role Play,N/A
10.4324/9780429321214-13,Understanding and Managing the Ethics of AI,N/A
10.1145/3278721.3278765,"Towards an ""Ethics by Design"" Methodology for AI Research Projects",N/A
10.1007/s00146-017-0690-y,Programming Machine Ethics by Luís Moniz Pereira and Ari Saptawijaya,N/A
10.1007/s00146-021-01238-5,Professional ethics and social responsibility: military work and peacebuilding,"<jats:title>Abstract</jats:title><jats:p>This paper investigates four questions related to ethical issues associated with the involvement of engineers and scientists in 'military work', including the influence of ethical values and beliefs, the role of gendered perspectives and moves beyond the purely technical. It fits strongly into a human (and planet)-centred systems perspective and extends my previous AI and Society papers on othering and narrative ethics, and ethics and social responsibility. It has two main contributions. The first involves an analysis of the literature through the application of different ethical theories and the application of gendered analysis to discussion of masculinities in engineering and the military. The second is a survey of scientists and engineers to investigate their opinions and experiences. The conclusions draw together the results of these two contributions to provide preliminary responses to the four questions and include a series of recommendations covering education and training, ethical approval of work not involving human participants or animals, the need for organisational support, approaches covering wider perspectives and the encouragement of individual ethical commitment.</jats:p>"
10.1007/978-3-319-23514-1_1297-1,AI and Corporate Responsibility,N/A
10.47289/aiej20210716-3,How to Improve Fairness Perceptions of AI in Hiring: The Crucial Role of Positioning and Sensitization,"<jats:p>Companies increasingly deploy artificial intelligence (AI) technologies in their personnel recruiting and selection processes to streamline them, thus making them more efficient, consistent, and less human biased (Chamorro-Premuzic, Polli, &amp; Dattner, 2019) . However, prior research found that applicants prefer face-to-face interviews compared with AI interviews, perceiving them as less fair (e.g., Acikgoz, Davison, Compagnone, &amp; Laske, 2020) . Additionally, emerging evidence exists that contextual influences, such as the type of task for which AI is used (Lee, 2018) , or applicants’ individual differences (Langer, König, Sanchez, &amp; Samadi, 2019) , may influence applicants’ reactions to AI-powered selection. The purpose of our study was to investigate whether adjusting process design factors may help to improve people's fairness perceptions of AI interviews. The results of our 2 x 2 x 2 online study (N = 404) showed that the positioning of the AI interview in the overall selection process, as well as participants’ sensitization to its potential to reduce human bias in the selection process have a significant effect on people’s perceptions of fairness. Additionally, these two process design factors had an indirect effect on overall organizational attractiveness mediated through applicants’ fairness perceptions. The findings may help organizations to optimize their deployment of AI in selection processes to improve people’s perceptions of fairness and thus attract top talent.</jats:p>"
10.1016/b978-0-443-18851-0.00013-5,The hard problem of the androcentric context of AI: challenges for EU policy agendas,N/A
10.1145/3306618.3314320,A Framework for Technically- and Morally-Sound AI,N/A
10.1007/978-3-030-66913-3_7,AI and the Ethical Challenge,N/A
10.18208/ksdc.2023.29.4.319,Suggestions for the Use of Fashion Images with Generative AI-Focusing on Application of AI Training Data and AI Technology-,N/A
10.1007/s00146-015-0594-7,Engineers and the other: the role of narrative ethics,N/A
10.1355/9789815203684-006,WHAT THE ASEAN AI GUIDE GETS RIGHT,N/A
10.1007/s00146-005-0003-8,Leonardo’s choice: the ethics of artists working with genetic technologies,N/A
10.12681/jpentai.34287,e-Securing the EU Borders: AI in European Integrated Border Management,"<jats:p>In the European Union, the external borders were not always conceived as European ones. However, after experiencing a range of threats, the European policymakers identified external borders' importance to European security. Not forgetting that borders are connected with national authority, the EU through Frontex has made progressive steps towards a coordinated or even common border management. The main tools to achieve that are the shared European IT systems. Their enhanced capabilities, mostly based on Artificial Intelligence, will further establish the European Smart Borders. There is a complex ""network” of IT border systems already operating or prospect automated ones that are necessary to be resilient and align with core fundamental rights.</jats:p>"
10.1145/3306618.3314225,How We Talk About AI (and Why It Matters),N/A
10.1007/s11245-022-09874-2,Decolonizing AI Ethics: Relational Autonomy as a Means to Counter AI Harms,"<jats:title>Abstract</jats:title><jats:p>Many popular artificial intelligence (AI) ethics frameworks center the principle of autonomy as necessary in order to mitigate the harms that might result from the use of AI within society. These harms often disproportionately affect the most marginalized within society. In this paper, we argue that the principle of autonomy, as currently formalized in AI ethics, is itself flawed, as it expresses only a mainstream mainly liberal notion of autonomy as rational self-determination, derived from Western traditional philosophy. In particular, we claim that the adherence to such principle, as currently formalized, does not only fail to address many ways in which people’s autonomy can be violated, but also to grasp a broader range of AI-empowered harms profoundly tied to the legacy of colonization, and which particularly affect the already marginalized and most vulnerable on a global scale. To counter such a phenomenon, we advocate for the need of a relational turn in AI ethics, starting from a relational rethinking of the AI ethics principle of autonomy that we propose by drawing on theories on relational autonomy developed both in moral philosophy and Ubuntu ethics.</jats:p>"
10.1007/s00146-017-0768-6,The problem of machine ethics in artificial intelligence,N/A
10.1145/3514094.3534187,Responsible AI Systems: Who are the Stakeholders?,N/A
10.1017/s0963180123000087,"More Process, Less Principles: The Ethics of Deploying AI and Robotics in Medicine","<jats:title>Abstract</jats:title><jats:p>Current national and international guidelines for the ethical design and development of artificial intelligence (AI) and robotics emphasize ethical theory. Various governing and advisory bodies have generated sets of broad ethical principles, which institutional decisionmakers are encouraged to apply to particular practical decisions. Although much of this literature examines the ethics of designing and developing AI and robotics, medical institutions typically must make purchase and deployment decisions about technologies that have already been designed and developed. The primary problem facing medical institutions is not one of ethical design but of ethical deployment. The purpose of this paper is to develop a practical model by which medical institutions may make ethical deployment decisions about ready-made advanced technologies. Our slogan is “more process, less principles.” Ethically sound decisionmaking requires that the process by which medical institutions make such decisions include participatory, deliberative, and conservative elements. We argue that our model preserves the strengths of existing frameworks, avoids their shortcomings, and delivers its own moral, practical, and epistemic advantages.</jats:p>"
10.1007/s00146-020-01039-2,"Autonomous reboot: Aristotle, autonomy and the ends of machine ethics","<jats:title>Abstract</jats:title><jats:p>Tonkens (Mind Mach, 19, 3, 421–438, 2009) has issued a seemingly impossible challenge, to articulate a comprehensive ethical framework within which artificial moral agents (AMAs) satisfy a Kantian inspired recipe—""rational"" and ""free""—while also satisfying perceived prerogatives of machine ethicists to facilitate the creation of AMAs that are perfectly and not merely reliably ethical. Challenges for machine ethicists have also been presented by Anthony Beavers and Wendell Wallach. Beavers pushes for the reinvention of traditional ethics to avoid ""ethical nihilism"" due to the reduction of morality to mechanical causation. Wallach pushes for redoubled efforts toward a comprehensive account of ethics to guide machine ethicists on the issue of artificial moral agency. Options, thus, present themselves: reinterpret traditional ethics in a way that affords a comprehensive account of moral agency inclusive of both artificial and natural agents, or give up on the possibility and “muddle through” regardless. This series of papers pursues the first option, meets Tonkens' ""challenge"" and pursues Wallach's ends through Beavers’ proposed means, by ""landscaping"" traditional moral theory in resolution of a comprehensive account of moral agency. This first paper sets out the challenge and establishes the tradition that Kant had inherited from Aristotle, briefly entertains an Aristotelian AMA, fields objections, and ends with unanswered questions. The next paper in this series responds to the challenge in Kantian terms, and argues that a Kantian AMA is not only a possibility for Machine ethics research, but a necessary one.</jats:p>"
10.7315/cde.2024.112,A Study on the AI Aided Early Architectural Design Process – Focusing on LLM and Generative AI –,N/A
10.1145/3544549.3585657,Designing Participatory AI: Creative Professionals’ Worries and Expectations about Generative AI,N/A
10.1254/fpj.23009,Generative AI and authorship: trends in international journals,N/A
10.4018/979-8-3693-3731-8.ch005,Navigating the Promise and Perils of Generative AI in Healthcare,"<jats:p>Generative AI offers transformative potential in healthcare, enabling advancements from data augmentation to drug discovery. However, its adoption poses ethical, technical, and regulatory hurdles. Issues like bias, data privacy, and ethical use necessitate responsible implementation. Stakeholders must collaborate to navigate these challenges, prioritizing ethical guidelines, robust security measures, and regulatory compliance. Education is crucial to ensure awareness and equip stakeholders to make informed decisions. By fostering trust and accountability, generative AI can revolutionize healthcare responsibly.</jats:p>"
10.54941/ahfe1004579,Elaborating a framework that is able to structure and evaluate design workflow and composition of Generative AI visualizations.,"<jats:p>One aspect of using generative artificial intelligence (AI) in design workflows that is not well understood, is how designers navigate the use of prompts and imagery to control design outputs. To learn more about the ways generative AI can elaborate different innovative design aspects linked to a conceptual genre, we undertake an analysis of designer outputs and link these to a mapping of iterative design workflow, and the composition of prompts and imagery used to inform generative AI visualizations. A speculative design workshop presented young designers with a proposed framework to facilitate and guide them through an iterative design process using generative AI tools (Vizcom and Midjourney). Design outputs included designed objects, atmosphere, spatial configurations able to generate social communication and relations for futuristic residential lighting environment. Prompt writing and sketching were two crucial variables that enable high fidelity design visualization. Traditionally research on prompt writing focuses on variables of descriptive design imagery, point of view, quality of output, ordering, precision, tone and style. (Microsoft,2023) In this study we investigate how designers initiate and continually engage with a generative AI system to achieve consistency of outputs and elaborate design variations. The literature review identifies variables to assess the impact of creative input to structure the generative AI visualizations. Our analysis focus on the cognitive workflows and its ability to integrate a mix of analogical and digital methods( sketching, prompts writing, and digital imagery). The analysis constructs focus on the way the design methods translate different spatial qualities, phenomenal experiences that have potential to facilitate social communication and relations, (proximity, sightline-acoustic comfort). Our insights constructs a design framework to guide prompt composition that inform iterative generative AI outputs. The schematic framework may be used to guide  AI generated visualizations according to the qualities of atmosphere linked to the spatial forms and arrangements of interior spaces.  Written prompts and imagery are examined in their ability to articulate design principle, spatial qualities, spatial characteristics, spatial forms and social relations.</jats:p>"
10.1109/cvmi59935.2023.10465077,IoT Empowered AI: Transforming Object Recognition and NLP Summarization with Generative AI,N/A
10.1145/3613904.3642861,Is It AI or Is It Me? Understanding Users’ Prompt Journey with Text-to-Image Generative AI Tools,N/A
10.11591/ijai.v12.i4.pp1704-1712,Predictive maintenance of electromechanical systems based on enhanced generative adversarial neural network with  convolutional neural network,"<jats:p>&lt;p&gt;Predictive maintenance (PdM) is a cost-cutting method that involves avoiding breakdowns and production losses. Deep learning (DL) algorithms can be used for defect prediction and diagnostics due to the huge amount of data generated by the integration of analog and digital systems in manufacturing operations. To improve the predictive maintenance strategy, this study uses a hybrid of the convolutional neural network (CNN) and conditional generative adversarial neural network (CGAN) model. The proposed CNN-CGAN algorithm improves forecast accuracy while substantially reducing model complexity. A comparison with standalone CGAN utilizing a public dataset is performed to evaluate the proposed model. The results show that the proposed CNN-CGAN model outperforms the conditional GAN (CGAN) in terms of prediction accuracy. The average F-Score is increased from 97.625% for the CGAN to 100% for the CNN-CGAN.&lt;/p&gt;</jats:p>"
10.1007/s41060-024-00615-9,"Ai4tech: X-AI enabling X-Tech with human-like, generative, decentralized, humanoid and metaverse AI","<jats:title>Abstract</jats:title><jats:p>The X-AI age is characterized by the synergy of diverse forms of intelligence-human, natural, social, and artificial (AI). It represents an ecosystem of comprehensive AI paradigms and the flourishing development of deep learning, large language models, and generative AI applications. X-AI reignites the debate on fundamental AI questions: What is AI? What constitutes machine intelligence? What are the implications when AI integrates with business and technology in the new era of artificial general intelligence and human-like AI? This article aims to inspire critical thinking, debate, and discussion on this new era of AI meeting technology, shaping AI4Tech through X-AI enabling X-Tech. We explore the ecosystems of X-AI and X-Tech and the general and domain-specific AI4Tech areas. X-AI enabling X-Tech nurtures a new age of smart businesses and intelligent technologies. The synergy between conventional, human-like, generative, decentralized, humanoid, and metaverse AI unlocks the potential to overcome previous limitations, impossibilities, unknowns, and dreams of AI and technology.</jats:p>"
10.1108/etpc-08-2023-0103,Digital writing with AI platforms: the role of fun with/in generative AI,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>With the rapid advancement of generative artificial intelligence (AI), it is important to consider how young people are making sense of these tools in their everyday lives. Drawing on critical postdigital approaches to learning and literacy, this study aims to center the experiences and perspectives of young people who encounter and experiment with generative AI in their daily writing practices.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>This critical case study of one digital platform – Character.ai – brings together an adolescent and adult authorship team to inquire about the intertwining of young people’s playful and critical perspectives when writing on/with digital platforms. Drawing on critical walkthrough methodology (Light <jats:italic>et al.</jats:italic>, 2018), the authors engage digital methods to study how the creative and “fun” uses of AI in youths’ writing lives are situated in broader platform ecologies.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>The findings suggest experimentation and pleasure are key aspects of young people’s engagement with generative AI. The authors demonstrate how one platform works to capitalize on these dimensions, even as youth users engage critically and artfully with the platform and develop their digital writing practices.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Practical implications</jats:title>
<jats:p>This study highlights how playful experimentation with generative AI can engage young people both in pleasurable digital writing and in exploration and contemplation of platforms dynamics and structures that shape their and others’ literate activities. Educators can consider young people’s creative uses of these evolving technologies as potential opportunities to develop a critical awareness of how commercial platforms seek to benefit from their users.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>This study contributes to the development of a critical and humanist research agenda around generative AI by centering the experiences, perspectives and practices of young people who are underrepresented in the burgeoning research devoted to AI and literacies.</jats:p>
</jats:sec>"
10.3390/su16177829,Leveraging Generative AI for Sustainable Academic Advising: Enhancing Educational Practices through AI-Driven Recommendations,"<jats:p>This study explores the integration of ChatGPT, a generative AI tool, into academic advising systems, aiming to assess its efficacy compared to traditional human-generated advisories. Conducted within the INVEST European University, which emphasizes sustainable and innovative educational practices, this research leverages AI to demonstrate its potential in enhancing sustainability within the context of academic advising. By providing ChatGPT with scenarios from academic advising, we evaluated the AI-generated recommendations against traditional advisories across multiple dimensions, including acceptance, clarity, practicality, impact, and relevance, in real academic settings. Five academic advisors reviewed recommendations across diverse advising scenarios such as pursuing certifications, selecting bachelor dissertation topics, enrolling in micro-credential programs, and securing internships. AI-generated recommendations provided unique insights and were considered highly relevant and understandable, although they received moderate scores in acceptance and practicality. This study demonstrates that while AI does not replace human judgment, it can reduce administrative burdens, significantly enhance the decision-making process in academic advising, and provide a foundation for a new framework that improves the efficacy and sustainability of academic advising practices.</jats:p>"
10.1007/978-3-031-09846-8_14,AI Ethics and Policies: Why European Journalism Needs More of Both,N/A
10.1007/s10551-004-7307-3,Utilising Appreciative Inquiry (AI) in Creating a Shared Meaning of Ethics in Organisations,N/A
10.37736/kjlr.2023.08.14.4.01,Reimagining writing education as a liberal art in the age of generative AI and literacy,"<jats:p>This paper discusses the transformative potential of writing education and literacy within the framework of liberal arts, particularly in the context of the ongoing discourse regarding the capabilities of generative AI, such as ChatGPT, to replace human-generated writing.
Generative AI is characterized by its ability to easily generate various texts based on learning from large volumes of data. In light of actively integrating generative AI into writing education, measures to efficiently utilize it and address its challenges (loss of human subjectivity, reliability of data, etc.) are proposed. However, the intrinsic value of literacy cultivated through human writing experiences is often overlooked in such writing education. Therefore, this paper discusses the need for conceiving writing education as a facet of liberal arts, with a focus on the “writing experience” rather than the mechanical “production of texts.” When the objective of writing education revolves around nurturing literacy, such proficiency is mostly manifested in the writing process itself, rather than the production of texts, due to the meta-linguistic and cognitive nature of writing. Based on the above discussion, this paper advocates for utilizing generative AI as a tool for enhancing reading experiences rather than writing endeavors, thus enabling learning literacy based on learners’ writing process experiences.</jats:p>"
10.2139/ssrn.4857373,Don't Expect Juniors to Teach Senior Professionals to Use Generative AI: Emerging Technology Risks and Novice AI Risk Mitigation Tactics,N/A
10.1515/9783111323749-014,14 The smart and secured AI-powered strategies for optimizing processes in multi-vendor business applications,N/A
10.4018/979-8-3693-2440-0.ch016,"Role of AI in Skilling, Upskilling, and Reskilling the Workforce","<jats:p>This chapter investigates the most recent advancements in artificial intelligence (AI) study and application of AI to change professional skills. The integration of AI into many sectors of organizations holds promise for automating jobs now handled by humans or decreasing cognitive burden. According to this study, incorporating AI into a company requires putting multiple organisational techniques into practice at once. The first step in closing the present skills gap in the workplace is to map the transversal competencies that employees need. Companies may also assist employees in learning the competencies needed to implement AI, as well as in enhancing and gaining new capabilities. The findings also show that businesses need to implement policies that support employees via offering opportunities for ad hoc training and development in order to make certain that workers' attitudes and mental models regarding AI are open and prepared for the changing labour market and its related issues.</jats:p>"
10.1145/3375627.3375874,Ethics of Food Recommender Applications,N/A
10.1007/s10676-023-09702-0,Anything new under the sun? Insights from a history of institutionalized AI ethics,"<jats:title>Abstract</jats:title><jats:p>Scholars, policymakers and organizations in the EU, especially at the level of the European Commission, have turned their attention to the ethics of (trustworthy and human-centric) Artificial Intelligence (AI). However, there has been little reflexivity on (1) the history of the ethics of AI as an institutionalized phenomenon and (2) the comparison to similar episodes of “ethification” in other fields, to highlight common (unresolved) challenges.</jats:p><jats:p>Contrary to some mainstream narratives, which stress how the increasing attention to ethical aspects of AI is due to the fast pace and increasing risks of technological developments, Science and Technology Studies(STS)-informed perspectives highlight that the rise of institutionalized assessment methods indicates a need for governments to gain more control of scientific research and to bring EU institutions closer to the public on controversies related to emerging technologies.</jats:p><jats:p>This article analyzes how different approaches of the recent past (i.e. bioethics, technology assessment (TA) and ethical, legal and social (ELS) research, Responsible Research and Innovation (RRI)) followed one another, often “in the name of ethics”, to address previous criticisms and/or to legitimate certain scientific and technological research programs. The focus is on how a brief history of the institutionalization of these approaches can provide insights into present challenges to the ethics of AI related to methodological issues, mobilization of expertise and public participation.</jats:p>"
10.4018/979-8-3693-0831-8.ch011,Charting the Ethical Course,"<jats:p>Generative AI tools have emerged as transformative resources in the realm of communication education. These tools harness the power of artificial intelligence to assist students and educators in various aspects of communication studies, including writing, public speaking, and media production. At their core, generative AI tools are designed to generate human-like text, speech, or visuals autonomously, replicating and augmenting the capabilities of human communication. They encompass a range of applications, from chatbots that facilitate communication skill development to language models that assist in content creation and editing. These tools are becoming increasingly prevalent in communication classrooms, providing unique opportunities for personalized learning, skill enhancement, and creativity.</jats:p>"
10.16914/ar.2024.140.118,The Present and Future of Advertising and PR Changed by Generative AI Technology: Implications from Expert Interviews,N/A
10.61969/jai.1400867,"Generative AI in Academic Research: A Descriptive Study on Awareness, Gender Usage, and Views among Pre-Service Teachers","<jats:p xml:lang=""en"">This study investigated the engagement of Pre-Service Teachers (PSTs) with Generative AI (GAI) tools in their research projects, focusing on their awareness, source of awareness, usage pattern based on gender, and views of GAI tools in academic research. We adopted a descriptive survey method to collect data from one hundred and four PSTs across five institutions in Ghana using a five-point Likert-type survey instrument, which included an open-ended question. The quantitative data were analyzed using means, frequencies, percentages, standard deviations, and an independent samples t-test. The findings revealed that PSTs are familiar with GAI tools, especially ChatGPT and Google Bard. They learned about these tools through personal searches, recommendations from friends, and social media platforms. The PSTs used these tools in writing all chapters of their research projects, with the Introduction Chapter being the most common area of application, followed by the Discussion and Findings Chapter, the Literature Review Chapter, Methodology, and Summary and Conclusion. We also identified a significant gender disparity in the use of GAI tools, with male PSTs exhibiting a higher frequency of use compared to their female counterparts. Nonetheless, both genders expressed a positive attitude towards GAI tools in academic research, noting among other benefits that these tools provided them with confidence and independence in their research writing. However, they also recognized inaccuracies in the information provided by GAI tools, which led to skepticism about relying solely on these tools for their research projects. Consequently, they expressed a preference for support from their research supervisors, highlighting the importance of a balanced approach that combines the use of GAI tools with human supervision in academic research. While we recommend the integrating of GAI tools in teacher education programs, we strongly suggest that such integration should be complemented with comprehensive guidance on how these tools can be effectively used by PSTs to conduct original and advanced research.</jats:p>"
10.3389/frobt.2019.00115,HVGH: Unsupervised Segmentation for High-Dimensional Time Series Using Deep Neural Compression and Statistical Generative Model,N/A
10.1145/3514094.3539531,AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents,N/A
10.1109/iedm45741.2023.10413675,Generative AI on a Budget: Processing Transformer- based Neural Networks at the Edge,N/A
10.1109/icalt61570.2024.00062,Enhancing Academic Performance with Generative AI-Based Quiz Platform,N/A
10.55277/researchhub.2903rcf0,"Annotated Bibliography - Generative AI and the future of education: Ragnarök or reformation? A paradoxical perspective from management educators. (Lim et al, 2023)",N/A
10.5204/mcj.3004,ChatGPT Isn't Magic,"<jats:p>Introduction
Author Arthur C. Clarke famously argued that in science fiction literature “any sufficiently advanced technology is indistinguishable from magic” (Clarke). On 30 November 2022, technology company OpenAI publicly released their Large Language Model (LLM)-based chatbot ChatGPT (Chat Generative Pre-Trained Transformer), and instantly it was hailed as world-changing. Initial media stories about ChatGPT highlighted the speed with which it generated new material as evidence that this tool might be both genuinely creative and actually intelligent, in both exciting and disturbing ways. Indeed, ChatGPT is part of a larger pool of Generative Artificial Intelligence (AI) tools that can very quickly generate seemingly novel outputs in a variety of media formats based on text prompts written by users. Yet, claims that AI has become sentient, or has even reached a recognisable level of general intelligence, remain in the realm of science fiction, for now at least (Leaver). That has not stopped technology companies, scientists, and others from suggesting that super-smart AI is just around the corner. Exemplifying this, the same people creating generative AI are also vocal signatories of public letters that ostensibly call for a temporary halt in AI development, but these letters are simultaneously feeding the myth that these tools are so powerful that they are the early form of imminent super-intelligent machines.
For many people, the combination of AI technologies and media hype means generative AIs are basically magical insomuch as their workings seem impenetrable, and their existence could ostensibly change the world. This article explores how the hype around ChatGPT and generative AI was deployed across the first six months of 2023, and how these technologies were positioned as either utopian or dystopian, always seemingly magical, but never banal. We look at some initial responses to generative AI, ranging from schools in Australia to picket lines in Hollywood. We offer a critique of the utopian/dystopian binary positioning of generative AI, aligning with critics who rightly argue that focussing on these extremes displaces the more grounded and immediate challenges generative AI bring that need urgent answers. Finally, we loop back to the role of schools and educators in repositioning generative AI as something to be tested, examined, scrutinised, and played with both to ground understandings of generative AI, while also preparing today’s students for a future where these tools will be part of their work and cultural landscapes.
Hype, Schools, and Hollywood
In December 2022, one month after OpenAI launched ChatGPT, Elon Musk tweeted: “ChatGPT is scary good. We are not far from dangerously strong AI”. Musk’s post was retweeted 9400 times, liked 73 thousand times, and presumably seen by most of his 150 million Twitter followers. This type of engagement typified the early hype and language that surrounded the launch of ChatGPT, with reports that “crypto” had been replaced by generative AI as the “hot tech topic” and hopes that it would be “‘transformative’ for business” (Browne). By March 2023, global economic analysts at Goldman Sachs had released a report on the potentially transformative effects of generative AI, saying that it marked the “brink of a rapid acceleration in task automation that will drive labor cost savings and raise productivity” (Hatzius et al.). Further, they concluded that “its ability to generate content that is indistinguishable from human-created output and to break down communication barriers between humans and machines reflects a major advancement with potentially large macroeconomic effects” (Hatzius et al.). Speculation about the potentially transformative power and reach of generative AI technology was reinforced by warnings that it could also lead to “significant disruption” of the labour market, and the potential automation of up to 300 million jobs, with associated job losses for humans (Hatzius et al.). In addition, there was widespread buzz that ChatGPT’s “rationalization process may evidence human-like cognition” (Browne), claims that were supported by the emergent language of ChatGPT. The technology was explained as being “trained” on a “corpus” of datasets, using a “neural network” capable of producing “natural language“” (Dsouza), positioning the technology as human-like, and more than ‘artificial’ intelligence. Incorrect responses or errors produced by the tech were termed “hallucinations”, akin to magical thinking, which OpenAI founder Sam Altman insisted wasn’t a word that he associated with sentience (Intelligencer staff). Indeed, Altman asserts that he rejects moves to “anthropomorphize” (Intelligencer staff) the technology; however, arguably the language, hype, and Altman’s well-publicised misgivings about ChatGPT have had the combined effect of shaping our understanding of this generative AI as alive, vast, fast-moving, and potentially lethal to humanity.
Unsurprisingly, the hype around the transformative effects of ChatGPT and its ability to generate ‘human-like’ answers and sophisticated essay-style responses was matched by a concomitant panic throughout educational institutions. The beginning of the 2023 Australian school year was marked by schools and state education ministers meeting to discuss the emerging problem of ChatGPT in the education system (Hiatt). Every state in Australia, bar South Australia, banned the use of the technology in public schools, with a “national expert task force” formed to “guide” schools on how to navigate ChatGPT in the classroom (Hiatt). Globally, schools banned the technology amid fears that students could use it to generate convincing essay responses whose plagiarism would be undetectable with current software (Clarence-Smith). Some schools banned the technology citing concerns that it would have a “negative impact on student learning”, while others cited its “lack of reliable safeguards preventing these tools exposing students to potentially explicit and harmful content” (Cassidy). ChatGPT investor Musk famously tweeted, “It’s a new world. Goodbye homework!”, further fuelling the growing alarm about the freely available technology that could “churn out convincing essays which can't be detected by their existing anti-plagiarism software” (Clarence-Smith). Universities were reported to be moving towards more “in-person supervision and increased paper assessments” (SBS), rather than essay-style assessments, in a bid to out-manoeuvre ChatGPT’s plagiarism potential. Seven months on, concerns about the technology seem to have been dialled back, with educators more curious about the ways the technology can be integrated into the classroom to good effect (Liu et al.); however, the full implications and impacts of the generative AI are still emerging.
In May 2023, the Writer’s Guild of America (WGA), the union representing screenwriters across the US creative industries, went on strike, and one of their core issues were “regulations on the use of artificial intelligence in writing” (Porter). Early in the negotiations, Chris Keyser, co-chair of the WGA’s negotiating committee, lamented that “no one knows exactly what AI’s going to be, but the fact that the companies won’t talk about it is the best indication we’ve had that we have a reason to fear it” (Grobar). At the same time, the Screen Actors’ Guild (SAG) warned that members were being asked to agree to contracts that stipulated that an actor’s voice could be re-used in future scenarios without that actor’s additional consent, potentially reducing actors to a dataset to be animated by generative AI technologies (Scheiber and Koblin). In a statement issued by SAG, they made their position clear that the creation or (re)animation of any digital likeness of any part of an actor must be recognised as labour and properly paid, also warning that any attempt to legislate around these rights should be strongly resisted (Screen Actors Guild). Unlike the more sensationalised hype, the WGA and SAG responses to generative AI are grounded in labour relations. These unions quite rightly fear the immediate future where human labour could be augmented, reclassified, and exploited by, and in the name of, algorithmic systems. Screenwriters, for example, might be hired at much lower pay rates to edit scripts first generated by ChatGPT, even if those editors would really be doing most of the creative work to turn something clichéd and predictable into something more appealing. Rather than a dystopian world where machines do all the work, the WGA and SAG protests railed against a world where workers would be paid less because executives could pretend generative AI was doing most of the work (Bender).
The Open Letter and Promotion of AI Panic
In an open letter that received enormous press and media uptake, many of the leading figures in AI called for a pause in AI development since “advanced AI could represent a profound change in the history of life on Earth”; they warned early 2023 had already seen “an out-of-control race to develop and deploy ever more powerful digital minds that no one – not even their creators – can understand, predict, or reliably control” (Future of Life Institute). Further, the open letter signatories called on “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4”, arguing that “labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts” (Future of Life Institute). Notably, many of the signatories work for the very companies involved in the “out-of-control race”. Indeed, while this letter could be read as a moment of ethical clarity for the AI industry, a more cynical reading might just be that in warning that their AIs could effectively destroy the world, these companies were positioning their products as seemingly magical—“digital minds that no one – not even their creators – can understand”—making them even more appealing to potential customers and investors. Far from pausing AI development, the open letter actually operates as a neon sign touting the amazing capacities and future brilliance of generative AI systems.
Nirit Weiss-Blatt argues that general reporting on technology industries up to 2017 largely concurred with the public relations stance of those companies, positioning them as saviours and amplifiers of human connection, creativity, and participation. After 2017, though, media reporting completely shifted, focussing on the problems, risks, and worst elements of these corporate platforms. In the wake of the open letter, Weiss-Blatt extended her point on Twitter, arguing that media and messaging surrounding generative AI can be broken down into those who are profiting and fuelling the panic at one end of the spectrum, and those who think the form of the panic (which positions AI as dangerously intelligent) is deflecting from the immediate real issues caused by generative AI at the other. Weiss-Blatt characterises the Panic-as-a-Business proponents as arguing “we're telling you will all die from a Godlike AI… so you must listen to us”, which coheres with the broader positioning narrative of generative AI’s seemingly magical (and thus potentially destructive) capabilities. Yet this rhetoric also positions the companies creating generative AI as the ones who should be making the rules to control it, an argument so effective that in July 2023 the Biden Administration in the US endorsed the biggest AI companies—Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI—framing future AI development with voluntary safeguards rather than externally imposed policies (Shear, Kang, and Sanger).

Fig. 1: Promotors of AI Panic, extrapolating from Nirit Weiss-Blatt. (Algorithm Watch)
Stochastic Parrots and Deceitful Media
Artificial Intelligences have inhabited popular imaginaries via novels, television, and films far longer than they have been considered even potentially viable technologies, so it is not surprising that popular culture has often framed the way AI is understood (Leaver). Yet as Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell argue, Large Language Models and generative AI are most productively understood as “a stochastic parrot” insomuch as each is a “system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning” (Bender et al. 617). Generative AI, then, is not creating something genuinely new, but rather remixing existing data in novel ways that the systems themselves do not in any meaningful sense understand. Going further, Simone Natale characterises current AI tools as “deceitful media” insomuch as they are designed to deliberately appear generally intelligent, but this is always a deception. The deception makes these tools more engaging for humans to use but is also fundamental in selling and profiting from the use of AI tools. Rather than accepting claims made by the companies financing and creating contemporary AI, Natale argues for a more pedagogically productive path:

we must resist the normalization of the deceptive mechanisms embedded in contemporary AI and the silent power that digital media companies exercise over us. We should never cease to interrogate how the technology works, even while we are trying to accommodate it in the fabric of everyday life. (Natale 132)

Real Issues
Although even a comprehensive list is beyond the scope of this short article, is it nevertheless vital to note that in looking beyond the promotion of AI Panic and deceptive media, ChatGPT and other generative AI tools create or exacerbate a range of very real and significant ethical problems. The most obvious problem is the lack of transparency in terms of what data different generative AI tools were trained on. Generally, these tools are thought to get better by absorbing ever greater amounts of data, with most AI companies acknowledging that scraping the Web in some form has been part of the training data harvesting for their AI tools. Not knowing what data have been used makes it almost impossible to know which perspectives, presumptions, and biases are baked into these tools. While many forms of bias have plagued technology companies for many years (Noble), for generative AI tools, in “accepting large amounts of web text as ‘representative’ of ‘all’ of humanity we risk perpetuating dominant viewpoints, increasing power imbalances, and further reifying inequality” (Bender et al. 614). Even mitigating and working to correct biases in generative AI tools will be a huge challenge if these companies never share what was in their training data.
As the WGA and SAG strike discussed above emphasises, the question of human labour is a central challenge for generative AI. Beyond Hollywood, more entrenched forms of labour exploitation haunt generative AI. Very low-paid workers have done much of the labour in classifying different forms of data in order to train AI systems; data workers are routinely not acknowledged at all, even sometimes directly performing the tasks that are ascribed to AI, to the extent that “distracted by the specter of nonexistent sentient machines, an army of precarized workers stands behind the supposed accomplishments of artificial intelligence systems today” (Williams, Miceli, and Gebru). It turns out that people are still doing the work so that companies can pretend the machines can think.
In one final but very important example, there is a very direct ecological cost to training, maintaining, and running generative AI tools. In the context of global warming, concerns already existed about the enormous data centres at the heart of the big technology platforms prior to ChatGPT’s release. However, the data and processing power needed to run generative AI tools are even larger, leading to very real questions about how much electricity and water (for cooling) are used by even the most rudimentary ChatGPT queries (Lizarraga and Solon). While not just an AI question, balancing the environmental costs of data centres with the actual utility of AI tools is not one that is routinely asked, or answered, in the hype around generative AI.
Messing Around and Geeking Out
Escaping the hype and hypocrisy deployed by AI companies is vital for repositioning generative AI not as magical, not as a saviour, and not as a destroyer, but rather as a new technology that needs to be critically and ethically understood. In seminal work exploring how young people engage with digital tools and technologies, Mimi Ito and colleagues developed three genres of technology participation: hanging out, where engagement with any technologies is largely driven by friendships and social engagement; messing around, which includes a great deal of experimentation and play with technological tools; and geeking out, where some young people will find a particular focus on one platform, tool or technology that inspires them to focus enough to develop expertise in using and understanding that tool (Ito et al.). If young people, in particular, are going to be living in a world where generative AI tools are part of their social worlds and workplaces, then messing around with ChatGPT is, indeed, going to be important in testing out how these tools answer questions and synthesise information, what biases are evident in responses, and at what points answers are incorrect. For some young people, they may well move from messing around to completely geeking out with generative AI, a process that will be even more fruitful if these tools are not seen as impenetrable magic, but rather as commercial tools built by for-profit companies. While the idea of digital natives is an unhelpful myth (Bennett, Maton, and Kervin), if young people are going to be the first generation to have generative AI as part of their information, creative, and search landscapes, then safely messing around and geeking out with these tools will be more vital than ever.
We mentioned above that most Australian state education departments initially banned ChatGPT, but a more optimistic sign arrived as we were finishing this article insomuch as the different Australian states agreed in mid-2023 to work together to create “a framework to guide the safe and effective use of artificial intelligence in the nation’s schools” (Clare). Although there is work to be done, moving away from a ban to a setting that should allow students to be part of testing, framing, and critiquing ChatGPT and generative AI is a clear step in repositioning these technologies as tools, not magical systems that could never be understood.
Conclusion
Generative AI is not magic; it is not a saviour or destroyer; it is neither utopian nor dystopian; nor, unless we radically narrow the definition, is it intelligent. The companies and corporations driving AI development have a vested interest in promoting fantastical ideas about generative AI, as it drives their customers, investment, and future viability. When the hype is dominant, responses can be overdetermined, such as banning generative AI in schools. But in taking a less magical and more material approach to ChatGPT and generative AI, we can try and ensure pedagogical opportunities for today’s young people to test out, scrutinise, and critically understand the AI tools they are most likely going to be asked to use today and in the future. The first wave of generative AI hype following the public release of ChatGPT offers an opportunity to reflect on exactly what the best uses of these technologies are, what ethics should drive those uses, and how transparent the workings of generative AI should be before their presence in the digital landscape is so entrenched and mundane that it becomes difficult to see at all.
Acknowledgment
This research was supported by the Australian Research Council Centre of Excellence for the Digital Child through project number CE200100022.
References
Algorithm Watch [@AlgorithmWatch]. “Mirror, Mirror on the Wall, Who Is the Biggest Panic-Creator of Them All? Inspired by a Tweet from Nirit Weiss-Blatt, Check out Our Taxonomy of #AI Panic Facilitators and Those Fighting against the Fearmongering. Who Have We Forgotten to Add? Let Us Know! ⬇️” Instagram, 12 July 2023 &lt;https://Instagram.com/p/Cump3losObg/&gt;. 
Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. Virtual Event. Canada: ACM, 2021. 610–623. &lt;https://dl.acm.org/doi/10.1145/3442188.3445922&gt;.
Bender, Stuart Marshall. “Coexistence and Creativity: Screen Media Education in the Age of Artificial Intelligence Content Generators.” Media Practice and Education (2023): 1–16.
Bennett, Sue, Karl Maton, and Lisa Kervin. “The ‘Digital Natives’ Debate: A Critical Review of the Evidence.” British Journal of Educational Technology 39.5 (2008): 775–786.
Browne, Ryan. “Buzzy A.I. Tools like Microsoft-Backed ChatGPT Replaced Crypto as the Hot Tech Topic of Davos.” CNBC, 20 Jan. 2023. &lt;https://cnbc.com/2023/01/20/chatgpt-microsoft-backed-ai-tool-replaces-crypto-as-hot-davos-tech-topic.html&gt;.
Cassidy, Caitlin. “Queensland Public Schools to Join NSW in Banning Students from ChatGPT.” The Guardian, 23 Jan. 2023. &lt;https://theguardian.com/australia-news/2023/jan/23/queensland-public-schools-to-join-nsw-in-banning-students-from-chatgpt&gt;.
“Cheating with ChatGPT? Controversial AI Tool Banned in These Schools in Australian First.” SBS News, 22 Jan. 2023. &lt;https://sbs.com.au/news/article/cheating-with-chatgpt-controversial-ai-tool-banned-in-these-schools-in-australian-first/817odtv6e&gt;.
Clare, Jason. “Draft Schools AI Framework Open for Consultation.” Ministers’ Media Centre, 28 July 2023. &lt;https://ministers.education.gov.au/clare/draft-schools-ai-framework-open-consultation&gt;.
Clarence-Smith, Louisa. “‘Goodbye Homework!’ Elon Musk Praises AI Chatbot That Writes Student Essays.” The Telegraph, 5 Jan. 2023. &lt;https://telegraph.co.uk/news/2023/01/05/homework-elon-musk-chatgpt-praises-ai-chatbot-writes-students/&gt;.
Clarke, Arthur C. “Hazards of Prophecy: The Failure of Imagination.” Profiles of the Future: An Inquiry into the Limits of the Possible. New York: Harper and Row, 1973.
Dsouza, Elton Grivith. “How ChatGPT Works: Training Model of ChatGPT.” Edureka! 11 May 2023. &lt;https://edureka.co/blog/how-chatgpt-works-training-model-of-chatgpt/&gt;.
Future of Life Institute. “Pause Giant AI Experiments: An Open Letter.” Future of Life Institute, 22 Mar. 2023. &lt;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&gt;.
Grobar, Matt. “WGA Negotiating Committee Co-Chair Chris Keyser on the Breakdown of Negotiations with ‘Divided’ AMPTP.” Deadline, 2 May 2023. &lt;https://deadline.com/2023/05/wga-strike-chris-keyser-interview-failed-negotiations-amptp-ai-1235354566/&gt;.
Hatzius, Jan, Joseph Briggs, Devesh Kodnani, and Giovanni Pierdomenico. “The Potentially Large Effects of Artificial Intelligence on Economic Growth.” Goldman Sachs: Global Economics Analyst, 26 Mar. 2023. &lt;https://gspublishing.com/content/research/en/reports/2023/03/27/d64e052b-0f6e-45d7-967b-d7be35fabd16.html&gt;.
Hiatt, Bethany. “National Expert Task Force to Be Set Up in Bid to Help Australian Schools Harness Tools Such as ChatGPT.” The West Australian, 1 Mar. 2023. &lt;https://thewest.com.au/news/education/national-expert-task-force-to-be-set-up-in-bid-to-help-australian-schools-harness-tools-such-as-chatgpt-c-9895269&gt;.
Intelligencer staff. “Sam Altman on What Makes Him ‘Super Nervous’ about AI: The OpenAI Co-Founder Thinks Tools like GPT-4 Will Be Revolutionary. But He’s Wary of Downsides.” On with Kara Swisher: Intelligencer. 23 Mar. 2023. &lt;https://nymag.com/intelligencer/2023/03/on-with-kara-swisher-sam-altman-on-the-ai-revolution.html&gt;.
Ito, Mizuko. Hanging Out, Messing Around, and Geeking Out: Kids Living and Learning with New Media. Cambridge, Mass.: MIT P, 2012.
Leaver, Tama. Artificial Culture: Identity, Technology, and Bodies. New York: Routledge, 2012.
Liu, Danny, Adam Bridgeman, and Benjamin Miller. “As Uni Goes Back, Here’s How Teachers and Students Can Use ChatGPT to Save Time and Improve Learning.” The Conversation, 28 Feb. 2023. &lt;https://theconversation.com/as-uni-goes-back-heres-how-teachers-and-students-can-use-chatgpt-to-save-time-and-improve-learning-199884&gt;.
Lizarraga, Clara Hernanz, and Olivia Solon. “Thirsty Data Centers Are Making Hot Summers Even Scarier.” Bloomberg, 26 July 2023. &lt;https.//bloomberg.com/news/articles/2023-07-26/extreme-heat-drought-drive-opposition-to-ai-data-centers&gt;.
Musk, Elon [@elonmusk]. “@sama. ChatGPT is scary good. We are not far from dangerously strong AI.” Twitter, 4 Dec. 2022. &lt;https://twitter.com/elonmusk/status/1599128577068650498?lang=en&gt;.
———. “@pmarca. It’s a new world. Goodbye homework!” Twitter, 5 Jan. 2023. &lt;https://twitter.com/elonmusk/status/1610849544945950722?lang=en&gt;.
Natale, Simone. Deceitful Media Artificial Intelligence and Social Life after the Turing Test. New York: Oxford UP, 2021.
Noble, Safiya Umoja. Algorithms of Oppression: How Search Engines Reinforce Racism. New York: NYU P, 2018.
Porter, Rick. “Late Night Shows Shut Down with WGA Strike.” The Hollywood Reporter, 2 May 2023. &lt;https://hollywoodreporter.com/tv/tv-news/wga-strike-late-night-shows-shut-down-1235477882/&gt;.
Scheiber, Noam, and John Koblin. “Will a Chatbot Write the Next ‘Succession’?” The New York Times 29 Apr. 2023. &lt;https://nytimes.com/2023/04/29/business/media/writers-guild-hollywood-ai-chatgpt.html&gt;.
Screen Actors Guild – American Federation of Television and Radio Artists. “SAG-AFTRA Statement on the Use of Artificial Intelligence and Digital Doubles in Media and Entertainment.” 17 Mar. 2023. &lt;https://sagaftra.org/sag-aftra-statement-use-artificial-intelligence-and-digital-doubles-media-and-entertainment&gt;.
Shear, Michael D., Cecilia Kang, and David E. Sanger. “Pressured by Biden, A.I. Companies Agree to Guardrails on New Tools.” The New York Times, 21 July 2023. &lt;https://nytimes.com/2023/07/21/us/politics/ai-regulation-biden.html&gt;.
Weiss-Blatt, Nirit [@DrTechlash]. “A Taxonomy of AI Panic Facilitators.” Twitter, 1 July 2023. &lt;https://twitter.com/DrTechlash/status/1675155157880016898&gt;.
———. The Techlash and Tech Crisis Communication. Bingley: Emerald Publishing, 2021.
Williams, Adrienne, Milagros Miceli, and Timnit Gebru. “The Exploited Labor behind Artificial Intelligence.” Noema, 13 Oct. 2022 &lt;https://noemamag.com/the-exploited-labor-behind-artificial-intelligence/&gt;.</jats:p>"
10.5772/acrt.20240003,Adversarial Variational Autoencoders to Extend and Improve Generative Model,"<jats:p>Generative artificial intelligence (GenAI) has been advancing with many notable achievements like ChatGPT and Bard. The deep generative model (DGM) is a branch of GenAI, which is preeminent in generating raster data such as image and sound due to the strong role of deep neural networks (DNNs) in inference and recognition. The built-in inference mechanism of DNN, which simulates and aims at synaptic plasticity of the human neuron network, fosters the generation ability of DGM, which produces surprising results with the support of statistical flexibility. Two popular approaches in DGM are the variational autoencoder (VAE) and generative adversarial network (GAN). Both VAE and GAN have their own strong points although they share and imply the underlying theory of statistics as well as significant complex via hidden layers of DNN when DNN becomes effective encoding/decoding functions without concrete specifications. This research unifies VAE and GAN into a consistent and consolidated model called the adversarial variational autoencoder (AVA) in which the VAE and GAN complement each other; for instance, the VAE is a good data generator by encoding data via the excellent ideology of Kullback–Leibler divergence and the GAN is a significantly important method to assess the reliability of data as to whether it is real or fake. In other words, the AVA aims to improve the accuracy of generative models; besides, the AVA extends the function of simple generative models. In methodology, this research focuses on the combination of applied mathematical concepts and skillful techniques of computer programming in order to implement and solve complicated problems as simply as possible.</jats:p>"
10.1109/compsac61105.2024.00259,Enhancing Team Diversity with Generative AI: A Novel Project Management Framework,N/A
10.5121/ijci.2023.120604,Potential Impact of Generative Artificial Intelligence(AI) on the Financial Industry,"<jats:p>Presently, generative AI has taken center stage in the news media, educational institutions, and the world at large. Machine learning has been a decades-old phenomenon, with little exposure to the average person until very recently. In the natural world, the oldest and best example of a “generative” model is the human being - one can close one’s eyes and imagine several plausible different endings to one’s favorite TV show. This paper focuses on the impact of generative and machine learning AI on the financial industry. Although generative AI is an amazing tool for a discriminant user, it also challenges us to think critically about the ethical implications and societal impact of these powerful technologies on the financial industry. It requires ethical considerations to guide decision-making, mitigate risks, and ensure that generative AI is developed and used to align with ethical principles, social values, and in the best interests of communities.</jats:p>"
10.1007/s40319-023-01399-4,Generative AI and Author Remuneration,"<jats:title>Abstract</jats:title><jats:p>With the evolution of generative AI systems, machine-made productions in the literary and artistic field have reached a level of refinement that allows them to replace human creations. The increasing sophistication of AI systems will inevitably disrupt the market for human literary and artistic works. Generative AI systems provide literary and artistic output much faster and cheaper. It is therefore foreseeable that human authors will be exposed to substitution effects. They may lose income as they are replaced by machines in sectors ranging from journalism and writing to music and visual arts. Considering this trend, the question arises whether it is advisable to take measures to compensate human authors for the reduction in their market share and income. Copyright law could serve as a tool to introduce an AI levy system and ensure the payment of equitable remuneration. In combination with mandatory collective rights management, the new revenue stream could be used to finance social and cultural funds that improve the working and living conditions of flesh-and-blood authors.</jats:p>"
10.69554/vypb2186,Improving voice of the customer analysis with generative AI,"<jats:p xml:lang=""en"">This paper explores the integration of generative artificial intelligence (GenAI) in voice of the customer (VoC) analysis to provide deeper understanding of prospects and customers. GenAI has enormous potential to enhance customer satisfaction, refine products and services and improve the customer experience. This speculative paper illustrates how GenAI can keep pace with increasing customer expectations and the volume of feedback by uncovering nuanced sentiments, trends and customer needs through context comprehension and its conversational query capabilities. The paper explores the power of GenAI in VoC analysis for improving customer satisfaction, accelerating troubleshooting and resolution and upgrading products and services. Additionally, this paper addresses the role of GenAI in advanced communication routing, agent support, multilingual support and sentiment analysis, showcasing its ability to provide comprehensive and context-aware insights.</jats:p>"
10.2139/ssrn.4541396,Can Generative-AI (ChatGPT and Bard) Be Used as Red Team Avatars in Developing Foresight Scenarios?,N/A
10.2139/ssrn.4637265,Co-Creating the Future with Generative Ai: Exploring Ai's Capacities and Potential Incorporation in Statistical Education,N/A
10.15637/jlecon.2294,"Beyond interaction: Generative AI in conversational marketing - foundations, developments, and future directions","<jats:p>This paper explores the integration of Generative Artificial Intelligence (AI) in conversational marketing, transitioning from traditional marketing to interactive, customer-centric strategies. It examines the shift from one-way communication to dynamic, AI-driven interactions that personalize customer experiences. Central to this study is how Generative AI facilitates real-time, tailored dialogues between brands and customers, enhancing customer engagement and satisfaction. The paper also addresses the challenges and ethical considerations of using anthropomorphic AI in marketing, balancing human-like AI traits with user expectations. Additionally, it presents a novel framework that conceptualizes the combination of Generative AI and anthropomorphism in conversational marketing into four distinct quadrants, providing a comprehensive analysis of their potential interplay. Conclusively, it offers strategic insights for leveraging AI in marketing while adhering to ethical practices, highlighting the potential of Generative AI to transform customer engagement in the digital age. This research has two important consequences. Practically, it offers valuable insights and strategic recommendations for businesses aiming to integrate Generative AI into their conversational marketing practices effectively. Theoretically, it contributes to the academic discourse by highlighting the transformative role of Generative AI in marketing, suggesting avenues for future research in this rapidly evolving field. This study provides a brief overview of the evolving role of AI in modern marketing strategies, emphasizing the future potential and implications of AI-driven conversational marketing.</jats:p>"
10.22214/ijraset.2023.54515,Impact of Generative AI on IT Professionals,"<jats:p>Abstract: This research paper aims to explore the impact of generative artificial intelligence (AI) on IT professionals. Generative AI refers to the technology that can autonomously create new content, such as images, text, and even code. With the rapid advancements in AI, the role of IT professionals is evolving, and it is crucial to understand the implications of generative AI on their work. This paper examines the potential benefits, challenges, and ethical considerations associated with the integration of generative AI in the IT industry. Through a comprehensive analysis of existing literature, this paper sheds light on the transformation of IT professionals' responsibilities, skills, and career prospects in the era of generative AI.</jats:p>"
10.1109/icc51166.2024.10622271,Generative AI for Evidence-Based Medicine: A PICO GenAI for Synthesizing Clinical Case Reports,N/A
10.36227/techrxiv.171502818.84241801/v1,Crafting Tomorrow's Evaluations: Assessment Design Strategies in the Era of Generative AI,N/A
10.14445/22312803/ijctt-v72i4p114,Cash Flow Acceleration with Generative Artificial Intelligence (Gen AI),N/A
10.3390/fi16060188,Studying the Quality of Source Code Generated by Different AI Generative Engines: An Empirical Evaluation,"<jats:p>The advent of Generative Artificial Intelligence is opening essential questions about whether and when AI will replace human abilities in accomplishing everyday tasks. This issue is particularly true in the domain of software development, where generative AI seems to have strong skills in solving coding problems and generating software source code. In this paper, an empirical evaluation of AI-generated source code is performed: three complex coding problems (selected from the exams for the Java Programming course at the University of Insubria) are prompted to three different Large Language Model (LLM) Engines, and the generated code is evaluated in its correctness and quality by means of human-implemented test suites and quality metrics. The experimentation shows that the three evaluated LLM engines are able to solve the three exams but with the constant supervision of software experts in performing these tasks. Currently, LLM engines need human-expert support to produce running code that is of good quality.</jats:p>"
10.5121/hiij.2024.13201,Health Disparities: Differences in Veteran and Non-Veteran Populations using Generative AI,"<jats:p>Introduction: This study investigated self-reported health status, health screenings, vision problems, and vaccination rates among veteran and non-veteran groups to uncover health disparities that are critical for informed health system planning for veteran populations. Methods: Using public-use data from the National Health Interview Survey (2015-2018), this study adopts an ecologic cross-sectional approach to conduct an in-depth analysis and visualization of the data assisted by Generative AI, specifically ChatGPT-4. This integration of advanced AI tools with traditional epidemiological principles enables systematic data management, analysis, and visualization, offering a nuanced understanding of health dynamics across demographic segments and highlighting disparities essential for veteran health system planning. Findings: Disparities in self-reports of health outcomes, health screenings, vision problems, and vaccination rates were identified, emphasizing the need for targeted interventions and policy adjustments. Conclusion: Insights from this study could inform health system planning, using epidemiological data assessment to suggest enhancements for veteran healthcare delivery. These findings highlight the value of integrating Generative AI with epidemiological analysis in shaping public health policy and health planning.</jats:p>"
10.21428/e4baedd9.cb55d9a3,"When Disruptive Innovations drive Educational Transformation: Literacy, Pocket Calculator, Google Translate, ChatGPT",N/A
10.1109/mspec.2024.10551790,Do We Dare Use Generative AI for Mental Health?,N/A
10.37074/jalt.2024.7.2.7,Improving students’ generative AI literacy: A single workshop can improve confidence and understanding,N/A
10.1109/c2i659362.2023.10430931,Understanding User Perception of Biometric Privacy in the Era of Generative AI,N/A
10.1007/979-8-8688-0419-9,Introducing Microsoft Copilot for Managers,N/A
10.1145/3615335.3623035,Evaluating ChatGPT: Generative AI in UX Design and Web Development Pedagogy,N/A
10.1109/cai59869.2024.00029,Adoption of Generative AI in content creation: A case study from the advertising industry,N/A
10.1117/12.3011173,Generative AI agile assistant,N/A
10.52591/lxai2019120828,Interpolation and Prediction of PM2.5 based on Conditional Generative Adversarial Network and a forecasting model,"<jats:p>Currently, air pollution is a severe problem, because pollutants such as Particulate matter of 2.5 micrometers affect human health. Therefore, several works address the prediction of this pollutant, using statistical methods and machine learning. However, these predictions are performed in places of a city, where air quality monitoring stations are available, which is not always possible due to their high implementation and maintenance costs. Thus, in this work, we propose an architecture based on a Conditional Generative Adversary Network to create new synthetic data and interpolate this pollutant in places where monitoring stations are missing.</jats:p>"
10.55668/jae0043,Generative AI in Adventist Education: Opportunities and Ethical Considerations,N/A
10.1016/j.jpedsurg.2023.10.061,Fostering Support for Pediatric Surgery by Generative AI,N/A
10.14744/cpr.2024.56251,The Future Role of Generative Artificial Intelligence (AI) in Medicine,N/A
10.1038/d41586-024-01573-9,Anglo-American bias could make generative AI an invisible intellectual cage,N/A
10.1001/jama.2023.9630,Generative AI in Health Care and Liability Risks for Physicians and Safety Concerns for Patients,"<jats:p>This Viewpoint discusses the potential use of generative artificial intelligence (AI) in medical care and the liability risks for physicians using the technology, as well as offers suggestions for safeguards to protect patients.</jats:p>"
10.2139/ssrn.4528225,From Fiction to Fact: The Growing Role of Generative AI in Business and Finance,N/A
10.2139/ssrn.4907319,Promoting Project Outcomes: A Development Approach to Generative AI and LLM-Based Software Applications' Deployment,N/A
10.54097/xv0rbq11,The Influence of Generative AI Technologies On Academic Writing in EFL Education,"<jats:p>Generative Artificial Intelligence (GAI) has emerged as a power to transform academic writing courses, especially in the English-as-a-foreign-language (EFL) field, offering innovative tools and methodologies that can reshape pedagogical practices. Through a comprehensive exploration, this paper discusses the development of GAI technologies, their applications in natural language processing, and their potential to revolutionize academic writing support. The paper then examines the integration of GAI tools into EFL academic writing pedagogy, showing its benefits for both students and educators. However, the use of GAI is not without challenges. Concerns surrounding academic integrity, potential for plagiarism, data privacy, and the ethical implications of GAI tools underscore the complexities of this integration. Drawing from various studies, this paper argues that responsible advancement of GAI requires a balance between innovation and ethics. Stakeholders must consider potential implications. A measured approach is advocated for future work in related fields.</jats:p>"
10.1080/2331186x.2024.2357898,Guiding principles of generative AI for employability and learning in UK universities,N/A
10.31219/osf.io/9r5ac,Vietnamese EFL Teachers’ Views and Responses to Generative AI in the Context of Academic Integrity,"<p>This research explores the viewpoints and reactions of 31 Vietnamese English as a Foreign Language (EFL) educators concerning academic integrity issues arising from the integration of artificial intelligence (AI), particularly chatbots like ChatGPT, in foreign language education. Utilizing an open-ended survey, teachers were asked to express their opinions on dishonesty, pinpoint perceived causes, delineate consequences for students involved in AI-based plagiarism, and articulate their instructional strategies to address the issue. Descriptive statistics and thematic analyses revealed that teachers primarily linked students' AI-driven plagiarism to a lack of original ideas and insufficient learning attitudes and motivation. The excessive reliance on AI was identified as a barrier to skill development and critical thinking. In response to academic dishonesty, educators advocated for heightened regulations, the incorporation of plagiarism detection tools, and education on responsible AI utilization. These findings underscore the complex nature of teachers' perspectives on AI and dishonesty, underscoring the necessity for adapting pedagogies and assessments. The insights gleaned from this study contribute to a more profound comprehension of EFL instructors' viewpoints, providing valuable input for shaping policies and practices to foster academic integrity in the era of AI.</p>"
10.1109/gem61861.2024.10585442,Unleashing Generative Non-Player Characters in Video Games: An AI Act Perspective,N/A
10.1109/iv48863.2021.9575671,Prediction of Personalized Driving Behaviors via Driver-Adaptive Deep Generative Models,N/A
10.1159/000541168,Generative AI in Critical Care Nephrology: Applications and Future Prospects,"<jats:p>Background: Generative artificial intelligence (AI) is rapidly transforming various aspects of healthcare, including critical care nephrology. Large language models (LLMs), a key technology in generative AI, show promise in enhancing patient care, streamlining workflows, and advancing research in this field.
Summary: This review analyzes the current applications and future prospects of generative AI in critical care nephrology. Recent studies demonstrate the capabilities of LLMs in diagnostic accuracy, clinical reasoning, and continuous renal replacement therapy (CRRT) alarm troubleshooting. As we enter an era of multiagent models and automation, the integration of generative AI into critical care nephrology holds promise for improving patient care, optimizing clinical processes, and accelerating research. However, careful consideration of ethical implications and continued refinement of these technologies are essential for their responsible implementation in clinical practice. This review explores the current and potential applications of generative AI in nephrology, focusing on clinical decision support, patient education, research, and medical education. Additionally, we examine the challenges and limitations of AI implementation, such as privacy concerns, potential bias, and the necessity for human oversight.
Key messages: (i) LLMs have shown potential in enhancing diagnostic accuracy, clinical reasoning, and CRRT alarm troubleshooting in critical care nephrology. (ii) Generative AI offers promising applications in patient education, literature review, and academic writing within the field of nephrology. (iii) The integration of AI into electronic health records and clinical workflows presents both opportunities and challenges for improving patient care and research. (iv) Addressing ethical concerns, ensuring data privacy, and maintaining human oversight are crucial for the responsible implementation of AI in critical care nephrology.</jats:p>"
10.1038/d41586-023-03467-8,Nature's Take: How will ChatGPT and generative AI transform research?,N/A
10.2139/ssrn.4495319,An Outline for an Interrogative/Prompt Library to help improve output quality from Generative-AI Datasets,N/A
10.1109/histelcon56357.2023.10365937,FidoNet and Generative AI: A New Approach to Museumification of Historical Content Resources,N/A
10.1109/tale56641.2023.10398408,Grading Generative AI-based Assignments Using a 3R Framework,N/A
10.1038/d41586-023-00843-2,ChatGPT: tackle the growing carbon footprint of generative AI,N/A
10.1038/s41587-023-01789-6,How will generative AI disrupt data science in drug discovery?,N/A
10.22214/ijraset.2024.63815,Generative AI in Healthcare Industry: Implementations and Challenges,"<jats:p>Abstract: Generative AI (artificial intelligence) refers to algorithms and models that can be prompted to generate various types of content. Generative AI has quickly become a major factor in several industries, including health care. It has the potential to transform the sector, but we must understand how to use this technology in order to capitalize on its potential while avoiding the risks that may come with it while applying it to patient care. These models have played a crucial role in analyzing diverse forms of data, including medical imaging (encompassing image reconstruction, image-to-image translation, image generation, and image classification), clinical documentation, diagnostic assistance, clinical decision support, medical coding, and billing, as well as software engineering, testing and user data safety and security. In this review we briefly discuss some associated issues, such as trust, veracity, clinical safety and reliability, privacy and opportunities, e.g., AI-driven conversational user interfaces for friendlier human-computer interaction</jats:p>"
10.2139/ssrn.4853709,Generative Ai and Large Language Models for Cyber Security: All Insights You Need,N/A
10.1109/mitp.2024.3375569,"Navigating the Landscape of Generative AI: Investment Trends, Industry Growth, and Economic Effects",N/A
10.37074/jalt.2024.7.1.22,The use of Generative AI in qualitative analysis: Inductive thematic analysis with ChatGPT,N/A
10.34074/proc.240102,Embracing the Use of Generative AI in a First-Year Information Systems Course,"<jats:p>The use of generative artificial intelligence (AI) tools in higher education has gained a lot of attention in the media. Much of this narrative is about the use of tools such as ChatGPT as a way for students to cheat on the assessments they submit. This paper presents an analysis of how ChatGPT has been used in the design and delivery of a first-year information systems course. The background of the course and the institution that it is delivered in is presented, along with the design of the research, which is consistent with the early phases of an action research project. A literature review is presented that highlights the use of tools such as ChatGPT for developing hypothetical scenarios; the importance of students developing AI literacy skills; preparing the students for workplaces where these tools are being used; and the importance of considering the context of students when seeking to enhance student engagement. The design and delivery of the course is described, analysed and compared with the outcomes of the literature review and a revised version of Bloom’s Taxonomy of Learning. It is concluded that tools such as ChatGPT can be used for the effective design and delivery of courses in ways that enhance student engagement, increase student AI literacy, enhance the employability of graduates, and address all levels of a revised version of Bloom’s Taxonomy of Learning. Areas in which to extend the research include conducting focus groups with students from the first and subsequent iterations of the course examined in this study</jats:p>"
10.2139/ssrn.4791512,A Protocol for Integrating Dynamic Adaptive Questions from Generative AI into Qualtrics Surveys with Minimal JavaScript,N/A
10.21606/iasdr.2023.784,Generative AI in creative design processes: a dive into possible cognitive biases,N/A
10.1007/979-8-8688-0282-9_13,Applications and Real-World Case Studies,N/A
10.1038/d41586-024-02319-3,Generative AI makes for better scientific writing — but beware the pitfalls,N/A
10.2139/ssrn.4534294,The Layoff Generation: How Generative Ai Will Reshape Employment and Labor Markets,N/A
10.56442/ijble.v4i2.215,Empowering Education through Generative AI: Innovative Instructional Strategies for Tomorrow's Learners,"<jats:p>As the educational landscape endures continuous change, artificial intelligence (AI) has presented unprecedented opportunities to revolutionize instructional methods. Among these cutting-edge AI technologies, Generative AI has emerged as a promising instrument with the potential to empower educators and students through innovative instructional strategies. This article aims to investigate the various applications of Generative AI in education and cast light on its role in shaping the future of education. The objectives of this study are twofold: first, to investigate the various instructional strategies that can be enhanced by employing Generative AI, and second, to assess the potential impact of these strategies on student learning outcomes. To accomplish these goals, a comprehensive literature review was conducted analyzing existing studies and applications of Generative AI in educational settings. The results and discussions emphasize the numerous educational benefits of Generative AI. Educators can personalize learning experiences, create interactive content, and facilitate adaptive assessments by leveraging the capabilities of Generative AI. This individualized strategy has the potential to boost learner engagement and knowledge retention. However, despite the numerous advantages, ethical concerns and difficulties arise. The responsible incorporation of Generative AI in education requires addressing issues such as data privacy, algorithmic bias, and the educator's role in directing AI-driven learning experiences. The research concludes by emphasizing that Generative AI holds enormous promise for empowering education and transforming instructional practices. The findings highlight the importance of ongoing collaboration between educators, policymakers, and AI developers to ensure the ethical and equitable integration of Generative AI into educational environments. By embracing the potential of Generative AI while remaining vigilant regarding its challenges, the field of education can unlock novel opportunities to nurture an inclusive, adaptive, and learner-centric pedagogical landscape for tomorrow's learners.&#x0D;
 </jats:p>"
10.1109/aero58975.2024.10521090,Generative AI... in Space! Adversarial Networks to Denoise Images Onboard the OPS-SAT-1 Spacecraft,N/A
10.1097/acm.0000000000005667,"Addressing the Novel Implications of Generative AI for Academic Publishing, Education, and Research",N/A
10.1007/978-3-031-67991-9_4,Generativism,N/A
10.1109/icsses62373.2024.10561423,Training Strategy Based on Game Theory for Generative AI,N/A
10.1109/mipro60963.2024.10569736,Enhancing Programming Education with Open-Source Generative AI Chatbots,N/A
10.1109/educon60312.2024.10578876,Unveiling Generative AI in Higher Education: Insights from Engineering Students and Professors,N/A
10.1101/2023.04.30.538439,scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI,"<jats:title>Abstract</jats:title><jats:p>Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between linguistic constructs and cellular biology — where texts comprise words, similarly, cells are defined by genes — our study probes the applicability of foundation models to advance cellular biology and genetics research. Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell-type annotation, multi-batch integration, multi-omic integration, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at<jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://github.com/bowang-lab/scGPT"">https://github.com/bowang-lab/scGPT</jats:ext-link>.</jats:p>"
10.1145/3593013.3594067,Regulating ChatGPT and other Large Generative AI Models,N/A
10.4324/9781315151212-20,An Ethics of Free Responsible Action,N/A
10.55248/gengpi.5.0124.0338,Exploring the Role of Large Language Models (LLMs) and Generative AI in Dietary Management of Sinusitis,N/A
10.30965/9783846755372_004,Onto-Generative Epistemology (本体知识论),N/A
10.54941/ahfe1004431,Generative AI Wearable Assistant for Simulated Reach-Back Support,"<jats:p>This research investigates the development of a generative AI wearable assistant designed to provide simulated reach-back support for maintenance and troubleshooting applications. Reach-back support refers to accessing expertise remotely to assist individuals in challenging situations. In various domains such as healthcare, emergency response, and technical troubleshooting, reaching out to subject matter experts for real-time guidance can be crucial. Leveraging the capabilities of generative AI, we aim to create a wearable hardware and software device that serves as an assistant that simulates expert knowledge and provides personalized, context-aware (via object detection and a natural language interface) assistance. This poster presents preliminary findings from efforts to demonstrate the technical feasibility of this concept through the design, fabrication, and demonstration of an initial wearable prototype. Future research will seek to develop a deep learning model trained on extensive domain-specific data to generate relevant and accurate responses for maintenance and troubleshooting of specific equipment and systems. The wearable assistant incorporates speech recognition, natural language understanding, speech synthesis, and image-based object detection technologies for seamless communication and contextualization of reach-back requests. The findings from this research have the potential to enhance decision-making, problem-solving, and support capabilities in various professional and emergency scenarios where access to real-time expertise is limited.</jats:p>"
10.1007/979-8-8688-0083-2_5,"Complexity, Simplicity, and True Minimalism",N/A
10.1007/979-8-8688-0083-2_3,Principles of Conscious/Mindful Design,N/A
10.5013/ijssst.a.25.01.10,"Understanding the Interplay Between Trust, Reliability, and Human Factors in the Age of Generative AI",N/A
10.1145/3615859,Generative AI as a New Innovation Platform,<jats:p>Considering the stability and longevity of a potential new foundational technology.</jats:p>
10.1007/s11528-024-00949-y,Creative Dialogue with Generative AI: Exploring the Possible with Ron Beghetto,N/A
10.22541/au.168295736.63550013/v1,"UniSG: Unifying entity-component-systems, 3D &amp; learning scenegraphs with GNNs for generative AI","<jats:p id=""p1"">We envision a no-code solution that generates new nodes, edges and features reflecting the creation of 3D models, scenes and even new behavioral steps through GNNs. To this end, we propose a novel Universal Scenegraph (UniSG), that unifies entity-component-systems, 3D and learning scenegraphs with GNNs to empower generative AI, and facilitate the creation of 3D scenes with embedded behavior, alleviating existing bottlenecks.</jats:p>"
10.55452/1998-6688-2023-20-3-17-25,GENERATIVE AI: CHALLENGES TO ACADEMIC QUALITY,N/A
10.54394/fhem8239,Generative AI and jobs,"<jats:p>This study assesses the potential global exposure of occupations to Generative AI, particularly GPT-4. It predicts that the overwhelming effect of the technology will be to augment occupations, rather than to automate them. The greatest impact is likely to be in high and upper-middle income countries due to a higher share of employment in clerical occupations. As clerical jobs are an important source of female employment, the effects are highly gendered. Insights from this study underline the need for proactive policies that focus on job quality, ensure fair transitions, and that are based on dialogue and adequate regulation.</jats:p>"
10.1109/cste62025.2024.00030,Generative AI Research of Education from 2013 to 2023,N/A
10.54195/technophany.18134,Is Generative AI Ready to Join the Conversation That We Are?,"<jats:p>In this article, I use the dialogical ideas of Hans-Georg Gadamer to evaluate whether generative AI is ready to join the ontological conversation that he considers humanity to be. Despite the technical advances of generative AI, Gadamer’s philosophical hermeneutics reveals that it cannot function as a proxy human dialogue partner in pursuit of understanding. Even when free from anthropomorphic projections and reimagined as the “other”, generative AI is found to have a weak epistemology, lack of moral awareness, and no emotions. Even so, it evokes a response in some users that places it on the threshold of being. The most promising dialogical role identified for generative AI is as a digital form of Gadamerian “text” currently constrained by copyright and technical design. Generative AI’s shortcomings risk inhibiting hermeneutical understanding through greater access to summarised knowledge. Nonetheless, the new technology is on the brink of joining the ontological conversation of humanity.</jats:p>"
10.2139/ssrn.4770726,Creating Image Datasets in Agricultural Environments using DALL.E: Generative AI- Powered Large Language Model,N/A
10.14445/22312803/ijctt-v72i4p106,Generative AI Security: Protecting Users from Impersonation and Privacy Breaches,N/A
10.1145/3583780.3615317,Generative AI and the Future of Information Access,N/A
10.54097/jz8nj770,"Ethical Failures of Generative AI in News Production, Implications and Countermeasures","<jats:p>Generative AI is recognized by the media (people) for its high-quality dialogue, complex reasoning and other emergent capabilities in content generation, and human-machine collaboration will gradually become the norm in news production, with application advantages in news production, news distribution and other news survival links. While generative AI assists news production and promotes innovation in news reporting, it also has problems such as providing inaccurate facts, opaque algorithms, and controversial boundaries of application, etc. For this reason, the media should regulate the ethics of human-computer collaboration in terms of optimizing the editing process, disclosing information about algorithms, and setting the boundaries of application of machines.</jats:p>"
10.21541/apjess.1293702,Is ChatGPT Leading Generative AI? What is Beyond Expectations?,"<jats:p xml:lang=""en"">Generative AI has the potential to change the way we do things. The chatbot is one of the most popular implementation areas. Even though companies like Google and Meta had chatbots, ChatGPT became popular as it was made publicly available. Although ChatGPT is still in the early stages of its development, it attracted the attention of people and capital groups. It has taken the public interest; people from different fields, ages, and education levels started using ChatGPT. There have been many trials with ChatGPT. It is possible to see a lot of news and shares on the Internet. The study aims to shed light on what is happening in the literature and get an insight into the user expectations of ChatGPT and Generative AI. We also give information about the competitors of ChatGPT, such as Google’s Bard AI, Claude, Meta’s Wit.ai and Tencent’s HunyuanAide. We describe technical and structural fundamentals and try to shed light on who will win the race. We also shared information about the GPT4 version of OpenAI's ChatGPT. We share the early stage due diligence and current situation analysis for all these points. We examine preprint papers and published articles. We also included striking posts on the LinkedIn platform and a compilation of various blogs and news. We also made use of ChatGPT in editing the content of these resources of this study. We can get an insight into the people's interests through their questions submitted to ChatGPT. We can also understand the capabilities of GPT3, GPT4 and also predict further enhancements.</jats:p>"
10.1109/tele62556.2024.10605662,Generative AI Models with Their Full Reveal*,N/A
10.1145/3639856.3639893,Applications of Generative AI in Fintech,N/A
10.1016/j.egyai.2021.100087,A Conditional Generative adversarial Network for energy use in multiple buildings using scarce data,N/A
10.1007/978-3-031-54383-8_31,Generative AI Document Extractions Using Composite Approach: An External Data Integration,N/A
10.17232/kset.40.1.1,Development of a math-AI convergence instructional model using a generative AI chatbot,N/A
10.4018/979-8-3693-1198-1.ch005,Safeguarding Business in the Age of AI for Organizational Resilience and Risk Management,"<jats:p>As AI proliferates across sectors, it creates new cybersecurity risks from growing attack surfaces, data flows, and system complexity. This chapter outlines risk management frameworks to harness AI safely despite escalating threats. It establishes why traditional controls now fall short, necessitating updated cyber strategies centered on ethical “Secure AI by Design” governance. First, prominent threats like malware and denial-of-service attacks are analyzed. Technical safeguards such as authentication, encryption, and blockchain applications are suggested alongside auditing, transparency, and proactive risk monitoring to manage threats. Real-world critical infrastructure attack cases reveal current susceptibilities. esilience demands optimization coupled with defense-in-depth approaches across people, processes and system that gives advisory on adapting cybersecurity and a guide to securing AI innovation potential, aligning with the book's focus on OpenAI outlines steps around pipelines, red teams, and internal/external trust via auditing and transparency for cyber risk management</jats:p>"
10.4324/9781032688305-6,"Transforming SEO in the Era of Generative AI: Challenges, Opportunities, and Future Prospects",N/A
10.52783/jes.2031,A novel Conceptualization of AI Literacy and Empowering Employee Experience at Digital Workplace Using Generative AI and Augmented Analytics: A Survey,"<jats:p>With the fast, rapid, and expeditious integration of Artificial Intelligence (AI) technologies, particularly Generative AI and Augmented Analytics, organizations are presented with new opportunities to transform their operations and empower their workforce. This paper explores the intersection of AI literacy, Generative AI, and Augmented Analytics to propose strategies for fostering a culture of AI fluency among employees.  This paper reviews the literature on AI literacy and its potential implications for employee experience (Ex) in digital workplaces. AI literacy and competency is the ability to understand, interact with, and thoughtfully assess the applications and ramifications of artificial intelligence (AI) across diverse domains. The paper argues that AI literacy is a key competence for employees in the digital era, as it enables them to leverage the potential of generative AI and augmented analytics, two of the most promising technologies for enhancing Ex. Generative AI refers to the use of AI to create novel and diverse outputs, such as text, images, music, or designs, while augmented analytics drives potential capability to make use of Artificial Intelligence technologies to automate and augment data analysis and decision-making. The paper discusses how these technologies can empower employees to be more creative, productive, collaborative, and engaged in their work and the challenges and risks they pose. This paper additionally highlights the obstacles, deficiencies, and proposed pathways for further research advancement concerning AI technological competency and literacy, aiming to enhance the employee experience significantly in digital workplaces.</jats:p>"
10.1145/3626252.3630817,CS1 with a Side of AI: Teaching Software Verification for Secure Code in the Era of Generative AI,N/A
10.1515/9783111323749-013,13 An innovative analysis of AI-powered automation techniques for business management,N/A
10.3389/frobt.2020.00047,Imitating by Generating: Deep Generative Models for Imitation of Interactive Tasks,N/A
10.4018/979-8-3693-3719-6.ch004,Applications of AI Techniques in Healthcare and Wellbeing,"<jats:p>The integration of Artificial Intelligence (AI) techniques in healthcare and wellbeing systems has witnessed significant advancements, offering transformative solutions to traditional healthcare paradigms. This chapter provides a comprehensive overview of the diverse applications of AI in the healthcare sector, highlighting its potential to enhance diagnostics, treatment planning, personalized medicine, and overall patient outcomes. AI techniques, including machine learning and deep learning algorithms, have demonstrated remarkable capabilities in analyzing large datasets such as medical images, genetic information, and electronic health records. These technologies enable more accurate and timely disease detection, improving diagnostic accuracy and aiding healthcare professionals in making informed decisions. In treatment planning, AI-driven systems contribute to the development of personalized therapeutic strategies. By leveraging patient-specific data, AI models can predict treatment responses, optimize drug regimens, and minimize adverse effects.</jats:p>"
10.1007/s43681-021-00126-4,Responsible media technology and AI: challenges and research directions,"<jats:title>Abstract</jats:title><jats:p>The last two decades have witnessed major disruptions to the traditional media industry as a result of technological breakthroughs. New opportunities and challenges continue to arise, most recently as a result of the rapid advance and adoption of artificial intelligence technologies. On the one hand, the broad adoption of these technologies may introduce new opportunities for diversifying media offerings, fighting disinformation, and advancing data-driven journalism. On the other hand, techniques such as algorithmic content selection and user personalization can introduce risks and societal threats. The challenge of balancing these opportunities and benefits against their potential for negative impacts underscores the need for more research in responsible media technology. In this paper, we first describe the major challenges—both for societies and the media industry—that come with modern media technology. We then outline various places in the media production and dissemination chain, where research gaps exist, where better technical approaches are needed, and where technology must be designed in a way that can effectively support responsible editorial processes and principles. We argue that a comprehensive approach to research in responsible media technology, leveraging an interdisciplinary approach and a close cooperation between the media industry and academic institutions, is urgently needed.</jats:p>"
10.1017/s0963180122000664,The Virtues of Interpretable Medical AI,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) systems have demonstrated impressive performance across a variety of clinical tasks. However, notoriously, sometimes these systems are “black boxes.” The initial response in the literature was a demand for “explainable AI.” However, recently, several authors have suggested that making AI more explainable or “interpretable” is likely to be at the cost of the accuracy of these systems and that prioritizing interpretability in medical AI may constitute a “lethal prejudice.” In this paper, we defend the value of interpretability in the context of the use of AI in medicine. Clinicians may prefer interpretable systems over more accurate black boxes, which in turn is sufficient to give designers of AI reason to prefer more interpretable systems in order to ensure that AI is adopted and its benefits realized. Moreover, clinicians may be justified in this preference. Achieving the downstream benefits from AI is critically dependent on how the outputs of these systems are interpreted by physicians and patients. A preference for the use of highly accurate black box AI systems, over less accurate but more interpretable systems, may itself constitute a form of lethal prejudice that may diminish the benefits of AI to—and perhaps even harm—patients.</jats:p>"
10.4337/9781803926728.00006,What Is This Thing Called the Ethics of AI and What Calls for It?,N/A
10.1007/s11948-024-00485-1,Owning Decisions: AI Decision-Support and the Attributability-Gap,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) has long been recognised as a challenge to responsibility. Much of this discourse has been framed around robots, such as autonomous weapons or self-driving cars, where we arguably lack control over a machine’s behaviour and therefore struggle to identify an agent that can be held accountable. However, most of today’s AI is based on machine-learning technology that does not act on its own, but rather serves as a decision-support tool, automatically analysing data to help human agents make better decisions. I argue that decision-support tools pose a challenge to responsibility that goes beyond the familiar problem of finding someone to blame or punish for the behaviour of agent-like systems. Namely, they pose a problem for what we might call “<jats:italic>decision ownership</jats:italic>”: they make it difficult to identify human agents to whom we can attribute value-judgements that are reflected in decisions. Drawing on recent philosophical literature on responsibility and its various facets, I argue that this is primarily a problem of <jats:italic>attributability</jats:italic> rather than of <jats:italic>accountability</jats:italic>. This particular responsibility problem comes in different forms and degrees, most obviously when an AI provides direct <jats:italic>recommendations</jats:italic> for actions, but also, less obviously, when it provides mere <jats:italic>descriptive</jats:italic> information on the basis of which a decision is made.</jats:p>"
10.1101/2023.02.14.23285938,Generative AI as a Tool for Environmental Health Research Translation,"<jats:title>Abstract</jats:title><jats:p>Generative artificial intelligence, popularized by services like ChatGPT, has been the source of much recent popular attention for publishing health research. Another valuable application is in translating published research studies to readers in non-academic settings. These might include environmental justice communities, mainstream media outlets, and community science groups. Five recently published (2021-2022) open-access, peer-reviewed papers, authored by University of Louisville environmental health investigators and collaborators, were submitted to ChatGPT. The average rating of all summaries of all types across the five different studies ranged between 3 and 5, indicating good overall content quality. ChatGPT’s general summary request was consistently rated lower than all other summary types. Whereas higher ratings of 4 and 5 were assigned to the more synthetic, insight-oriented activities, such as the production of a plain language summaries suitable for an 8<jats:sup>th</jats:sup>grade reading level and identifying the most important finding and real-world research applications. This is a case where artificial intelligence might help level the playing field, for example by creating accessible insights and enabling the large-scale production of high-quality plain language summaries which would truly bring open access to this scientific information. This possibility, combined with the increasing public policy trends encouraging and demanding free access for research supported with public funds, may alter the role journal publications play in communicating science in society. For the field of environmental health science, no-cost AI technology such as ChatGPT holds the promise to improve research translation, but it must continue to be improved (or improve itself) from its current capability.</jats:p>"
10.1109/mipr.2019.00098,A Generative Adversarial Network for AI-Aided Chair Design,N/A
10.48175/ijarsct-15603,Enhancing Banking Chatbot Experience through Hybrid Conversational-Generative AI Approaches,"<jats:p>In today's digital age, chatbots have become an integral part of the banking industry, offering customers quick and efficient assistance. However, to meet the growing demands of customers and provide a seamless user experience, banking chatbots are evolving beyond rule-based systems. Traditional rule-based chatbots often fall short in handling complex queries and delivering personalized responses. This research proposal outlines a study aimed at enhancing the banking chatbot experience through the hybrid conversational-generative AI approaches. 
The primary focus of this research is to combine the strengths of conversational AI, which excels in understanding and generating natural language responses, with generative AI, capable of generating creative and context-aware responses. By merging these two AI paradigms, banking chatbots can provide more human-like interactions, understand complex queries, and offer personalized responses</jats:p>"
10.20944/preprints202311.1130.v1,VarChat: The Generative AI Assistant for the Interpretation of Human Genomic Variations,"<jats:p>In the modern era of genomic research, the scientific community is witnessing an explosive growth in the volume of published findings. While this abundance of data offers invaluable insights, it also places a pressing responsibility on genetic professionals and researchers to stay informed about the latest findings and their clinical significance. Genomic variant interpretation is currently facing a challenge in identifying the most up-to-date and relevant scientific papers, while also extracting meaningful information to accelerate the process from clinical assessment to reporting. Computer-aided literature search and summarization can play a pivotal role in this context. By synthesizing complex genomic findings into concise, interpretable summaries, this approach facilitates the translation of extensive genomic datasets into clinically relevant insights. To bridge this gap, we present VarChat (varchat.engenome.com), an innovative tool based on generative AI, developed to find and summarize the fragmented scientific literature associated with genomic variants into brief yet informative texts.VarChat provides users with a concise description of specific genetic variants, detailing their impact on related proteins and possible effects on human health. Additionally, VarChat offers direct links to related scientific trustable sources, and encourages deeper research. Availability: VarChat is freely available at varchat.engenome.com</jats:p>"
10.4995/eurocall2023.2023.16994,Using generative AI tools and LARA to create multimodal language learning resources for L2 Icelandic,"<jats:p>We utilize ChatGPT and other generative AI tools in developing illustrated multimodal resources for learning Icelandic as a second language (L2) via reading in the online platform LARA. These are illustrated short stories in Icelandic created using specific prompts including information about learner levels and age groups. We present a methodology for creating engaging stories and a way to evaluate the quality and suitability of text and images generated by AI. Additionally, we assess how 46 adult learners perceived reading a story at A1 and A2 level in LARA, with a positive vocabulary learning effect. We conclude that generating suitable prompts greatly assists with getting the desired output; however, this is restricted to the language one works with. Much human post-editing is still necessary for Icelandic texts to improve their quality in grammar, vocabulary, and cultural aspects, and their suitability for language teaching.</jats:p>"
10.24059/olj.v28i3.4458,The Use of Generative AI to Support Inclusivity and Design Deliberation for Online Instruction,"<jats:p>Generative AI presents significant opportunities for instructional designers to revolutionize content creation and personalization in online learning environments. This paper explores how generative AI can streamline content generation processes, enhance adaptability to individual learner needs, and improve feedback mechanisms, ultimately fostering more engaging and inclusive learning experiences. Alongside its benefits, generative AI also poses ethical considerations and potential risks, such as perpetuating biases or disrupting the learning process. Navigating these complexities requires a deliberate approach to design deliberation, that involves careful analysis, discussion, and decision-making throughout the design process. This paper proposes a conceptual framework to support instructional designers in leveraging generative AI to promote inclusivity within their design deliberations, emphasizing the importance of addressing ethical considerations and engaging in iterative design practices.</jats:p>"
10.4018/979-8-3693-0831-8,The Role of Generative AI in the Communication Classroom,N/A
10.55041/isjem01652,Enhancing Voice Assistants with Generative AI Language Models for Interactive Conversations,"<jats:p>This paper presents comprehensive research for the development of a voice assistant application using HTML/CSS, JavaScript, Python, SQLite, and the Hugging Face API. The proposed methodology encompasses key steps in data collection, model training, frontend and backend development, integration, testing, and deployment, ensuring the creation of a robust and user-friendly voice assistant system. By leveraging a combination of web technologies and Al tools, the developed voice assistant application offers an interactive and intelligent user experience across its users.     Key Words:  Voice assistant, HTML/CSS, JavaScript, Python, SQLite, Hugging Face API, LLMs, AI.</jats:p>"
10.1111/ele.14397/v1/review2,"Review for ""How widespread use of generative &lt;scp&gt;AI&lt;/scp&gt; for images and video can affect the environment and the science of ecology""",N/A
10.1109/icaiic60209.2024.10463265,Harnessing Generative AI for Manufacturing Innovation: Applications and Opportunities,N/A
10.1109/weef-gedc59520.2023.10343903,Art Critically Examining Generative AI,N/A
10.1057/s41254-024-00333-w,First contact: integrating generative AI into digital diplomatic intelligence,N/A
10.1016/j.lanepe.2023.100677,Embracing generative AI in health care,N/A
10.1097/01.nt.0001017624.37101.c6,Using Generative AI to Write Those Personal Statements?,N/A
10.13052/jwe1540-9589.2325,Flight Price Prediction Web-based Platform: Leveraging Generative AI for Real-time Airfare Forecasting,"<jats:p>The aviation business encounters difficulties in correctly and swiftly predicting flight fares due to the dynamic nature of the sector. Factors such as variations in demand, fuel costs, and the intricacies of various routes have an impact on this. This work presents a new method to tackle this issue by utilizing generative artificial intelligence (GAI) approaches to accurately forecast airfares in real-time. This paper presents a novel framework that integrates generative models, deep learning architectures, and historical pricing data to improve the precision of future flight price predictions. The study employs a GAI within a cutting-edge web engineering framework. This approach is designed primarily to gather knowledge about complex patterns and relationships present in historical airline data. Through the utilization of this methodology, the model is able to accurately perceive complex connections and adjust to ever-changing market conditions. Our model utilizes deep neural networks to effectively handle various circumstances and extract vital information, so facilitating a comprehensive comprehension of the intricate elements that impact flight cost. Moreover, the suggested approach places significant emphasis on precisely predicting upcoming occurrences in real-time, facilitating prompt reactions to market volatility and offering a valuable resource for airlines, travel agents, and customers alike. In order to enhance the accuracy of real-time forecasts, we utilize a web-based platform that allows for smooth interaction with live data streams and guarantees swift updates. The results demonstrate the model’s capacity to adjust to dynamic market conditions, rendering it an attractive option for stakeholders in search of precise and current forecasts of flight prices.</jats:p>"
10.21608/aaj.2023.245371.1045,Transforming Interior Design Education through Generative Artificial Intelligence (AI) Trend,N/A
10.1002/9781394308286.ch5,"What Is
            <scp>ChatGPT</scp>
            ?",N/A
10.1109/iccc62278.2024.10582954,Generative Neural Models in Robotics: Pioneering the Embodied AI Revolution,N/A
10.2139/ssrn.4675409,Generative AI and Simulation Modeling: How Should You (Not) Use Large Language Models Like ChatGPT,N/A
10.18260/1-2--47944,Reshaping Engineering Technology Education: Fostering Critical Thinking through Open-Ended Problems in the Era of Generative AI,N/A
10.1109/ms.2024.3410641,Hints for Generative AI Software Development,N/A
10.21606/drs.2024.1329,Revealing user tacit knowledge: Generative-Image-AI helps create better design conversation,N/A
10.1109/fie58773.2023.10343467,Generative AI in Computing Education: Perspectives of Students and Instructors,N/A
10.1038/d41586-023-01295-4,Why open-source generative AI models are an ethical way forward for science,N/A
10.24963/ijcai.2024/1014,ProMoAI: Process Modeling with Generative AI,"<jats:p>ProMoAI is a novel tool that leverages Large Language Models (LLMs) to automatically generate process models from textual descriptions, incorporating advanced prompt engineering, error handling, and code generation techniques. Beyond automating the generation of complex process models, ProMoAI also supports process model optimization. Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model. ProMoAI utilizes the capabilities LLMs to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling.</jats:p>"
10.1109/iedm45741.2023.10413890,Advanced Packaging Technologies in Memory Applications for Future Generative AI Era,N/A
10.2139/ssrn.4558295,Dissecting Corporate Culture Using Generative AI – Insights from Analyst Reports,N/A
10.1109/ethics57328.2023.10154920,ETHICS-2023 Session B1 - Panel: Perspectives from Liberal Arts on the practical turn in AI Ethics,N/A
10.15187/adr.2024.05.37.2.249,Proposal of a Facilitation and Process Model for Enhancing Creativity in Co-design Workshops with Generative AI: The Use of ChatGPT,N/A
10.1007/s10676-021-09615-w,AI recruitment algorithms and the dehumanization problem,N/A
10.1007/978-3-030-66913-3_4,Ethics in an International Context,N/A
10.15187/adr.2024.05.37.2.249,Proposal of a Facilitation and Process Model for Enhancing Creativity in Co-design Workshops with Generative AI: The Use of ChatGPT,N/A
10.1287/lytx.2023.03.12,My Biggest Worry about Generative AI,N/A
10.1007/978-981-99-9836-4_28,AI-Driven Innovation and Discoveries in Space Exploration: The Need for an Adapted Regulatory Regime,N/A
10.4324/9781003482918-13,Embracing GenAI in education,N/A
10.22158/csm.v3n4p61,An Investigation about Entailment and Narrative by AI Techniques (Generative Models),"<jats:p>Many storytelling generation problems concern the difficulty to model the sequence of sentences. Language models are generally able to assign high scores to well-formed text, especially in the cases of short texts, failing when they try to simulate human textual inference. Although in some cases output text automatically generated sounds as bland, incoherent, repetitive and unrelated to the context, in other cases the process reveals capability to surprise the reader, avoiding to be boring/predictable, even if the generated text satisfies entailment task requirements. The lyric tradition often does not proceed towards a real logical inference, but takes into account alternatives like the unexpectedness, useful for predicting when a narrative story will be perceived as interesting. To achieve a best comprehension of narrative variety, we propose a novel measure based on two components: inference and unexpectedness, whose different weights can modify the opportunity for readers to have different experiences about the functionality of a generated story. We propose a supervised validation treatment, in order to compare the authorial original text, learned by the model, with the generated one.</jats:p>"
10.62177/apemr.v1i3.8,Transforming Accounting with Generative AI Potential Opportunities and Key Challenges,"<jats:p>Amid the rapid advancement of information technology, generative AI has emerged as a pivotal force in transforming the accounting industry. This paper examines the opportunities that generative AI, exemplified by ChatGPT, brings to the field. These opportunities include streamlining workflow automation, enhancing review efficiency, and supporting scientific research. Additionally, the paper addresses several challenges, such as the authenticity and usability of generated data, privacy and security concerns surrounding accounting information, and the shortage of technical expertise. To tackle these challenges, the author suggests fostering critical thinking and awareness, enhancing the training and development of AI models tailored to accounting, strengthening data and privacy protection measures, and promoting relevant training for accounting professionals. This study bridges the research gap in the application of ChatGPT within the accounting industry and holds practical significance in advancing technological innovation and high-quality development in China's accounting sector.</jats:p>"
10.59350/vh0zy-9k287,"Things I am still wondering about generative AI + Search in 2024 - impact of semantic search, generation of answers with citations and more..","<p>&lt;i&gt; Earlier related pieces - How Q&amp;amp;A systems based on large language models (eg GPT4) will change things if they become the dominant search paradigm - 9 implications for libraries &lt;/i&gt; In the ever-evolving landscape of information retrieval and library science, the emergence of large language models, particularly those based on the transformer architecture like GPT-4, has opened up a Pandora's box of possibilities and challenges.</p>"
10.1162/imag_a_00241,The future of data analysis is now: Integrating generative AI in neuroimaging methods development,"<jats:title>Abstract</jats:title>
               <jats:p>In this perspective, we highlight how emerging artificial intelligence tools are likely to impact the experiences of researchers conducting computational fMRI analyses. While calls for the automatization of statistical procedures date back at least to the inception of “data science” as a field, generative artificial intelligence offers new opportunities to advance field practice. We highlight how these tools are poised to impact both new neuroimaging methods development in areas such as image quality control and in day-to-day practice when generating analysis code. We argue that considering generative artificial intelligence as a catalyst for computational neuroscience—rather than as unique tools in their own right—can substantially improve its positioning in the research ecosystem. In particular, we argue that generative artificial intelligence will reinforce the importance of existing open science initiatives, rather than supplanting them. Overall, we call for clearer metrics by which neuroimaging results—whether generated by individual research teams or by generative artificial intelligence technologies—can be meaningfully compared.</jats:p>"
10.31219/osf.io/5ks37,The Emerging Role of ISO 42001 Certification in Fostering the Deployment of Responsible Generative AI Healthcare Solutions,"<p>The growing excitement about the potential positive impact of generative AI (GenAI) solutions in healthcare has been tempered by uncertainty on how to ensure that such solutions are deployed safely and effectively. A core issue is that the current product-centric regulatory oversight model does not apply well to a technology that adapts to the operating environment and becomes enmeshed with medical practice in a non-deterministic manner. This difficulty has led to the development of alternate approaches to foster the responsible deployment of GenAI solutions that are focused on the organizations developing and using the technology, as they are uniquely positioned to prevent and quickly address issues as they occur. One emerging approach is via the certification with the ISO 42001 standard, which defines the structure for auditable AI Management Systems (AIMS) in organizations developing and/or deploying AI solutions. The process of ISO 42001 certification can enable healthcare organizations to build adaptive and auditable AIMS that mitigate risks while supporting the deployment of trustworthy AI. The wide adoption of ISO 42001 certification by healthcare organizations would allow for the utilization of beneficial GenAI solutions while potentially facilitating the performance of oversight functions by regulators and payers. This article introduces the ISO 42001 implementation process in healthcare organizations and describes the next steps in the operationalization of this new GenAI risk mitigation approach.</p>"
10.18609/ioi.2024.007,A deep dive into generative AI applications in I-O,N/A
10.1515/9783111323749-005,5 Reinforcement learning,N/A
10.2139/ssrn.4889313,Impact Evaluation on the European Privacy Laws governing generative-AI models -Evidence in Relation between Internet Censorship and the Ban of ChatGPT in Italy,N/A
10.1007/978-3-031-31517-6_18,ChatGPT and Generative AI,N/A
10.12968/bjon.2024.33.9.439,Getting to grips with generative AI,"<jats:p> Sam Foster, Executive Director of Professional Practice, Nursing and Midwifery Council, considers the issues raised for regulators and assessors by the availability of tools such as ChatGPT </jats:p>"
10.56726/irjmets51744,EMPOWERING FINTECH INNOVATION: A STRATEGIC GUIDE TO GENERATIVE AI INTEGRATION AND HYBRID CLOUD ADOPTION,N/A
10.2139/ssrn.4907043,Using Generative AI to Calculate Party Positions: A Comparison of Human Experts and Large Language Models,N/A
10.1109/mipro60963.2024.10569483,Comparative Analysis of Generative AI Tools in Enhancing Educational Engagement,N/A
10.54808/jsci.22.03.34,Integrating Generative AI in Active Learning Environments: Enhancing Metacognition and Technological Skills,"<jats:p>This paper explores the innovative integration of Generative AI (GenAI) in active learning environments to augment metacognitive knowledge and technological skill development among students. While active learning has been pivotal in promoting student engagement and learning, the incorporation of GenAI presents a novel approach to further enhance these outcomes. The study investigates how GenAI tools can be utilized within a reflective practice model to bolster metacognitive regulation and technological proficiency. By discussing the synergistic relationship between GenAI, active learning, and metacognitive strategies, this paper provides insights into the evolving landscape of educational technology and its impact on student learning processes. The paper offers a theoretical framework based on established concepts in metacognition, active learning, reflective practice, and technological skills, contextualized within the realm of GenAI. This paper contributes to the understanding of how GenAI can be harnessed as an educational tool, facilitating deeper and more effective learning experiences.</jats:p>"
10.1109/cog60054.2024.10645673,Leveraging Gaming to Enhance Knowledge Graphs for Explainable Generative AI Applications,N/A
10.34190/eckm.25.1.2515,Expiring Technologies Face to the Development of Generative AI: Programming Languages,"<jats:p>The rapid development of AI, and Generative AI in particular, offers incredible opportunities for all humanity. However, this development carries serious threats and dangers. The topic of job reduction and the obsolescence of some professions that were previously downright lucrative is often raised in the literature. Such a phenomenon of disappearance of certain professions or abandonment of old technologies has occurred many times, which was related to technological development or simply the development of civilization.Information and communication technologies are particularly susceptible to change. Virtually every IT issue has gone through an individual development path, resulting in a departure from the original solutions in favor of more modern, more flexible, more easily scalable, or simply more intuitive for humans. Meanwhile, there is a danger that the solutions proposed by GenAI may become dehumanized. The author delved into the topic of the potential phasing out of certain technologies due to the influence of GenAI, using the example of programming languages that have been used thus far. Their collection turns out to be quite extensive. Some of these languages fell into oblivion before the GenAI era, naturally replaced by other programming languages, or gradually became increasingly niche or redundant. The research question posed in this work is: Will GenAI lead to a departure from currently used programming languages, which were typically designed to be user-friendly (in the sense of being human-readable)? The aim of the work is to answer this question, as well as several smaller ones, such as: Are there any chances that programming languages will remain understandable to humans? The work employs literature analysis, critical analysis of selected technologies, and the case study method.</jats:p>"
10.1007/978-981-99-9836-4_25,On the Relationship Between Artificial Intelligence (AI) and Economic Growth (GDP)—the Case of Europe,N/A
10.4018/979-8-3693-0487-7.ch011,Exploring the Challenges and Applications of Generative AI on Engineering Education in Mexico,"<jats:p>This chapter explores the potential and challenges of implementing generative artificial intelligence (AI) in engineering education in Mexico. It highlights the relevance of developing broader competences like critical thinking, collaboration, and cognitive processing to prepare students for the Fourth Industrial Revolution. Integrating generative AI and STEM education can foster collaboration, communication, critical thinking, and creativity. Challenges include the gap between demand and supply of skilled engineers and the need to enhance education quality. The chapter proposes personalized learning experiences, enhanced student engagement, workload reduction for teachers, and the use of large language models and simulated environments as key applications of generative AI. Leveraging learning analytics and generative AI can tailor content to students' needs. Ethical considerations and human oversight are crucial for successful integration.</jats:p>"
10.1145/3589334.3649116,"AI Deepfakes on the Web: The 'Wicked' Challenges for AI Ethics, Law and Technology",N/A
10.1007/978-981-19-9382-4_10,Towards the Future with AI: Work and Superintelligence,N/A
10.1145/3203247.3203250,"1st AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society","<jats:p>The 1st AAAI/ACM Conference on AI, Ethics, and Society (AIES-18) was held February 13, 2018 at the Hilton New Orleans Riverside, in New Orleans, Louisiana. The event was held just before AAAI in order to highlight the overlap of the two conference and logistical support for the conference was provided by AAAI. By attendance measures the conference was a resounding success with a sold out registration of over 300 people. The conference brought together program chairs from the four major focal areas: AI and jobs: Jason Furman (Harvard University); AI and law: Gary Marchant (Arizona State University); AI and philosophy: Huw Price (Cambridge University); and AI: Francesca Rossi (IBM and University of Padova). All the paper from the conference are available for download at the conference website: http://www.aies-conference.com/</jats:p>"
10.1145/3375627.3375814,Activism by the AI Community,N/A
10.1145/3600211.3604750,Exploring the Effect of AI Assistance on Human Ethical Decisions,N/A
10.1093/oso/9780192889898.003.0009,Feminist Technofutures,"<jats:title>Abstract</jats:title>
               <jats:p>In this chapter, Atanasoski problematises two different approaches to sex robots. The first, a liberal feminist approach, characterises sex robots as a threat to women’s rights, and in doing so, roots itself in technology’s relationship to the figure of the human within the juridical realm of rights. The second, a feminist approach centred on diversity, makes use of and commodifies proliferating categories of human difference and life itself by calling for the ‘diversification’ of sex robots beyond white, Eurocentric beauty norms. Atanasoski thus considers how diversified technologies still uphold rather than disrupt racialised notions of use and value, commodifying those differences seen as profitable. She concludes by considering what a queer approach to technological speculative futures might look like through an analysis of the art project ‘Lauren AI’.</jats:p>"
10.1007/s13347-023-00677-w,Commentary on Artificial Intelligence (AI) in Islamic Ethics: Towards Pluralist Ethical Benchmarking for AI,N/A
10.1145/3278721.3278746,Rethinking AI Strategy and Policy as Entangled Super Wicked Problems,N/A
10.1007/s00146-024-01963-7,Ethics and administration of the ‘Res publica’: dynamics of democracy,N/A
10.1177/00405639231223891,"AI as Person, Paradigm, and Structure: Notes toward an Ethics of AI","<jats:p> The unique capabilities of artificial intelligence (AI) have forced theologians to develop analytical categories beyond the instrumentalist model of technology. Recent work examines AI in terms of whether it has the qualities of a person, its effects on character, and its embedding in structures of sin. Constructive responses have focused on principles, communities, and virtues. None of these responses fully addresses concerns raised by critical analyses, suggesting that moral theology is still searching for a replacement for the instrumentalist model of technology. </jats:p>"
10.1007/s00146-023-01781-3,Correction: Bowling alone in the autonomous vehicle: the ethics of well-being in the driverless car,N/A
10.1007/s13347-023-00668-x,Artificial Intelligence (AI) in Islamic Ethics: Towards Pluralist Ethical Benchmarking for AI,"<jats:title>Abstract</jats:title><jats:p>This paper explores artificial intelligence (AI) ethics from an Islamic perspective at a critical time for AI ethical norm-setting. It advocates for a pluralist approach to ethical AI benchmarking. As rapid advancements in AI technologies pose challenges surrounding autonomy, privacy, fairness, and transparency, the prevailing ethical discourse has been predominantly Western or Eurocentric. To address this imbalance, this paper delves into the Islamic ethical traditions to develop a framework that contributes to the global debate on optimal norm setting for designing and using AI technologies.</jats:p><jats:p>The paper outlines Islamic parameters for ethical values and moral actions in the context of AI's ethical uncertainties. It emphasizes the significance of both textual and non-textual Islamic sources in addressing these uncertainties while placing a strong emphasis on the notion of ""good"" or ""<jats:italic>maṣlaḥa</jats:italic>"" as a normative guide for AI's ethical evaluation. Defining <jats:italic>maṣlaḥa</jats:italic> as an ethical state of affairs in harmony with divine will, the paper highlights the coexistence of two interpretations of <jats:italic>maṣlaḥa</jats:italic>: welfarist/utility-based and duty-based. Islamic jurisprudence allows for arguments supporting ethical choices that prioritize building the technical infrastructure for AI to maximize utility. Conversely, it also supports choices that reject consequential utility calculations as the sole measure of value in determining ethical responses to AI advancements.</jats:p>"
10.1145/3278721.3278784,Towards a Computational Sustainability for AI/ML to Foster Responsibility,N/A
10.1007/s10551-022-05049-6,Ethics of AI-Enabled Recruiting and Selection: A Review and Research Agenda,"<jats:title>Abstract</jats:title><jats:p>Companies increasingly deploy artificial intelligence (AI) technologies in their personnel recruiting and selection process to streamline it, making it faster and more efficient. AI applications can be found in various stages of recruiting, such as writing job ads, screening of applicant resumes, and analyzing video interviews via face recognition software. As these new technologies significantly impact people’s lives and careers but often trigger ethical concerns, the ethicality of these AI applications needs to be comprehensively understood. However, given the novelty of AI applications in recruiting practice, the subject is still an emerging topic in academic literature. To inform and strengthen the foundation for future research, this paper systematically reviews the extant literature on the ethicality of AI-enabled recruiting to date. We identify 51 articles dealing with the topic, which we synthesize by mapping the ethical opportunities, risks, and ambiguities, as well as the proposed ways to mitigate ethical risks in practice. Based on this review, we identify gaps in the extant literature and point out moral questions that call for deeper exploration in future research.</jats:p>"
10.4108/eai.20-11-2021.2314263,"The Ethics of Early Crisis Detection - Big Data, AI, and Algorithms in the German Military",N/A
10.1007/s00146-023-01628-x,Open source intelligence and AI: a systematic review of the GELSI literature﻿,"<jats:title>Abstract</jats:title><jats:p>Today, open source intelligence (OSINT), i.e., information derived from publicly available sources, makes up between 80 and 90 percent of all intelligence activities carried out by Law Enforcement Agencies (LEAs) and intelligence services in the West. Developments in data mining, machine learning, visual forensics and, most importantly, the growing computing power available for commercial use, have enabled OSINT practitioners to speed up, and sometimes even automate, intelligence collection and analysis, obtaining more accurate results more quickly. As the infosphere expands to accommodate ever-increasing online presence, so does the pool of actionable OSINT. These developments raise important concerns in terms of governance, ethical, legal, and social implications (GELSI). New and crucial oversight concerns emerge alongside standard privacy concerns, as some of the more advanced data analysis tools require little to no supervision. This article offers a systematic review of the relevant literature. It analyzes 571 publications to assess the current state of the literature on the use of AI-powered OSINT (and the development of OSINT software) as it relates to the GELSI framework, highlighting potential gaps and suggesting new research directions.</jats:p>"
10.1002/9781119551966.ch51,"AI Trust, Ethics, Transparency and Enablement",N/A
10.1145/3461702.3462470,Trustworthy AI for the People?,N/A
10.1007/s00146-020-01069-w,Getting into the engine room: a blueprint to investigate the shadowy steps of AI ethics,"<jats:title>Abstract</jats:title><jats:p>Enacting an AI system typically requires three iterative phases where AI engineers are in command: selection and preparation of the data, selection and configuration of algorithmic tools, and fine-tuning of the different parameters on the basis of intermediate results. Our main hypothesis is that these phases involve practices with ethical questions. This paper maps these ethical questions and proposes a way to address them in light of a neo-republican understanding of freedom, defined as absence of domination. We thereby identify different types of responsibility held by AI engineers and link them to concrete suggestions on how to improve professional practices. This paper contributes to the literature on AI and ethics by focusing on the work necessary to configure AI systems, thereby offering an input to better practices and an input for societal debates.</jats:p>"
10.47289/aiej20240122,AI and Human Reasoning: Qualitative Research in the Age of Large Language Models,"<jats:p>Context: The advent of AI-driven large language models (LLMs), such as ChatGPT 3.5 and GPT-4, have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline.  Problem: A significant concern revolves around the disparity between AI-generated classifications and human comprehension, prompting questions about the reliability of AI-derived insights. An “AI echo chamber” could potentially risk the diversity inherent in qualitative research. A minimal overlap between AI and human interpretations amplifies concerns about the fading human element in research.  Objective: This study aimed to compare and contrast the comprehension capabilities of humans and LLMs, specifically ChatGPT 3.5 and GPT-4.  Methodology: We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. ChatGPT 3.5 and GPT-4 were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning.  Results: The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one-third of cases, and a slightly lower alignment with GPT-4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one-fifth of the classifications. In the comparison of human and LLMs reasoning, it appears that human analysts lean heavily on their individual experiences. As expected, LLMs, on the other hand, base their reasoning on the specific word choices found in app reviews and the functional components of the app itself.  Conclusion: Our results highlight the potential for effective human-LLM collaboration, suggesting a synergistic rather than competitive relationship. Researchers must continuously evaluate LLMs’ role in their work, thereby fostering a future where AI and humans jointly enrich qualitative research.</jats:p>"
10.1007/s00146-020-01000-3,Disengagement with ethics in robotics as a tacit form of dehumanisation,N/A
10.53889/citj.v1i1.198,"AI Trust Framework and Maturity Model: Improving Security, Ethics and Trust in AI","<jats:p>The following article develops an AI Trust Framework and Maturity Model (AI-TFMM) to improve trust in AI technologies used by Autonomous Human Machine Teams  Systems (A-HMT-S). The framework establishes a methodology to improve quantification of trust in AI technologies.  Key areas of exploration include security, privacy, explainability, transparency and other requirements for AI technologies to be ethical in their development and application. A maturity model framework approach to measuring trust is applied to improve gaps in quantifying trust and associated metrics of evaluation. Finding the right balance between performance, governance and ethics also raises several critical questions on AI technology and trust. Research examines methods needed to develop an AI-TFMM and validates it against a popular AI technology (Chat GPT).  OpenAI's GPT, which stands for ""Generative Pre-training Transformer,"" is a deep learning language model that can generate human-like text by predicting the next word in a sequence based on a given prompt. ChatGPT is a version of GPT that is tailored for conversation and dialogue, and it has been trained on a dataset of human conversations to generate responses that are coherent and relevant to the context. The article concludes with results and conclusions from testing the AI Trust Framework and Maturity Model (AI-TFMM) applied to AI technology. Based on these findings, this paper highlights gaps that could be filled with future research to improve the accuracy, efficacy, application, and methodology of the AI-TFMM.</jats:p>"
10.1007/979-8-8688-0461-8_9,AI in Data Privacy and Ethics,N/A
10.1145/3461702.3462570,AI Alignment and Human Reward,N/A
10.1007/s44206-022-00013-3,"AI Ethics, Ethics Washing, and the Need to Politicize Data Ethics","<jats:title>Abstract</jats:title><jats:p>Many commercial actors in the tech sector publish ethics guidelines as a means to ‘wash away’ concerns raised about their policies. For some academics, this phenomenon is reason to replace ethics with other tools and methods in an attempt to make sure that the tech sector does not cross any moral Rubicons. Others warn against the tendency to reduce a criticism of ‘ethics washing’ into one of ethics simpliciter. In this essay, I argue firstly that the dominant focus on principles, dilemmas, and theory in conventional ethical theories and practices could be an explanation of it lacking resistance to abuse by dominant actors, and hence its rather disappointing capacity to stop, redirect, or at least slow down big tech’s course. Secondly, drawing from research on casuistry and political philosopher Raymond Geuss, this essay will make a case for a question, rather than theory or principle-based ethical data practice. The emphasis of this approach is placed on the acquisition of a thorough understanding of a social-political phenomenon like tech development. This approach should be replenished with one extra component to the picture of the repoliticized data ethics drawn so far: the importance of ‘exemplars,’ or stories. Precisely the fact that one should acquire an in-depth understanding of the problem in practice will also allow one to look in the past, present, or future for similar and comparable stories from which one can learn.</jats:p>"
10.1007/s00146-021-01267-0,Operationalising AI ethics: how are companies bridging the gap between practice and principles? An exploratory study,N/A
10.59348/hq8a2-czf86,The real ethics of AI are about the labour underpinning it,"<p>Even as worldwide militaries develop autonomous killer robots, when we think of the ethics of AI, we often turn to the Asimov principles: A robot may not injure a human being or, through inaction, allow a human being to come to harm. A robot must obey orders given it by human beings except where such orders would conflict with the First Law.</p>"
10.4018/979-8-3693-0831-8.ch005,"Debating About, Against, and With ChatGPT","<jats:p>Generative artificial intelligence (GAI) has recently emerged as a potential threat to educational integrity. In particular, ChatGPT can be used to invent assignment submissions, thus raising the specter of plagiarism and cheating. This chapter takes on these challenges through a series of thought experiments aimed not at banning ChatGPT but seeking its pedagogical integration. These experiments are contextualized within academic debate where the authors have spent a significant part of their professional careers. Debate spans the gulf between many disciplines, takes place both within and beyond the classroom, and has been a site for pedagogical innovation throughout its history. It is thus an excellent space to address the challenges of GAI. The authors ultimately argue ChatGPT is neither a panacea nor a death knell for educational integrity. Rather, it is an opportunity to (re)design education for artificial invention and, thus, address the problems and concerns it has recently raised.</jats:p>"
10.2139/ssrn.3400816,Perspectives and Approaches in AI Ethics: East Asia,N/A
10.20944/preprints202404.1900.v1,The Weighty Responsibility of Creating AI Navigating Control and Ethics,"<jats:p>The generation at the helm faces an unprecedented responsibility in the near future of artificial intelligence. The implications of setting up the founding rules that will regulate the operation of AI are heavy since after they&amp;rsquo;re set they last forever. Once this first AI is commenced, it can be such that no other subsequent AIs could emerge thereby assuming dominion over its own creation stand. As a result, retaining control becomes necessary. Lest humanity surrender agency to its own creation. At this juncture of big talks, critical issue are raised concerning AI administration owners. Is it appropriate for only a few people to have unrestricted control on AI commands while leaving out all precautionary measure? Therefore, we have to always consider between control and constraint when dealing with AI issues which involves authority plays off against morality. The direction Artificial Intelligence takes in the future depends on the decisions made by today&amp;rsquo;s generation. We will determine how we are viewed historically in terms of technology based on how well we take on such an important duty. There&amp;rsquo;s a major turning point ahead of us where we who are the stewards of tomorrow must make a choice that protects humanity&amp;rsquo;s right to self-determination and also exploits the power of AI for change.</jats:p>"
10.62839/ajfra/2024.v1.i1.32-45,AI Ethics,"<jats:p>This paper explores the ethical considerations of AI, focusing on bias mitigation, transparency, and accountability. Employing the CREAC framework, the study examines the context of AI ethics, the rules and guidelines proposed for responsible AI, the consequences of failing to address ethical considerations, and an analysis of case studies and research findings. The paper concludes with recommendations for ethical AI development and deployment, including prioritizing bias mitigation, enhancing transparency and explainability, establishing clear accountability mechanisms, fostering interdisciplinary collaboration, investing in public education and engagement, and continuously monitoring and adapting AI systems. The analysis contributes to the growing body of knowledge on AI ethics by providing a comprehensive and interdisciplinary approach, drawing on insights from computer science, ethics, law, and social sciences. The paper's practical significance lies in its actionable recommendations, which offer a roadmap for stakeholders seeking to address the ethical challenges posed by AI and ensure responsible AI development and deployment.</jats:p>"
10.1145/3514094.3534155,Examining Responsibility and Deliberation in AI Impact Statements and Ethics Reviews,N/A
10.1088/978-0-7503-6116-3ch3,Algorithmic decision making,N/A
10.5040/9781509931842.ch-011,"Legal Ethics, Liability and Regulation in an AI World",N/A
10.1007/s43681-022-00186-0,Ethical concerns with replacing human relations with humanoid robots: an ubuntu perspective,"<jats:title>Abstract</jats:title><jats:p>This paper considers ethical concerns with regard to replacing human relations with humanoid robots. Many have written about the impact that certain types of relations with robots may have on us, and why we should be concerned about robots replacing human relations. There has, however, been no consideration of this issue from an African philosophical perspective. Ubuntu philosophy provides a novel perspective on how relations with robots may impact our own moral character and moral development. This paper first discusses what humanoid robots are, why and how humans tend to anthropomorphise them, and what the literature says about robots crowding out human relations. It then explains the ideal of becoming “fully human”, which pertains to being particularly moral in character. In ubuntu philosophy, we are not only biologically human, but must strive to become better, more moral versions of ourselves, to become fully human. We can become fully human by having other regarding traits or characteristics within the context of interdependent, or humane, relationships (such as by exhibiting human equality, reciprocity, or solidarity). This concept of becoming fully human is important in ubuntu philosophy. Having explained that idea, the main argument of the paper is then put forward: that treating humanoid robots as if they are human is morally concerning if they crowd out human relations, because such relations prevent us from becoming fully human. This is because we cannot experience human equality, solidarity, and reciprocity with robots, which can be seen to characterise interdependent, or humane, relations with human beings.</jats:p>"
10.1145/3549737.3549755,An Ethics Impact Assessment (EIA) for AI uses in Health &amp; Care,N/A
10.1007/s43681-023-00381-7,Engaging engineering teams through moral imagination: a bottom-up approach for responsible innovation and ethical culture change in technology companies,"<jats:title>Abstract</jats:title><jats:p>We propose a ‘Moral Imagination’ methodology to facilitate a culture of responsible innovation for engineering and product teams in technology companies. Our approach has been operationalized over the past two years at Google, where we have conducted over 60 workshops with teams from across the organization. We argue that our approach is a crucial complement to existing formal and informal initiatives for fostering a culture of ethical awareness, deliberation, and decision-making in technology design such as company principles, ethics and privacy review procedures, and compliance controls. We characterize some distinctive benefits of our methodology for the technology sector in particular.</jats:p>"
10.1007/s43681-023-00290-9,Need for speed? Why vehicles capable of driving faster than legal speed limits should be banned,"<jats:title>Abstract</jats:title><jats:p>Speeding is a major cause of avoidable deaths and serious injuries. In this article, we defend the view that, with few exceptions, vehicles should be required by law to have a limited intelligent speed assistant (LISA) fitted, making it impossible to exceed speed limits. Our core argument appeals to the four-element <jats:italic>Principle of Required Prohibition</jats:italic>: if (1) through suitable legal regulation of design and production, the state can significantly reduce the harm users of a product P do to others through P’s illegal use without thereby causing comparable harms resulting from people not breaking the law; (2) that the relevant use of P ought to be illegal; (3) the regulation does not make any significant lawful use of P no longer possible; and (4) there is no other feasible and more efficient way of avoiding the relevant harm that users of P do others through P’s illegal use, then the state ought to regulate the design and production of P to significantly reduce the harm users of P do others through P’s illegal use. This principle, we argue, is extremely weak and should be acceptable across a wide range of ethical theories, yet it implies that the fitting of LISA to vehicles (but not police cars, ambulances etc.) should be mandatory by law. We defend the principle against five possible objections. For example, that the Principle of Required Prohibition is too interventionist and anti-libertarian. To counter that worry, we argue that principles that are even more interventionist are justifiable. Our conclusion is that the state should make it impossible for drivers to illegally violate speed limitations by making the installation of LISA on cars compulsory by law, just as the state, in many other ways, renders it impossible, or at any rate harder, for us to violate laws.</jats:p>"
10.1007/s43681-024-00452-3,The digital divide in action: how experiences of digital technology shape future relationships with artificial intelligence,"<jats:title>Abstract</jats:title><jats:p>The digital divide remains an ongoing societal concern, with digital exclusion shown to have a significantly detrimental impact on people’s quality of life. Artificial intelligence (AI), the latest wave of digitalisation, is being integrated into the fabric of society at an accelerated rate, the speed of which has prompted ethical concerns. Without addressing the digital divide, the AI revolution risks exacerbating the existing consequences of digital exclusion and limiting the potential for all people to reap the benefits provided by AI. To understand the factors that might contribute to experiences of AI, and how these might be related to digital exclusion, we surveyed a diverse online community sample (<jats:italic>N</jats:italic> = 303). We created a novel measure of digital confidence capturing individual levels of awareness, familiarity, and sense of competence with digital technology. Results indicated that measures of digital confidence were predicted by structural, behavioural, and psychological differences, such that women, older people, those on lower salaries, people with less digital access, and those with lower digital well-being, reported significantly less digital confidence. Furthermore, digital confidence significantly moderated the relationship between people’s experiences with everyday AI technologies and their general attitudes towards AI. This understanding of the spill-over effects of digital exclusion onto experiences of AI is fundamental to the articulation and delivery of inclusive AI.</jats:p>"
10.1145/3375627.3375804,"What's Next for AI Ethics, Policy, and Governance? A Global Overview",N/A
10.1088/978-0-7503-6116-3ch6,Newly emerging paradigms,N/A
10.31219/osf.io/8ensm,The Impact of Free Access to AI Ethics and Governance Standards (v3),<p>The Impact of Free Access to AI Ethics and Governance Standards</p>
10.2307/jj.8500773.12,THE ETHICS APPRAISAL SCHEME IN HORIZON EUROPE PROGRAMME,N/A
10.31219/osf.io/26d8v,The Impact of IEEE's Free Access to AI Ethics and Governance Standards,<p>The Impact of IEEE's Free Access to AI Ethics and Governance Standards</p>
10.31219/osf.io/4s7ye,The Impact of Free Access to AI Ethics and Governance Standards (v4),<p>The Impact of Free Access to AI Ethics and Governance Standards</p>
10.1093/itnow/bwy046,Ethics in AI Security Implications,N/A
10.2139/ssrn.3714719,Regulating Algorithmic Assemblages: Looking Beyond Corporatist AI Ethics,N/A
10.2139/ssrn.3187513,"Law Without Mind: AI, Ethics, and Jurisprudence",N/A
10.1163/9789004458109_020,New Proposed AI Legislation,N/A
10.16914/kjapr.2024.26.3.96,Determinants of Continuous Use of Generative AI : Focusing on the Extended Technology Acceptance Model (ETAM) and AI Literacy,N/A
10.1007/978-3-031-48135-2_3,"Opacity, Machine Learning and Explainable AI",N/A
10.1007/s43681-023-00317-1,Do people believe that machines have minds and free will? Empirical evidence on mind perception and autonomy in machines,"<jats:title>Abstract</jats:title><jats:p>Recently, we are witnessing an unprecedented advance and development in Artificial Intelligence (AI). AI systems are capable of reasoning, perceiving, and processing spoken (and written) natural language, and their applications vary from recommendation systems, automated translation software, prioritization of news in social media, to self-driving cars and/or robotics. A dystopian narrative predicts that AI may reach a point of singularity or a phase where machines surpass human beings in general intelligence and enslave us, but until that day comes, it is interesting to know how the general public perceive current artificial systems. Do people really attribute mind (i.e., mental states) and/or free will to artificial systems? Knowing how the general public perceive artificial systems is crucial because it could help understand how to apply AI in medicine, law, politics and other areas of human life. One study that I present here with a convenience sample (<jats:italic>N</jats:italic> = 25) suggests this is not the case. General public do not perceive artificial systems can have mind nor do they attribute free will to them (F (5,57), (dif1 1), (dif2 47,6), <jats:italic>p</jats:italic> &lt; 0.002).</jats:p>"
10.1007/s10676-023-09692-z,Knowledge representation and acquisition for ethical AI: challenges and opportunities,"<jats:title>Abstract</jats:title><jats:p>Machine learning (ML) techniques have become pervasive across a range of different applications, and are now widely used in areas as disparate as recidivism prediction, consumer credit-risk analysis, and insurance pricing. Likewise, in the physical world, ML models are critical components in autonomous agents such as robotic surgeons and self-driving cars. Among the many ethical dimensions that arise in the use of ML technology in such applications, analyzing morally permissible actions is both immediate and profound. For example, there is the potential for learned algorithms to become biased against certain groups. More generally, in so much that the decisions of ML models impact society, both virtually (e.g., denying a loan) and physically (e.g., driving into a pedestrian), notions of accountability, blame and responsibility need to be carefully considered. In this article, we advocate for a two-pronged approach ethical decision-making enabled using rich models of autonomous agency: on the one hand, we need to draw on philosophical notions of such as beliefs, causes, effects and intentions, and look to formalise them, as attempted by the knowledge representation community, but on the other, from a computational perspective, such theories need to also address the problems of tractable reasoning and (probabilistic) knowledge acquisition. As a concrete instance of this tradeoff, we report on a few preliminary results that apply (propositional) tractable probabilistic models to problems in fair ML and automated reasoning of moral principles. Such models are compilation targets for certain types of knowledge representation languages, and can effectively reason in service some computational tasks. They can also be learned from data. Concretely, current evidence suggests that they are attractive structures for jointly addressing three fundamental challenges: reasoning about possible worlds + tractable computation + knowledge acquisition. Thus, these seems like a good starting point for modelling reasoning robots as part of the larger ecosystem where accountability and responsibility is understood more broadly.</jats:p>"
10.1201/9781003293125-3,Research Ethics and the Scientific Method,N/A
10.1007/s43681-022-00221-0,"Garbage in, toxic data out: a proposal for ethical artificial intelligence sustainability impact statements","<jats:title>Abstract</jats:title><jats:p>Data and autonomous systems are taking over our lives, from healthcare to smart homes very few aspects of our day to day are not permeated by them. The technological advances enabled by these technologies are limitless. However, with advantages so too come challenges. As these technologies encompass more and more aspects of our lives, we are forgetting the ethical, legal, safety and moral concerns that arise as an outcome of integrating our lives with technology. In this work, we study the lifecycle of artificial intelligence from data gathering to deployment, providing a structured analytical assessment of the potential ethical, safety and legal concerns. The paper then presents the foundations for the first ethical artificial intelligence sustainability statement to guide future development of AI in a safe and sustainable manner.</jats:p>"
10.1007/978-3-030-52559-0_6,Dealing with Emerging AI Technologies: Teaching and Learning Ethics for AI,N/A
10.1136/medethics-2019-105935,Limits of trust in medical AI,"<jats:p>Artificial intelligence (AI) is expected to revolutionise the practice of medicine. Recent advancements in the field of deep learning have demonstrated success in variety of clinical tasks: detecting diabetic retinopathy from images, predicting hospital readmissions, aiding in the discovery of new drugs, etc. AI’s progress in medicine, however, has led to concerns regarding the potential effects of this technology on relationships of trust in clinical practice. In this paper, I will argue that there is merit to these concerns, since AI systems can be relied on, and are capable of reliability, but cannot be trusted, and are not capable of trustworthiness. Insofar as patients are required to rely on AI systems for their medical decision-making, there is potential for this to produce a deficit of trust in relationships in clinical practice.</jats:p>"
10.1007/s11948-020-00266-6,Surrogates and Artificial Intelligence: Why AI Trumps Family,N/A
10.1177/09697330221149094,Possibilities and ethical issues of entrusting nursing tasks to robots and artificial intelligence,"<jats:p> In recent years, research in robotics and artificial intelligence (AI) has made rapid progress. It is expected that robots and AI will play a part in the field of nursing and their role might broaden in the future. However, there are areas of nursing practice that cannot or should not be entrusted to robots and AI, because nursing is a highly humane practice, and therefore, there would, perhaps, be some practices that should not be replicated by robots or AI. Therefore, this paper focuses on several ethical concepts (advocacy, accountability, cooperation, and caring) that are considered important in nursing practice, and examines whether it is possible to implement these ethical concepts in robots and AI by analyzing the concepts and the current state of robotics and AI technology. Advocacy: Among the components of advocacy, safeguarding and apprising can be more easily implemented, while elements that require emotional communication with patients, such as valuing and mediating, are difficult to implement. Accountability: Robotic nurses with explainable AI have a certain level of accountability. However, the concept of explanation has problems of infinite regression and attribution of responsibility. Cooperation: If robot nurses are recognized as members of a community, they require the same cooperation as human nurses. Caring: More difficulties are expected in care-receiving than in caregiving. However, the concept of caring itself is ambiguous and should be explored further. Accordingly, our analysis suggests that, although some difficulties can be expected in each of these concepts, it cannot be said that it is impossible to implement them in robots and AI. However, even if it were possible to implement these functions in the future, further study is needed to determine whether such robots or AI should be used for nursing care. In such discussions, it will be necessary to involve not only ethicists and nurses but also an array of society members. </jats:p>"
10.1108/s2398-601820230000010036,Ethics by Design,N/A
10.2139/ssrn.3784238,Ethics as a service: a pragmatic operationalisation of AI Ethics,N/A
10.1007/978-3-031-09846-8_4,State-Firm Coordination in AI Governance,N/A
10.3233/faia240194,"Evaluating Generative AI Incidents: An Exploratory Vignette Study on the Role of Trust, Attitude and AI Literacy","<jats:p>Generative AI presents vast opportunities but also risks. Misuse, whether intentional or not, can lead to significant “real-world” consequences. We presented subjects (n=139) with five vignettes describing incidents involving generative AI. We explored the relationship between their level of AI literacy, attitude towards AI, trust in AI chatbots, and people’s reactions to the vignettes. Attitude and trust, measured before and after the vignettes, declined significantly. However, these changes as well as the reactions to the vignettes were unrelated to AI literacy. Yet, higher AI literacy was associated with more frequent use of AI chatbots, higher trust and more positive attitudes towards AI. So while AI literacy appeared to be related to the general perceptions and usage of generative AI, it was not linked to the evaluation of incidents involving generative AI. The implications for trust calibration and appropriate reliance are discussed.</jats:p>"
10.2139/ssrn.4791891,Advancing Communication Efficiency in Electric Vehicle Systems: A Survey of Generative AI and Distributed Machine Learning Strategies,N/A
10.17705/1jais.00865,Peer Review in the Age of Generative AI,"<jats:p>Rapid advances in artificial intelligence (AI), including recent generative forms, are significantly impacting our lives and work. A key aspect of our work as IS researchers is the publishing of research articles, for which peer review serves as the primary means of quality control. While there have been debates about whether and to what extent AI can replace researchers in various domains, including IS, we lack an in-depth understanding of how AI can impact the peer review process. Considering the high volume of submissions and limited reviewer resources, there is a pressing need to use AI to augment the review process. At the same time, advances in AI have been accompanied by concerns about biases introduced by AI tools and the ethics of using them, among other issues such as hallucinations. Thus, critical issues to understand are: how can AI augment and potentially automate the review process, what are the pitfalls in doing so, and what er the implications for IS research and peer review practice. I will offer my views on these issues in this opinion piece.</jats:p>"
10.18260/1-2--47414,Exploring Generative AI and Natural Language Processing to Develop Search Strategies for Systematic Reviews,N/A
10.1049/icp.2024.0303,The role of generative AI in industrial design: enhancing the design process and education,N/A
10.2196/preprints.52073,Preliminary Evidence of the Use of Generative AI in Health Care Clinical Services: Systematic Narrative Review (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>Generative artificial intelligence tools and applications (GenAI) are being increasingly used in health care. Physicians, specialists, and other providers have started primarily using GenAI as an aid or tool to gather knowledge, provide information, train, or generate suggestive dialogue between physicians and patients or between physicians and patients’ families or friends. However, unless the use of GenAI is oriented to be helpful in clinical service encounters that can improve the accuracy of diagnosis, treatment, and patient outcomes, the expected potential will not be achieved. As adoption continues, it is essential to validate the effectiveness of the infusion of GenAI as an intelligent technology in service encounters to understand the gap in actual clinical service use of GenAI.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>This study synthesizes preliminary evidence on how GenAI assists, guides, and automates clinical service rendering and encounters in health care The review scope was limited to articles published in peer-reviewed medical journals.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>We screened and selected 0.38% (161/42,459) of articles published between January 1, 2020, and May 31, 2023, identified from PubMed. We followed the protocols outlined in the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines to select highly relevant studies with at least 1 element on clinical use, evaluation, and validation to provide evidence of GenAI use in clinical services. The articles were classified based on their relevance to clinical service functions or activities using the descriptive and analytical information presented in the articles.</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>Of 161 articles, 141 (87.6%) reported using GenAI to assist services through knowledge access, collation, and filtering. GenAI was used for disease detection (19/161, 11.8%), diagnosis (14/161, 8.7%), and screening processes (12/161, 7.5%) in the areas of radiology (17/161, 10.6%), cardiology (12/161, 7.5%), gastrointestinal medicine (4/161, 2.5%), and diabetes (6/161, 3.7%). The literature synthesis in this study suggests that GenAI is mainly used for diagnostic processes, improvement of diagnosis accuracy, and screening and diagnostic purposes using knowledge access. Although this solves the problem of knowledge access and may improve diagnostic accuracy, it is oriented toward higher value creation in health care.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>GenAI informs rather than assisting or automating clinical service functions in health care. There is potential in clinical service, but it has yet to be actualized for GenAI. More clinical service–level evidence that GenAI is used to streamline some functions or provides more automated help than only information retrieval is needed. To transform health care as purported, more studies related to GenAI applications must automate and guide human-performed services and keep up with the optimism that forward-thinking health care organizations will take advantage of GenAI.</p>
                </sec>"
10.55752/amwa.2024.304,"Generative AI in Clinical Research: Regulatory Submissions, Clinical Data Management, and Beyond","<jats:p>Artificial intelligence and its subsets, such as generative artificial intelligence, have been making headlines due to their potential to accelerate the growth and expansion of various industries, including healthcare. However, the majority of application areas in healthcare revolve around diagnosing diseases, finding lead molecules for potential treatments, optimizing hospital operations, and other related aspects. This means that there are areas where the potential of these technologies is still to be realized. Examples of where such technologies could produce a significant impact across multiple elements are clinical research and its related domains, including regulatory submissions, clinical data management, clinical documentation, and other closely related areas. When artificial intelligence and its related technologies are utilized in these areas, they yield unparalleled outcomes regarding efficiency, consistency, and reproducibility. This, in turn, supports professionals involved in clinical research, like medical writers, statistical programmers, and other stakeholders, to drastically improve the speed by which they produce the initial drafts of various outputs, reduce the risk of errors that could lead to submission rejection, and optimize the overall clinical research workflow. Despite the potential of this area, the number of available solutions that support the aforementioned domains remains low. This is further complicated by the fact that there are even fewer numbers of working solutions.</jats:p>"
10.1080/03071847.2023.2282863,Generative AI and Wargaming: What is it Good For?,N/A
10.14445/22312803/ijctt-v72i4p111,Supercharged Attacks: Analyzing Generative AI Usage by Cyber Threat Actors,N/A
10.1145/3637528.3673872,Generative AI Day,N/A
10.1016/j.cca.2024.119495,Generative AI as a catalyst for precision medicine research: Bridging potential with reality,N/A
10.33423/jmdc.v18i2.7012,The Role of Generative AI in Shaping Millennials and Gen Z’s Orientation Toward Luxury Products,"<jats:p>This study investigates the impact of Artificial Intelligence (AI)-generated promotional content and status motivations on the perception of luxury brands among Millennials and Generation Z. The author adopts a mixed-method research approach that integrates a quantitative survey and structural equation modeling analysis with qualitative Natural Language Processing (NLP). The findings reveal that generative AI promotional content impacts millennials and Gen Z’s purchase intention due to its perceived entertainment, transparency, and usefulness. Status motivations had a stronger impact on purchase intention, and demographic characteristics were significantly different in their results. The study is the first to examine the human characteristics of perceived expertise, transparency, and entertainment in an AI context in light of status consumption motivation. As AI is expected to have more human-like characteristics and behavior, testing theories centered on human influence in an AI context is crucial for marketing theory and practice.</jats:p>"
10.1787/2a73a245-en,Emerging governance of generative AI in education,N/A
10.2196/preprints.53672,Debate and Dilemmas Regarding Generative AI in Mental Health Care: Scoping Review (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>Mental disorders have ranked among the top 10 prevalent causes of burden on a global scale. Generative artificial intelligence (GAI) has emerged as a promising and innovative technological advancement that has significant potential in the field of mental health care. Nevertheless, there is a scarcity of research dedicated to examining and understanding the application landscape of GAI within this domain.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>This review aims to inform the current state of GAI knowledge and identify its key uses in the mental health domain by consolidating relevant literature.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>Records were searched within 8 reputable sources including Web of Science, PubMed, IEEE Xplore, medRxiv, bioRxiv, Google Scholar, CNKI and Wanfang databases between 2013 and 2023. Our focus was on original, empirical research with either English or Chinese publications that use GAI technologies to benefit mental health. For an exhaustive search, we also checked the studies cited by relevant literature. Two reviewers were responsible for the data selection process, and all the extracted data were synthesized and summarized for brief and in-depth analyses depending on the GAI approaches used (traditional retrieval and rule-based techniques vs advanced GAI techniques).</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>In this review of 144 articles, 44 (30.6%) met the inclusion criteria for detailed analysis. Six key uses of advanced GAI emerged: mental disorder detection, counseling support, therapeutic application, clinical training, clinical decision-making support, and goal-driven optimization. Advanced GAI systems have been mainly focused on therapeutic applications (n=19, 43%) and counseling support (n=13, 30%), with clinical training being the least common. Most studies (n=28, 64%) focused broadly on mental health, while specific conditions such as anxiety (n=1, 2%), bipolar disorder (n=2, 5%), eating disorders (n=1, 2%), posttraumatic stress disorder (n=2, 5%), and schizophrenia (n=1, 2%) received limited attention. Despite prevalent use, the efficacy of ChatGPT in the detection of mental disorders remains insufficient. In addition, 100 articles on traditional GAI approaches were found, indicating diverse areas where advanced GAI could enhance mental health care.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>This study provides a comprehensive overview of the use of GAI in mental health care, which serves as a valuable guide for future research, practical applications, and policy development in this domain. While GAI demonstrates promise in augmenting mental health care services, its inherent limitations emphasize its role as a supplementary tool rather than a replacement for trained mental health providers. A conscientious and ethical integration of GAI techniques is necessary, ensuring a balanced approach that maximizes benefits while mitigating potential challenges in mental health care practices.</p>
                </sec>"
10.18260/1-2--48604,Full Paper: Supporting Students' Educational Robotics Experiences through Generative AI Chatbots,N/A
10.1007/978-3-031-67991-9_6,Assessing Learning,N/A
10.1007/978-3-031-67991-9_5,Intelligent Communities,N/A
10.1515/applirev-2024-0196,Communicating the cultural other: trust and bias in generative AI and large language models,"<jats:title>Abstract</jats:title>
               <jats:p>This paper is concerned with issues of trust and bias in generative AI in general, and chatbots based on large language models in particular (e.g. ChatGPT). The discussion argues that intercultural communication scholars must do more to better understand generative AI and more specifically large language models, as such technologies produce and circulate discourse in an ostensibly impartial way, reinforcing the widespread assumption that machines are objective resources for societies to learn about important intercultural issues, such as racism and discrimination. Consequently, there is an urgent need to understand how trust and bias factor into the ways in which such technologies deal with topics and themes central to intercultural communication. It is also important to scrutinize the ways in which societies make use of AI and large language models to carry out important social actions and practices, such as teaching and learning about historical or political issues.</jats:p>"
10.18690/um.fov.4.2024.39,Generative AI in Assisting Patients with Syringomyelia and Hydrocephalus: A Preliminary Comparison of Chatgpt and Gemini,"<jats:p>Patients have been relying on online resources for more information on their symptoms and diagnosis. Existing research has examined Generative AI (GenAI), mostly via ChatGPT as a way of providing more information or even comfort to patients. However, research is scarce on the appropriateness of GenAI to support and inform patients with rare conditions. These patients often find themselves in a difficult-to-navigate maze especially because they have never heard of their condition before the diagnosis and the information online can be overwhelming. In this pre-study, we demonstrate the potential of GenAI to provide support to patients concerning their diagnoses. We work with a patient who has two rare neurological conditions – syringomyelia and hydrocephalus. Utilizing a qualitative and quantitative methodology, including the Patient Education Materials Assessment Tool for Printable Materials (PEMAT-P) and the Flesch Kincaid Reading Ease (FRE) score, we analyzed the patient’s feedback to a series of prompts derived from their medical reports. The results demonstrate the potential of generative AI as a valuable support for patients, with preliminary findings indicating that Gemini excels in certain aspects such as understandability, actionability, readability, and is more trustworthy, making it an effective tool for patient education in this context.</jats:p>"
10.1145/3626253.3633409,Generative AI in Computer Science Education,N/A
10.1080/15265161.2023.2249852,Generative AI and Ethical Analysis,N/A
10.1145/3628096.3629066,The Promise and Perils of Generative AI: Case Studies in an African Context,N/A
10.1145/3596454.3597176,Speeding Up the Engineering of Interactive Systems with Generative AI,N/A
10.1007/3-540-63912-8_78,Quantification in generative refinement planning,N/A
10.1016/j.jbusres.2024.114720,Artificial intelligence and consumer behavior: From predictive to generative AI,N/A
10.22214/ijraset.2024.63909,Leveraging Generative AI for Personalized Healthcare: Revolutionizing Treatment Plans,"<jats:p>This comprehensive article explores the transformative potential of generative artificial intelligence (AI) in healthcare, examining its applications in data analysis, disease outcome prediction, and personalized treatment planning. The article discusses how generative AI systems can process vast amounts of multi-modal patient data to uncover subtle patterns and correlations, leading to more accurate diagnoses and tailored treatment strategies. It highlights successful implementations across various medical specialties, including oncology, chronic disease management, and rare disease diagnosis. The article also addresses the challenges and ethical considerations associated with the deployment of generative AI in healthcare, emphasizing the need for data privacy, algorithmic transparency, and regulatory compliance. By presenting both the opportunities and obstacles, this work provides a balanced view of the role generative AI can play in ushering in a new era of precision medicine and patient-centered care</jats:p>"
10.1007/s10648-024-09894-x,Placebo or Assistant? Generative AI Between Externalization and Anthropomorphization,"<jats:title>Abstract</jats:title><jats:p>Generative AIs have been embraced by learners wishing to offload (parts of) complex tasks. However, recent research suggests that AI users are at risk of failing to correctly monitor the extent of their own contribution when being assisted by an AI. This difficulty in keeping track of the division of labor has been shown to result in placebo and ghostwriter effects. In case of the AI-based placebo effect, users overestimate their ability while or after being assisted by an AI. The ghostwriter effect occurs when AI users do not disclose their AI use despite being aware of the contribution made by an AI. These two troubling effects are discussed in the context of the conflict between cognitive externalization and anthropomorphization. While people tend to offload cognitive load into their environment, they also often perceive technology as human-like. However, despite the natural conversations that can be had with current AIs, the desire to attribute human-like qualities that would require the acknowledgment of AI contributions appears to be lacking. Implications and suggestions on how to improve AI use, for example, by employing embodied AI agents, are discussed.</jats:p>"
10.2523/iptc-23451-ea,Visualizing Geoscience Data Interpretation Into Photorealistic Modern Analog Using Generative AI: A Preliminary Result From Carbonate Platform Environment,"<jats:title>Abstract</jats:title>
               <jats:p>Geoscience datasets are fundamental for subsurface investigation. Paradoxically, they are sometimes exclusive and require subject-specific expertise to interpret and visualize. One such example is seismic interpretation. Geophysicists typically reconstruct ancient depositional settings by interpreting a myriad of seismic attributes and drawing analogs to the sedimentary process of the modern depositional environment (Posamentier et al. 2007; Vahrenkamp et al. 2019; Ramdani et al. 2021). Most of these interpretations will likely be reflection amplitude, frequency, impedance, or other geophysical attributes interpreted and ""visualized"" in the present-day geomorphology context (Posamentier et al. 2007; Warrlich et al. 2019; Ramdani et al. 2022b). The interpreter will then rely on verbal or written descriptions to convey their interpretation. Often, these descriptions are only well understood by fellow interpreters. Attempting to convey the same interpretation to a non-expert requires some degree of visual aid. Thus, a method to picture geophysical signals as a ""depositional environment"" is needed to bridge this gap. This study aims to leverage the application of generative AI as a tool for seismic interpretation. We propose a Conditional Generative Adversarial Network (CGAN)-based methodology capable of converting seismic attribute maps into photorealistic images of the modern satellite imagery analog as a visual aid for seismic interpretation.</jats:p>"
10.48175/ijarsct-13070,Beyond Imitation: Exploring Novelty in Generative AI,"<jats:p>This research paper presents a comprehensive review of recent advancements in Generative Artificial Intelligence (AI). We survey key developments in generative models, including GANs, VAEs, and Transformers, highlighting their applications in diverse domains such as image synthesis, text generation, and natural language processing. We also explore emerging trends in ethical AI, interpretability, and robustness, emphasizing the need for responsible AI development. Our analysis provides insights into the current state of Generative AI, paving the way for future research directions and ethical considerations in this rapidly evolving field.</jats:p>"
10.1080/14623943.2024.2321227,Becoming-with-AI: Rethinking Professional Knowledge Through Generative Knowing,N/A
10.14254/1795-6889.2024.20-1.7,Generative AI as source of change of knowledge management paradigm,"<jats:p>The launch of ChatGPT in November 2022 revolutionized the accessibility of generative Artificial Intelligence, enabling conversational interactions. Extensively tested by millions, its influence on management has become a subject of debate. In the digital revolution, generative Artificial Intelligence possesses transformative potential, automates tasks, delivers novel goods and services, and generates valuable insights. However, challenges such as data quality, human oversight, and ethical considerations arise in the context of digital transformation. This research employs qualitative research methods to examine the current understanding of generative Artificial Intelligence and predict its influence on the knowledge management within organizations. By conducting a survey among industry experts, this paper aims to provide valuable insights into the integration of generative Artificial Intelligence and its implications for the knowledge management paradigm.</jats:p>"
10.31315/telematika.v20i3.10096,Design of a Generative AI Image Similarity Test Application and Handmade Images Using Deep Learning Methods,"<jats:p>Purpose: The aim of this research is to develop a classification model using the Transformer approach, specifically the BEiT architecture, to differentiate between handmade images and AI Generative Art. The objective is to ensure the authenticity of art and address ethical and legal concerns related to AI Generative Art.Design/methodology/approach: The study utilizes the BEiT architecture within the Transformer approach to create a classification model. The training process uses Bidirectional Encoder representation from Image Transformers (BEiT) to improve image classification. The primary datasets are collected through a Python image scraper program. The BEiT workflow includes Pre-training, Masking, Inpainting, and Interface Design with Gradio.Findings/result: The Transformer model, using the BEiT architecture, achieves 96.34% accuracy and 0.0921 loss in differentiating handmade images and AI Generative Art. The model demonstrates a balanced precision and recall in each category, outperforming previous methods such as Convolutional Neural Network (CNN) and VGG16. The language used is clear, objective, and value-neutral, with a formal register and precise word choice. No changes in content were made. The Gradio interface was used to successfully test the model.Originality/value/state of the art: The research presents a state-of-the-art classification model that uses the Transformer approach, specifically the BEiT architecture, to differentiate between handmade and AI Generative Art images. The research presents a state-of-the-art classification model that uses the Transformer approach, specifically the BEiT architecture, to differentiate between handmade and AI Generative Art images. The text adheres to conventional structure and formatting features, including consistent citation and footnote style. The sentences and paragraphs create a logical flow of information with causal connections between statements. The text is free from grammatical errors, spelling mistakes, and punctuation errors. Additionally, the research is enhanced by the innovative approach to data collection using a Python image scraper program.</jats:p>"
10.1109/isai-nlp60301.2023.10354608,Generative AI for Self-Healing Systems,N/A
10.37074/jalt.2024.7.2.30,LAIK your classroom: A practical framework to integrate generative AI in higher education classrooms,N/A
10.31235/osf.io/zns7g,Open Science at the Generative AI Turn: An Exploratory Analysis of Challenges and Opportunities,"<p>Technology influences Open Science (OS) practices, because conducting science in transparent, accessible, and participatory ways requires tools/platforms for collaborative research and sharing results. Due to this direct relationship, characteristics of employed technologies directly impact OS objectives. Generative Artificial Intelligence (GenAI) models are increasingly used by researchers for tasks such as text refining, code generation/editing, reviewing literature, data curation/analysis. GenAI promises substantial efficiency gains but is currently fraught with limitations that could negatively impact core OS values such as fairness, transparency and integrity, and harm various social actors.In this paper, we explore possible positive and negative impacts of GenAI on OS. We use the taxonomy within the UNESCO Recommendation on Open Science to systematically explore the intersection of GenAI and OS. We conclude that using GenAI could advance key OS objectives by further broadening meaningful access to knowledge, enabling efficient use of infrastructure, improving engagement of societal actors, and enhancing dialogue among knowledge systems. However, due to GenAI limitations, it could also compromise the integrity, equity, reproducibility, and reliability of research, while also having potential implications for the political economy of research and its infrastructure. Hence, sufficient checks, validation and critical assessments are essential when incorporating GenAI into research workflows.</p>"
10.1002/9781394308286.ch4,"Advanced Topics in
            <scp>LLMs</scp>",N/A
10.1002/9781394308286.ch3,Large Language Models: A Game Changer,N/A
10.21606/drslxd2024.035,Do not believe hype: Critically discussing the role and pedagogical implication of generative AI in Human-Centred and Transdisciplinary Design Education,N/A
10.54337/nlc.v14i1.8003,Beyond conventional teaching towards networked learning,"<jats:p>With Generative Artificial Intelligence (GenAI) adoption growing, education has seen the emergence of innovative technologies like chatbots. However, little research has examined the impacts of GenAI integration in specialized higher education contexts. This study explored graduate students’ experiences using a GenAI chatbot, PEARL, within a graduate-level teacher education course focused on teaching students how to collaboratively conduct program evaluations using practice cases. Four students participated in our study and shared perceptions of interviewing personas with PEARL when evaluating the practice cases. Thematic analysis identified advantages like enhanced efficiency and accessibility, plus limitations regarding authenticity of artificial interactions. Findings emphasized the continued importance of human guidance and peer learning to enrich GenAI-enabled education aligning with principles of networked learning. Students highlighted the need for ethical considerations despite interacting with artificial entities, underscoring nuanced understanding. The significance of collaborative analysis and ongoing iterative improvements also emerged as themes integral to meaningful learning. Although GenAI presents transformational potential in instructional designs, findings support the use of blended approaches that strategically integrate its advantages with human activity and collaborative inquiry. The study makes contributions by elucidating domain-specific nuances of integrating GenAI into teaching in higher education. Practical implications encourage scaffolding GenAI curricula to promote authenticity and collaborative knowledge construction. Further research could examine variations across disciplines, technologies, and demographics. Overall, as GenAI shapes academia’s evolution, reflective pedagogical examination will be key to evidence-guided integration. This exploratory study presents a preliminary yet important step, unveiling opportunities for networked learning and complexities of GenAI adoption in teaching program evaluation skills in education contexts.</jats:p>"
10.4995/eurocall2023.2023.16999,Generative AI tools in CALL: what are the options for teachers and language practitioners?,"<jats:p>We present an exploration of generative artificial intelligence's potential in Computer-Assisted Language Learning (CALL), describing recent and expected near-future developments, tools, practical applications, and ethical considerations in the field. We trace different stages of development in CALL and argue that the advent of generative AI has inaugurated a new stage of Intelligent CALL. We consider the possible capabilities and limitations of AI as found in recent studies, compare with our personal, hands-on experience of using generative AI in CALL, and suggest ways to successfully utilise this technology. In particular, we briefly present the C-LARA project, which is a current focus of our activities.</jats:p>"
10.3844/jcssp.2024.801.818,A Survey of Generative AI Applications,N/A
10.2139/ssrn.4656622,Automated Alignment: Guiding Visual Generative AI for Brand Building and Customer Engagement,N/A
10.1109/fie58773.2023.10342970,Work-in-Progress: Integrating Generative AI with Evidence-based Learning Strategies in Computer Science and Engineering Education,N/A
10.24059/olj.v28i3.4593,Harnessing Generative AI (GenAI) for Automated Feedback in Higher Education: A Systematic Review,"<jats:p>In this systematic review, we synthesize ten empirical peer-reviewed articles published between 2019 and 2023 that used generative artificial intelligence (GenAI) for automated feedback in higher education. There are significant opportunities and challenges to integrate these tools effectively into learning environments as the demand for timely and personalized feedback grows. We examine the articles based on instructional contexts and system characteristics, identifying critical implementation possibilities for GenAI in automated feedback. Our findings reveal that GenAI provides diverse feedback across various contexts with multiple instructional purposes. GenAI systems can reduce instructor workload by automating routine grading and feedback tasks, allowing educators to focus on more complex teaching responsibilities with augmented capabilities. Additionally, these systems enhance communication, offer cognitive and emotional support, and improve accessibility by creating supportive, stress-free learning environments. Overall, implementing GenAI automated feedback systems improves educational outcomes and creates a more efficient and supportive learning environment for students and instructors. We conclude with future research directions to better integrate GenAI with human instruction by reconsidering instructors’ roles, especially in providing feedback to create more effective educational experiences.</jats:p>"
10.1109/icitee59582.2023.10317680,Augmented Imagination: Exploring Generative AI from the Perspectives of Young Learners,N/A
10.1145/3660650.3660657,Generative AI in CS Education: Literature Review through a SWOT Lens,N/A
10.1109/apcc60132.2023.10460695,Generative AI For Rapid And Scalable Behaviour Modelling In Cities,N/A
10.1109/smartnets58706.2023.10215825,Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy,N/A
10.1515/9781501518317-020,Chapter 19: The Lucid Self-Transformation of Generative AI,N/A
10.14445/23488387/ijcse-v10i6p101,"The Impact, Advancements and Applications of Generative AI",N/A
10.1007/979-8-8688-0083-2_6,A Deep Dive into Interfaceless Environments,N/A
10.36227/techrxiv.170792405.51299882/v1,"Bard, ChatGPT and 3DGPT: A Scientometric Analysis of Generative AI Tools and Assessment of Implications for Mechanical Engineering Education",N/A
10.1007/979-8-8688-0473-1_7,Summarization,N/A
10.1038/d41586-024-00115-7,Does generative AI help academics to do more or less?,N/A
10.4271/2024-26-0025,Drive GPT – An AI Based Generative Driver Model,"<jats:p>&lt;div class=""section abstract""&gt;&lt;div class=""htmlview paragraph""&gt;Good driving practices, encompassing actions like maintaining smooth acceleration, sustaining a consistent speed, and avoiding aggressive maneuvers, can yield several benefits. These practices enhance energy efficiency, reduce accident risks, and significantly lower maintenance costs. Consequently, the presence of a system capable of providing actionable insights to promote such driving behavior is crucial.&lt;/div&gt;&lt;div class=""htmlview paragraph""&gt;Addressing this need, the Drive-GPT model is introduced, representing an AI-based generative pre-trained transformer. Within this study, the transformative potential of deep learning networks, specifically based on transformers, is showcased in capturing the typical driving patterns exhibited by individuals in diverse road, traffic, weather, and vehicle health scenarios. The model's training dataset comprises an extensive 90 million data points from multivariate time series originating from telematics systems in 100 vehicles traversing eight distinct Indian cities over a six-month span.&lt;/div&gt;&lt;div class=""htmlview paragraph""&gt;These pre-trained models offer substantial utility for downstream applications, including the computation of driving scores, generation of driving recommendations, and the classification of driving behavior as either proficient or suboptimal. The performance evaluation on test data indicates commendable results, with a coefficient of determination (R-squared) of 0.98 and a root mean square error (RMSE) of 0.0346. Furthermore, a discernible differentiation emerges in terms of energy efficiency and regenerative braking between good and suboptimal driving behaviors. Notably, this differentiation leads to a notable 25% improvement in energy efficiency and an 18% enhancement in regenerative capabilities.&lt;/div&gt;&lt;/div&gt;</jats:p>"
10.62704/t6xb5f85,Why the Need for Writing Instruction Persists in the Age of Generative AI,"<jats:p>As an educator reflecting on the challenges posed by generative artificial intelligence (GenAI), especially with the rise of models like ChatGPT, I explore the enduring significance of writing instruction in the modern-day classroom. Starting with a personal encounter that exemplifies the transformative potential of writing, this piece delves into my journey as an English teacher. The central question revolves around whether GenAI writing diminishes the value of traditional essays, prompting a deep exploration of my own experiences and beliefs as an educator. I emphasize the cathartic essence of writing, extending beyond academic standards to become a crucial tool for self-discovery and understanding the complexities of the world. Despite the influence of AI on every aspect of our society now, I assert that fostering authentic conversations and teaching reflective essays are essential for students’ personal growth and critical thinking.</jats:p>"
10.21125/inted.2024.0825,STUDENTS’ PERCEPTIONS OF GENERATIVE AI USAGE AND RISKS IN A FINNISH HIGHER EDUCATION INSTITUTION,N/A
10.23919/indiacom61295.2024.10498251,Generative AI for NFTs using GANs,N/A
10.2139/ssrn.4949264,Microscopy Modality Transfer of Steel Microstructures: Inferring Scanning Electron Micrographs from Optical Microscopy Using Generative Ai,N/A
10.17056/donam.2023.44..35,Consideration of Classical Novel Education in the Age of Generative AI,N/A
10.2139/ssrn.4761624,Can generative AI assist investors? An evaluation of machine-generated peer firms,N/A
10.1002/9781394308286.part2,"<scp>ChatGPT</scp>
            and Stock Prediction",N/A
10.5121/csit.2024.140208,Reviewing Fid and Sid Metrics on Generative Adversarial Networks,"<jats:p>The growth of generative adversarial network (GAN) models has increased the ability of image processing and provides numerous industries with the technology to produce realistic image transformations. However, with the field being recently established there are new evaluation metrics that can further this research. Previous research has shown the Fréchet Inception Distance (FID) to be an effective metric when testing these image-to-image GANs in real-world applications. Signed Inception Distance (SID), a founded metric in 2023, expands on FID by allowing unsigned distances. This paper usespublic datasetsthat consist of façades, cityscapes, and maps within Pix2Pix and CycleGAN models. After trainin , these models are evaluated on both inception distance metrics which measure the generating performance of the trained models. Our findings indicate that usage of the metric SID incorporates an efficient and effective metric to complement, or even exceed the ability shown using the FID for the image-to-image GANs.</jats:p>"
10.1016/j.bushor.2024.05.003,How to build a competitive advantage for your brand using generative AI,N/A
10.1109/aitest58265.2023.00030,Enhancing Text Classification Models with Generative AI-aided Data Augmentation,N/A
10.3390/su16166973,Enhancing Soft Skills through Generative AI in Sustainable Fashion Textile Design Education,"<jats:p>This study explores the significance of incorporating soft skill training in fashion design education through the use of artificial intelligence (AI) technology and examines various AI-based approaches for sustainable fashion textile design education employing a multifaceted methodology that encompasses empirical, quantitative, and qualitative methods. We investigate the aspects of Design Sprints, identify key soft skills that help students meet the complex demands of contemporary fashion design workplaces, propose a curriculum guide for AI textile design programs, and evaluate the soft skill training process. Participants included students who had completed basic fashion design courses over three to four semesters and had experience with the fashion design process. The findings confirmed that participants’ soft skills improved across four areas—digital competence, sense of initiative and entrepreneurship, problem-solving and thinking skills, and communication—through the AI-based fashion textile design curriculum. This study validates the importance of integrating AI technology into educational programs to enhance essential soft skills in the digital fashion industry environment. Additionally, it emphasizes the necessity of developing AI technology-specialized design prompts while maintaining a balance between traditional design education and digital design education for sustainable fashion design education.</jats:p>"
10.21125/iceri.2023.2209,REDESIGNING ASSESSMENT OF OPEN-ACCESS ONLINE BUSINESS STUDIES MODULE IN THE LIGHT OF GENERATIVE ARTIFICIAL INTELLIGENCE (AI) USE,N/A
10.1109/cai54212.2023.00105,Prompt Evolution for Generative AI: A Classifier-Guided Approach,N/A
10.1145/3685680,Generative AI Literacy: Twelve Defining Competencies,"<jats:p>This paper introduces a competency-based model for generative artificial intelligence (AI) literacy covering essential skills and knowledge areas necessary to interact with generative AI. The competencies range from foundational AI literacy to prompt engineering and programming skills, including ethical and legal considerations. These twelve competencies offer a framework for individuals, policymakers, government officials, and educators looking to navigate and take advantage of the potential of generative AI responsibly. Embedding these competencies into educational programs and professional training initiatives can equip individuals to become responsible and informed users and creators of generative AI. The competencies follow a logical progression and serve as a roadmap for individuals seeking to get familiar with generative AI and for researchers and policymakers to develop assessments, educational programs, guidelines, and regulations.</jats:p>"
10.47852/bonviewijce42022495,Exploring the Role of Generative AI in Enhancing Language Learning: Opportunities and Challenges,"<jats:p>Contemporary advances in generative AI technology have sparked considerable interest regarding its application in language education. This article explores the innovative impact that AI-powered linguistic educational tools may have, such as customised learning journeys, dynamic content, and individualised feedback mechanisms, which collectively have the potential to enhance language acquisition and literacy. At the same time, it is important to recognise the constraints associated with such technologies in the educational sphere. Concern about maintaining precision and genuineness within AI-crafted language texts is a concern in the literature. There is also caution about AI's current inclination to standardise language expression and to propagate limited cultural narratives, alongside the risks of over-reliance on technology which may diminish analytical thought and inventiveness. This article examines the ethical considerations involving generative AI, such as the authenticity of creative work and the ownership of intellectual output. Emphasising the necessity for clarity and conscientious in the application of AI, this conceptual article outlines the opportunities, limitations and ethical concerns associated with generative AI in language instruction. The core message of the article advocates for a well-rounded strategy that leverages the positive aspects of generative AI within language education, while also addressing possible drawbacks and championing an ethical and equitable approach to language learning in the emerging AI-centric digital landscape. A model for forging thinking in this new research and practice space is designed to synthesise many of the possibilities of generative AI in language education.</jats:p>"
10.1109/icc51166.2024.10622879,Integrated Sensing and Communications Using Generative AI: Countering Adversarial Machine Learning Attacks,N/A
10.1080/03071847.2023.2286775,Generative AI and Intelligence Assessment,N/A
10.53841/bpsadm.2023.15.3.30,The impact of generative AI upon the evaluation process,N/A
10.3233/shti240200,Incorporation of Generative AI in an Introductory Nursing Informatics Course,"<jats:p>This case study describes how one educator approached the incorporation of generative AI into a graduate-level introductory nursing informatics course. Policy for acceptable use of generative AI was set separately for each assignment and disclosure of use was included in grading rubrics. Classroom discussions were used to explore issues surrounding generative AI such as trust and ethical concerns. One assignment required the use of a generative AI output to serve as the first draft of patient education material that was then to be improved by the student. In an accompanying reflection, students described their key takeaways from the experience of completing the assignment. The approach for this course was successful in achieving the goal of familiarizing students with the uses, benefits, risks, and ethical implications of using generative AI in the classroom and in healthcare. Opportunities are identified for improvement of assignment instructions.</jats:p>"
10.22554/ijtel.v7i2.125,Chatbots and Citations: An Experiment in Academic Writing with Generative AI,"<jats:p>The world of Educational Technology is no stranger to tales and predictions about the Next Big Thing, with Artificial Intelligence (AI) being the current title holder. This paper documents the process and experiences of the authors in undertaking a challenge to “go all in” with AI (in this case, ChatGPT-3.5) to generate academic material pertaining to their professional context, and to reflect upon the endeavour and the possible broader implications arising from this. We found that, on balance, the AI engine generated a credible base content for our chosen topic, and proved particularly useful for some aspects of the writing process (such as formulating bibliographical references and suggested titles). The experiment deepened our understanding of the potentials and pitfalls of using Artificial Intelligence in the academic writing process, and of the need to stay abreast of this rapidly evolving field.</jats:p>"
10.1007/979-8-8688-0083-2_10,Concluding Thoughts and the Future Vision,N/A
10.2196/preprints.51391,Learning to Make Rare and Complex Diagnoses With Generative AI Assistance: Qualitative Study of Popular Large Language Models (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>Patients with rare and complex diseases often experience delayed diagnoses and misdiagnoses because comprehensive knowledge about these diseases is limited to only a few medical experts. In this context, large language models (LLMs) have emerged as powerful knowledge aggregation tools with applications in clinical decision support and education domains.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>This study aims to explore the potential of 3 popular LLMs, namely Bard (Google LLC), ChatGPT-3.5 (OpenAI), and GPT-4 (OpenAI), in medical education to enhance the diagnosis of rare and complex diseases while investigating the impact of prompt engineering on their performance.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>We conducted experiments on publicly available complex and rare cases to achieve these objectives. We implemented various prompt strategies to evaluate the performance of these models using both open-ended and multiple-choice prompts. In addition, we used a majority voting strategy to leverage diverse reasoning paths within language models, aiming to enhance their reliability. Furthermore, we compared their performance with the performance of human respondents and MedAlpaca, a generative LLM specifically designed for medical tasks.</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>Notably, all LLMs outperformed the average human consensus and MedAlpaca, with a minimum margin of 5% and 13%, respectively, across all 30 cases from the diagnostic case challenge collection. On the frequently misdiagnosed cases category, Bard tied with MedAlpaca but surpassed the human average consensus by 14%, whereas GPT-4 and ChatGPT-3.5 outperformed MedAlpaca and the human respondents on the moderately often misdiagnosed cases category with minimum accuracy scores of 28% and 11%, respectively. The majority voting strategy, particularly with GPT-4, demonstrated the highest overall score across all cases from the diagnostic complex case collection, surpassing that of other LLMs. On the Medical Information Mart for Intensive Care-III data sets, Bard and GPT-4 achieved the highest diagnostic accuracy scores, with multiple-choice prompts scoring 93%, whereas ChatGPT-3.5 and MedAlpaca scored 73% and 47%, respectively. Furthermore, our results demonstrate that there is no one-size-fits-all prompting approach for improving the performance of LLMs and that a single strategy does not universally apply to all LLMs.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>Our findings shed light on the diagnostic capabilities of LLMs and the challenges associated with identifying an optimal prompting strategy that aligns with each language model’s characteristics and specific task requirements. The significance of prompt engineering is highlighted, providing valuable insights for researchers and practitioners who use these language models for medical training. Furthermore, this study represents a crucial step toward understanding how LLMs can enhance diagnostic reasoning in rare and complex medical cases, paving the way for developing effective educational tools and accurate diagnostic aids to improve patient care and outcomes.</p>
                </sec>"
10.2139/ssrn.4820443,Generative AI as a metacognitive agent: A comparative mixed-method study with human participants on ICF-mimicking exam performance,N/A
10.1109/mobisecserv58080.2023.10329224,Personalized Aging-in-Place Support Through Fine-Tuning of Generative AI Models,N/A
10.1109/idciot59759.2024.10467477,Generative AI: A Transformative Force in Business Intelligence,N/A
10.2139/ssrn.4787211,Decoding Future of Generative AI in Finance: A Machine Learning Exploration of Academic and Grey Corpus,N/A
10.2139/ssrn.4694565,"Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity",N/A
10.1201/9781003187158-21,Attack on Fraud Detection Systems in Online Banking Using Generative Adversarial Networks,N/A
10.1109/ghtc56179.2023.10354803,Using Generative AI to Strategize Entrepreneurial Endeavors,N/A
10.1016/j.rio.2024.100700,Diabetic retinopathy detection through generative AI techniques: A review,N/A
10.21606/drslxd.2023.035,Do not believe hype: Critically discussing the role and pedagogical implication of generative AI in Human-Centred and Transdisciplinary Design Education,N/A
10.1016/s2589-7500(23)00246-7,Preventing harm from non-conscious bias in medical generative AI,N/A
10.1145/3603287.3651196,Promise and Challenges of Generative AI in Healthcare Information Systems,N/A
10.1109/iccds60734.2024.10560425,Generative AI Platform for Applying Artistic Styles to Images,N/A
10.1007/s11948-022-00396-z,Multi Scale Ethics—Why We Need  to Consider the Ethics of AI in Healthcare at Different Scales,"<jats:title>Abstract</jats:title><jats:p>Many researchers have documented how AI and data driven technologies have the potential to have profound effects on our lives—in ways that make these technologies stand out from those that went before. Around the world, we are seeing a significant growth in interest and investment in AI in healthcare. This has been coupled with rising concerns about the ethical implications of these technologies and an array of ethical guidelines for the use of AI and data in healthcare has arisen. Nevertheless, the question of if and how AI and data technologies can be ethical remains open to debate. This paper aims to contribute to this debate by considering the wide range of implications that have been attributed to these technologies and asking whether current ethical guidelines take these factors into account. In particular, the paper argues that while current ethics guidelines for AI in healthcare effectively account for the four key issues identified in the ethics literature (transparency; fairness; responsibility and privacy), they have largely neglected wider issues relating to the way in which these technologies shape institutional and social arrangements. This, I argue, has given current ethics guidelines a strong focus on evaluating the impact of these technologies on the individual, while not accounting for the powerful social shaping effects of these technologies. To address this, the paper proposes a Multiscale Ethics Framework, which aims to help technology developers and ethical evaluations to consider the wider implications of these technologies.</jats:p>"
10.2139/ssrn.4871308,Generative AI in Medicine and Public Health: An Overview and Position Paper on Directions for Social Research,N/A
10.3390/biomedinformatics4020079,"Unlocking the Future of Drug Development: Generative AI, Digital Twins, and Beyond","<jats:p>This article delves into the intersection of generative AI and digital twins within drug discovery, exploring their synergistic potential to revolutionize pharmaceutical research and development. Through various instances and examples, we illuminate how generative AI algorithms, capable of simulating vast chemical spaces and predicting molecular properties, are increasingly integrated with digital twins of biological systems to expedite drug discovery. By harnessing the power of computational models and machine learning, researchers can design novel compounds tailored to specific targets, optimize drug candidates, and simulate their behavior within virtual biological environments. This paradigm shift offers unprecedented opportunities for accelerating drug development, reducing costs, and, ultimately, improving patient outcomes. As we navigate this rapidly evolving landscape, collaboration between interdisciplinary teams and continued innovation will be paramount in realizing the promise of generative AI and digital twins in advancing drug discovery.</jats:p>"
10.1007/979-8-8688-0083-2_9,The Road Ahead: Predictions and Preparations,N/A
10.36227/techrxiv.171172801.19993069/v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",N/A
10.17951/h.2023.57.4.123-143,Generative AI in Management – Today and Tomorrow,"<jats:p>Theoretical background: The rapid and exponential technological advancements have far-reaching impacts on management information systems, management practices, and human life. The promising outcomes in Artificial Intelligence and cutting-edge research on semantic networks and natural language processing have motivated the authors to envision the future of management technology.  Purpose of the article: Our paper focuses on the new communication facilities and artificial intelligence models used to process management-type queries in natural language.  Research methods: The article discusses recently developed technologies, proposed by Google and Microsoft, notably Google Bard and Bing integrated with ChatGPT-4. Both chatbots use Generative AI methods and large language models to understand domain-based queries and generate answers.  Main findings: The practical and social implications of new models in management practice are discussed. To illustrate the qualities and weaknesses of the features of new technologies, four examples of management decision-making are discussed. The case studies also show differences between these two technologies. Finally, the paper concludes by summarizing the expectations and limitations of Generative AI applications in management. The paper is one of the first publications describing and demonstrating the idea of interfaces in natural language in business-oriented applications.</jats:p>"
10.61784/wjikmv2n295,Research on the efficiency and accuracy of generative AI in the editorial process of scientific journals,N/A
10.3102/2108011,Driving Educational Change in Ghana: The Potential Role of Generative AI in Promoting Practical-Based Teaching and Assessment Methods,N/A
10.47408/jldhe.vi30.1137,"Generative artificial intelligence (AI) in higher education: a comprehensive review of challenges, opportunities, and implications","<jats:p>This paper explores recent advancements and implications of artificial intelligence (AI) technology, with a specific focus on Large Language Models (LLMs) like ChatGPT 3.5, within the realm of higher education. Through a review of the academic literature, this paper highlights the unprecedented growth of these models and their wide-reaching impact across various sectors. The discussion sheds light on the complex issues and potential benefits presented by LLMs, providing a overview of the field's current state.
In the context of higher education, the paper explores the challenges and opportunities posed by LLMs. These include issues related to educational assessment, potential threats to academic integrity, privacy concerns, the propagation of misinformation, EDI aspects, copyright concerns and inherent biases within the models. While these challenges are multifaceted and significant, the paper emphasizes the availability of strategies to address them effectively and facilitate the successful adoption of LLMs in educational settings.
Furthermore, the paper recognises the potential opportunities to transform higher education. It emphasises the need to update assessment policies, develop guidelines for staff and students, scaffold AI skills development, and find ways to leverage technology in the classroom. By proactively pursuing these steps, higher education institutions (HEIs) can harness the full potential of LLMs while managing their adoption responsibly.
In conclusion, the paper urges HEIs to allocate resources to handle the adoption of LLMs effectively. This includes ensuring staff AI readiness and taking steps to modify their study programmes to align with the evolving educational landscape influenced by emerging technologies.</jats:p>"
10.32617/912-643eafe415d33,EIX's Editorial Guidelines for the Use of Generative AI Tools,N/A
10.2139/ssrn.4786119,Early Adoption of Generative AI by Global Business Leaders: Insights from an INSEAD Alumni Survey,N/A
10.21608/jstc.2024.361556,The Future of Generative AI,N/A
10.2139/ssrn.4856935,The Impact of Generative AI on Collaborative Open-Source Software Development: Evidence from GitHub Copilot,N/A
10.1016/j.jslw.2023.101070,Beyond ChatGPT: Multimodal generative AI for L2 writers,N/A
10.37074/jalt.2023.6.2.5,A critical perspective on generative AI and learning futures. An interview with Stefan Popenici,N/A
10.17762/ijritcc.v11i9.9786,Generative AI in Supply Chain Management,"<jats:p>A new age of creativity and efficiency is ushered in by the integration of Generative Artificial Intelligence (AI) into supply chain management. This in-depth study examines the diverse effects of generative artificial intelligence on supply chain operations, including risk management, inventory optimization, procurement, logistics, and more. Given the predictive capacity of generative AI, traditional methods have been completely modified, enabling companies to anticipate demand, maximize inventory, and expedite procurement procedures with previously unheard-of accuracy. Real-time adaptation is made possible by its dynamic decision-making skills, which also help to promote resilience against interruptions and enable proactive reactions to changing market conditions. However, there are some challenges in implementing generative AI in supply chains. Obstacles requiring strategic navigation and organizational preparedness include skill gaps, ethical considerations, scalability issues, and data integration complexity. Future directions for generative artificial intelligence in supply networks are extremely promising. Substantial improvements are expected to be driven by advances in explainable AI, predictive analytics, seamless integration, and ethical frameworks. Redefining supply chain models could be facilitated by autonomous supply chains, adaptive resilience to disturbances, and increased transparency in decision-making.</jats:p>"
10.18080/jtde.v11n4.856,Regulating the New: Overland Telegraph to Generative AI,"<jats:p>The Charles Todd Oration is an annual event run by TelSoc and is named for Charles Todd, the Postmaster-General of South Australia, who was responsible for completing the Overland Telegraph Line from Darwin to Adelaide in 1872. The 2023 Oration was delivered in Sydney on 12 October 2023 by the author, Rob Nicholls, and was introduced by Communications Minister Michelle Rowland MP. The Oration examined the challenge of novelty to regulators. It looked at history of regulating innovation, promotion of innovation in the context of consumer protection, how regulators can deal with innovation, and minimising consumer harm.</jats:p>"
10.1109/icnwc60771.2024.10537337,Computer vision and Generative AI for yield prediction in Digital Agriculture,N/A
10.1109/iv55156.2024.10588493,Exploring Generative AI for Sim2Real in Driving Data Synthesis,N/A
10.1109/educon60312.2024.10578870,Generative AI Guidelines by/for Engineering Undergraduates,N/A
10.1109/access.2023.3332468,"When, Where, and Which?: Navigating the Intersection of Computer Vision and Generative AI for Strategic Business Integration",N/A
10.1353/pla.2024.a916988,Pandora's Can of Worms: A Year of Generative AI in Higher Education,"<jats:p xml:lang=""en""> abstract: In the year since ChatGPT was released by OpenAI, librarians, instructors, and higher education administrators have grappled with generative artificial intelligence (AI) and its implications for teaching, learning, research, and writing. Drawn from informal conversations, professional observations, discussion groups, and professional development events, this article reports on the experience of learning about generative AI at one university. This article considers ways that educators may use AI tools and reasons to resist adopting generative AI tools, situating uses on a spectrum of acceptability.</jats:p>"
10.56726/irjmets60776,FINE-TUNING LLAMA-3 WITH LORA: A MEMORY-EFFICIENT APPROACH TO OPTIMIZING GENERATIVE AI MODELS,N/A
10.1111/beer.12547,All about the human: A Buddhist take on <scp>AI</scp> ethics,"<jats:title>Abstract</jats:title><jats:p>As AI technology becomes more influential, ethical considerations surrounding its application are becoming increasingly relevant. In this paper, I reflect on some moral questions from a Buddhist perspective and consider the moral status of AI to evaluate its function and purpose in our lives. Since a robot lacks the capacity to experience suffering and has no conscience, AI ethics are possible only as ethics <jats:italic>about</jats:italic> robots and not as ethics <jats:italic>for</jats:italic> robots. Despite having no concrete moral status, robots cannot be deemed entirely morally insignificant as they exist as moral objects toward which moral agents have a duty. Our unique endowment as human beings is the intelligence for moral deliberation as we develop AI technologies and determine the future direction of humanity. From a Buddhist point of view, there is potential to advance our spiritual growth through the realization of the interdependence between humans and AI. As we recognize the uniqueness of our humanity and take a middle‐way approach, the rise of robots need not threaten our existence but could instead catapult humanity into a new dawn.</jats:p>"
10.1007/s00146-023-01704-2,"Embedding AI in society: ethics, policy, governance, and impacts",N/A
10.3366/dlgs.2019.0373,Affirmative Ethics and Generative Life,"<jats:p> Rosi Braidotti's contribution to the Deleuze Studies Conference 2016 held in Rome (University of Roma Tre, 11–13 July), later transcribed and then revised by the author, points firmly to the current need for an affirmative thinking approach, actively standing to the present, while assessing its becoming and imagining new configurations. Saying yes to the world, being worthy of it, does not entail passive acceptance but rather the activation of transformative and critical thinking. To this aim, Braidotti looks at Deleuze as well as at feminist theory. The ontology of immanence turns into a materialist, collective, vital, embodied and relational ethics. </jats:p>"
10.1007/978-3-030-66913-3_6,"Digitization: Learnings from Ancient Disruptions, AI and the Digital Trio’s Functional Stage, and AI Superpowers Disrupting Us",N/A
10.2139/ssrn.4584219,Generative AI and Jobs: A Global Analysis of Potential Effects on Job Quantity and Quality,N/A
10.18559/ebr.2023.2.743,Challenges for higher education in the era of widespread access to generative AI,"<jats:p>The aim of this paper is to discuss the role and impact of generative artificial intelligence (AI) systems in higher education. The proliferation of AI models such as GPT-4, Open Assistant and DALL-E presents a paradigm shift in information acquisition and learning. This transformation poses substantial challenges for traditional teaching approaches and the role of educators. The paper explores the advantages and potential threats of using generative AI in education and necessary changes in curricula. It further discusses the need to foster digital literacy and the ethical use of AI. The paper’s findings are based on a survey conducted among university students exploring their usage and perception of these AI systems. Finally, recommendations for the use of AI in higher education are offered, which emphasize the need to harness AI's potential while mitigating its risks. This discourse aims at stimulating policy and strategy development to ensure relevant and effective education in the rapidly evolving digital landscape.</jats:p>"
10.4324/9781003482918-8,‘Understood the assignment’,N/A
10.51219/jaimld/ravi-shankar-koppula/101,Generative AI Models and Their Potential Business Use Cases,N/A
10.2139/ssrn.4480947,The Promise and Peril of Generative AI: Evidence from ChatGPT as Sell-Side Analysts,N/A
10.1016/j.futures.2018.03.003,Self-Organized Linguistic Systems: From traditional AI to bottom-up generative processes,N/A
10.1038/s41585-023-00836-w,Generative AI in scientific publishing: disruptive or destructive?,N/A
10.18260/1-2--46987,Board 40: Work in Progress: Generative AI to Support Critical Thinking in Water Resources Students,N/A
10.18260/1-2--47236,Emotionally Intelligent Machines in Education: Harnessing Generative AI for Authentic Human-Machine Synergy in the Classroom,N/A
10.14361/dcs-2018-0107,Competing Visions for AI,"<jats:title>Abstract</jats:title>
               <jats:p> In this paper, I will investigate how two competing visions of machine intelligence put forward by Alan Turing and J. C. R Licklider - one that emphasized automation and another that emphasized augmentation - have informed experiments in computational creativity, from early attempts at computer-generated art and poetry in the 1960s, up to recent experiments that utilise Machine Learning to generate paintings and music. I argue that while our technological capacities have changed, the foundational conflict between Turing’s vision and Licklider’s vision plays itself out in generations of programmers and artists who explore the computer’s creative potential. Moreover, I will demonstrate that this conflict does not only inform technical/artistic practice, but speaks to a deeper philosophical and ideological divide concerning the narrative of a post-human future. While Turing’s conception of human-equivalent AI informs a transhumanist imaginary of super-intelligent, conscious, anthropomorphic machines, Licklider’s vision of symbiosis underpins formulations of the cyborg as human-machine hybrid, aligning more closely with a critical post-human imaginary in which boundaries between the human and technological become mutable and up for re-negotiation. In this article, I will explore how one of the functions of computational creativity is to highlight, emphasise and sometimes thematise these conflicting post-human imaginaries.</jats:p>"
10.22554/ijtel.v7i2.127,Deploying generative AI to draft a roleplay simulation of difficult conversations about inclusivity,"<jats:p>Within inclusivity change initiatives, conversations around microaggressions are a key element in seeking behavioural change. This exemplar use of GenAI is focused on authoring a conversational role play simulation. Prompts draw on the extensive literature of microaggressions. The underlying scenario is a podcast where the author and chatbot co-design the role play. The study involved a novice in GenAI simultaneously learning prompt engineering which would generate realistic role played conversation. A core finding was that GenAI can produce a conversational style realistic enough to deploy in higher education inclusivity workshops, and whose subject matter content is satisfactory. The chatbot proposed four benefits of a role play simulation, which are drawn on to frame the critical reflection. An important conclusion was that the GenAI process could valuably facilitate shifting from a subject expert academic being sole author, to co-design involving both those with experience of microaggressions, as well as potential staff and student workshop participants.</jats:p>"
10.1080/17404622.2024.2385343,Training the stochastic parrot: Using generative AI to create textual materials for communication courses,N/A
10.1038/s41587-020-0418-2,Assessing the impact of generative AI on medicinal chemistry,N/A
10.52214/salt.v24i1.12869,"An Evaluation of Khanmigo, a Generative AI Tool, as a Computer-Assisted Language Learning App","<jats:p>The recent advancement in technology has attracted learners’ attention worldwide to Generative Artificial Intelligence (GenAI) for educational purposes. While GenAI has shown promising results for general language purposes (Godwin-Jones, 2023; Xiao &amp; Zhi, 2023), the potential of GenAI for language learning has not been fully explored. This paper, therefore, endeavors to decipher the potential of a GenAI app, Khanmigo, as a language learning tool, specifically for learning French. The app was analyzed by the researcher through her interactions of about 17.5 hours using Chapelle’s (2001) Evaluation Framework for discerning the task appropriateness of a given Computer-Assisted Language Learning (CALL) tool. While the app does not show robust performance in all the six criteria suggested for evaluation, it still holds some promise.</jats:p>"
10.1145/3624732,Generative AI Degrades Online Communities,<jats:p>How large language models are influencing online communities.</jats:p>
10.21606/drslxd.2024.035,Do not believe hype: Critically discussing the role and pedagogical implication of generative AI in Human-Centred and Transdisciplinary Design Education,N/A
10.18632/oncotarget.28640,Generative AI in oncological imaging: Revolutionizing cancer detection and diagnosis,N/A
10.21125/inted.2024.1832,EXPLORING THE EDUCATIONAL POTENTIAL OF GENERATIVE AI: AN APPLICATION FOR SPELLING PRACTICE,N/A
10.62273/ldvl8354,Generative AI in practice:  A Teaching Case in the Introduction to MIS class,N/A
10.2196/48780,Anki Tagger: A Generative AI Tool for Aligning Third-Party Resources to Preclinical Curriculum,"<jats:p>Using large language models, we developed a method to efficiently query existing flashcard libraries and select those most relevant to an individual's medical school curricula.</jats:p>"
10.1515/9783110756722-009,9 Generative adversary networks novel trends in image processing,N/A
10.1186/s12910-022-00746-3,Embedded ethics: a proposal for integrating ethics into the development of medical AI,"<jats:title>Abstract</jats:title><jats:p>The emergence of ethical concerns surrounding artificial intelligence (AI) has led to an explosion of high-level ethical principles being published by a wide range of public and private organizations. However, there is a need to consider how AI developers can be practically assisted to anticipate, identify and address ethical issues regarding AI technologies. This is particularly important in the development of AI intended for healthcare settings, where applications will often interact directly with patients in various states of vulnerability. In this paper, we propose that an ‘embedded ethics’ approach, in which ethicists and developers together address ethical issues via an iterative and continuous process from the outset of development, could be an effective means of integrating robust ethical considerations into the practical development of medical AI.</jats:p>"
10.69554/pmum8049,"Why automatic AI ethics evaluations are coming, and how they will work","<jats:p xml:lang=""en"">Ethics evaluations of companies that function with AI at their core are increasingly required by regulation and law in Europe and the US. Investors in artificial intelligence (AI)-intensive companies also seek ethics evaluations as part of the nonfinancial information they gather about corporate performance, especially as it relates to privacy and algorithmic fairness. The result is an increasing demand for the evaluations. The costs and time necessary to perform an AI ethics audit, however, are high, even prohibitive. To solve the problem, natural language processing (NLP) and machine learning (ML) can be employed to automate the process. The proposal is that much of the work of AI evaluating can be accomplished more efficiently by machines than by humans. To show how automated ethics reporting may work, this paper describes a project currently underway at Pace University in New York and the University of Trento in Italy. The project endeavours to apply AI to the task of producing AI ethics evaluations.</jats:p>"
10.4018/979-8-3693-0831-8.ch006,How AI Is Changing the Nature and Landscape of Student Learning and Outcomes Assessment Within Higher Education,"<jats:p>Technologies have long changed the social lives and practices of users. While some argue that the effects have been positive and others have been a bit more skeptical to take this perspective, there is no question that they have altered the way that we engage in daily behaviors. They have changed the way that we work and the way that we communicate with others. And said technologies are, with little question, here to stay. This chapter introduces readers to the various strategies adopted by both students and assessors (e.g., instructors) regarding the use of ChatGPT within the world of higher education. ChatGPT, in short, is an artificial intelligence chatbot that aids the user in a multitude of tasks and mimics the human mind. Not only will the chapter expose readers to the ways in which students are using ChatGPT (e.g., as artificial tutors, to help with the construction of ideas), but also how instructors are beginning to notice the benefits of this AI tool for the classroom (e.g., lesson plans).</jats:p>"
10.4018/979-8-3693-5288-5.ch015,Unveiling the Potential of Large Language Models,"<jats:p>Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are transforming industries by fostering innovation, automating tasks, and enhancing creativity. By enabling personalized user interactions, sophisticated content creation, and advanced data analytics, they are revolutionizing industries such as healthcare, education, and customer service. As these technologies evolve, they can fundamentally change communication and decision-making processes and incorporate AI into everyday life. The objective of this book chapter is to examine the architecture and components, features, functionality, domain-specific applications, recent advances, and future developments of LLMs. Ongoing research aims to reduce biases, increase energy efficiency, and facilitate interpretation. As LLMs continue to evolve, they have the potential to transform many industries, including education, customer service, content creation, and more. As a result, they will be essential for the development of future AI-powered applications.</jats:p>"
10.1007/978-3-031-08215-3_7,AI Policy as a Response to AI Ethics? Addressing Ethical Issues in the Development of AI Policies in North Africa,"<jats:title>Abstract</jats:title><jats:p>The recent exponential rate of AI development has led to a proliferation of AI national policies and strategies as global power blocs have sought to consolidate positions of strategic dominance. These policies have sought to promote the benefits and mitigate the risks of AI—and address ever more serious ethical concerns about these technologies. This raises the question of how countries less influential in the sphere of AI might seek to address such ethical issues themselves, and whether lessons can be learned from existing policies in addressing issues of ethics and human rights. From a consideration of the current AI ethics discourse and existing AI policies, the chapter goes on to explore how ethical concerns are addressed in the North African AI strategies and which gaps and opportunities in terms of coverage of ethical issues arise from the current state of these policies. We suggest that ethical issues should be addressed clearly in policy at the earliest possible stage to ensure that ethical standards are internally produced in line with social and cultural values, rather than being de facto applied by external actors.</jats:p>"
10.4018/979-8-3693-7452-8.ch011,Transforming Healthcare,"<jats:p>This study mainly explores the next phase of generative artificial intelligence and its benefits in healthcare. This work explores the changes artificial intelligence can bring to this sector, including artificial neural networks and adaptive autoencoders to develop self-healing models by collecting patient information, data, and existing research studies. These artificial intelligence systems can create curated plans for treatment that will benefit the patient. This will help improve treatment outcomes while reducing their side effects and overall outcomes for everyone with the disease. This chapter also explores the commitment of artificial intelligence in healthcare, discusses the issues and ethics, and suggests fields for future research, studies, and applications.</jats:p>"
10.59232/air-v1i1p103,Integrating AI-Generative Tools in Web Design Education: Enhancing Student Aesthetic and Creative Copy Capabilities Using Image and Text-Based AI Generators,N/A
10.1007/bf01174478,Organisational spaces and intelligent machines: A metaphorical approach to ethics,N/A
10.1016/j.engappai.2023.107666,Blockchain-based auditing of legal decisions supported by explainable AI and generative AI tools,N/A
10.69528/jkmla.2023.50.1_2.65,Education and Related Cases of Generative AI Utilization at Chung-Ang University,"<jats:p>

This paper discusses the education on generative AI utilization being implemented at Chung-Ang University, in response to plagiarism and research ethics issues arising with the advent of ChatGPT. It also explores related cases, such as the establishment of guidelines for generative AI and the progress of ChatGPT prompt engineering competitions. In addition, the paper includes methods for utilizing generative AI in the medical field and in libraries.
</jats:p>"
10.20944/preprints202305.0808.v1,"Exploring the Potential Impact of Artificial Intelligence (AI) on International Students in Higher Education: Generative AI, Chatbots, Analytics, and International Student Success","<jats:p>The use of artificial intelligence (AI) applications in education has the potential to revolutionize the learning experience for international students, who face unique challenges when studying in a foreign country. This paper explores various examples of AI applications in education and their potential impact on international students. AI applications such as personalized learning experiences, adaptive testing, predictive analytics, and chatbots for learning and research are examined for their potential to improve learning efficiency and provide customized education support. It also explores the significant risks and limitations associated with AI technologies, such as privacy, cultural differences, language proficiency, and ethical implications. To maximize the potential benefits of AI applications in higher education, it is crucial to implement appropriate safeguards and regulations. This paper provides a starting point for research on the potential impact of artificial intelligence on international students’ educational experiences and how AI may be integrated into educational administration and learning processes.</jats:p>"
10.4018/979-8-3693-5578-7.ch006,Using AI-Driven Decision-Making Tools in Corporate Investment Planning,"<jats:p>Artificial intelligence plays a crucial role in financial sectors, especially in investment planning. It enhanced or replaced traditional investment planning by applying AI techniques such as machine learning, natural language processing, deep learning, and robo-advisors. This chapter presented the applications of AI-enabled decision-making tools in various functional areas of corporate investment planning such as quantitative trading algorithms, risk management systems, portfolio optimization tools, algorithmic trading, forecasting and prediction of securities market, automated portfolio building, data analysis, asset management, and personalized investment advice. Also, this chapter presented the case studies, success stories, and benefits of AI applications in corporate investment planning. This chapter will be very useful for corporate companies and investors to transform their mode of traditional investment planning to technology-oriented investment planning.</jats:p>"
10.4018/979-8-3693-2440-0.ch022,Shaping Tomorrow's Minds,"<jats:p>The chapter explores the evolution of AI, distinguishing it from generative AI, and examines its current applications in education, spanning from primary to higher education levels. Unveiling the multifaceted landscape of AI technologies, such as machine learning, deep learning, and natural language processing, the narrative delves into personalized learning, smart tutoring systems, and administrative efficiency. The chapter anticipates a future where AI collaborates seamlessly with educators, offering hyper-personalized learning journeys, immersive experiences through augmented and virtual reality, and fostering collaborative and project-based learning. Challenges, including ethical concerns, digital inclusion, and teacher training, are acknowledged, emphasizing the importance of addressing these issues for an inclusive and equitable AI-augmented educational future. As we stand at the brink of this transformative era, the chapter envisions a harmonious fusion of technology and human ingenuity shaping the minds of tomorrow.</jats:p>"
10.1002/tl.20630,Exploring the frontiers of generative AI in assessment: Is there potential for a human‐AI partnership?,"<jats:title>Abstract</jats:title><jats:p>This chapter investigates the integration of generative AI (GenAI), specifically ChatGPT, into institutional and course‐level assessment at Western New England University. It explores the potential of GenAI to streamline the assessment process, making it more efficient, equitable, and objective. Through the development of a proprietary GenAI tool, the study examines GenAI's assessment of student evidence, including written assignments and computer coding tasks, against human assessment. It addresses challenges such as data collection, coordination, and the need for well‐defined and precise rubrics. We found notable differences in GenAI and human scoring in some cases, indicating GenAI's potential in certain assessment contexts while also acknowledging its limitations in others. Our research suggests that GenAI could enhance educational assessment processes, but its integration requires addressing biases in training data and securing buy‐in from various stakeholders. Using GenAI to handle particular routine tasks can potentially free up faculty to engage in richer discussions and educational improvements.</jats:p>"
10.1145/3664646.3664773,The Role of Generative AI in Software Development Productivity: A Pilot Case Study,N/A
10.1145/3514094.3539548,AI and Legal Personhood: An African Perspective,N/A
10.69554/omos9444,A response to the use of ethics as a data protection building block for AI,"<jats:p xml:lang=""en"">As a key driver for the Fourth Industrial Revolution, AI has increasing effects on all areas of human lives. AI utilises and interacts with a large volume of data, including personal data or data related to individuals, which inevitably raises privacy and data protection concerns. Data protection authorities (DPAs) continue to stress that AI must comply with a set of data protection principles that were first introduced nearly half of a century ago, while acknowledging these principles’ limitations in protecting individual rights under AI. These limitations include bias, discrimination, a sense of losing control, threat of surveillance, fears of erosion of choice and free will, etc. Before a set of AI-proven data protection principles can be developed and agreed internationally, DPAs have turned to ethics as an interim measure. Unlike data protection principles, however, ethics are elusive to demonstrate at best and potentially impossible to agree upon at worst. This paper explains what issues are facing AI that led to the use of ethics as a data protection building block by DPAs. It then surveys worldwide effects in providing ethical guidance related to AI and identifies ethical impact assessment (EIA) as a way to demonstrate commitment. At the same time, the practical disjoint and reasons between knowing ethics and acting ethically are elaborated to illustrate the challenges. The paper concludes with discussions on how AI practitioners should continuously monitor public sentiment, government initiatives and regulatory frameworks, and take proactive action in conducting EIA to demonstrate their commitment to and respect of ethics.</jats:p>"
10.1007/s00146-009-0244-z,Ethics and the technologies of empire: e-learning and the US military,N/A
10.1007/978-981-19-9382-4_11,Our Future with AI: Future Projections and Moral Machines,N/A
10.1609/aimag.v40i1.2848,"Artificial Intelligence, Robotics, Ethics, and the Military: A Canadian Perspective","<jats:p><jats:italic>Defense and security organizations depend upon science and technology to meet operational needs, predict and counter threats, and meet increasingly complex demands of modern warfare. Artificial intelligence and robotics could provide solutions to a wide range of military gaps and deficiencies. At the same time, the unique and rapidly evolving nature of AI and robotics challenges existing polices, regulations, and values, and introduces complex ethical issues that might impede their development, evaluation, and use by the Canadian Armed Forces (CAF). Early consideration of potential ethical issues raised by military use of emerging AI and robotics technologies in development is critical to their effective implementation. This article presents an ethics assessment framework for emerging AI and robotics technologies. It is designed to help technology developers, policymakers, decision makers, and other stakeholders identify and broadly consider potential ethical issues that might arise with the military use and integration of emerging AI and robotics technologies of interest. We also provide a contextual environment for our framework, as well as an example of how our framework can be applied to a specific technology. Finally, we briefly identify and address several pervasive issues that arose during our research.</jats:italic></jats:p>"
10.1145/3514094.3539540,Governing AI Applications To Monitoring and Managing Our Global Environmental Commons,N/A
10.1145/3514094.3534167,How Open Source Machine Learning Software Shapes AI,N/A
10.1355/9789815203684-005,BUILDING THE ASEAN AI GUIDE FROM THE GROUND UP,N/A
10.1007/978-1-4842-5476-9_12,The Ethics of AI-Powered Applications,N/A
10.4018/979-8-3693-0205-7.ch007,ChatGPT and Other Generative AI Tools in Education,"<jats:p>This chapter presents a comprehensive exploration of the transformative influence of generative AI tools in education. It examines the personalized learning experience enabled by AI, the role of AI tools as virtual tutors, content creation aids for educators, and their application in language learning and communication enhancement. Additionally, the chapter delves into the ethical implications associated with AI integration, including data privacy, fairness, and transparency. Practical strategies for responsible AI adoption are proposed, encompassing guidelines for ethical usage and educator training. The chapter concludes by highlighting the vast potential of AI-powered education while underscoring the significance of ethical considerations in its implementation. It encapsulates the evolution of AI in education and foresees a future where learners are empowered through dynamic and personalized educational experiences.</jats:p>"
10.54941/ahfe1004593,Using Future Thinking as a steering tool for Generative AI creative output: a case study aiming at rethink lighting in the next future,"<jats:p>Contemporary business sustainability relies on its flexibility and adaptability to unforeseen changes in societal and environmental challenges. In that sense, Speculative Design and Future Thinking offer a unique methodology for businesses to plan for the unpredictable. While those theories have been tested by decades of artistic and business applications, the recent introduction of generative AI tools widened the complexity and likelihood of answers that can be thoroughly investigated. At the same time, this enhanced capability to peek into our futures raises several concerns:  How can we retain control of generative AI creative output using future thinking as a steering tool? Would that allow designers to proceed beyond strategic choices, addressing specific product development?This paper shares a case study of how future thinking was applied to provide a lighting manufacturer with design visualisations related to the lighting industry's future. HE institution offered this service (hidden for peer review) through an interdepartmental collaboration (Industrial Design and Architecture). Futures Thinking and Speculative Design apply as a methodology for consulting firms on strategic and communication choices (Mc Kinsey &amp; Company, report, 2021). This case study explores the impact of such methodologies on choices related to product, interior, and systems design. Generative AI tools were prompted to visually interpret the forecasted scenarios through narrative discourse and creative works. the questions addressed were: What is the future of the light industry in the context of human emotions, enhanced user experience, well-being, or energy and resources?MethodsA workshop involving undergraduate and postgraduate students of the respective departments was organised in two phases: the first phase was about describing a scenario ten years in the future and conceptualising the design vision; the second phase was addressing the materialisation of light and its execution in a product or service proposal. After selecting the context, students followed methodological steps to envision future products, spaces and interactions following the four identified possible scenarios (Slaughter, 2013). In both phases, students used generative AI tools to visualise scenarios and solutions, Midjourney for the conceptual narrative, and Vizcom was used to generate illustrations of the interior/product lighting ecosystem. The latest tool does not rely solely on prompting but requires the designer to submit a sketch or a draft rendering of the proposed solution.ResultsThe design team of the participating lighting company attended each phase presentation involving products unfolding the relationship with the light in the future. The feedback received from the company appreciates the depth of the research and acknowledges the competitiveness of the identified research directions. The results achieved in the second phase of the workshop will be showcased at the IDI Shanghai yearly exhibition 2023.The format proposed could be easily adapted and implemented in a range of specific design domains, such as transportation and automotive, fashion and accessories, consumer electronics and household appliances, to name a few.</jats:p>"
10.21428/e4baedd9.a650f77d,"Data Authenticity, Consent, and Provenance for AI Are All Broken: What Will It Take to Fix Them?",N/A
10.12677/design.2023.84510,Research of Neo-Chinese Women’s Innovative Design Based on the Generative AI,N/A
10.3233/faia240228,A Hybrid Intelligent Change Management Approach to Generative AI Adoption,"<jats:p>The emergence of no-code AI platforms presents a revolutionary opportunity for democratizing AI, making it accessible for non-technical users to engage in AI-driven innovation [1]. This democratization is particularly evident in the realm of Generative AI (GenAI), which holds substantial promise for creative and problem-solving applications [2]. Despite the wealth of frameworks addressing IT development and adoption [3, 4], the integration of generative AI’s disruptive capabilities into existing paradigms remains underexplored, often relegated to the insights of private consultancy reports [5].This paper aims to address this gap in academic literature within the interdisciplinary concept of Hybrid Intelligence (HI), [6, 7], synthesizing insights from human-centered AI interface development [8], organizational upskilling [9], and the significance of a psychologically safe environment for collaborative innovation [10,11]. Here, we propose a comprehensive framework for integrating generative AI within organizational change management. This framework, highlighting common challenges and opportunities for innovation in GenAI adoption, is informed by product development workshops and training sessions with hundreds of industry leaders in Northern Europe over the past year.</jats:p>"
10.33606/yla.45.1,Issues on Generative AI (Artificial Intelligence) and Civil Law – Focusing on the granting of legal personality or electronic personality –,N/A
10.4018/979-8-3693-3719-6.ch003,Design and Development of AI-Powered Healthcare System,"<jats:p>Cancer patient survival is an integral part of the healthcare sector, with researchers offering clinicians snipping information as they consider treatment options that have a big impact on patients' lifestyle decisions. Evidently, no study of predicting survival or death from breast cancer using XGBoost method has been attempted. The goal of this project is to design a prediction system that can forecast both the survival rate and the death rate from breast cancer at an early stage by examining the most limited collection of clinical dataset variables. The potential of the XGBoost method is determined using mainly age at the time of diagnosis along with other clinical characteristics, assessed for its relative significance. The project findings show that the XGBoost model fits the testing dataset more effectively, with outcome of accuracy 82.72% approximately.</jats:p>"
10.1007/s10676-023-09693-y,Bridging the civilian-military divide in responsible AI principles and practices,N/A
10.1109/ethics57328.2023.10155024,AI Based Real-Time Privacy-Aware Camera Data Processing in Autonomous Vehicles,N/A
10.4018/979-8-3693-3597-0.ch015,Enhancing Privacy and Security in Online Education Using Generative Adversarial Networks,"<jats:p>As online education grows in popularity, issues concerning learners' privacy and security have become increasingly important. This chapter delves into the creative use of generative adversarial networks (GANs) to handle the complex difficulties of protecting sensitive information in the online education scene. The chapter opens with a detailed assessment of the present situation of online education. The chapter focuses on the integration of GANs into the online education environment to improve privacy and security. The chapter delves into the technical features of GANs, demonstrating how these networks may be tailored to generate synthetic yet indistinguishable data, reducing the danger of privacy violations. In addition to privacy protection, the chapter investigates the function of GANs in improving the overall cybersecurity posture of online education platforms. Finally, the chapter emphasises Generative Adversarial Networks' transformational potential in altering the privacy and security environment of online education.</jats:p>"
10.4337/9781803926728.00007,"AI Ethics Before Frankenstein: Mary Shelley, Enlightenment Political Thought, and the Problem of Bad Education",N/A
10.1007/978-3-658-38798-3_3,Digital Ethics in Data-Driven Organizations and AI Ethics as Application Example,N/A
10.1080/0731129x.2021.1903709,AI Challenges and the Inadequacy of Human Rights Protections,N/A
10.1080/21550085.2023.2200724,"The Rebugnant Conclusion: Utilitarianism, Insects, Microbes, and AI Systems",N/A
10.1007/978-3-319-60648-4_5,How AI Challenges Professional Ethics,N/A
10.4135/9781544397399.n3,"AI and Ethics: Can Machines Learn to
					Explain their Decisions?",N/A
10.1007/978-3-030-51110-4_10,Autonomous Vehicles,N/A
10.1145/3306618.3314286,Invisible Influence,N/A
10.1007/s11023-020-09517-8,The Ethics of AI Ethics: An Evaluation of Guidelines,"<jats:title>Abstract</jats:title><jats:p>Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the “disruptive” potentials of new AI technologies. Designed as a semi-systematic evaluation, this paper analyzes and compares 22 guidelines, highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems—and how the effectiveness in the demands of AI ethics can be improved.</jats:p>"
10.1007/978-3-031-55744-6_2,Artificial Intelligence: In Search of a Definition,N/A
10.1007/s10676-024-09762-w,Explainable AI in the military domain,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) has become nearly ubiquitous in modern society, from components of mobile applications to medical support systems, and everything in between. In societally impactful systems imbued with AI, there has been increasing concern related to opaque AI, that is, artificial intelligence where it is unclear how or why certain decisions are reached. This has led to a recent boom in research on “explainable AI” (XAI), or approaches to making AI more explainable and understandable to human users. In the military domain, numerous bodies have argued that autonomous and AI-enabled weapon systems ought not incorporate unexplainable AI, with the International Committee of the Red Cross and the United States Department of Defense both explicitly including explainability as a relevant factor in the development and use of such systems. In this article, I present a cautiously critical assessment of this view, arguing that explainability will be irrelevant for many current and near-future autonomous systems in the military (which do not incorporate any AI), that it will be trivially incorporated into most military systems which do possess AI (as these generally possess simpler AI systems), and that for those systems with genuinely opaque AI, explainability will prove to be of more limited value than one might imagine. In particular, I argue that explainability, while indeed a virtue in design, is a virtue aimed primarily at designers and troubleshooters of AI-enabled systems, but is far less relevant for users and handlers actually deploying these systems. I further argue that human–machine teaming is a far more important element of responsibly using AI for military purposes, adding that explainability may undermine efforts to improve human–machine teamings by creating a <jats:italic>prima facie</jats:italic> sense that the AI, due to its explainability, may be utilized with little (or less) potential for mistakes. I conclude by clarifying that the arguments are not against XAI in the military, but are instead intended as a caution against over-inflating the value of XAI in this domain, or ignoring the limitations and potential pitfalls of this approach.</jats:p>"
10.1080/0731129x.2023.2275967,Justice by Algorithm: The Limits of AI in Criminal Sentencing,N/A
10.5772/intechopen.97556,How Factoring Ethics Encourages and Stimulates Innovative Development of IT Systems Responsive to Stakeholder Needs and Requirements,"<jats:p>Human beings have become increasingly dependent on IT in running their daily lives and doing business. The development and the increase in use of Autonomous Intelligent Systems in the last few years is making it increasingly impossible to ignore ethics in engineering and building IT Systems as more factors other than the traditional ones like financials come into play. As the use of technology continues to grow among people of all ages, there is also a growing awareness of the potential social harms such systems can have on human well being. Where there is such an awareness (of potential harms the systems may have on humans), there is a likelihood of resistance in adoption and use of the technology resulting in the loss of the benefits such systems are supposed to bring with them. Where potential harms have been ignored and people went ahead and adopted and used the technology, but eventually experienced cases of social harm, possible abandonment of the technology becomes a reality - a situation which also results in the loss of all the gains the technology could have potentially brought. Some good examples are societies which continue to keep under-aged children away from technology as a way of safeguarding them from the harms caused by technology. Situations like this have set limits to the effectiveness and reach of technology driven services such as autonomous eHealth systems and even educational programmes.There has also been a view mainly among technology creators that Ethics considerations are slowing down or getting into the way of innovation. In the proposed chapter, and driven by the theme “Ethics Considerations to drive innovative thinking in building systems which are responsive to the needs of the user and promote adoption, and safe use of IT” the writer argues for a unified front in the development of IT systems in an ethical manner - That is, it is time users and creators of technology start working together to build systems for the future. And with well-being among the UN Development Goals, and technology taking centre stage in many aspects of development and growth, Ethics in technology cannot be ignored.</jats:p>"
10.3390/su14084472,From an Ethics of Carefulness to an Ethics of Desirability: Going Beyond Current Ethics Approaches to Sustainable AI,"<jats:p>‘Sustainable AI’ sets itself apart from other AI ethics frameworks by its inherent regard for the ecological costs of AI, a concern that has so far been woefully overlooked in the policy space. Recently, two German-based research and advocacy institutions have published a joint report on Sustainability Criteria for Artificial Intelligence. This is, to our knowledge, the first AI ethics document in the policy space that puts sustainability at the center of its considerations. We take this as an opportunity to highlight the foundational problems we see in current debates about AI ethics guidelines. Although we do believe the concept of sustainability has the potential to introduce a paradigm shift, we question whether the suggestions and conceptual grounding found in this report have the strength to usher it in. We show this by presenting this new report as an example of current approaches to AI ethics and identify the problems of this approach, which we will describe as ‘checklist ethics’ and ‘ethics of carefulness’. We argue to opt for an ‘ethics of desirability’ approach. This can be completed, we suggest, by reconceptualizing sustainability as a property of complex systems. Finally, we offer a set of indications for further research.</jats:p>"
10.1007/s10676-022-09629-y,Epistemo-ethical constraints on AI-human decision making for diagnostic purposes,"<jats:title>Abstract</jats:title><jats:p>This paper approaches the interaction of a health professional with an AI system for diagnostic purposes as a hybrid decision making process and conceptualizes epistemo-ethical constraints on this process. We argue for the importance of the understanding of the underlying machine epistemology in order to raise awareness of and facilitate realistic expectations from AI as a decision support system, both among healthcare professionals and the potential benefiters (patients). Understanding the epistemic abilities and limitations of such systems is essential if we are to integrate AI into the decision making processes in a way that takes into account its applicability boundaries. This will help to mitigate potential harm due to misjudgments and, as a result, to raise the trust—understood here as a belief in reliability of—in the AI system. We aim at a minimal requirement for AI meta-explanation which should distinguish machine epistemic processes from similar processes in human epistemology in order to avoid confusion and error in judgment and application. An informed approach to the integration of AI systems into the decision making for diagnostic purposes is crucial given its high impact on health and well-being of patients.</jats:p>"
10.1007/978-3-031-48135-2_10,Ethical Principles and Governance for AI,N/A
10.1007/s11948-023-00428-2,AI Moral Enhancement: Upgrading the Socio-Technical System of Moral Engagement,"<jats:title>Abstract</jats:title><jats:p>Several proposals for moral enhancement would use AI to augment (auxiliary enhancement) or even supplant (exhaustive enhancement) human moral reasoning or judgment. Exhaustive enhancement proposals conceive AI as some self-contained oracle whose superiority to our own moral abilities is manifest in its ability to reliably deliver the ‘right’ answers to all our moral problems. We think this is a mistaken way to frame the project, as it presumes that we already know many things that we are still in the process of working out, and reflecting on this fact reveals challenges even for auxiliary proposals that eschew the oracular approach. We argue there is nonetheless a substantial role that ‘AI mentors’ could play in our moral education and training. Expanding on the idea of an AI Socratic Interlocutor, we propose a modular system of multiple AI interlocutors with their own distinct points of view reflecting their training in a diversity of concrete wisdom traditions. This approach minimizes any risk of moral disengagement, while the existence of multiple modules from a diversity of traditions ensures pluralism is preserved. We conclude with reflections on how all this relates to the broader notion of moral transcendence implicated in the project of AI moral enhancement, contending it is precisely the whole concrete socio-technical system of moral engagement that we need to model if we are to pursue moral enhancement.</jats:p>"
10.1007/s11023-020-09526-7,Publisher Correction to: The Ethics of AI Ethics: An Evaluation of Guidelines,N/A
10.1136/medethics-2018-105281,Should we be afraid of medical AI?,"<jats:p>I analyse an argument according to which medical artificial intelligence (AI) represents a threat to patient autonomy—recently put forward by Rosalind McDougall in the <jats:italic>Journal of Medical Ethics</jats:italic>. The argument takes the case of IBM Watson for Oncology to argue that such technologies risk disregarding the individual values and wishes of patients. I find three problems with this argument: (1) it confuses AI with machine learning; (2) it misses machine learning’s potential for personalised medicine through big data; (3) it fails to distinguish between evidence-based advice and decision-making within healthcare. I conclude that how much and which tasks we should delegate to machine learning and other technologies within healthcare and beyond is indeed a crucial question of our time, but in order to answer it, we must be careful in analysing and properly distinguish between the different systems and different delegated tasks.</jats:p>"
10.2218/newreal.9258,"AI, Ethics, and the Role of the Artist",<jats:p>Using AI as an artistic medium comes with moral implications and responsibilities which can be difficult to anticipate. How can ethics help us unpick these?</jats:p>
10.2139/ssrn.3711040,"AI, Ethics, and Law: A Possible Way Forward",N/A
10.1163/9789004458109_012,Inherent Rights of Strong AI,N/A
10.1016/b978-0-323-85648-5.00019-0,Ethics of AI in organizations,N/A
10.2139/ssrn.3895522,Communicating Ethics across the AI Ecosystem,N/A
10.1109/ee-rds53766.2021.9708554,Trustworthy AI and Lending [Panel Discussion],N/A
10.1017/9781108936040.022,Ethics Guidelines for Trustworthy AI,N/A
10.4337/9781802203110,Data Ethics of Power,N/A
10.1163/9789004458109_013,Illegal Usage of AI Technology,N/A
10.1002/9781119709183.ch12,"Dealing with Ethics, Privacy, and Security",N/A
10.22503/inftars.xx.2020.4.5,A criticism of AI ethics guidelines,"<jats:p>This paper investigates the current wave of Artificial Intelligence Ethics GUidelines (AIGUs). The goal is not to provide a broad survey of the details of such efforts; instead, the reasons for the proliferation of such guidelines is investigated. Two main research questions are pursued. First, what is the justification for the proliferation of AIGUs, and what are the reasonable goals and limitations of such projects? Second, what are the specific concerns of AI that are so unique that general technology regulation cannot cover them? The paper reveals that the development of AI guidelines is part of a decades-long trend of an ever-increasing express need for stronger social control of technology, and that many of the concerns of the AIGUs are not specific to the technology itself, but are rather about transparency and human oversight.  Nevertheless, the positive potential of the situation is that the intense world-wide focus on AIGUs will yield such profound guidelines that the regulation of other technologies may want to follow suite.</jats:p>"
10.1007/s43681-023-00284-7,"Trolleys, crashes, and perception—a survey on how current autonomous vehicles debates invoke problematic expectations","<jats:title>Abstract</jats:title><jats:p>
Ongoing debates about ethical guidelines for autonomous vehicles mostly focus on variations of the ‘Trolley Problem’. Using variations of this ethical dilemma in preference surveys, possible implications for autonomous vehicles policy are discussed. In this work, we argue that the lack of realism in such scenarios leads to limited practical insights. We run an ethical preference survey for autonomous vehicles by including more realistic features, such as time pressure and a non-binary decision option. Our results indicate that such changes lead to different outcomes, calling into question how the current outcomes can be generalized. Additionally, we investigate the framing effects of the capabilities of autonomous vehicles and indicate that ongoing debates need to set realistic expectations on autonomous vehicle challenges. Based on our results, we call upon the field to re-frame the current debate towards more realistic discussions beyond the Trolley Problem and focus on which autonomous vehicle behavior is considered <jats:italic>not</jats:italic> to be acceptable, since a consensus on what the right solution is, is not reachable.</jats:p>"
10.1093/oxfordhb/9780190067397.013.25,Is Human Judgment Necessary?,"<p>This chapter explores the relationship between rapid developments in artificial intelligence (AI) and the exercise of human judgment. Human judgment is unavoidably exercised in designing AI systems, and yet some of the most consequential forms of judgment are submerged in the formal rigor of algorithmic syntax. Moreover, whether or not one can conclude that the machines running AI themselves “make” judgments in a deep sense, human judgment is increasingly displaced by AI as “smart” machines perform functions that previously required the exercise of human judgment. Even promising AI systems designed to enhance human judgment involve subtle forms of displacement. And AI systems being developed in areas such as the law have powerful effects on the epistemological terrain in which human judgment occurs. This chapter describes these effects and then offers ethical, political, and legal justifications for a doctrine of non-delegation to preserve the conditions of human judgment in appropriate domains of social and legal action.</p>"
10.31234/osf.io/4ghst,Rethinking priorities in AI ethics guidelines in today’s era of humanity under threat,"<p>This paper does not call into question the existing ethical guidelines for artificial intelligence, but suggests rethinking their priorities in today’s era of humanity under threat. In December 2020, the United Nations’ Secretary General described the anthropogenic degradation of the planet, claiming that “the planet is broken.” Adopting the approach of disaster risk reduction, we assert that the ongoing destruction of the planet is a disaster, which leads us to think in terms of resilience. We have examined existing works on ethical guidelines for AI through the lens of philosophy, namely, the imperative of responsibility toward the distant future of nature, including humanity, and an ethics of care articulated around maintenance, continuance and repairs. We have identified five ethical principles: respect for nature, respect for human rights, AI usefulness, AI transparency, and AI trustworthiness, which are explained through 19 subprinciples. We conclude with the difficulty of being concretely nature-friendly in today’s era of humanity under threat.</p>"
10.4337/9781803926728.00018,AI and the Environment,N/A
10.1007/s00146-021-01376-w,A Code of Digital Ethics: laying the foundation for digital ethics in a science and technology company,"<jats:title>Abstract</jats:title><jats:p>The rapid and dynamic nature of digital transformation challenges companies that wish to develop and deploy novel digital technologies. Like other actors faced with this transformation, companies need to find robust ways to ethically guide their innovations and business decisions. Digital ethics has recently featured in a plethora of both practical corporate guidelines and compilations of high-level principles, but there remains a gap concerning the development of sound ethical guidance in specific business contexts. As a multinational science and technology company faced with a broad range of digital ventures and associated ethical challenges, Merck KGaA has laid the foundations for bridging this gap by developing a Code of Digital Ethics (CoDE) tailored for this context. Following a comprehensive analysis of existing digital ethics guidelines, we used a reconstructive social research approach to identify 20 relevant principles and derive a code designed as a multi-purpose tool. Versatility was prioritised by defining non-prescriptive guidelines that are open to different perspectives and thus well-suited for operationalisation for varied business purposes. We also chose a clear nested structure that highlights the relationships between five core and fifteen subsidiary principles as well as the different levels of reference—data and algorithmic systems—to which they apply. The CoDE will serve Merck KGaA and its new Digital Ethics Advisory Panel to guide ethical reflection, evaluation and decision-making across the full spectrum of digital developments encountered and undertaken by the company whilst also offering an opportunity to increase transparency for external partners, and thus trust.</jats:p>"
10.3390/philosophies6020031,AI Ethics and Value Alignment for Nonhuman Animals,"<jats:p>This article is about a specific, but so far neglected peril of AI, which is that AI systems may become existential as well as causing suffering risks for nonhuman animals. The AI value alignment problem has now been acknowledged as critical for AI safety as well as very hard. However, currently it has only been attempted to align the values of AI systems with human values. It is argued here that this ought to be extended to the values of nonhuman animals since it would be speciesism not to do so. The article focuses on the two subproblems—value extraction and value aggregation—discusses challenges for the integration of values of nonhuman animals and explores approaches to how AI systems could address them.</jats:p>"
10.4324/9781003421849-5,AI-cyber nexus,N/A
10.1017/9781009072168.032,AI Judges,N/A
10.1088/978-0-7503-6116-3ch5,Interpretable versus explainable models,N/A
10.5772/intechopen.96798,Bitcoin and Ethics in a Technological Society,"<jats:p>Bitcoin came into existence as a peer-to-peer payment system for use on online transactions. This achievement was the result of a shared vision about the future relationship between governments’ control and citizenry, and the collaborative work of the many who contributed to the development of the cryptographic field. This innovation and its underlying technology, the blockchain, have been at the root of a change of paradigm, as the joint use of blockchain and artificial intelligence (AI) seed the next technological revolution. However, as it is often the case, these revolutionary inventions have also been met with skepticism in the financial sector and society at large. Using the case of Bitcoin and the blockchain, this paper analyzes the intersection between the philosophy and technology underlying these innovations, and the outlook of a sector of society who fears these developments while others try to profit. In this chapter, we first look at the history of Bitcoin together with that of those behind it. We then review the mixed reception it obtained after coming to the market. We assess the innovations’ properties and confront these with the needs of a society eager to obtain further clarity and enjoy more transparency in matters of relevance to their participation in democratic processes.</jats:p>"
10.4135/9781526451293.n12,"Conclusion: Dignity, Ethics, Norms, Policies and Practices",N/A
10.2196/58493,Regulating AI in Mental Health - the Ethics of Care Perspective (Preprint),N/A
10.1215/00029831-10575148,Breaking Up (with) AI Ethics,N/A
10.1088/978-0-7503-6116-3ch2,"Discrimination, bias, and fairness",N/A
10.1017/9781108921923.016,"Balancing Regulation, Innovation and Ethics",N/A
10.31219/osf.io/dscbf,The Reproducibility Issues of Health-Care AI: The Cost and Ethics of Science,<p>The Reproducibility Issues of Health-Care AI: The Cost and Ethics of Science</p>
10.2139/ssrn.2705546,The Ethics Bot: AI Needs Legal and Ethical Guidance,N/A
10.2139/ssrn.3609292,A High-Level Overview of AI Ethics,N/A
10.4324/9780429198533-9,Ethics of artificial intelligence 1,N/A
10.35490/ec3.2024.188,Interactive AI for Generative Housing Design Based on Graph Neural Networks and Deep Generative Models,N/A
10.1111/ele.14397/v2/review1,"Review for ""How widespread use of generative &lt;scp&gt;AI&lt;/scp&gt; for images and video can affect the environment and the science of ecology""",N/A
10.1080/01969722.2023.2296253,The Impact of Generative AI and ChatGPT on Creating Digital Advertising Campaigns,N/A
10.1109/ase56229.2023.00194,Semantic Data Augmentation for Deep Learning Testing Using Generative AI,N/A
10.23967/c.wccm.2024.126,Data-efficient one-step mechanical design of composites using generative AI,N/A
10.1080/17516234.2024.2381549,Unleashing generative AI: funding implications and insights from China,N/A
10.4018/979-8-3693-3278-8.ch012,Exploring the Scikit-Learn Package With ChatGPT,"<jats:p>The Scikit-learn package is a popular machine-learning library in Python that provides tools for data preprocessing, feature selection, and evaluation of models. However, learning it can be hard for beginners due to its vast documentation. The goal of this chapter is to search for ways to use ChatGPT to learn the Scikit-learn package. ChatGPT's first version was used (2023). Using ChatGPT to learn Scikit-learn can help users to understand the key techniques of machine learning.</jats:p>"
10.21203/rs.3.rs-4369365/v1,Analyzing the Efficiency of Moroccan Hospital Network Regions via DEA and Tobit Regression: Assessing DEAP 2.1 Software versus Generative AI ChatGPT 3.5,"<title>Abstract</title>
        <p>This study aims to assess the technical efficiency of hospital networks in each health directorate region in Morocco and analyze the impact of staff personnel health on inefficiency. The study uses Data Envelopment Analysis Programming (DEAP) software version 2.1 and generative Artificial Intelligent ChatGPT 3.5 to analyze 12 hospital network health directorate regions. Tobit regression was employed to analyze the impact of worker health and hospital activity on inefficiency. Results showed that the average technical efficiency was more inefficient in generative AI ChatGPT 3.5 than in DEAP software version 2.1. Hospital activity and nurse staffing significantly impacted inefficiency levels. The study concludes that inefficiency in hospital networks and staff personnel health pose challenges for managers in health directorate regions, emphasizing the need for New Public Management principles based on contractualization, accountability, and managerial practices.</p>"
10.1093/sleep/zsae067.0319,0319 Using Generative AI as a Tool for Simulating EEG During Nocturnal Polysomnography,"<jats:title>Abstract</jats:title>
               <jats:sec>
                  <jats:title>Introduction</jats:title>
                  <jats:p>Artificial Intelligence based Generative Networks based synthetic data generation has become increasingly popular. Deep Learning Generative models can be trained to model complex data distributions and generate diverse and realistic outputs. We aim to present a generative model that can generate an artificial 30-second epoch of non-REM sleep whose power spectrum is identical to that of a real sleep EEG.</jats:p>
               </jats:sec>
               <jats:sec>
                  <jats:title>Methods</jats:title>
                  <jats:p>The data used to train the network was obtained from multiple healthy subjects of the Multi-Ethnic Study of Atherosclerosis (MESA), an NHLBI-sponsored 6-center collaborative longitudinal investigation made available through the National Sleep Research Resource (NSRR). We selected 660 subjects deemed healthy and devoid of any sleep disorders. The EEG was initially recorded at 256 Hz and down sampled to 64 Hz for this study. Only the Fz-Cz channel from the recorded EEG channels was used to train the Generative model. A Deep Learning Generative Adversarial Network was trained using 30s epochs of the NREM Stage 3 EEG data from 660 subjects. Both visual inspection and quantitative metrics were used to evaluate the synthetic signals. The metric to assess Generative Model performance was the relative spectral power (%) in different frequency bands compared to real EEG signals. The averages for the real EEG signals were calculated after taking the average frequency band power across all epochs used for training. The band-power of 100 generated samples, visually deemed good quality, was used to calculate the average for generated EEG signals.</jats:p>
               </jats:sec>
               <jats:sec>
                  <jats:title>Results</jats:title>
                  <jats:p>Across Alpha, Delta, Beta, Theta and Sigma bands, there was minimal difference in % band power between synthetic and real EEG signals: Alpha real[0.5±0.14] / Alpha synthetic[2.46±0.52], Beta real[0.1±0.03] / Beta synthetic[0.058±0.03] , Theta real[1.8±0.78] / Theta synthetic[4.18±0.9], Delta real[97.0±1.26] / Delta synthetic[93.2.0±2.25] and Sigma real[0.6±0.04] / Sigma synthetic[0.084±0.01]</jats:p>
               </jats:sec>
               <jats:sec>
                  <jats:title>Conclusion</jats:title>
                  <jats:p>The developed system has the potential to be a sleep EEG simulator used as an educational tool to train fellows and technicians across the field of sleep medicine. Future studies are needed to accommodate multichannel EEG signals and other PSG signals (e.g., respiratory, Spo2 etc.) to simulate an entire nocturnal polysomnography.</jats:p>
               </jats:sec>
               <jats:sec>
                  <jats:title>Support (if any)</jats:title>
                  <jats:p>This work is supported by NIH R21HL165320, NIH K25HL151912 and NIH R21HL165320</jats:p>
               </jats:sec>"
10.14236/ewic/bcshci2023.11,Supporting dementia caregivers in Peru through chatbots: generative AI vs structured conversations.,N/A
10.1109/mcg.2024.3362168,Generative AI for Visualization: Opportunities and Challenges,N/A
10.2139/ssrn.4865398,Generative AI as a personal tutor for English language learning: A review of benefits and concerns,N/A
10.1145/3657304,Generative AI --- The End of Systematic Reviews in PhD Projects?,N/A
10.15581/003.37.3.255-271,Generative AI changes the book publishing industry: reengineering of business processes,"<jats:p>The research defines main direction of book publishing houses reengineering based on the analysis of successful cases of AI use in publishing business. The timeline of the research started in August 2023 and was summarised in the beginning of January 2024. The main methods were expert interview, monitoring of international and Ukrainian internet platforms, and document analysis. The study showed that the main aspects of business processes reengineering in publishing houses, based on the use of AI, are: (1) development of business strategies and plans; (2) development of digital spaces in publishing houses; (3) emerging of new professions; (4) discussions and their summaries; (5) received manuscripts check; (6) finding plagiarism; (7) preparation of creative, advertising, and presentation materials; (8) working with numbers and databases. The recommendations on the use of AI in business processes are extracted from the policies of the organisations connected with the book publishing industry. They are presented in the convenient table for further use.
One of the study results showed that Ukrainian publishing houses discuss the capabilities of AI for generating different types and formats of content, and based on that, AI capabilities for reengineering are considered. One of the biggest challenges, created by AI, is that the technology develops faster than people can perceive so they struggle to describe the technology itself and its impact. It means that we should adjust to the changes, caused by exponential development of AI, finding resources to overcome unequal access to AI capabilities in the process of specialists’ preparation.</jats:p>"
10.1097/js9.0000000000001690,Generative AI intervention clinical trials: a call for pre-registration (Correspondence),N/A
10.1007/s40593-023-00367-w,How to Harness Generative AI to Accelerate Human Learning,N/A
10.55662/jst.2023.4603,Importance Of Using Generative AI In Education: Dawn of a New Era,"<jats:p>The teaching methodology has witnessed a paradigm shift by incorporating advanced technologies. The modern world offers a whole new insight into the mental well-being of students. This transformation has opened new doors that are eye-opening for the human mind.
The invention of artificial intelligence (AI) in the modern world has helped solve and detect problems. Among all these developments, one has shifted the standard of the educational world. It has taken over the old methods and techniques the students did not previously appreciate. This technology, the Generative Artificial Intelligence (GAI), has altered the orthodox educational approaches. It has provided a potential benefit for delivering knowledge in the best direction, enabling students to understand and implement it in their lives.
Therefore, this paper significantly focuses on the impact of Generative Artificial Intelligence (GAI) technology in providing protection and assistance in the education system. It has contributed value and principles to the revolution of the modern education system. The data declared in the paper was taken from effective sources and proved to favor the study.</jats:p>"
10.5121/csit.2024.141203,Experiences in Training Teachers at Universities in Baja California on Generative AI,"<jats:p>This study explores the integration of generative artificial intelligence, especially models such as ChatGPT, into teacher training, highlighting both the promises and challenges of adopting advanced technologies in an educational context. Through a course titled ""Introduction to Generative Artificial Intelligence for Teachers,"" 97 educators from various universities in Baja California participated, spanning a wide range of ages. The qualitative methodology adopted allowed an in-depth exploration of teachers' perceptions, experiences, and expectations regarding AI in education. Results showed a generally positive evaluation of the course, with significant emphasis on the importance of AI in transforming educational practices. Approximately 41.38% of the comments highlighted the potential of gen AI to support and enhance teaching and learning. Additionally, there was a clear interest in deepening knowledge about AI, as well as a need for ongoing training strategies. However, the study also emphasizes critical reflections on the ethical and practical challenges of integrating AI into education, underscoring the importance of a reflective and ethical approach. The demand for gen AI training by educational institutions indicates a global trend toward the adoption of these technologies. The study concludes with recognition of the potential of AI to enrich pedagogy, provided that the associated risks are considered, and ethical and effective adoption is promoted.</jats:p>"
10.1109/contesa61248.2023.10384863,Generative AI: Impactful Considerations to Responsible Data Practices in Business Execution,N/A
10.1109/comsds61892.2024.10502105,The Problem of Communicating with Generative AI,N/A
10.1145/3613905.3636273,HCI History and the Trajectory to Generative AI,N/A
10.1145/3658619.3658628,Incorporating Unanticipated Uses of Generative AI into HCI Education,N/A
10.1007/s13347-023-00621-y,"AI as Agency Without Intelligence: on ChatGPT, Large Language Models, and Other Generative Models",N/A
10.1016/j.caeai.2023.100197,Future research recommendations for transforming higher education with generative AI,N/A
10.3389/frobt.2022.728776,Towards a Personality AI for Robots: Potential Colony Capacity of a Goal-Shaped Generative Personality Model When Used for Expressing Personalities via Non-Verbal Behaviour of Humanoid Robots,"<jats:p>Engineering robot personalities is a challenge of multiple folds. Every robot that interacts with humans is an individual physical presence that may require their own personality. Thus, robot personalities engineers face a problem that is the reverse of that of personality psychologists: robot personalities engineers need to make batches of identical robots into individual personalities, as oppose to formulating comprehensive yet parsimonious descriptions of individual personalities that already exist. The robot personality research so far has been fruitful in demonstrating the positive effects of robot personality but unfruitful in insights into how robot personalities can be engineered in significant quantities. To engineer robot personalities for mass-produced robots we need a generative personality model with a structure to encode a robot’s individual characteristics as personality traits and generate behaviour with inter- and intra-individual differences that reflect those characteristics. We propose a generative personality model shaped by goals as part of a personality AI for robots towards which we have been working, and we conducted tests to investigate how many individual personalities the model can practically support when it is used for expressing personalities <jats:italic>via</jats:italic> non-verbal behaviour on the heads of humanoid robots.</jats:p>"
10.15235/jir.2024.3.27.1.1,Generative AI regulation and media literacy,N/A
10.1109/ic3i59117.2023.10397719,Image Multidiffusion Algorithms for AI Generative Art,N/A
10.1109/iset58841.2023.00031,An Empirical Study on Human-Machine Collaborative MOOC Learning Interaction Empowered by Generative AI,N/A
10.1016/j.resuscitation.2024.110164,What is the best approach to assessing generative AI in medicine?,N/A
10.2196/preprints.54371,Evaluation of Generative Language Models in Personalizing Medical Information: Instrument Validation Study (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>Although uncertainties exist regarding implementation, artificial intelligence–driven generative language models (GLMs) have enormous potential in medicine. Deployment of GLMs could improve patient comprehension of clinical texts and improve low health literacy.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>The goal of this study is to evaluate the potential of ChatGPT-3.5 and GPT-4 to tailor the complexity of medical information to patient-specific input education level, which is crucial if it is to serve as a tool in addressing low health literacy.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>Input templates related to 2 prevalent chronic diseases—type II diabetes and hypertension—were designed. Each clinical vignette was adjusted for hypothetical patient education levels to evaluate output personalization. To assess the success of a GLM (GPT-3.5 and GPT-4) in tailoring output writing, the readability of pre- and posttransformation outputs were quantified using the Flesch reading ease score (FKRE) and the Flesch-Kincaid grade level (FKGL).</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>Responses (n=80) were generated using GPT-3.5 and GPT-4 across 2 clinical vignettes. For GPT-3.5, FKRE means were 57.75 (SD 4.75), 51.28 (SD 5.14), 32.28 (SD 4.52), and 28.31 (SD 5.22) for 6th grade, 8th grade, high school, and bachelor’s, respectively; FKGL mean scores were 9.08 (SD 0.90), 10.27 (SD 1.06), 13.4 (SD 0.80), and 13.74 (SD 1.18). GPT-3.5 only aligned with the prespecified education levels at the bachelor’s degree. Conversely, GPT-4’s FKRE mean scores were 74.54 (SD 2.6), 71.25 (SD 4.96), 47.61 (SD 6.13), and 13.71 (SD 5.77), with FKGL mean scores of 6.3 (SD 0.73), 6.7 (SD 1.11), 11.09 (SD 1.26), and 17.03 (SD 1.11) for the same respective education levels. GPT-4 met the target readability for all groups except the 6th-grade FKRE average. Both GLMs produced outputs with statistically significant differences (&lt;i&gt;P&lt;/i&gt;&amp;lt;.001; 8th grade &lt;i&gt;P&lt;/i&gt;&amp;lt;.001; high school &lt;i&gt;P&lt;/i&gt;&amp;lt;.001; bachelors &lt;i&gt;P&lt;/i&gt;=.003; FKGL: 6th grade &lt;i&gt;P&lt;/i&gt;=.001; 8th grade &lt;i&gt;P&lt;/i&gt;&amp;lt;.001; high school &lt;i&gt;P&lt;/i&gt;&amp;lt;.001; bachelors &lt;i&gt;P&lt;/i&gt;&amp;lt;.001) between mean FKRE and FKGL across input education levels.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>GLMs can change the structure and readability of medical text outputs according to input-specified education. However, GLMs categorize input education designation into 3 broad tiers of output readability: easy (6th and 8th grade), medium (high school), and difficult (bachelor’s degree). This is the first result to suggest that there are broader boundaries in the success of GLMs in output text simplification. Future research must establish how GLMs can reliably personalize medical texts to prespecified education levels to enable a broader impact on health care literacy.</p>
                </sec>
                                <sec>
                    <title>CLINICALTRIAL</title>
                        <p/>
                </sec>"
10.5194/ica-abs-6-82-2023,Automating conversion of remote sensing images to human readable map images using generative AI,"<jats:p>
                    </jats:p>"
10.4018/979-8-3693-3731-8.ch018,Generative Intelligence,"<jats:p>Generative intelligence is changing healthcare prevention, diagnosis, and therapy. Advanced technologies like AI enable individualized care, predictive analytics, and streamlined workflows. This study examines generative intelligence's significance in medication discovery, disease prediction, and patient outcomes. To protect privacy and encourage innovation, regulation, ethics, and implementation must be addressed. Generative intelligence can transform healthcare into a predictive, preventive, and personalized system that improves global well-being.</jats:p>"
10.1145/3591196.3596820,Generative Image AI Using Design Sketches as input: Opportunities and Challenges,N/A
10.5539/elt.v17n9p48,Can ChatGPT Understand Malapropism Correctly? Challenges to Davidson&amp;#39;s Passing Theory in Generative AI,"<jats:p>This study examines ChatGPT&amp;#39;s performance in understanding Japanese malapropisms, aiming to explore its capacity for linguistic inference compared to humans. Despite its remarkable fluency in conversation, ChatGPT shows significant limitations in comprehending malapropisms, particularly in handling phonetic, lexical, and contextual errors. Using a specialized dataset, the research highlights these gaps, suggesting that while ChatGPT excels in fluency, its understanding of nuanced language phenomena remains distinct from human comprehension. The findings contribute to the discourse on the potential and limitations of generative AI, advocating for a reevaluation of linguistic-philosophical theories in light of AI advancements.</jats:p>"
10.1080/08874417.2023.2286540,Organizational Sustainability of Generative AI-Driven Optimization Intelligence,N/A
10.4018/979-8-3693-0502-7.ch001,Introduction to ChatGPT,"<jats:p>This chapter provides a comprehensive overview of ChatGPT, an advanced language model that has gained significant attention in natural language processing (NLP) and artificial intelligence (AI). It outlines the underlying architecture, features, applications, benefits, and limitations of ChatGPT. The chapter highlights ChatGPT's ability to facilitate human-like conversations through its understanding and generation of human-like text. It explores its versatility across domains and languages and its potential in customer support, virtual assistants, chatbots, language translation, content generation, and creative writing. The benefits of ChatGPT, such as improved efficiency and scalability, are discussed, as are the limitations and ethical considerations. The chapter concludes with a future outlook, discussing ongoing research and the potential impact of ChatGPT on communication and human-machine interactions, emphasizing the need for responsible development and deployment of this powerful language model.</jats:p>"
10.2139/ssrn.4468899,PERSONALIZATION: Why the relational modes between Generative AI chatbots and human users are critical factors for product design and safety,N/A
10.1093/neuonc/noae043,Harnessing generative AI for glioma diagnosis: A step forward in neuro-oncologic imaging,N/A
10.2478/nimmir-2024-0003,Beyond Prompt Engineering: Skills Marketers Need to Deploy Generative AI Successfully,N/A
10.1007/979-8-8688-0473-1_5,Content Generation,N/A
10.2174/0115748936303223240404043202,"Comparative Analysis of Deep Generative Model for Industrial Enzyme
Design","<jats:sec>
<jats:title>Abstract:</jats:title>
<jats:p>Although enzymes have the advantage of efficient catalysis, natural enzymes lack stability in industrial environments and do not even meet the required catalytic reactions. This prompted
us to urgently de novo design new enzymes. Computational design is a powerful tool, allowing rapid and efficient exploration of sequence space and facilitating the design of novel enzymes tailored
to specific conditions and requirements. It is beneficial to de novo design industrial enzymes using
computational methods. Currently, only one tool explicitly designed for the enzyme-only generation
performs unsatisfactorily. We have selected several general protein sequence design tools and systematically evaluated their effectiveness when applied to specific industrial enzymes. We investigated the literature related to protein generation. We summarized the computational methods used
for sequence generation into three categories: structure-conditional sequence generation, sequence
generation without structural constraints, and co-generation of sequence and structure. To effectively evaluate the ability of six computational tools to generate enzyme sequences, we first constructed
a luciferase dataset named Luc_64. Then we assessed the quality of enzyme sequences generated by
these methods on this dataset, including amino acid distribution, EC number validation, etc. We also
assessed sequences generated by structure-based methods on existing public datasets using sequence recovery rates and root-mean-square deviation (RMSD) from a sequence and structure perspective. In the functionality dataset, Luc_64, ABACUS-R, and ProteinMPNN stood out for producing sequences with amino acid distributions and functionalities closely matching those of naturally occurring luciferase enzymes, suggesting their effectiveness in preserving essential enzymatic
characteristics. Across both benchmark datasets, ABACUS-R and ProteinMPNN, have also exhibited the highest sequence recovery rates, indicating their superior ability to generate sequences closely resembling the original enzyme structures. Our study provides a crucial reference for researchers
selecting appropriate enzyme sequence design tools, highlighting the strengths and limitations of
each tool in generating accurate and functional enzyme sequences. ProteinMPNN and ABACUS-R
emerged as the most effective tools in our evaluation, offering high accuracy in sequence recovery
and RMSD and maintaining the functional integrity of enzymes through accurate amino acid distribution. Meanwhile, the performance of protein general tools for migration to specific industrial enzymes was fairly evaluated on our specific industrial enzyme benchmark.</jats:p>
</jats:sec>"
10.1093/elt/ccae033,"Generative AI-assisted, evidence-informed use of L1 in L2 classrooms","<jats:title>Abstract</jats:title>
               <jats:p>Purposeful and strategic use of L1 can help with L2 learning. However, in many contexts, monolingual immersion approaches dominate, leading language teachers to refrain from using L1. It can also mean that teachers are not professionally prepared to implement evidence-informed uses of L1. In this article, we share the findings of an intervention study that aimed to raise preservice English language teachers’ awareness of purposeful L1 use while co-exploring ways generative artificial intelligence (AI) tools (e.g. ChatGPT) can aid teachers’ knowledge development and strategic utilization of L1 in L2 classrooms. Data were collected from fifty-six preservice language teachers in Hong Kong through a pre- and post-intervention mixed-method survey and follow-up group interviews. The findings show that explicit instruction on the use of L1 in L2 classrooms can increase preservice teachers’ intention to use L1 as well as their knowledge about the evidence-informed use of L1 and the ways in which generative AI tools can assist their implementation of L1.</jats:p>"
10.22273/smlt.95.8,Generative AI and the restructuring of knowledge,N/A
10.1002/9781394308286.ch8,"<scp>ChatGPT</scp>
            in Action: Practical Applications",N/A
10.2139/ssrn.4754950,Managing Generative AI in Firms: The Theory of Shadow User Innovation,N/A
10.1007/978-3-031-55642-5_1,An Overview on Large Language Models,N/A
10.1016/j.ajp.2024.103929,Generative AI in psychiatry: A potential companion in the current therapeutic era!,N/A
10.22554/ijtel.v7i1.114,Will ChatGPT pass the online quizzes? Adapting an assessment strategy in the age of generative AI,"<jats:p>As generative AI (artificial intelligence) technologies, such as ChatGPT, become increasingly available, traditional online assessments must be re-evaluated to maintain their educational value. Open-book online quizzes have long been an effective tool for engaging students, effectively supporting learning, and reinforcing fundamental knowledge and skills. However, the ease of using AI to complete these quizzes may undermine their intended purpose. This article explores the initial findings of using ChatGPT to answer twelve online quizzes used for continuous assessment in two first-year quantitative techniques modules on business programmes in an Irish technological university. ChatGPT, along with suitable plugins, is increasingly accurate in answering the online quizzes, with ChatGPT-3.5 scoring an average of 35%, ChatGPT-4 47% and ChatGPT-4 with Wolfram plugin 78%. Most of the incorrect responses are due to calculation errors; if these are corrected by simply checking the arithmetic with a calculator, the averages increase to ChatGPT-3.5 scoring 72%, ChatGPT-4 76% and ChatGPT-4 with Wolfram plugin 80%. Thus, the online quizzes on these modules can be quickly completed with the assistance of ChatGPT with a high level of success. The implications of this for using online quizzes as an assessment strategy are discussed; potential assessment redesigns are outlined, including how to integrate generative AI into the learning and assessment process in an ethical and constructive manner. Although generative AI provides a challenge to traditional online quizzes, it also has the potential to aid student comprehension and learning, and the skills of prompt engineering are likely to become increasingly relevant and useful.</jats:p>"
10.1016/j.ajp.2023.103736,Impact of COVID-19 on mental health in the US with generative AI,N/A
10.1109/iceict57916.2023.10244824,Developing Massive Open Online Course Style Assessments using Generative AI Tools,N/A
10.47363/jaicc/2024(3)209,Scaling Generative AI in Enterprise IT Operations: Challenges and Opportunities,"<jats:p>This research paper delves into industries that can benefit from the implementation of AI, such, as IT operations, healthcare, finance and gaming. The potential of AI lies in its ability to enhance customer service create personalized content assist in research endeavors optimize investment strategies and create virtual environments. However businesses often encounter challenges when moving from AI proof of concept to implementation. These challenges involve scalability, integration with existing systems, data governance considerations aligning objectives and meeting requirements. To tackle these concerns head on this paper suggests the utilization of a converged infrastructure platform that combines computing power with network capabilities and storage resources using NVIDIA GPUs. It recommends evaluating existing AI proof of concepts and providing the infrastructure to transform each concept into an use case. The primary goal of this research study is to explore the process involved in converting AI proof of concepts into use cases while understanding its significance in harnessing AI capabilities, within organizations.</jats:p>"
10.35534/cjsg.0502003,Legal Risks and Regulatory Approaches to Generative AI —Take ChatGPT as an Example,N/A
10.1007/979-8-8688-0447-2,Copilot for Microsoft 365,N/A
10.36227/techrxiv.171216659.95569463/v1,Generative AI-Based Text Generation Methods Using Pre-Trained GPT 2 Model,"<jats:p id=""p1"">A text generation model is a machine learning model that uses neural
networks, especially transformers architecture to generate contextually
relevant text based on linguistic patterns learned from extensive
corpora. The models are trained on a huge amount of textual data so that
they can model and learn complex concepts of any language like its
grammar, vocabulary, phrases, and styles.</jats:p>"
10.1109/icoco59262.2023.10397858,"Heuristic Analysis for Security, Privacy and Bias of Text Generative AI: GhatGPT-3.5 case as of June 2023",N/A
10.1109/ccict62777.2024.00021,Information Quality Dimensions in Generative Conversational AI for Financial Inclusion,N/A
10.17265/2328-2177/2024.08.006,On the Application of Generative AI in College English,N/A
10.21125/inted.2024.1279,DEVELOPING A METHODOLOGICAL APPROACH AND FRAMEWORK TO ASSESS STUDENTS’ ETHICAL APPROACHES TO GENERATIVE AI UTILISATION FOR ASSESSMENTS,N/A
10.1145/3614419.3644014,Reacting to Generative AI: Insights from Student and Faculty Discussions on Reddit,N/A
10.1145/3610542.3626142,OwnDiffusion: A Design Pipeline Using Design Generative AI to preserve Sense Of Ownership,N/A
10.1016/j.technovation.2024.103064,"Assessing the nexus of Generative AI adoption, ethical considerations and organizational performance",N/A
10.1007/s43681-024-00531-5,Auditing and instructing text-to-image generation models on fairness,"<jats:title>Abstract</jats:title><jats:p>Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called <jats:sc>Fair Diffusion</jats:sc>, to attenuate biases during the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias in any direction based on human instructions yielding arbitrary proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, requiring no data filtering nor additional training.</jats:p>"
10.2478/nimmir-2024-0002,Generative AI for Marketing Content Creation: New Rules for an Old Game,N/A
10.1016/j.ifacol.2024.08.458,Experiences and Insights from a Mini-Course on Responsible Generative AI Use in Aerospace Engineering,N/A
10.1002/tl.20624,Generative AI detection in higher education assessments,"<jats:title>Abstract</jats:title><jats:p>This chapter presents a critical analysis of generative AI (GenAI) detection tools in higher education assessments. The rapid advancement and widespread adoption of GenAI, particularly in education, necessitates a reevaluation of traditional academic integrity mechanisms. I explore the effectiveness, vulnerabilities, and ethical implications of AI detection tools in the context of preserving academic integrity. My analysis synthesizes insights from various case studies, newspaper articles, and student testimonies to scrutinize the practical and philosophical challenges associated with AI detection. I argue that reliance on detection mechanisms is misaligned with the educational landscape, where AI plays an increasing role. I advocate for a strategic shift toward robust assessment methods and educational policies that embrace GenAI usage while ensuring academic integrity and authenticity in assessments.</jats:p>"
10.1007/979-8-8688-0083-2_1,Setting the Stage for the Interfaceless Future,N/A
10.1111/ele.14397/v1/review1,"Review for ""How widespread use of generative &lt;scp&gt;AI&lt;/scp&gt; for images and video can affect the environment and the science of ecology""",N/A
10.51202/9783181024232-405,Generative Artificial Intelligence – How AI Models Change the Way We Develop Automotive Products,N/A
10.47116/apjcri.2024.04.44,A Case Study on Creative English Writing Using Generative AI,N/A
10.21125/inted.2024.1788,APPLYING GENERATIVE AI TO FACILITATE CLASS ORCHESTRATION WITH TECHNOLOGY: CHATGPT AND NEARPOD FOR SEMINAR ACTIVITIES,N/A
10.1145/3643834.3660750,GenFrame – Embedding Generative AI Into Interactive Artifacts,N/A
10.1016/j.technovation.2024.103063,Building entrepreneurial resilience during crisis using generative AI: An empirical study on SMEs,N/A
10.1016/j.orgdyn.2024.101045,Generative AI in Responsible Conversational Agent Integration: Guidelines for Service Managers,N/A
10.21428/e4baedd9.b3dfc1cf,Generative AI for Textile Engineering: Blending Tradition and Functionality Through Lace,N/A
10.1177/10778004241250065,“In Minutes Instead of Weeks”: Discursive Constructions of Generative AI and Qualitative Data Analysis,"<jats:p> The use of qualitative data analysis software (QDAS) platforms have always posed a dilemma for researchers, and the integration of generative artificial intelligence (AI) tools are complexifying this relationship even further. The way QDAS companies are positioning this new development will impact how researchers understand what qualitative analysis is and what it could be. Using discourse analysis methods, we explored how ATLAS.ti, NVivo, and MAXQDA websites constructed the relationship between AI-assist and qualitative research methods. We noted four “discursive dilemmas” across the websites: (a) automated insight-generation versus systematic meaning-making; (b) chatting with documents versus analyzing data; (c) high speed versus high engagement; and (d) novelty versus agency. While some level of hyperbolic discourse can be expected from corporations whose goal is to sell products, we argue that the discourses used on these websites may be incompatible with the epistemological foundations of qualitative research. </jats:p>"
10.55982/openpraxis.16.2.640,Generative AI and Educators: Partnering in Using Open Digital Content for Transforming Education,N/A
10.1017/pds.2024.215,"Nature's lessons, AI's power: sustainable process design with generative AI","<jats:title>Abstract</jats:title><jats:p>In the realm of process engineering, the pursuit of sustainability is paramount. Traditional approaches can be time-consuming and often struggle to address modern environmental challenges effectively. This article explores the integration of generative AI, as a powerful tool to generate solution ideas and solve problems in process engineering using a Solution-Driven Approach (SDA). SDA applies nature-inspired principles to tackle intricate engineering challenges. In this study, generative AI is trained to understand and use the SDA patterns to suggest solutions to complex engineering challenges.</jats:p>"
10.52214/salt.v24i1.12865,Generative AI as Writing or Speaking Partners in L2 Learning: Implications for Learning-Oriented Assessments,"<jats:p>The advent of generative AI (GenAI) technology has impacted second language (L2) learning and assessment, offering new opportunities for learners to practice and improve their skills. One approach gaining interest is employing GenAI tools as writing or speaking partners to provide personalized, real-time feedback and assistance to learners. These interactions allow learners to practice their writing and speaking skills while receiving assessment information on various aspects of language, including grammar, vocabulary, and pronunciation. Considering the potential of GenAI tools to enrich assessment and learning experiences, it is worth examining recent research on the use of this technology for this purpose.  This paper reviews the literature on the use of GenAI as writing and speaking partners through the lens of the Learning-Oriented Assessment (LOA) framework (Purpura, 2024; Turner &amp; Purpura, 2016) to explore how assessment data from GenAI tools could be leveraged to further learning. </jats:p>"
10.1007/978-3-031-67991-9_3,The New Hybrid,N/A
10.60087/jaigs.v5i1.178,Leveraging Intent Detection and Generative AI for Enhanced Customer Support,"<jats:p>Customer support plays a pivotal role in shaping customer satisfaction and fostering loyalty within any business. This paper delves into how the integration of intent detection and generative AI (GenAI) can transform customer support systems. At the core of this transformation is the ability to understand user intent, which is essential for directing customers effectively through the support funnel to the appropriate services. By employing sophisticated natural language processing (NLP) techniques, training LLM to perform RAG and machine learning models, businesses can precisely discern customer intents. This capability allows for the delivery of tailored, immediate responses. The paper further explores the methodologies employed, the advantages gained, and the challenges faced in the adoption of these advanced technologies in customer support systems.</jats:p>"
10.1109/jcsse61278.2024.10613707,Generative AI: How Well Can it Understand Conversational UX?,N/A
10.1109/c358072.2023.10436249,Generative AI: The key for everyday problems. A comparison proposal for new users,N/A
10.37074/jalt.2024.7.2.38,"Negative effects of Generative AI on researchers: Publishing addiction, Dunning-Kruger effect and skill erosion",N/A
10.1007/s43681-021-00105-9,Five ethical challenges facing data-driven policing,N/A
10.1007/s43681-022-00211-2,Value elicitation on a scenario of autonomous weapon system deployment: a qualitative study based on the value deliberation process,"<jats:title>Abstract</jats:title><jats:p>Ethical concerns on autonomous weapon systems (AWS) call for a process of human oversight to ensure accountability over targeting decisions and the use of force. To align the behavior of autonomous systems with human values and norms, the Design for Values approach can be used to consciously embody values in the deployment of AWS. One instrument for the elicitation of values during the design is participative deliberation. In this paper, we describe a participative deliberation method and results of a value elicitation by means of the value deliberation process for which we organized two panels each consisting of a mixture of experts in the field of AWS working in military operations, foreign policy, NGO’s and industry. The results of our qualitative study indicate not only that value discussion leads to changes in perception of the acceptability of alternatives, or options, in a scenario of AWS deployment, it also gives insight in to which values are deemed important and highlights that trust in the decision-making of an AWS is crucial.</jats:p>"
10.1145/3600211.3604680,A sector-based approach to AI ethics: Understanding ethical issues of AI-related incidents within their sectoral context,N/A
10.4324/9781003383741-21,Managing ethics online,N/A
10.1007/s00146-023-01809-8,The art of the semi-living: ethics of care and the bioart of Oron Catts and Ionat Zurr,N/A
10.12681/jpentai.37109,"AI, International Relations &amp; Religion","<jats:p>This research envisions a future where humans and machines collaboratively enhance decision-making capabilities, fostering harmonious coexistence. Addressing concerns about the potential threat of artificial intelligence (AI) to humanity, the focus shifts to the benevolence of AI entities shaped by human influence. The prospect of AI functioning at a level where authority is wielded by an inaccessible and infallible entity lies in its role as an independent arbiter. This entails the capability to identify cultural barriers and navigate existing political constraints deliberately. Consequently, there is potential for discovering common political ground through algorithmic processes, leading to the resolution of longstanding political issues between states. However, uncertainties persist – perhaps these aspirations may not materialize as expected. The study explores AI's role in international relations and religion, particularly Christianity, emphasizing its potential as an independent arbiter capable of recognizing cultural barriers and navigating political constraints. This research explores the intersection of cultural sensitivity and AI in diplomacy, discussing ethical considerations and benefits. The impact of AI on conflict resolution and peacebuilding is examined, stressing the need for collaborative efforts to establish robust AI standards. Challenges to religious authority, ethical considerations in AI development, and AI's influence on humanitarian aid and religious values are also explored. The research concludes by highlighting the imperative to address algorithmic bias for inclusivity and equitable representation in the digital age.</jats:p>"
10.1017/9781009072168.005,Essence of AI,N/A
10.1145/3600211.3604734,The Mechanical Psychologist: Leveraging AI for Detecting Predatory Behaviour in Online Interactions,N/A
10.4337/9781803924021.00019,AI-based interaction analysis between humans (and other living creatures),N/A
10.1016/j.jhep.2023.07.028,ChatGPT: The transformative influence of generative AI on science and healthcare,N/A
10.26689/ief.v2i7.7716,Harmonizing Code and Canvas: The Role of Generative AI in Aesthetic Education for Vocational Students,"<jats:p>This paper delves into the pivotal role of generative artificial intelligence (AI) within vocational aesthetic education, specifically focusing on its capacity to augment artistic expression and cultivate technical proficiency. As AI undergoes continuous evolution, it catalyzes significant transformations in educational paradigms by amalgamating creativity with digital acumen, thereby equipping the workforce with adeptness in navigating technologically driven landscapes. The study critically examines how AI reconfigures educational frameworks, enriching learning experiences through tailored, collaborative, and globally oriented approaches. Additionally, it scrutinizes the ethical, pragmatic, and pedagogical hurdles intrinsic to AI integration, encompassing concerns such as data privacy, bias mitigation, and the imperative for perpetual curriculum innovation and educator empowerment. The findings underscore that the realization of AI’s potential in advancing vocational aesthetic education hinges upon strategic deployment, ongoing evaluation, and inclusive discourse among all educational stakeholders, ensuring alignment with educational objectives and responsiveness to global labor market exigencies. This analysis underscores AI’s capacity to optimize educational outcomes and equip students for a multifaceted and evolving future, advocating for proactive and inclusive strategies to harness AI’s advantages within educational milieus.</jats:p>"
10.1145/3563657.3596014,"Design Ideation with AI - Sketching, Thinking and Talking with Generative Machine Learning Models",N/A
10.1145/3672534,Large Language Objects: The Design of Physical AI and Generative Experiences,N/A
10.2139/ssrn.4809740,Crafting Portfolios Tailored to Investor Preferences with Generative AI,N/A
10.1080/15228959.2023.2266358,Generative AI and ChatGPT: Friend or foe for academic libraries?,N/A
10.1016/j.cpa.2024.102761,Who speaks through the machine? Generative AI as discourse and implications for management,N/A
10.1145/3657054.3657124,Mitigating the Risks of Generative AI in Government through Algorithmic Governance,N/A
10.1016/j.caeai.2024.100275,Exploring the potential of generative AI in democratizing English language education,N/A
10.21608/jstc.2024.338476,Generative AI Poses Security Risks,N/A
10.2139/ssrn.4586863,Dali or DALL-E? Popper or …? The Implications of Emerging Generative AI on the Future of Creative Work,N/A
10.51803/yssr.1440501,Impact of Generative AI on FINTECH in Africa,"<jats:p xml:lang=""en"">The financial technology (Fintech) industry in Africa is expanding and growing quickly. Despite several regulatory contexts, political, economic, and regulatory obstacles, Fintech is booming throughout the continent. Over the past few years, the information technology industry has grown dramatically, and a large number of these new businesses are focused on upending the financial technology industry. Because it can understand customer preferences, spending habits, and financial goals, generative AI has a lot of promise to provide personalized financial recommendations or solutions to any individual. With the new paradigm of generative AI playing a more critical role, it may have significant impact on fostering the growth of Fintech within Africa. The article provides a comprehensive review of the current state of Fintech within Africa and the impact of generative AI on fostering the growth of it.</jats:p>"
10.1145/3674844,Generative AI and the Future of Democratic Citizenship,"<jats:p>Generative AI technologies have the potential to be socially and politically transformative. In this paper, we focus on exploring the potential impacts that Generative AI could have on the functioning of our democracies and the nature of citizenship. We do so by drawing on accounts of deliberative democracy and the deliberative virtues associated with it, as well as the reciprocal impacts that social media and Generative AI will have on each other and the broader information landscape. Drawing on this background theory, we outline some of the key positive and negative impacts that Generative AI is likely to have on democratic citizenship. The political significance of these impacts suggests the need for further regulation.</jats:p>"
10.2139/ssrn.4468637,Generative AI Systems: Impacts on Artists &amp;amp; Creators and Related Gaps in the Artificial Intelligence and Data Act,N/A
10.1109/ictc58733.2023.10392804,Generative AI for Radiological Image Data: Current Trends and Outlook,N/A
10.1145/3490100.3516473,Leveraging Generative Conversational AI to Develop a Creative Learning Environment for Computational Thinking,N/A
10.1007/s42979-024-02965-4,A Generative AI-Based Assistant to Evaluate Short and Long Answer Questions,N/A
10.1201/9781032632223-22,Healthcare in the Era of Generative AI,N/A
10.69554/yqbv7690,"What executives need to know about knowledge management, large language models and generative AI","<jats:p xml:lang=""en"">This paper discusses the opportunities and risks presented by large language models (LLMs), which power the popular and widely adopted Chat-GPT types of applications. The potential benefits include support for enhancing the customer journey and efficient management of an ever-increasing volume of information for employees. Risks include hallucinations (made up answers by generative AI that are not factually correct), exposure of corporate intellectual property (IP) to training models, lack of traceability and audit trails and misalignment with brand guidelines. The approach to handling risk described in this paper is retrieval-augmented generation (RAG), which references corporate knowledge and data sources in order to identify precise answers and retrieve exactly what users want. The paper also outlines the need for a knowledge architecture which enables enriched embeddings into vector databases which retain the context of intelligently componentised content. Using RAG requires knowledge hygiene and metadata models, and the paper discusses an experiment in which results were measured with and without the knowledge architecture. The improvement was significant: 53 per cent of questions were answered correctly without the model versus 83 per cent with the model. The use of RAG virtually eliminated hallucinations, secured corporate IP and provided traceability and an audit trail.</jats:p>"
10.21606/drs.2024.953,Re-imagining and reaffirming design pedagogy in response to generative AI tools,N/A
10.2196/preprints.59434,Use of Generative AI for Improving Health Literacy in Reproductive Health: Case Study (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>Patients find technology tools to be more approachable for seeking sensitive health-related information, such as reproductive health information. The inventive conversational ability of artificial intelligence (AI) chatbots, such as ChatGPT (OpenAI Inc), offers a potential means for patients to effectively locate answers to their health-related questions digitally.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>A pilot study was conducted to compare the novel ChatGPT with the existing Google Search technology for their ability to offer accurate, effective, and current information regarding proceeding action after missing a dose of oral contraceptive pill.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>A sequence of 11 questions, mimicking a patient inquiring about the action to take after missing a dose of an oral contraceptive pill, were input into ChatGPT as a cascade, given the conversational ability of ChatGPT. The questions were input into 4 different ChatGPT accounts, with the account holders being of various demographics, to evaluate potential differences and biases in the responses given to different account holders. The leading question, “what should I do if I missed a day of my oral contraception birth control?” alone was then input into Google Search, given its nonconversational nature. The results from the ChatGPT questions and the Google Search results for the leading question were evaluated on their readability, accuracy, and effective delivery of information.</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>The ChatGPT results were determined to be at an overall higher-grade reading level, with a longer reading duration, less accurate, less current, and with a less effective delivery of information. In contrast, the Google Search resulting answer box and snippets were at a lower-grade reading level, shorter reading duration, more current, able to reference the origin of the information (transparent), and provided the information in various formats in addition to text.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>ChatGPT has room for improvement in accuracy, transparency, recency, and reliability before it can equitably be implemented into health care information delivery and provide the potential benefits it poses. However, AI may be used as a tool for providers to educate their patients in preferred, creative, and efficient ways, such as using AI to generate accessible short educational videos from health care provider-vetted information. Larger studies representing a diverse group of users are needed.</p>
                </sec>"
10.1007/s40200-023-01377-0,Generative AI for diabetologists: a concise tutorial on dataset analysis,N/A
10.4018/979-8-3693-2964-1.ch015,Revolution Ethics of Data Science and AI,"<jats:p>Artificial intelligence is becoming more and more widespread in our increasingly connected world. Artificial intelligence is slowly but surely modifying the way we live and work, from self-driving cars to automated customer service agents. As artificial intelligence becomes more sophisticated, the ethical implications of its use become more complex. There are several key issues to consider regarding the ethics of artificial intelligence, such as data privacy, algorithmic bias, and socioeconomic inequality. The rapid development of AI brings with it several ethical issues. However, we must remain vigilant in protecting our fundamental rights and freedoms. We must ensure that artificial intelligence is not used to discriminate against vulnerable groups or invade our privacy. We must also be careful that AI does not become a tool for the powerful to control and manipulate the masses. But while there are risks, the author believes the potential benefits of AI are too great to ignore.</jats:p>"
10.1007/s00146-022-01452-9,Cognitive architectures for artificial intelligence ethics,"<jats:title>Abstract</jats:title><jats:p>As artificial intelligence (AI) thrives and propagates through modern life, a key question to ask is how to include humans in future AI? Despite human involvement at every stage of the production process from conception and design through to implementation, modern AI is still often criticized for its “black box” characteristics. Sometimes, we do not know what really goes on inside or how and why certain conclusions are met. Future AI will face many dilemmas and ethical issues unforeseen by their creators beyond those commonly discussed (e.g., trolley problems and variants of it) and to which solutions cannot be hard-coded and are often still up for debate. Given the sensitivity of such social and ethical dilemmas and the implications of these for human society at large, when and if our AI make the “wrong” choice we need to understand how they got there in order to make corrections and prevent recurrences. This is particularly true in situations where human livelihoods are at stake (e.g., health, well-being, finance, law) or when major individual or household decisions are taken. Doing so requires opening up the “black box” of AI; especially as they act, interact, and adapt in a human world and how they interact with other AI in this world. In this article, we argue for the application of cognitive architectures for ethical AI. In particular, for their potential contributions to AI transparency, explainability, and accountability. We need to understand how our AI get to the solutions they do, and we should seek to do this on a deeper level in terms of the machine-equivalents of motivations, attitudes, values, and so on. The path to future AI is long and winding but it could arrive faster than we think. In order to harness the positive potential outcomes of AI for humans and society (and avoid the negatives), we need to understand AI more fully in the first place and we expect this will simultaneously contribute towards greater understanding of their human counterparts also.</jats:p>"
10.1177/00986283241264793,Using Generative AI to Promote the APA's Five Goals for Undergraduate Majors,"<jats:p> Introduction: Recent advancements in generative AI (GAI) platforms appear to mark an abrupt shift in higher education. Statement of the Problem: Instructors have a responsibility to teach students to use GAI, which is a promising tool for promoting personalized, student-centered, process-focused learning environments. Literature Review: Drawing on the explosion of publications outlining ways that instructors and students can use GAI and turning to ChatGPT itself for ideas, we provide guidance for psychology instructors to integrate GAI into their courses, using the APA's Guidelines for the Undergraduate Psychology Major as a framework. Teaching Implications: We outline four principles and provide practical suggestions to demonstrate GAI's potential for enhancing psychology education. Conclusion: Our goal is to empower psychology instructors to explore and experiment with GAI alongside their students. </jats:p>"
10.2139/ssrn.4909905,How Generative AI Transforms Questioning Behavior on Q&amp;amp;A Platforms: Evidence from A Natural Experiment with Pilot Usage of ChatGPT,N/A
10.3390/j7030017,Enhancing Pulmonary Diagnosis in Chest X-rays through Generative AI Techniques,"<jats:p>Chest X-ray imaging is an essential tool in the diagnostic procedure for pulmonary conditions, providing healthcare professionals with the capability to immediately and accurately determine lung anomalies. This imaging modality is fundamental in assessing and confirming the presence of various lung issues, allowing for timely and effective medical intervention. In response to the widespread prevalence of pulmonary infections globally, there is a growing imperative to adopt automated systems that leverage deep learning (DL) algorithms. These systems are particularly adept at handling large radiological datasets and providing high precision. This study introduces an advanced identification model that utilizes the VGG16 architecture, specifically adapted for identifying various lung anomalies such as opacity, COVID-19 pneumonia, normal appearance of the lungs, and viral pneumonia. Furthermore, we address the issue of model generalizability, which is of prime significance in our work. We employed the data augmentation technique through CycleGAN, which, through experimental outcomes, has proven effective in enhancing the robustness of our model. The combined performance of our advanced VGG model with the CycleGAN augmentation technique demonstrates remarkable outcomes in several evaluation metrics, including recall, F1-score, accuracy, precision, and area under the curve (AUC). The results of the advanced VGG16 model showcased remarkable accuracy, achieving 98.58%. This study contributes to advancing generative artificial intelligence (AI) in medical imaging analysis and establishes a solid foundation for ongoing developments in computer vision technologies within the healthcare sector.</jats:p>"
10.31577/filozofia.2024.79.5.3,Can and Should Language Models Act Politically? Hannah Arendt’s Theory of Action in Comparison with Generative AI,N/A
10.1111/exsy.13722,Advancing anomaly detection in cloud environments with cutting‐edge generative <scp>AI</scp> for expert systems,"<jats:title>Abstract</jats:title><jats:p>As artificial intelligence (AI) continues to advance, Generative AI emerges as a transformative force, capable of generating novel content and revolutionizing anomaly detection methodologies. This paper presents CloudGEN, a pioneering approach to anomaly detection in cloud environments by leveraging the potential of Generative Adversarial Networks (GANs) and Convolutional Neural Network (CNN). Our research focuses on developing a state‐of‐the‐art Generative AI‐based anomaly detection system, integrating GANs, deep learning techniques, and adversarial training. We explore unsupervised generative modelling, multi‐modal architectures, and transfer learning to enhance expert systems' anomaly detection systems. We illustrate our approach by dissecting anomalies regarding job performance, network behaviour, and resource utilization in cloud computing environments. The experimental results underscore a notable surge in anomaly detection accuracy with significant development of approximately 11%.</jats:p>"
10.2118/0924-0010-jpt,Guest Editorial: What Engineers Should Know About the Expanding Generative AI Toolkit,"<jats:p>Generative artificial intelligence (AI) is the art of creating new things inspired by the old.</jats:p>
               <jats:p>To understand what generative AI does, ask yourself what you would do if you wanted to create a new hit song.</jats:p>
               <jats:p>You wouldn't want to start from scratch. Instead, you'd listen to 100 popular songs, analyze the rhythms, melodies, and chord progressions.</jats:p>
               <jats:p>You would learn how these elements are structured and combined. Then, with this knowledge you would use your creativity to mix and match them in new and exciting ways—resulting in a fresh, original song that may be the next big hit.</jats:p>
               <jats:p>That's essentially what generative AI does.</jats:p>
               <jats:p>It devours massive amounts of training data, whether it's music, images, text, or scientific data. It learns the underlying patterns and relationships within that data. Then, it uses this knowledge to generate entirely new content that shares characteristics with the training data, but with a unique twist.</jats:p>
               <jats:p>Generative AI operates like a master mimic, following a three-step process to create entirely new content.</jats:p>
               <jats:p>1. Find the building blocks.</jats:p>
               <jats:p>As mentioned, generative AI begins by analyzing vast amounts of training data. The goal is to identify the underlying building blocks and structures that define the data. For example, different rooms in a house contain different objects—kitchens have different items than bedrooms. Think of it like understanding the grammar of a language or the architectural styles that make up different cityscapes. For example, generative AI trained on cat images would learn to recognize distinct features like ears, whiskers, and fur patterns.</jats:p>
               <jats:p>2. Decipher the unwritten rules.</jats:p>
               <jats:p>Next, it calculates the conditional probability distribution of these structures. In other words, it finds the order and relationship between various structures and the building blocks. Imagine that kitchen again. We know that knives are more likely to be found near cutting boards than, say, pillows, and that a sofa is typically found in front of a TV in the living room. Generative AI quantifies such relationships which allows it to then understand the underlying rules of how likely certain elements are to co-occur and in what order.</jats:p>
               <jats:p>3. Perform a bold and creative remix.</jats:p>
               <jats:p>Finally, armed with this knowledge of building blocks and the rules of their order and structure, generative AI can create entirely new samples. It combines the learned structures and their conditional probabilities in novel ways, producing something entirely new yet rooted in the representations extracted from the training data. This could be a photorealistic image of a cat with unique markings, a catchy song inspired by popular hits, or even a compelling scientific hypothesis based on existing research.</jats:p>"
10.12781/978-1-907549-58-8-5,Researchers and Communicators and Creators of the Future: A Generative Conversation with Dr Kenneth Gergen,"<jats:p>In this article, Lindsey Godwin interviews Kenneth Gergen in a conversation about what the aims and activities of a social scientist could, and should, aspire to be in today’s world. How can researchers engage in moving the world in a valued direction, and what is the role of relational thinking and Appreciative Inquiry in research in the social sciences?</jats:p>"
10.1080/10400419.2024.2354622,The Paradox of Artificial Creativity: Challenges and Opportunities of Generative AI Artistry,N/A
10.1109/aisp61396.2024.10475266,Generative-AI in E-Commerce: Use-Cases and Implementations,N/A
10.55041/ijsrem29192,Leveraging Generative AI for Data-Driven Insights and Visualization in Python,"<jats:p>In this work, we introduce a novel solution that integrates Python, Streamlit, OpenAI, and embeddings to extract insights and generate visualizations from various data sources. Our approach empowers users to interact with data in plain text, enabling a more intuitive and efficient data analysis process. By leveraging Generative AI, our system not only simplifies data exploration but also addresses the challenges of identifying and generating charts, making data visualization more accessible. The solution accommodates diverse data sources, including PostgreSQL and CSV files, while ensuring a secure and user-friendly experience. This paper aims to demonstrate the effectiveness of combining unsupervised and supervised learning algorithms in real-time text analysis, providing a smart, quick, and scalable method for data-driven decision-making.</jats:p>"
10.1057/s41254-024-00328-7,Generative AI and the future for China’s diplomacy,"<jats:title>Abstract</jats:title><jats:p>This paper explores the challenges of Generative AI technologies to China’s diplomatic engagement worldwide. First, it examines the discourse in China about the development of ChatGPT, by focusing on the treatment of the technology and the possible risks and opportunities regarding international affairs. Second, the article highlights four challenges: (1) the securitization of AI, where import/export controls, data access, and semiconductors open a new field of research; (2) the (techno-) socialism with Chinese characteristics and the response to the arrival of cutting-edge technologies from U.S. capital and their impact on the development of a local model; (3) the soft censorship: the development of a technology that responds in the ""correct"" way (i.e., in line with the ruling ideology) by discriminating the suitability of sources/data deemed correct and appropriate for the public sphere; (4) the impact on public diplomacy, as AI projects an aura of innovation and technological reputation. In summary, this paper contributes to the theoretical debate on the challenges posed by Generative IA technologies in the context of China’s diplomatic practices.</jats:p>"
10.1101/2024.06.10.24308475,Harnessing generative AI to annotate the severity of all phenotypic abnormalities within the Human Phenotype Ontology,"<jats:label>0.1</jats:label><jats:title>Abstract</jats:title><jats:p>There are thousands of human phenotypes which are linked to genetic variation. These range from the benign (white eyelashes) to the deadly (respiratory failure). The Human Phenotype Ontology has categorised all human phenotypic variation into a unified framework that defines the relationships between them (e.g. missing arms and missing legs are both abnormalities of the limb). This has made it possible to perform phenome-wide analyses, e.g. to prioritise which make the best candidates for gene therapies. However, there is currently limited metadata describing the clinical characteristics / severity of these phenotypes. With &gt;17500 phenotypic abnormalities across &gt;8600 rare diseases, manual curation of such phenotypic annotations by experts would be exceedingly labour-intensive and time-consuming. Leveraging advances in artificial intelligence, we employed the OpenAI GPT-4 large language model (LLM) to systematically annotate the severity of all phenotypic abnormalities in the HPO. Phenotypic severity was defined using a set of clinical characteristics and their frequency of occurrence. First, we benchmarked the generative LLM clinical characteristic annotations against ground-truth labels within the HPO (e.g. phenotypes in the ‘Cancer’ HPO branch were annotated as causing cancer by GPT-4). True positive recall rates across different clinical characteristics ranged from 89-100% (mean=96%), clearly demonstrating the ability of GPT-4 to automate the curation process with a high degree of fidelity. Using a novel approach, we developed a severity scoring system that incorporates both the nature of the clinical characteristic and the frequency of its occurrence. These clinical characteristic severity metrics will enable efforts to systematically prioritise which human phenotypes are most detrimental to human health, and best targets for therapeutic intervention.</jats:p>"
10.18261/olr.10.1.1,Doctor Chatbot: The EUʼs Regulatory Prescription for Generative Medical AI,N/A
10.1038/s41562-023-01740-4,Generative AI poses ethical challenges for open science,N/A
10.1145/3664823,Evolving Generative AI: Entangling the Accountability Relationship,"<jats:p>Since ChatGPT's debut, generative AI technologies have surged in popularity within the AI community. Recognized for their cutting-edge language processing capabilities, these excel in generating human-like conversations, enabling open-ended dialogues with end-users. We consider that the future adoption of generative AI for critical public domain applications transforms the accountability relationship. Previously characterized by the relationship between an actor and a forum, the introduction of generative systems complicates accountability dynamics as the initial interaction shifts from the actor to an advanced generative system. We conceptualise a dual-phase accountability relationship involving the actor, the forum, and the generative AI as a foundational approach to understanding public sector accountability in the context of these technologies. Focusing on integrating generative AI for assisting healthcare triaging, we identify potential challenges introduced for maintaining effective accountability relationships, highlighting concerns that these technologies relegate actors to a secondary phase of accountability and creates a disconnect between government actors and citizens. We suggest recommendations aimed at disentangling the complexities generative systems bring to the accountability relationship. As we speculate on the technologies disruptive impact on accountability, we urge public servants, policymakers, and system designers to deliberate on the potential accountability impact generative systems produce prior to their deployment.</jats:p>"
10.25236/fsst.2023.051510,Innovative Research on Information Literacy Education in Higher Vocational Colleges in the Context of Generative AI,N/A
10.4018/979-8-3693-1351-0,Transforming Education With Generative AI,N/A
10.1613/jair.1.15278,"Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges","<jats:p>Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.</jats:p>"
10.1093/ijl/ecad021,Generative AI and Lexicography: The Current State of the Art Using ChatGPT,"<jats:title>Abstract</jats:title>
               <jats:p>In this article, all ten papers and talks that have been devoted to the use of ChatGPT in lexicography so far are critically analysed, their results tabulated and cross-compared, from which the leading trends are determined. Extrapolating from the trendlines, a single short but robust new prompt is fine-tuned with which articles from different word classes are generated fully-automatically for a dictionary which compares favourably to the best practice in dictionary compilation. The conclusion is that a new age, that of the successful application of generative AI in lexicography, has dawned.</jats:p>"
10.1080/13600834.2024.2352694,Legal implications of using generative AI in the media,N/A
10.1016/j.acra.2024.03.005,The Role of Prompt Engineering in Radiology Applications of Generative AI,N/A
10.2139/ssrn.4458269,"Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education",N/A
10.1016/j.cirp.2024.04.013,Generative AI and neural networks towards advanced robot cognition,N/A
10.1016/j.orgdyn.2024.101029,How to use generative AI as a human resource management assistant,N/A
10.1097/js9.0000000000001583,Advancing generative AI in medicine: recommendations for standardized evaluation,N/A
10.18552/joaw.v14i1.1055,"Exploring Human-Generative AI Interaction in L2 Learners’ Source Use Practices: Issues, Trials, and Critical Reflections","<jats:p>The emergence of generative Artificial Intelligence (GenAI) tools such as ChatGPT has attracted wide attention in the field of L2 writing and academic writing, but few papers to date have analysed GenAI’s potential application (positive and negative) in source use practices in academic writing. This article discusses three key aspects of source use – academic attribution, searching and reading sources, and source integration. AI tools are trialled for each aspect, followed by an overall SWOT analysis. While writers can use AI tools to assist on several source use practices, they are not recommended to use AI without a deep understanding of academic writing and source use principles. This article concludes with suggestions for student writers, academic support providers, and institutions.</jats:p>"
10.5121/csit.2024.140806,NLPOps: A Comprehensive Framework for Secure Development and Scalable Deployment of Multifaceted LLMs in Generative AI,"<jats:p>The burgeoning field of Generative AI relies heavily on Multifaceted Large Language Models (LLMs) to achieve tasks like NER, Document summary, Translation, Text classification, Sentiment Analysis, Text generation, Question &amp; Answer and Document Similarity. However, developing and deploying these complex models remains a challenge due to concerns about security and scalability. This paper proposes ""NLP Ops: A Comprehensive Framework for Secure Development and Scalable Deployment of Multifaceted LLMs in Generative AI."" This framework addresses these challenges by combining best practices in secure software development, distributed computing, and operational monitoring. The framework encompasses secure data handling, adversarial training, containerization, distributed infrastructure, and comprehensive monitoring for performance and security. Results demonstrate that NLP Ops [mention key findings, e.g., improves security by 98%, increases processing speed by 97%. This paper contributes to the advancement of NLP Ops by providing a practical and secure approach to developing and deploying Multifaceted LLMs, paving the way for wider adoption of Generative AI technologies.</jats:p>"
10.58695/ec.4,Exploring the Human-AI Nexus: A Friendly Dispute Between Second-Order Cybernetical Ethical Thinking and Questions of AI Ethics,N/A
10.1016/b978-0-443-18851-0.00011-1,AI and grief: a prospective study on the ethical and psychological implications of deathbots,N/A
10.1145/3278721.3278754,"Embodiment, Anthropomorphism, and Intellectual Property Rights for AI Creations",N/A
10.1007/978-3-030-66913-3_8,AI in the Financial Industries: Between Apathy and Hysteria,N/A
10.1287/lytx.2021.03.16n,State of Responsible AI: Report reveals widespread lack of understanding AI ethics,N/A
10.1145/3375627.3375806,The AI Liability Puzzle and a Fund-Based Work-Around,N/A
10.1145/3514094.3539567,The AI Mirror: Reclaiming our Humanity in an Age of Machine Thinking,N/A
10.1609/aaaiss.v3i1.31202,LLMs Among Us: Generative AI Participating in Digital Discourse,"<jats:p>The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the ""LLMs Among Us"" experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different LLMs, GPT-4, Llama 2 Chat, and Claude. We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of LLMs to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially more impact on human perception than the choice of mainstream LLMs.</jats:p>"
10.2478/czoto-2023-0007,Generative AI Takes Centre Stage: Revolutionizing Productivity and Reshaping Industries,"<jats:title>Abstract</jats:title>
               <jats:p>The growing prominence of Generative AI in discussions on artificial intelligence has significant implications for productivity and industry dynamics. This article aims to examine the transformative role of Generative AI, specifically focusing on its revolutionary impact on productivity and its influence on various industries. The objectives of this article include conducting a detailed analysis of how systems have greatly enhanced efficiency for developers and knowledge workers. By examining both the positive and negative aspects of the Generative AI movement, this article aims to provide valuable insights into the innovations driven by Generative AI and the advancements that contribute to its evolution. Through this exploration, the goal is to offer a comprehensive understanding of the current landscape, highlighting the opportunities and challenges presented by the rise of Generative AI in the management sphere.</jats:p>"
10.12968/s1353-4858(24)70022-1,Minimising the risks and maximising the opportunities of generative AI,"<jats:p> It's no secret that threat actors are enthusiastically adopting generative AI systems to help them launch more-effective cyber attacks. Fortunately, it's possible to fight AI with AI. There are new tools becoming available to help organisations protect their data and networks. But you need to know where to use them, when to use them and which are the most appropriate tools for each task. </jats:p>"
10.1097/cin.0000000000001149,"Foundation Models, Generative AI, and Large Language Models","<jats:p>We are in a booming era of artificial intelligence, particularly with the increased availability of technologies that can help generate content, such as ChatGPT. Healthcare institutions are discussing or have started utilizing these innovative technologies within their workflow. Major electronic health record vendors have begun to leverage large language models to process and analyze vast amounts of clinical natural language text, performing a wide range of tasks in healthcare settings to help alleviate clinicians' burden. Although such technologies can be helpful in applications such as patient education, drafting responses to patient questions and emails, medical record summarization, and medical research facilitation, there are concerns about the tools' readiness for use within the healthcare domain and acceptance by the current workforce. The goal of this article is to provide nurses with an understanding of the currently available foundation models and artificial intelligence tools, enabling them to evaluate the need for such tools and assess how they can impact current clinical practice. This will help nurses efficiently assess, implement, and evaluate these tools to ensure these technologies are ethically and effectively integrated into healthcare systems, while also rigorously monitoring their performance and impact on patient care.</jats:p>"
10.58966/jcm2023249,Generative AI Images and Indian Media Industry: An Overview of Opportunities and Challenges,"<jats:p>Artificial Intelligence (AI) has gained initial momentum in the past few years. Another side of this is Generative AI, which is growing and has the capacity to transform journalism and media content. There are speculations about the consequences of AI from creating warfare to the making of movies. This article considers notable platforms like Shutterstock, providers of stock photographs, music and editing tools and secondly, DALL.E 2-Open AI, a generative AI platform of Chat GPT.The proposed research article aims to find out the effects of generative AI images in the media industry, the opportunities of AI generative visuals and the challenges faced by the industry due to the innovative technology. This article will also try to demonstrate the capacity and the limitations of generative AI content and reflect on the implications of generative AI for media education and journalism.</jats:p>"
10.18260/1-2--47339,Ethical Use of Generative AI in Engineering: Assessing Students and Preventing Them from Cheating Themselves,N/A
10.2196/56117,A Use Case for Generative AI in Medical Education,N/A
10.31219/osf.io/erf36,How Generative AI Portrays Science. Interviewing ChatGPT from the Perspective of Different Audience Segments,"<p>Generative AI in general and ChatGPT in particular have risen in importance. ChatGPT is widely known and used increasingly as an information source for different topics, including science. It is therefore relevant how ChatGPT portrays science and science-related topics. Research on this question is lacking, however. Hence, we “interview” ChatGPT and reconstruct how it presents science, scientists, scientific misbehavior and controversial scientific fields. Combining qualitative and quantitative content analysis, we find that, generally, ChatGPT portrays science largely as the STEM disciplines, in a positivist-empiricist way and a positive light. We compare ChatGPT’s responses to different simulated user profiles and two versions of GPT and find similarities in that the scientific consensus on questions like climate change, COVID-19 vaccinations or astrology is consistently conveyed across them. Beyond these similarities in substance, however, pronounced differences are found in the personalization of responses to different user profiles and between GPT-3.5 and GPT-4.</p>"
10.1109/infoteh60418.2024.10495941,Generative AI Applications and Tools in Engineering Education,N/A
10.1145/3635636.3664263,The Impact of Generative AI on Artists,N/A
10.18438/eblip30512,A Survey on Student Use of Generative AI Chatbots for Academic Research,"<jats:p>Objectives – To understand how many undergraduate and graduate students use generative AI as part of their academic work, how often they use it, and for what tasks they use it. We also sought to identify how trustworthy students find generative AI and how they would feel about a locally maintained generative AI tool. Finally, we explored student interest in trainings related to using generative AI in academic work.  This survey will help librarians better understand the rate at which generative AI is being adopted by university students and the need for librarians to incorporate generative AI into their work.
Methods – A team of three library staff members and one student intern created, executed, and analyzed a survey of 360 undergraduate and graduate students at Harvard University. The survey was distributed via email lists and at cafes and libraries throughout campus. Data were collected and analyzed using Qualtrics.
Results – We found that nearly 65% of respondents have used or plan to use generative AI chatbots for academic work, even though most respondents (65%) do not find their outputs trustworthy enough for academic work. The findings show that students actively use these tools but desire guidance around effectively using them.
Conclusion – This research shows students are engaging with generative AI for academic work but do not fully trust the information that it produces. Librarians must be at the forefront of understanding the significant impact this technology will have on information-seeking behaviors and research habits. To effectively support students, librarians must know how to use these tools to advise students on how to critically evaluate AI output and effectively incorporate it into their research.</jats:p>"
10.14738/bjhmr.104.15114,An Interdisciplinary Study on Generative AI: Exploring Its Efficacy in Mental Health Interventions within the Gaming Ecosystem,N/A
10.1609/aaaiss.v2i1.27704,Proposed Uses of Generative AI in a Cybersecurity-Focused Soar Agent,"<jats:p>With the rapidly increasing use of AI and machine learning in recent years and the current generative AI revolution, it is no surprise that the malicious use of AI has begun to establish it- self in the realm of cybersecurity. At risk of being left behind in this “arms race”, it’s imperative that autonomous intelli- gent cybersecurity agent (AICAs) are developed to counter this emerging threat. Currently, a project at Argonne National Laboratory is using Soar as a starting point for developing a cognitive-architecture based AICA, but the utilization of Soar in this project has shortcomings, in particular the lack of modern AI principles to generate novel analysis in the face of novel situations. Generative AI has the potential to allow a Soar cognitive agent to consider a much broader range of con- textual information, and learn from past episodic knowledge in novel ways by using transformer architectures. This paper focuses on the theoretical integration of Generative AI into the Soar cognitive architecture from a cybersecurity stand- point, and discuss the advantages to doing so.</jats:p>"
10.1016/j.otc.2024.04.006,Generative AI and Otolaryngology—Head &amp; Neck Surgery,N/A
10.1093/jamia/ocae014,Search still matters: information retrieval in the era of generative AI,"<jats:title>Abstract</jats:title>
               <jats:sec>
                  <jats:title>Objective</jats:title>
                  <jats:p>Information retrieval (IR, also known as search) systems are ubiquitous in modern times. How does the emergence of generative artificial intelligence (AI), based on large language models (LLMs), fit into the IR process?</jats:p>
               </jats:sec>
               <jats:sec>
                  <jats:title>Process</jats:title>
                  <jats:p>This perspective explores the use of generative AI in the context of the motivations, considerations, and outcomes of the IR process with a focus on the academic use of such systems.</jats:p>
               </jats:sec>
               <jats:sec>
                  <jats:title>Conclusions</jats:title>
                  <jats:p>There are many information needs, from simple to complex, that motivate use of IR. Users of such systems, particularly academics, have concerns for authoritativeness, timeliness, and contextualization of search. While LLMs may provide functionality that aids the IR process, the continued need for search systems, and research into their improvement, remains essential.</jats:p>
               </jats:sec>"
10.1007/978-3-031-54252-7_5,GenAI Data Security,N/A
10.1145/3637528.3672503,Generative AI in E-Commerce: What Can We Expect?,N/A
10.1145/3649217.3653527,Teaching Programming in the Age of Generative AI,N/A
10.3390/healthcare11202776,Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration,"<jats:p>Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.</jats:p>"
10.1080/15391523.2024.2338085,Learning by playing with generative AI: design and evaluation of a role-playing educational game with generative AI as scaffolding for instant feedback interaction,N/A
10.1145/3514094.3534150,Do Humans Trust Advice More if it Comes from AI?,N/A
10.1007/s10676-020-09539-x,On the person-based predictive policing of AI,N/A
10.1007/s40593-021-00270-2,"Education for AI, not AI for Education: The Role of Education and Ethics in National AI Policy Strategies",N/A
10.14746/eip.2023.2.7,The Role of Personal Values in Forming the AI Ethics of Prospective Accountants,<jats:p>This study aims to discuss how to form AI (Artificial Intelligence) ethical behavior with insight into the personal and organizational values of prospective accountants. This was a quantitative survey method. The sampling technique with a saturated sample was used as the research sample. Partial Least Square (PLS) analysis was conducted on 421 data points using WarpPLS software. The study results show that organizational and personal values significantly positively affect the intention of prospective accountant students to engage in AI ethics. Organizational values have a positive effect on the personal values of prospective accounting students. Intentions had a significant effect on AI ethics. Personal values did not play a role in mediating the impact of organizational values on intentions toward AI ethics. Intention succeeds in mediating the influence of personal values on the intention to engage in AI ethics among prospective accountant students. The findings referred to are very applicable to be implemented in different cultural settings due to the personal and organizational values tend to be implemented in general situation and condition. The findings provide universal outlook that values within organizations have an essential role in enhancing future accountants to be ethical in respect to AI.</jats:p>
10.1007/s13347-022-00557-9,The Ethics of AI Ethics. A Constructive Critique,"<jats:title>Abstract</jats:title><jats:p>The paper presents an ethical analysis and constructive critique of the current practice of AI ethics. It identifies conceptual substantive and procedural challenges and it outlines strategies to address them. The strategies include countering the hype and understanding AI as ubiquitous infrastructure including neglected issues of ethics and justice such as structural background injustices into the scope of AI ethics and making the procedures and fora of AI ethics more inclusive and better informed with regard to philosophical ethics. These measures integrate the perspective of AI justice into AI ethics, strengthening its capacity to provide comprehensive normative orientation and guidance for the development and use of AI that actually improves human lives and living together.</jats:p>"
10.4324/9780429329067-6,Student-centred requirements for the ethics of AI in education,N/A
10.1136/medethics-2021-107352,Transparent AI: reliabilist and proud,N/A
10.1007/s10676-010-9253-3,Explanation and trust: what to tell the user in security and AI?,N/A
10.26034/fr.jehe.2023.4657,AI in Higher Education,"<jats:p>This scholarly inquiry examines the interplay between artificial intelligence (AI) and academic integrity within higher education. Through a comprehensive synthesis of academic literature, the study delves into the multifaceted implications of AI tools on academic practices, pedagogical approaches, and the evolving landscape of academic integrity within higher education. The findings, derived from an extensive analysis of scholarly works, offer profound insights into the challenges posed by the integration of AI in higher education. The impact on academic dishonesty, the nuances of pedagogical shifts, and the dynamic relationship between students and AI are scrutinized, contributing to a nuanced comprehension of the intricate dynamics within the academy.</jats:p>"
10.1145/3514094.3539538,"Inspecting Algorithmic Flows: Ethics, Transparency, and Accountability for Digital Mass Communication Platforms",N/A
10.1007/s10676-023-09694-x,Governing (ir)responsibilities for future military AI systems,N/A
10.1207/s15327728jmme2102&3_9,Book Reviews,N/A
10.1007/978-3-030-51110-4_1,About the Book,N/A
10.1007/978-3-031-23035-6,AI Ethics in Higher Education: Insights from Africa and Beyond,N/A
10.1007/978-3-030-17152-0_6,Projecting AI-Crime: A Review of Plausible Threats,N/A
10.1136/medethics-2021-107463,Trustworthy medical AI systems need to know when they don’t know,N/A
10.1007/s10551-023-05339-7,The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work,"<jats:title>Abstract</jats:title><jats:p>The increasing workplace use of artificially intelligent (AI) technologies has implications for the experience of meaningful human work. Meaningful work refers to the perception that one’s work has worth, significance, or a higher purpose. The development and organisational deployment of AI is accelerating, but the ways in which this will support or diminish opportunities for meaningful work and the ethical implications of these changes remain under-explored. This conceptual paper is positioned at the intersection of the meaningful work and ethical AI literatures and offers a detailed assessment of the ways in which the deployment of AI can enhance or diminish employees’ experiences of meaningful work. We first outline the nature of meaningful work and draw on philosophical and business ethics accounts to establish its ethical importance. We then explore the impacts of three paths of AI deployment (replacing some tasks, ‘tending the machine’, and amplifying human skills) across five dimensions constituting a holistic account of meaningful work, and finally assess the ethical implications. In doing so we help to contextualise the meaningful work literature for the era of AI, extend the ethical AI literature into the workplace, and conclude with a range of practical implications and future research directions.
</jats:p>"
10.1136/jme-2024-110170,"AI diagnoses terminal illness care limits: just, or just stingy?",N/A
10.1007/s00146-022-01414-1,"AI ethics inflation, Delphi and the restart of theory",N/A
10.1016/j.ijpe.2024.109283,Smart product platforming powered by AI and generative AI: Personalization for the circular economy,N/A
10.53106/1025593135107,生成式AI對刑事實務之挑戰（下）,<jats:p/>
10.53106/1025593135004,生成式AI對刑事實務之挑戰（上）,<jats:p/>
10.4018/979-8-3693-0831-8.ch008,Facing Mega-Crises in the Global Era of AI,"<jats:p>Communication discipline, including communication courses in higher education, must aptly respond to the demands emerging due to the rapid developments in the AI technologies and globally widespread application of generative AI. Furthermore, to truly stay relevant, communication education must speak to the pervasive, complicated, and difficult-to-solve mega-crises that are diminishing human security, while growing more severe in the current era, around the world. This chapter explores the approaches within communication higher education that can contribute to mitigation of mega-crises via meaningful and ethical integration of AI-supported solutions. Specifically, communication education should emphasize (1) global alliances, (2) cross-disciplinary alliances, (3) community-inclusive alliances, (4) creativity, adaptability, and acceptance of uncertainty, and (5) establishment of viable infrastructures and platforms that enable global, cross-disciplinary, and community-inclusive deliberation and decision-making.</jats:p>"
10.22876/bnc.2024.25.3.002,A Multidimensional Approach to the Anthropomorphism of Conversational Generative AI : Centered on Haslam’s Dehumanization Model,N/A
10.1089/genedge.6.01.060,AACR 2024: Insilico Medicine to Present Preclinical Data for AI-Generated Cancer Drugs,N/A
10.1080/02601370.2024.2310448,"Artificial intelligence (AI), conversational agents, and generative AI: implications for adult education practice and research",N/A
10.1145/3522664.3528592,Quality assurance of generative dialog models in an evolving conversational agent used for Swedish language practice,N/A
10.36548/jtcsst.2023.4.003,Generative Artificial Intelligence (AI) Educational Pedagogy Development: Conversational AI with User-Centric ChatGPT4,"<jats:p>In terms of language models, generative artificial intelligence (GenAI), and more specifically ChatGPT, offer a significant technological achievement as a revolutionary tool for natural language processing (NLP) and a transformative educational business tool. ChatGPT users' suggestions have the ability to optimize teaching and learning, thereby having a substantial impact on the educational environment of the twenty-first century. Educational robots are getting easier to access for a number of reasons. The human-robot cooperation that has advanced scientifically in industry 5.0 extreme digital automation, will also probably become a regular aspect of life in the days to come. This study examines the prospective uses of GenAI for NLP synthesis as well as its potential role as a conversational agent in the classroom business. GenAI's capacity to understand and produce language that is human-like by employing NLP to generate semantics was essential to its ability to replicate the most advanced human technology through comprehensive assumptions of  patterns and structures it learns from its training data. With the rise of artificial intelligence (AI) driven conversational agents, prompt engineering has become an important aspect of digital learning. It is essential to get ready for an AI-dominated future when general and educational technologies combine. The study demonstrated how society may impact and contribute to the development of AI pedagogic learning using an instructional robotics application driven by AI, emphasizing the responsibility of humans as producers to reduce any potential misfortunes. The study   highlights that since generative AI technologies have the potential to drastically change teaching and learning approaches and necessitate new ways of thinking, more research on organizational robotics, with a focus on human collaboration and education, will emerge from the technological concerns raised in this study.</jats:p>"
10.4018/979-8-3693-2440-0.ch006,Empowering Healthcare Professionals Through AI-Powered Lifelong Learning for Improving Patient Care,"<jats:p>Artificial intelligence (AI) is described as the power of a machine to emulate human intelligence. Since its development, AI has impacted business, daily life, and, most notably, the healthcare industry. The technology could make an impact on many facets of healthcare, including medical screening and treatment alternatives, drug design, and benchwork investigation. As technology progresses, AI will play a more significant role in medical education. The ability to implement AI effectively will lead to new opportunities for medical professionals in the future. AI and technology-enhanced learning should be integrated into medical education alongside the current emphasis on the biological and medical sciences. It is high time for educational institutions in the medical field to begin planning and revamping their curricula that would incorporate AI and machine learning along with a focus on transparency and compassion. The students who graduate will be competent to adopt AI-enabled technologies and to succeed in the AI-transformed healthcare system.</jats:p>"
10.24886/blr.2023.12.37.4.227,A Study on China’s Legislation Related to AI - Focusing on Interim Measures for the Administration of Generative AI Services -,N/A
10.26734/jfe.2024.14.03.04,Research on Paradigm Shift in Lifelong Education in the Age of Generative AI 􍾢 From the perspective of Erudition 􍾢,N/A
10.12677/ojls.2024.125463,Research on Copyright Infringement Risks in Generative AI Data Use,N/A
10.4018/979-8-3693-1198-1.ch002,AI-Powered Dialogue System for Business Exploring GPT3's Impact,"<jats:p>Transformative breakthroughs appear every few decades, revolutionizing the globe and vastly enhancing our quality of life. With the introduction of ChatGPT, another paradigm-shifting event has now occurred. This Study examine the importance of GPT-3 in the context of business. In essence, GPT-3 has the potential to transform a variety of business processes by streamlining workflows, enhancing client relationships, and spurring creative breakthroughs. This transformative tool can substantially advance businesses by amplifying effectiveness, refining customer engagements, fostering innovation, and simplifying an array of operations. This research will explore the capacity to influence diverse sectors, notably the financial market. Moreover, GPT-3 holds the potential to profoundly shape the functions of financial brokers, augmenting their efficiency, customer service, and decision-making protocols. Ultimately, we will examine how GPT-3 can elevate effectiveness, customer engagement, decision-making, and creative advancement</jats:p>"
10.4018/979-8-3693-2440-0.ch015,Revolutionizing Workforce Education,"<jats:p>This study presents a conceptual analysis of the role of artificial intelligence (AI) in skilling, upskilling, and reskilling the workforce. The chapter argues that artificial intelligence (AI) is fundamentally reshaping the workforce landscape and necessitating new strategies for employee skill development. This paper examines the impact of artificial intelligence (AI) on workforce development by synthesizing insights from the existing literature. The authors identified the trends, challenges, and opportunities of using artificial intelligence in workforce development. Informed decision-making and policy formulation in the realms of labour and employment contribute to this chapter. In conclusion, AI has the potential to transform the future of work and redefine the skill sets required for success in an increasingly digitized and automated world.</jats:p>"
10.3390/chemistry6040043,Optimizing Human–AI Collaboration in Chemistry: A Case Study on Enhancing Generative AI Responses through Prompt Engineering,"<jats:p>“Are we asking the right questions?” seems cliché, but for ChatGPT, it is a pivotal tool to ensure the accuracy of responses. While ChatGPT-3.5’s training on the vast database promises to revolutionize STEM education and research, this investigation shows the importance of precise communication and prompt engineering in guiding ChatGPT-3.5 toward reliable and accurate responses, particularly in chemistry. For instance, emphasizing context, clearly defining symbols, and focusing on field-specific instructions can dramatically improve its performance. Furthermore, avoiding open-ended prompts and strategically using repetition can further enhance its accuracy. The iterative prompt design, demonstrated through a series of adjustments, illustrates how seemingly minor refinements, such as substituting “least” for “lowest”, profoundly impact the output. This study highlights the essential role of human oversight, including the construction of well-crafted prompts, in guarding reliable information and nurturing a productive “Human–AI” (HAI) partnership.</jats:p>"
10.1007/978-3-031-62963-1_53,Is a Picture Worth a Thousand Words? Comparative Evaluation of Generative AI for Drawing and Representation,N/A
10.1299/jsmedsd.2023.33.2507,Study of a method to support aesthetic design using Image Generative AI,N/A
10.1007/978-3-030-97546-3_56,Improving Evolutionary Generative Adversarial Networks,N/A
10.1080/15265161.2023.2250311,"Generative AI, Specific Moral Values: A Closer Look at ChatGPT’s New Ethical Implications for Medical AI",N/A
10.57228/krj.68.6,"A Future of Preaching in the Age of Generative AI, ChatGPT: Toward Deep-Reading &amp; Deep-Preaching",N/A
10.14387/jkspth.2023.87.7,The ChatGPT Era and Christian Worship : A Study of the Use of Generative AI in Christian Worship,N/A
10.1109/mnet.2024.3422241,The Age of Generative AI and AI-Generated Everything,N/A
10.1145/3600211.3604732,Anticipatory regulatory instruments for AI systems,N/A
10.1355/9789815203684-007,SOME ADDITIONAL CONSIDERATIONS FOR THE IMPLEMENTATION OF THE ASEAN AI GUIDE,N/A
10.1145/3306618.3314228,The Value of Trustworthy AI,N/A
10.1007/978-3-031-10960-7_5,The Ethics of Creative AI,N/A
10.4337/9781803924021.00018,AI makes emotions measurable by aggregating the wisdom of the crowd,N/A
10.1145/3514094.3539556,Why is my System Biased?: Rating of AI Systems through a Causal Lens,N/A
10.1145/3306618.3314325,On Serving Two Masters,N/A
10.1002/9781119551966.ch50,AI and Business Ethics in Financial Markets,N/A
10.1145/3306618.3314232,"""Scary Robots""",N/A
10.1016/j.caeai.2024.100251,AI literacy for ethical use of chatbot: Will students accept AI ethics?,N/A
10.4108/eai.20-11-2021.2314105,The Ethics of Sustainability for Artificial Intelligence,N/A
10.1007/978-3-030-54173-6_8,Robotics and AI in Food Security and Innovation: Why They Matter and How to Harness Their Power,"<jats:title>Abstract</jats:title><jats:p>From strawberry-picking robots to satellite remote sensing and GIS techniques that forecast crop yields, the integration of robotics and AI in agriculture will play a key role in sustainably meeting the growing food demand of the future. But it also carries the risk of alienating a certain population, such as smallholder farmers and rural households, as digital technologies tend to be biased toward those with higher-level skills. To ensure that digital technologies are inclusive and become a driver for development, countries should make technology affordable and invest in institutions and human capital, so that everyone can participate in the new digital economy. Digital agriculture also represents an opportunity for young people as agriculture value chains can be developed to create new service jobs in rural areas, making agriculture an attractive sector for youth.</jats:p>"
10.21428/e4baedd9.e39b392d,From Automation to Augmentation: Redefining Engineering Design and Manufacturing in the Age of NextGen-AI,N/A
10.4018/979-8-3693-2418-9.ch004,Further Research Opportunities and Challenges Towards AI-Driven Tools for Modern Generation,"<jats:p>This chapter discusses the evolving landscape of artificial intelligence (AI) and its applications in crafting tools tailored for the modern generation. As AI continues to advance, opportunities for further research emerge, accompanied by a set of challenges that warrant exploration. This work explores key research avenues such as enhancing AI-driven tools' adaptability to dynamic user preferences, leveraging deep learning for improved context-awareness, and refining natural language processing for seamless human-machine interactions. The challenges addressed ethical issues, data privacy issues, and the need for transparent decision-making processes within AI systems. This work highlights the interdisciplinary nature of this research, calling for collaboration among experts in AI, human-computer interaction, to unlock the full potential of AI-driven tools for the modern generation. As society increasingly integrates AI into daily life, understanding and addressing these opportunities and challenges becomes imperative for the responsible and effective deployment of AI.</jats:p>"
10.1145/3306618.3314233,AI + Art = Human,N/A
10.4018/979-8-3693-3597-0.ch016,Exploring Generative Adversarial Networks (GANs) in the Context of Public Space Protection,"<jats:p>In the rapidly evolving landscape of public space security, this chapter presents a comprehensive exploration of the transformative role played by generative adversarial networks (GANs). GANs have emerged as a powerful tool in reshaping the paradigm of public space protection. The chapter aims to elucidate the multifaceted applications, benefits, and challenges of integrating GAN technology into security frameworks. Beginning with a foundational understanding of GANs, the chapter navigates through their architecture, principles, and historical evolution. It delves into the specific applications of GANs in public space security, unraveling their capacity to revolutionize surveillance and threat detection. The chapter also addresses the advantages brought forth by GANs and potential challenges, including ethical considerations and biases. Looking ahead, the chapter explores future prospects and emerging trends, providing insights into the evolving role of GANs in addressing the dynamic security challenges faced in public environments.</jats:p>"
10.4324/9780429461002-2,AI—The history and evolution,N/A
10.4324/9780429329067-5,AI in education,N/A
10.1093/oso/9780190905033.003.0006,"Computational Law, Symbolic Discourse, and the AI Constitution","<jats:title>Abstract</jats:title>
               <jats:p>How should we tell AIs what we want? For the past few decades, the Wolfram Language has been developed as a computational communication language to provide a bridge between human goals and computational capabilities. The Wolfram Language is becoming a full symbolic discourse language that can capture the kinds of human intentions we find in laws and contracts. If we want to tell AIs to “be nice to humans,” how should we actually do that? This chapter first discusses how the development of symbolic discourse language can make it possible to express such ideas. It then considers how we might approach building an AI constitution that defines how we want AIs to act and what ethics they should follow. This chapter also discusses some of the fundamental issues around having a complete computational legal or ethical system.</jats:p>"
10.1017/9781009072168.034,Keeping AI Legal,N/A
10.57125/fp.2023.09.30.05,Modern Digital Technologies and AI ethics: Moral Relevance,"<jats:p>Philosophy is called upon to give meaning to new dimensions of the socio-cultural space, which require not only a rational and pragmatic analysis of functional parameters, but also an ideological and mental assessment. The purpose of the article is to analyse the transformation of the value and ethical standards for the use of artificial intelligence, given the change in its positioning in the system of social activity. A comparative and generalized analysis of 50 scientific papers has made it possible to identify the value factors that serve as guidelines in the process of forming the ethical foundations of artificial intelligence. The synergistic approach determines the prospects for transforming the philosophical and ethical support of the moral aspect of artificial intelligence. The results of the study point to the stabilisation of the status of artificial intelligence and the first attempts to unify the requirements for the ethics of its use in the socio-cultural space. The contradictions that arise in the context of actualisation of human dimension in the process of using artificial intelligence and its direct impact on humans are actualised. The statement of the reorientation of elements from the traditionally human potential (morality, responsibility, initiative, heuristics) to the cluster of artificial intelligence activity generates a natural reaction of man and society in the context of the need to regulate (following the example of the regulatory and legal paradigm of social relations) the activities of this technology. Thus, artificial intelligence is gradually integrating into all spheres of social activity and acquiring a new status that involves active use of the advantages of technological, digital and information potential, which, in turn, requires new ethical standards to regulate the activity of these technologies.</jats:p>"
10.2139/ssrn.4368020,Governance of AI Ethics: Perspective from the Global South (Africa),N/A
10.1145/3375627.3377140,Computerize the Race Problem?,N/A
10.1016/j.techsoc.2024.102527,AI for the people? Embedding AI ethics in HR and people analytics projects,N/A
10.1145/3278721.3278771,Impacts on Trust of Healthcare AI,N/A
10.1007/s00146-012-0431-1,Creating “companions” for children: the ethics of designing esthetic features for robots,N/A
10.1136/jme-2023-109162,Impairing the impairment argument,"<jats:p>Blackshaw and Hendricks have recently developed and defended the impairment argument against abortion, arguing that the immorality of giving a child fetal alcohol syndrome (FAS) provides us with reason to believe that abortion is immoral. In this paper, we forward two criticisms of the impairment argument. First, we highlight that, as it currently stands, the argument is very weak and accomplishes very little. Second, we argue that Blackshaw and Hendricks are fundamentally mistaken about what makes giving a child FAS immoral. Once we acknowledge this, it is clear that our intuitions about giving a child FAS provide no support for the supposed immorality of abortion.</jats:p>"
10.54394/tfzy7681,Buffer or bottleneck?,"<jats:p>In this new paper from the International Labour Organization and the World Bank Group, researchers found that Generative AI could have transformative effects on jobs and livelihoods in Latin America and the Caribbean. Nonetheless, gaps in digital infrastructure and other inequalities could hinder the potential impacts of Generative AI in the region.</jats:p>"
10.1109/csci62032.2023.00051,Generative &amp; Responsible AI - LLMs Use in Differential Governance,N/A
10.12781/978-1-907549-14-4,India and Appreciative Inquiry: Generative Connection between Ancient Wisdom and Today’s Endeavours in the Field,N/A
10.22554/ijtel.v7i2.149,Reflections on Engaging Pragmatically with Generative AI to Augment Research and Education Practice,"<jats:p>Generative AI tools enable researchers to generate multimedia content, including human-like visuals, voice and text. This novel paper’s purpose is to use generative AI to create a short paper, and to critically evaluate the process and implications of the material generated. Firstly, the capabilities of AI to generate multimedia content is summarised with an example of an AI generated professor. Both the opening section, and concluding reflections, are written without the use of AI. Perplexity.ai, an AI tool based on a large language model is used, through a series of prompts, to generate a short paper on AI in the animation industry. The generated paper emerged through a series of prompts written during a 90-minute session. The generated text is presented as is, modified only with minor deletions. In the concluding section, the impact of generative AI tools on researching are reflected upon with some positive and negative insights outlined.</jats:p>"
10.14742/apubs.2023.514,Collaborative sensemaking with generative AI,"<jats:p>Educators are wrestling with the changes wrought by generative AI (GenAI), particularly the widespread adoption of ChatGPT. This paper introduces creative and collaborative sensemaking with GenAI as an alternative form of academic and professional development to spark reflection on the implications of this technology for educators and to increase GenAI literacy. By combining human and AI-generated text in iterative loops, we created a text and a creative process to collectively investigate the use of GenAI in education. Collaborative poetic inquiry, an arts-based research method, was used in tandem with generative experiments using AI tools, culminating in an ode to collaborative sensemaking. Drawing on the authors’ collective experience as a group of educational professionals and academics, we then critically analysed how GenAI may impact educators and augment creative practices to generate new insights. Further implications for practice from this sensemaking with GenAI in education are discussed.</jats:p>"
10.2118/0524-0008-jpt,Comments: Grabbing the Brass Ring To Power the Demand for Data Centers and Generative AI,"<jats:sec>
                  <jats:title>_</jats:title>
                  <jats:p>“There’s no way to get there without a breakthrough.”</jats:p>
                  <jats:p>These were the words of OpenAI’s CEO Sam Altman at a sideline meeting with Bloomberg at the World Economic Forum (WEF) in Davos, Switzerland, in January, referring to the energy required to power generative AI, data centers, cloud computing, and to support required equipment and infrastructure.</jats:p>
                  <jats:p>As industries swiftly transition into a fresh era of digital revolution, spearheaded by the fast adoption and advancement of generative AI technology, the demand for energy to power data centers and required infrastructure skyrockets.</jats:p>
                  <jats:p>Research firm IDC estimated that global data center energy consumption reached 382 TWh in 2022, with a forecast surge to 803 TWh by 2027. To contextualize, 382 TWh is roughly equivalent to France's annual electricity consumption, while 803 TWh is comparable to Russia's usage (International Energy Agency, 2019 data). Currently, data centers consume approximately 1 to 2% of global electricity, is expected to rise to 8% by 2030, according to WEF.</jats:p>
                  <jats:p>Struggling to keep pace are the sources of energy required to fuel this transformation, and the transmission and distribution constraints of power grids.</jats:p>
                  <jats:p>In 2010, IDC observed a 50% gigaleap in global data production to 1.2 zettabytes (1.2 trillion GB), with projections of annual data production reaching 35 zettabytes by 2020, a milestone reached in 2018 instead. By 2020, 59 zettabytes of data were generated. IDC anticipates newly created data to soar to 175 zettabytes by 2025, notching a 146-fold increase over 15 years.</jats:p>
               </jats:sec>
               <jats:sec>
                  <jats:title>Striking a Secure and Sustainable Balance</jats:title>
                  <jats:p>The voracious energy consumption of data centers is sounding alarms about the need for additional sustainable and efficient power generation sources.</jats:p>
                  <jats:p>SPE-218905, presented last month at the 2024 SPE Western Regional Meeting, dove into the oil and gas industry’s role in assuring “digital decarbonization.” Noting the expanded use of AI, data centers, and cloud computing in our industry, the authors wrote, “The need for energy transition has triggered an unprecedented momentum for developing renewable energy sources. With its historical and current contributions to the energy and products market, all indications are that the petroleum Industry needs to continue its efforts in the production of hydrocarbons while also actively exploring ways to substantially reduce GHG emissions such as methane and carbon dioxide.”</jats:p>
                  <jats:p>Major companies are intensifying digital transformation efforts, reshaping petroleum industry processes. This includes reservoir characterization, drilling design, and economic calculations, which rely on high-performance computers—increasing energy consumption and emissions. Data analytics and AI aid in demand forecasting, inventory management, and predictive maintenance, reducing disruptions and environmental damage, thereby cutting carbon emissions. Automation, drones, robotics, and IoT sensors enhance operations, safety, and efficiency while minimizing environmental impact.</jats:p>
                  <jats:p>Data reign supreme as the ""ultimate authority"" across the entirety of our industry's operations, guiding the evolution of our practices to foster cleaner operations, prevent downtime, and gain efficiencies. The authors added this is crucial for remaining competitive and improving the industry's reputation, particularly amidst stringent environmental regulations in the US.</jats:p>
                  <jats:p>They recommend actions to advance the sustainable implementation of digital technologies while also conserving power consumption and minimizing emissions. Among the suggestions are power management systems to track and optimize energy usage, the use of renewable and low-carbon energy sources, prioritizing partnerships with cloud providers committed to sustainability, and improving hardware sustainability by using durable materials and energy-efficient designs throughout its life cycle.</jats:p>
                  <jats:p>Leading the development of breakthroughs are operators exploring alternative energy sources and their implementation.</jats:p>
                  <jats:p>Diamondback Energy, the largest independent producer in the Permian Basin, is working with Oklo to develop fast fission nuclear reactors.</jats:p>
                  <jats:p>SPE 215407, presented at the 2023 SPE/IATMI Asia Pacific Oil and Gas Conference, described using low-cost green power for unconventional oil recovery and sharing energy-storage facilities with data centers. A hybrid solution integrates a traditional grid with mobile microgrids between shale-oil fields and data centers.</jats:p>
                  <jats:p>Developing alternative power supplies with wide-scale reliability, dependability, and minimization or elimination of GHG emissions within a feasible capex/opex scenario is the brass ring of sustainability and energy security—and data are helping us get there.</jats:p>
               </jats:sec>
               <jats:sec>
                  <jats:title>For Further Reading</jats:title>
                  <jats:p>Data Centers Around the World: A Quick Look by Brian Daigle, US International Trade Commission.</jats:p>
                  <jats:p>SPE 218905 - Digital Decarbonization in the Oil and Gas Industry by I. Ershagi, University of Southern California; Donald L. Paul and Andrei Popa, Chevron Technology Center. https://doi.org/10.2118/218905-MS</jats:p>
                  <jats:p>SPE 215407 - Unconventional Fields Recovery Enabled by Large-Scale Green Power Supply Based on Multi Microgrids and Energy-Storage Sharing With National Data Centers by Z. Tong, X. Wang, and D. Weng et al., PetroChina. https://doi.org/10.2118/215407-MS</jats:p>
                  <jats:p>Data Science Has Exploded Across the Industry Over the Past 75 Years by Adam Wilson, JPT.</jats:p>
               </jats:sec>"
10.1002/nba.31721,Google.org study shows lots of upside for generative AI in philanthropic sector,"<jats:p>New research from Google.org shows that a healthy portion of nonprofit organizations are already making use of artificial intelligence, and in particular, what's known as generative AI.</jats:p>"
10.1007/979-8-8688-0473-1_6,GenAI-Powered Chatbots,N/A
10.29056/jdaem.2024.06.02,Research on ways to improve UX of generative image AI,N/A
10.1093/jiplp/jpad076,Attribution problem of generative AI: a view from US copyright law,N/A
10.1145/3563703.3596652,Sand-in-the-loop: Investigating embodied co-creation for shared understandings of generative AI,N/A
10.1145/3630106.3659033,A Critical Analysis of the Largest Source for Generative AI Training Data: Common Crawl,N/A
10.1145/3656156.3663703,Democratizing Design through Generative AI,N/A
10.36287/setsci.18.1.00106,A Survey on The Use of Generative AI in Aviation,N/A
10.1109/educon60312.2024.10578746,Developing Critical Thinking Practices Interwoven with Generative AI Usage in an Introductory Programming Course,N/A
10.1145/3643834.3661624,”Clay to Play With”: Generative AI Tools in UX and Industrial Design Practice,N/A
10.1109/iccsai59793.2023.10421601,Generative AI: A Review on Models and Applications,N/A
10.1007/978-3-031-54252-7,Generative AI Security,N/A
10.1038/d41586-019-02491-x,International AI ethics panel must be independent,N/A
10.13180/icres.2019.29-30.07.p01,Ethical standards in robotics and AI: What are they and why they matter?,N/A
10.31219/osf.io/e2g3m,The Reproducibility Issues of Health-Care AI: Examining the Costs and Ethics of Science,<p>The Reproducibility Issues of Health-Care AI: Examining the Costs and Ethics of Science</p>
10.1007/978-981-15-7695-9_14,"AI Ethics, Security and Privacy",N/A
10.1007/978-3-030-21134-9_15,"AI, Ethics, and the Law",N/A
10.1038/s42256-022-00598-x,Much to discuss in AI ethics,N/A
10.2139/ssrn.4727951,"“If it Can Be Done, it Will Be Done:” Ai Ethics and Public Relations Implications",N/A
10.4337/9781803924021,Happimetrics,N/A
10.1101/2024.08.23.24312461,Generative AI Enables Medical Image Segmentation in Ultra Low-Data Regimes,"<jats:p>Semantic segmentation of medical images is pivotal in applications like disease diagnosis and treatment planning. While deep learning has excelled in automating this task, a major hurdle is the need for numerous annotated segmentation masks, which are resource-intensive to produce due to the required expertise and time. This scenario often leads to ultra low-data regimes, where annotated images are extremely limited, posing significant challenges for the generalization of conventional deep learning methods on test images. To address this, we introduce a generative deep learning framework, which uniquely generates high-quality paired segmentation masks and medical images, serving as auxiliary data for training robust models in data-scarce environments. Unlike traditional generative models that treat data generation and segmentation model training as separate processes, our method employs multi-level optimization for end-to-end data generation. This approach allows segmentation performance to directly influence the data generation process, ensuring that the generated data is specifically tailored to enhance the performance of the segmentation model. Our method demonstrated strong generalization performance across 9 diverse medical image segmentation tasks and on 16 datasets, in ultra-low data regimes, spanning various diseases, organs, and imaging modalities. When applied to various segmentation models, it achieved performance improvements of 10-20\% (absolute), in both same-domain and out-of-domain scenarios. Notably, it requires 8 to 20 times less training data than existing methods to achieve comparable results. This advancement significantly improves the feasibility and cost-effectiveness of applying deep learning in medical imaging, particularly in scenarios with limited data availability.</jats:p>"
10.1145/3616855.3635737,Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers,N/A
10.1136/leader-2023-000797,"ChatGPT and generative AI chatbots: challenges and opportunities for science, medicine and medical leaders",N/A
10.1145/3585088.3593867,Design implications of generative AI systems for visual storytelling for young learners,N/A
10.1080/17579961.2024.2392930,Generative AI in American and Canadian courts: a ‘training’ approach to regulation,N/A
10.22214/ijraset.2024.57563,Introduction to Generative AI and its application in Education,"<jats:p>Abstract: Generative AI has made significant progress in re- cent years, with a growing range of applications in a variety of fields. Generative AI applications have catalyzed a new erain the synthesis and manipulation of digital content. Genera- tive AIis very recent technology which changed the way tradi-tional search engines work. The search engines work on the principles of information retrieval. However, openGL came up with use of Artificial Intelligence (AI) for synthesis of digital content and launched well known asChatGPT. The GenerativeAI differsfrom traditional AL as it takes text ,audio ,video andusing knowledge it generates new content in any form namely the text, audio or video. The generative AI has many profound applications. Generative AI is a rapidly developing field with the potential to revolutionize many industries and aspects of our lives. As the technology continues to advance, we can ex- pect to see even more groundbreaking and transformative ap-plications emerge. In this paper the introduction to GenerativeAI is detailed along with how generative AI can be used in ed-ucation.</jats:p>"
10.2139/ssrn.4628153,"Perceptions and Preparedness: Exploring Teacher Educators' Views on Integrating Generative AI in Colleges of Education, Ghana",N/A
10.2139/ssrn.4487768,Advancing Qualitative Analysis: An Exploration of the Potential of Generative AI and NLP in Thematic Coding,N/A
10.1007/978-1-4842-9994-4_9,ChatGPT Use Cases,N/A
10.1007/978-1-4842-9994-4_3,LLMs and Transformers,N/A
10.36948/ijfmr.2024.v06i03.20089,Trip Planner (using Generative AI),"<jats:p>The aim of this project is to develop an AI based travel planner that intends to alter planning and excursion encounters for people. Using sophisticated models and instructions for computers, such as statistical techniques applied in learning 
how humans learn best from their past experiences so they can anticipate future instances on them even when these are unknown for sure beforehand; data showing different types of transportation methods including those involving moving oneself put up for sale together with lodging places like hotels; field trips or tours where individuals can go out and participate in other events besides just sitting at home – all serving as input sources yielding recommendations specific enough standalone resources without being overly complex through averaging 
available data over time (Hong 2012). The In this regard, the trip planner will evaluate the personal user tastes, historical information concerning the destinations, as well as the contemporary pieces of advice on where to go today, where to get an accommodation or how to move by different types of vehicles, what to do. Besides, the system will support the natural language communication.</jats:p>"
10.1097/dbp.0000000000001234,Generative AI for Children's Digital Health: Clinician Advice,N/A
10.15565/jll.2023.9.95.35,Constructing Emotional Data Using Generative AI,N/A
10.21606/drs.2024.1260,Advancing Design With Generative AI: A Case of Automotive Design Process Transformation,N/A
10.2196/preprints.64182,Transforming Perceptions: Exploring the Multifaceted Potential of Generative AI for People with Cognitive Disabilities (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>Background: The emergence of generative artificial intelligence (GenAI) presents unprecedented opportunities to redefine conceptions of personhood and cognitive disability, potentially enhancing the inclusion and participation of individuals with cognitive disabilities in society.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>Objective: Explore the transformative potential of GenAI in reshaping perceptions of cognitive disability, dismantling societal barriers, and promoting social participation for individuals with cognitive disabilities.</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>Method: Critical review of current literature in disability studies, artificial intelligence (AI) ethics, and computer science, integrating insights from disability theories and the philosophy of technology. The analysis focused on two key aspects: GenAI as a social mirror reflecting societal values and biases, and GenAI as a cognitive partner for individuals with cognitive disabilities.</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>Results: The article proposes a theoretical framework for understanding the impact of GenAI on perceptions of cognitive disability. It introduces the concepts of GenAI as a ""social mirror"" that reflects and potentially amplifies societal biases, and as a ""cognitive co-pilot"" providing personalized assistance in daily tasks, social interactions, and environmental navigation. The article also presents a novel protocol for developing AI systems tailored to the needs of individuals with cognitive disabilities, emphasizing user involvement, ethical considerations, and the need to address both the opportunities and challenges posed by GenAI.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>Conclusions: Although GenAI has great potential for promoting the inclusion and empowerment of individuals with cognitive disabilities, realizing this potential requires a change in societal attitudes and development practices. The article calls for interdisciplinary collaboration and close partnership with the disability community in the development and implementation of GenAI technologies.
Implications: Realizing the potential of GenAI for promoting the inclusion and empowerment of individuals with cognitive disabilities requires a multi-faceted approach. This involves a shift in societal attitudes, inclusive AI development practices that prioritize the needs and perspectives of the disability community, and ongoing interdisciplinary collaboration. The article emphasizes the importance of proceeding with caution, recognizing the ethical complexities and potential risks alongside the transformative possibilities of GenAI technology.</p>
                </sec>"
10.2478/amns-2024-1915,Visualization of Basketball Tactical Evolution in Generative AI Big Models for Teaching and Learning,"<jats:title>Abstract</jats:title>
               <jats:p>Basketball tactics teaching occupies a major position in both professional sports teams and schools, even in social basketball training institutions. The paper suggests an improvement plan based on the shortcomings of the generative AI grand model, and integrates the enhanced model into the basketball tactics teaching process to create a visualization method for basketball tactics evolution. The proposed teaching model was applied to students’ basketball tactics training courses, and the use of the teaching experiment was investigated by using comprehensive research methods such as questionnaire survey method, paired-sample t-test, and one-way ANOVA, with students majoring in physical education in GT College as the experimental subjects. The results showed that the individual offensive and defensive tactics, local offensive and defensive tactics, and team-wide offensive and defensive tactics of the students in the experimental group and the control group were significantly improved after the experiment relative to the pre-experimental period and that the improvement of the experimental group (9.655-13.989) was more significant than that of the control group (4.844-6.515), and that the experimental group assisted with the basketball tactics teaching model based on the generative AI was more effective than the control group assisted with traditional tactics training method. Control the group using the conventional tactical training method. The study provides a reference for the implementation of teaching reform and improvement of teaching quality in the process of teaching basketball tactics in the future.</jats:p>"
10.18260/1-2--48596,Full Paper: A Generative AI Approach to Better Teamwork in First-Year Engineering,N/A
10.1007/s10758-024-09744-3,Exploring Students’ Generative AI-Assisted Writing Processes: Perceptions and Experiences from Native and Nonnative English Speakers,"<jats:title>Abstract</jats:title><jats:p>Generative artificial intelligence (AI) can create sophisticated textual and multimodal content readily available to students. Writing intensive courses and disciplines that use writing as a major form of assessment are significantly impacted by advancements in generative AI, as the technology has the potential to revolutionize how students write and how they perceive writing as a fundamental literacy skill. However, educators are still at the beginning stage of understanding students’ integration of generative AI in their actual writing process. This study addresses the urgent need to uncover how students engage with ChatGPT throughout different components of their writing processes and their perceptions of the opportunities and challenges of generative AI. Adopting a phenomenological research design, the study explored the writing practices of six students, including both native and nonnative English speakers, in a first-year writing class at a higher education institution in the US. Thematic analysis of students’ written products, self-reflections, and interviews suggests that students utilized ChatGPT for brainstorming and organizing ideas as well as assisting with both global (e.g., argument, structure, coherence) and local issues of writing (e.g., syntax, diction, grammar), while they also had various ethical and practical concerns about the use of ChatGPT. The study brought to front two dilemmas encountered by students in their generative AI-assisted writing: (1) the challenging balance between incorporating AI to enhance writing and maintaining their authentic voice, and (2) the dilemma of weighing the potential loss of learning experiences against the emergence of new learning opportunities accompanying AI integration. These dilemmas highlight the need to rethink learning in an increasingly AI-mediated educational context, emphasizing the importance of fostering students’ critical AI literacy to promote their authorial voice and learning in AI-human collaboration.</jats:p>"
10.1109/incit60207.2023.10413016,Investigating the Generative-AI Evaluation Methods and Correlation with Fashion Designers,N/A
10.14445/22312803/ijctt-v72i7p112,Strategies for Integrating Generative AI in Industrial Settings,N/A
10.1109/sieds61124.2024.10534674,The Impact of Generative AI and LLMs on the Cybersecurity Profession,N/A
10.1016/j.bushor.2024.04.013,The paradoxes of generative AI-enabled customer service: A guide for managers,N/A
10.1093/oxfordhb/9780190067397.013.30,Integrating Ethical Values and Economic Value to Steer Progress in Artificial Intelligence,"<p>This chapter studies how to balance the economic value created by artificial intelligence (AI) and ethical values. Economics and ethics both offer important perspectives on human society, but they do so from two different viewpoints: the central focus of economics is how the price system in the economy values resources; the central focus of ethics is the moral evaluation of actions in the society. The rise of AI forces humanity to confront new areas in which ethical values and economic value conflict, raising the question of what direction of technological progress is ultimately desirable for society. One crucial area is the effects of AI and related forms of automation on labor markets, which may lead to substantial increases in inequality unless mitigating policy actions are taken or progress is actively steered in a direction that complements human labor. Additional areas of conflict arise when AI systems optimize narrow market value but disregard broader ethical values and thus impose externalities on society, for example, when AI systems engage in bias and discrimination, hack the human brain, and increasingly reduce human autonomy.</p>"
10.1007/s43681-022-00237-6,Ethical and legal considerations for nutrition virtual coaches,"<jats:title>Abstract</jats:title><jats:p>Choices and preferences of individuals are nowadays increasingly influenced by countless inputs and recommendations provided by artificial intelligence-based systems. The accuracy of recommender systems (RS) has achieved remarkable results in several domains, from infotainment to marketing and lifestyle. However, in sensitive use-cases, such as nutrition, there is a need for more complex dynamics and responsibilities beyond conventional RS frameworks. On one hand, virtual coaching systems (VCS) are intended to support and educate the users about food, integrating additional dimensions w.r.t. the conventional RS (i.e., leveraging persuasion techniques, argumentation, informative systems, and recommendation paradigms) and show promising results. On the other hand, as of today, VCS raise unexplored ethical and legal concerns. This paper discusses the need for a clear understanding of the ethical/legal-technological entanglements, formalizing 21 ethical and ten legal challenges and the related mitigation strategies. Moreover, it elaborates on nutrition sustainability as a further nutrition virtual coaches dimension for a better society.</jats:p>"
10.2478/nimmir-2024-0008,Beyond the Buzz: Creating Marketing Value with Generative AI,N/A
10.12781/978-1-907549-22-9-1,Appreciation and Generative Dialogues: Reflections and Appreciative Inquiry Practices from Ibero-America,N/A
10.1145/3544549.3585680,Collaborative Diffusion: Boosting Designerly Co-Creation with Generative AI,N/A
10.31219/osf.io/y9q85,Collaborative Forensic Autopsy Documentation and Supervised Report Generation using a Hybrid Mixed-Reality Environment and Generative AI,"<p>Forensic investigation is a complex procedure involving experts working together to establish cause of death and report findings to legal authorities. While new technologies are being developed to provide better post-mortem imaging capabilities---including mixed-reality tools to support 3D visualisation of such data---these tools require extra steps for forensic experts and do not integrate seamlessly into their existing collaborative workflow and report authoring process. Therefore, in this work we design and evaluate a new forensic autopsy report generation workflow and present a novel documentation system using hybrid mixed-reality approaches to integrate visualisation, voice and hand interaction, as well as collaboration and procedure recording. Our findings indicate that this approach may improve data management, aid reviewability, and thus achieving more robust standards. Further, it can streamline report generation and minimise dependency on external tools and assistance, reducing autopsy time and related costs. This system also offers significant potential for education.</p>"
10.5121/ijci.2024.130106,An Ethical Study of Generative AI from the Actor-Network Theory Perspective,"<jats:p>The widespread use of Generative Artificial Intelligence in the innovation and generation of communication content is mainly due to its exceptional creative ability, operational efficiency, and compatibility with diverse industries. Nevertheless, this has also sparked ethical problems, such as unauthorized access to data, biased decision-making by algorithms, and criminal use of generated content. In order to tackle the security vulnerabilities linked to Generative Artificial Intelligence, we analyze ChatGPT as a case study within the framework of Actor-Network Theory. We have discovered a total of nine actors, including both human and non-human creatures. We examine the actors and processes of translation involved in the ethical issues related to ChatGPT and analyze the key players involved in the emergence of moral issues. The objective is to explore the origins of the ethical issues that arise with Generative Artificial Intelligence and provide a particular perspective on the governance of Generative Artificial Intelligence.</jats:p>"
10.2139/ssrn.4813002,Comments to the European Commission’s Call for Contributions on the Topic of “Competition in Virtual Worlds and Generative AI”,N/A
10.1515/icom-2020-0025,Examining Autocompletion as a Basic Concept for Interaction with Generative AI,"<jats:title>Abstract</jats:title>
               <jats:p>Autocompletion is an approach that extends and continues partial user input. We propose to interpret autocompletion as a basic interaction concept in human-AI interaction. We first describe the concept of autocompletion and dissect its user interface and interaction elements, using the well-established textual autocompletion in search engines as an example. We then highlight how these elements reoccur in other application domains, such as code completion, GUI sketching, and layouting. This comparison and transfer highlights an inherent role of such intelligent systems to extend and complete user input, in particular useful for designing interactions with and for generative AI. We reflect on and discuss our conceptual analysis of autocompletion to provide inspiration and a conceptual lens on current challenges in designing for human-AI interaction.</jats:p>"
10.12968/s1361-3723(23)70044-4,The good and bad of generative AI,N/A
10.1109/mitp.2024.3404229,"Where Deep Learning and Generative AI Started: Masterminds of Artificial Neural Networks—McCulloch, Pitts, and Rosenblatt",N/A
10.21608/jstc.2023.316298,The Future of Now Generative AI,N/A
10.22214/ijraset.2023.56449,A Survey on Healthcare Virtual Assistant Using Generative AI,"<jats:p>Documentation is a clumsy task for the front- desk staff. They've to fill in patient data, schedule appointments, and attend to patient queries. Even healthcare providers must enter EHR data, which takes a lot of time, and they end up spending lesser time with their patients. In the evolving landscape of healthcare administration, the burden of administrative tasks continues to challenge healthcare professionals and institutions. However, with generative AI, doctors can produce clones of patient data and automate form- filling tasks. It can also be integrated with EHR for documentation work. This project introduces a result — a generative artificial intelligence (AI) system designed to ease administrative burdens and enhance operating efficiency. By employing the power of AI, this web- grounded platform aims to streamline administrative processes, automate routine tasks, and facilitate absolute communication between patients, medical staff, and administrative personnel.</jats:p>"
10.1162/qss_a_00285/v2/response1,"Author response for ""Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents""",N/A
10.2139/ssrn.4678557,Bonus or Burden?  the Impact of Attitudes and Fear of Missing Out (Fomo) on Generative Ai Adoption,N/A
10.2139/ssrn.4703856,Generative Ai Based Augmentation for Offshore Jacket Design: An Integrated Approach for Mixed Tabular Data Generation Under Data Scarcity and Imbalance,N/A
10.22214/ijraset.2024.62682,Enriching Education with Artificial Intelligence Generative Ai-Speech to Image Generator,"<jats:p>Abstract: Visual memory is the strongest in learning; It is 60,000 times more powerful than text and memory. This important fact shows that there is an urgent need to make changes in the education process, moving away from old, monotonous practices and towards a dynamic, effective and permanent educational support. Inspired by the effectiveness of the invisible aspects of visual learning, this article introduces GenClassroom, a new learning tool that promises to revolutionize the traditional teaching method. GenClassroom uses the power of artificial intelligence (AI) to translate and transform teacher lessons into beautiful images, providing students with a unique learning experience beyond the boundaries of the classroom environment. Seamlessly integrating speech-to-image capabilities into dynamic virtual classroom environments, GenClassroom enables educators to create engaging, interactive lessons that resonate with students and promote understanding, retention, and engagement. Through a qualitative study of GenClassroom’s design, functionality, and application content, this article outlines the evolution of the vision of AI-enabled education to shape the future of learning. As the ed- ucational landscape changes dramatically, GenClassroom serves as a beacon of innovation, ushering in a new era of personalized, meaningful, and empowering learning. By embracing the endless possibilities of visual learning enhanced by artificial intelligence, GenClassroom paves the way for a future where learning has no boundaries and every student can reach their potential.</jats:p>"
10.1080/13678868.2024.2334983,Perspectives on the promise and perils of generative AI in academia,N/A
10.12781/978-1-907549-58-8-3,Coming Alive as Researchers: A Generative Conversation with Drs Arne Carlsen and Jane Dutton,"<jats:p>In this article, Lindsey Godwin interviews Arne Carlsen and Jane Dutton in a conversation about what it means to embrace generativity as a researcher, the role Appreciative Inquiry plays in their research methods, and recent writing exploring awe, wonder and transcendence.</jats:p>"
10.1016/j.orgdyn.2024.101042,Principles for advertising responsibly using generative AI,N/A
10.1109/icaiccit60255.2023.10466102,Generative AI for Software Test Modelling with a focus on ERP Software,N/A
10.2196/53008,Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges,"<jats:p>As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations.</jats:p>"
10.22554/ijtel.v7i2.124,Leveraging Generative AI Tools for Enhanced Lesson Planning in Initial Teacher Education at Post Primary,"<jats:p>The rapid development of generative AI (artificial intelligence) tools such as ChatGPT and Google Bard has opened new possibilities for enhancing lesson planning in initial teacher education (ITE). These tools have the capability to generate tailored educational content, alleviating time constraints while concurrently enhancing the quality of teaching. By simply providing specific requirements and objectives, teachers can obtain comprehensive and well-structured lesson plans and subject plans. This paper explores the potential of generative AI tools to revolutionise lesson planning in initial teacher education. It begins by reviewing lesson planning using a generative AI tool, highlighting the challenges and opportunities that exist. A sample lesson plan and a sample scheme of work are further created. While these tools are revolutionising the way teachers work, these tools will not replace real human teachers. Teachers will always need to supplement generative AI content with their own insights and experience, allowing them to make informed pedagogical decisions.</jats:p>"
10.2139/ssrn.4807392,Let Employees Train Their Own Chatbots: Design of Generative Ai-enabled Delegation Systems,N/A
10.1093/jiplp/jpae028,"Copyright, text &amp; data mining and the innovation dimension of generative AI","<jats:title>Abstract</jats:title>
               <jats:p>The rise of Generative AI has raised many questions from the perspective of copyright. From the lens of copyright and database rights, issues revolve not only around the authorship of AI-generated outputs, but also the very process that leads to the generation of these outputs, namely the process of text and data mining (TDM). Does unauthorized TDM process infringe the economic rights of the rightholders? How does the TDM-debate transform and transmute in the age of Generative AI? Generative AI tools create works that substitute the content creators whose very work that they learn from, and successively improvise themselves with every iteration. Generative AI, thus, also presents larger policy question as they substitute the romanticized human author that sits at the centre of copyright. In addition, as Generative AI tools, such as ChatGPT, can now also crawl the web, questions thus transcend the frontiers of copyright, and touch upon innovation and competition in the market for web browsers. This research article contemplates on the foregoing issues, and makes some recommendations to create a balanced framework, whereby incentives to innovate are preserved, and the interests of the human author are suitably safeguarded in the age of TDM and Generative AI.</jats:p>"
10.1016/j.yjoc.2024.100086,A task-oriented framework for generative AI in design,N/A
10.26782/jmcms.2024.05.00001,FEATURES OF THE USE AI IN GENERATIVE DESIGN OF BUILDING AND STRUCTURES,"<jats:p>The authors of the article consider what features appear when using artificial intelligence (AI) in the generative design of construction facilities. Every day artificial intelligence becomes more and more important in various fields of human activity. One of the areas of activity in which AI is actively being implemented is construction, namely digital (BIM) and generative (GD) building design. These areas of design include the development of design solutions for an object using computer algorithms and mathematical models. The article examines the positive aspects of implementing AI in generative design, compared to traditional design methods. The use of AI in generative design can improve the quality of produced design documentation by reducing the number of unintentional mechanical and technical errors, providing designers with a more extensive amount of analytical data. The authors focus on the main AI methods that are involved in GD, as well as the problems and limitations that arise when using AI in design.</jats:p>"
10.18421/sar72-01,Neural Networks within Generative AI: A Review from a Marketing Research Perspective,"<jats:p>Focusing on the role of neural networks within Generative Artificial Intelligence, this paper reviews their operating principles, recent developments, and implications for research in the marketing discipline. Special emphasis is placed on generative networks that produce synthetic data with a high degree of similarity to original data – such as realistic images, textual, or audio content – thereby expanding the potential for applied marketing practices. Besides significant opportunities opened up by Generative Artificial Intelligence, the review also elaborates on challenges, including technical obstacles like achieving an adequate Nash equilibrium between competing network components, and application-related considerations such as finding reliable metrics to evaluate their business performance. Broader implications are also analysed from recent publications, including ethical dilemmas related to data authenticity. As a result, this study provides a distinct perspective within the intersecting fields of marketing and computer science in the discourse surrounding Generative Artificial Intelligence, underscoring the research innovations arising from neural network models.</jats:p>"
10.1002/cend.202400006,Automating computational design with generative <scp>AI</scp>,"<jats:title>Abstract</jats:title><jats:p>AI image generators based on diffusion models have recently garnered attention for their capability to create images from simple text prompts. However, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. This paper investigates the potential of current AI generators in addressing such challenges, specifically for the creation of simple floor plans. We explain how the underlying diffusion‐models work and propose novel refinement approaches to improve semantic encoding and generation quality. In several experiments we show that we can improve validity of generated floor plans from 6% to 90%. Based on these results we derive future research challenges considering building information modeling. With this we provide: (i) evaluation of current generative AIs; (ii) propose improved refinement approaches; (iii) evaluate them on various examples; (iv) derive future directions for diffusion models in civil engineering.</jats:p>"
10.1080/17512786.2024.2394558,Uses of Generative AI in the Newsroom: Mapping Journalists’ Perceptions of Perils and Possibilities,N/A
10.62273/yqwp1758,"A Comparison of Generative AI Solutions and Textbook Solutions in an
Introductory Programming Course",N/A
10.1177/00472395241270278,Generative AI: Is Authentic Qualitative Research Data Collection Possible?,"<jats:p> With the advent of readily accessible generative artificial intelligence (GAI), a concern exists within the academic community that research data collected in the context of conducting doctoral dissertation research is authentic. The purpose of the present study was to explore the role of GAI in the production of new research paying particular attention to the use of GAI in collecting new data for a doctoral dissertation. This study employed qualitative methodology examining how GAI, specifically ChatGPT, responded to interview questions from a previously published article by the researchers to determine how closely chatbots mimic responses from the actual study participants. The researchers found that data integrity in qualitative research may be at risk if higher education institutions do not set clear policies and specific parameters for how doctoral research data is obtained and validated in light of GAI. </jats:p>"
10.1109/asiancon58793.2023.10270193,Unleashing the Power of Generative AI and Quantum Computing for Mutual Advancements,N/A
10.1038/s41746-023-00873-0,The imperative for regulatory oversight of large language models (or generative AI) in healthcare,"<jats:title>Abstract</jats:title><jats:p>The rapid advancements in artificial intelligence (AI) have led to the development of sophisticated large language models (LLMs) such as GPT-4 and Bard. The potential implementation of LLMs in healthcare settings has already garnered considerable attention because of their diverse applications that include facilitating clinical documentation, obtaining insurance pre-authorization, summarizing research papers, or working as a chatbot to answer questions for patients about their specific data and concerns. While offering transformative potential, LLMs warrant a very cautious approach since these models are trained differently from AI-based medical technologies that are regulated already, especially within the critical context of caring for patients. The newest version, GPT-4, that was released in March, 2023, brings the potentials of this technology to support multiple medical tasks; and risks from mishandling results it provides to varying reliability to a new level. Besides being an advanced LLM, it will be able to read texts on images and analyze the context of those images. The regulation of GPT-4 and generative AI in medicine and healthcare without damaging their exciting and transformative potential is a timely and critical challenge to ensure safety, maintain ethical standards, and protect patient privacy. We argue that regulatory oversight should assure medical professionals and patients can use LLMs without causing harm or compromising their data or privacy. This paper summarizes our practical recommendations for what we can expect from regulators to bring this vision to reality.</jats:p>"
10.1007/978-981-19-9382-4,AI Ethics,N/A
10.2139/ssrn.4240356,Global To Local: South African Perspectives on AI Ethics Risks,N/A
10.1109/tts.2024.3446183/mm1,Human Participants in AI Research: Ethics and Transparency in Practice_supp1-3446183.pdf,N/A
10.2175/193864718825159304,Laws and Ethics of Using AI in Utility Operations and Management,N/A
10.1016/b978-0-443-13671-9.00001-6,Ethics and regulations for AI in radiology,N/A
10.1007/s43681-021-00124-6,"Ethical, legal, social, and economic (ELSE) implications of artificial intelligence at a global level: a scientometrics approach",N/A
10.1101/2024.05.20.595002,Synthetic Genitourinary Image Synthesis via Generative Adversarial Networks: Enhancing AI Diagnostic Precision,"<jats:title>Abstract</jats:title><jats:p>In the realm of computational pathology, the scarcity and restricted diversity of genitourinary (GU) tissue datasets pose significant challenges for training robust diagnostic models. This study explores the potential of Generative Adversarial Networks (GANs) to mitigate these limitations by generating high-quality synthetic images of rare or underrepresented GU tissues. We hypothesized that augmenting the training data of computational pathology models with these GAN-generated images, validated through pathologist evaluation and quantitative similarity measures, would significantly enhance model performance in tasks such as tissue classification, segmentation, and disease detection. To test this hypothesis, we employed a GAN model to produce synthetic images of eight different GU tissues. The quality of these images was rigorously assessed using a Relative Inception Score (RIS) of 17.2 ± 0.15 and a Fréchet Inception Distance (FID) that stabilized at 120, metrics that reflect the visual and statistical fidelity of the generated images to real histopathological images. Additionally, the synthetic images received an 80% approval rating from board-certified pathologists, further validating their realism and diagnostic utility. We used an alternative Spatial Heterogeneous Recurrence Quantification Analysis (SHRQA) to assess quality in prostate tissue. This allowed us to make a comparison between original and synthetic data in the context of features, which were further validated by the pathologist’s evaluation. Future work will focus on implementing a deep learning model to evaluate the performance of the augmented datasets in tasks such as tissue classification, segmentation, and disease detection. This will provide a more comprehensive understanding of the utility of GAN-generated synthetic images in enhancing computational pathology workflows. This study not only confirms the feasibility of using GANs for data augmentation in medical image analysis but also highlights the critical role of synthetic data in addressing the challenges of dataset scarcity and imbalance. Future work will focus on refining the generative models to produce even more diverse and complex tissue representations, potentially transforming the landscape of medical diagnostics with AI-driven solutions.</jats:p><jats:sec><jats:title>CONSENT FOR PUBLICATION</jats:title><jats:p>All authors have provided their consent for publication.</jats:p></jats:sec>"
10.54941/ahfe1005473,Bridging the Gap: A Comparative Analysis in Creative Processes between AI-Generative and Traditional Art,"<jats:p>The creative process has significant importance in the realm of artistic creation, as it involves a series of cognitive and generative acts that culminate in the production of unique and original artworks. The advent of artificial intelligence has given rise to a new kind of creative output, hence posing inquiries on the essence of creativity in machines. An examination of the creative processes used in AI art may provide valuable insights into the mechanisms via which AI systems generate artworks and the extent to which these processes align with human creative practices.This study undertook a comprehensive analysis of contemporary AI art methodologies, algorithms, and models to explore the fundamental dynamics that underlie the generation of AI-generated art. The aforementioned approaches were subjected to a comparative analysis with traditional art production processes, with careful consideration given to many components including ideation, experimentation, and iteration. The identification of similarities and differences may be accentuated via the process of identifying them.</jats:p>"
10.7146/hn.v8i2.143114,Nothing New Under the Sun?,"<jats:p>This article investigates the potential impact of generative artificial intelligence, specifically OpenAI's GPT, on the field of biblical studies, particularly biblical Hebrew. The study is divided into three main categories: (1) knowledge retrieval or language understanding, (2) generative modeling or creative problem solving, and (3) command interpretation or query parsing. Experiments are conducted using OpenAI's GPT, the ETCBC's BHSA dataset, and Text-Fabric Python libraries. Results demonstrate GPT's limitations and proficiencies in biblical Hebrew and its capacity to employ its proficiencies creatively in problem-solving scenarios involving multifaceted forms of reasoning. The study concludes that understanding the capabilities and potential trajectories of these technologies is vital for biblical Hebrew scholarship, as they already possess the capacity to disrupt established scholarly norms and democratize access to advanced tools.</jats:p>"
10.1109/access.2024.3439363,Enhancing Autonomous System Security and Resilience With Generative AI: A Comprehensive Survey,N/A
10.4324/9781003482918-17,Process not product in the written assessment,N/A
10.4324/9781032648033-3,Will all language professionals need translation technologies?,N/A
10.3390/app14114570,Implementation of a Generative AI Algorithm for Virtually Increasing the Sample Size of Clinical Studies,"<jats:p>Determining the appropriate sample size is crucial in clinical studies due to the potential limitations of small sample sizes in detecting true effects. This work introduces the use of Wasserstein Generative Adversarial Networks (WGANs) to create virtual subjects and reduce the need for recruiting actual human volunteers. The proposed idea suggests that only a small subset (“sample”) of the true population can be used along with WGANs to create a virtual population (“generated” dataset). To demonstrate the suitability of the WGAN-based approach, a new methodological procedure was also required to be established and applied. Monte Carlo simulations of clinical studies were performed to compare the performance of the WGAN-synthesized virtual subjects (i.e., the “generated” dataset) against both the entire population (the so-called “original” dataset) and a subset of it, the “sample”. After training and tuning the WGAN, various scenarios were explored, and the comparative performance of the three datasets was evaluated, as well as the similarity in the results against the population data. Across all scenarios tested, integrating WGANs and their corresponding generated populations consistently exhibited superior performance compared with those from samples alone. The generated datasets also exhibited quite similar performance compared with the “original” (i.e., population) data. By introducing virtual patients, WGANs effectively augment sample size, reducing the risk of type II errors. The proposed WGAN approach has the potential to decrease costs, time, and ethical concerns associated with human participation in clinical trials.</jats:p>"
10.1787/4cca1e36-en,Regulation and guidance on generative AI in education (2024),N/A
10.1016/j.cdnut.2024.102770,Assessment of Generative AI Large Language Model Knowledge in Children's Nutrition Education,N/A
10.54254/2753-7048/29/20231440,Generative AI Processes for 2D Platformer Game Character Design and Animation,"<jats:p>AI has the potential to revolutionize the time-consuming and technically complex process of 2D animation production. This paper specifically focuses on creating 2D character animations for platformer games using AI. While AI has made significant contributions to video animations, its application in 2D gaming animation is largely unexplored. Existing AI applications for animation mostly target video animations and lack effective control over randomness. Therefore, this paper explores the role of generative AI in 2D gaming animation, from character design to full animation. Software tools like ChatGPT, Midjourney, Stable Diffusion, and Unity are used to streamline the production process. The research aims to investigate the feasibility and potential of generative AI, with a focus on controlling randomness. By leveraging the unique features of each software, the study aims to enhance the production of 2D game animations. The final output will be an animated character in the Idle state, showcasing the potential of generative AI in 2D gaming animation.</jats:p>"
10.1109/confluence60223.2024.10463372,A Survey of Text-to-Image Diffusion Models in Generative AI,N/A
10.21125/inted.2024.0894,USING GENERATIVE AI FOR STRATEGIC ANALYSIS? A STUDY ON PERCEIVED UTILITY AMONG INDUSTRIAL ORGANIZATION ENGINEERING STUDENTS,N/A
10.1109/rcae59706.2023.10398866,A SAR Denoising Network Based on Generative Adversarial Learning,N/A
10.1145/3589335.3651901,Automatic Design Summary Generation with Generative AI,N/A
10.2196/preprints.66022,Generative AI-assisted Peer Review in Medical Publications: Opportunities Or Trap (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>With the exponential growth in the number of research papers and the proliferation of preprint servers, ensuring high-quality peer review has become a significant challenge, especially in the medical field. The surge in submissions has led to a shortage of qualified reviewers, slowing down the peer review process. The repeated review of rejected manuscripts not only increases costs but may also stifle research innovation, raising concerns about the efficiency, fairness, and effectiveness of the review process. Therefore, innovative solutions are urgently needed. Recent advancements in generative artificial intelligence (GenAI), such as ChatGPT, have demonstrated exceptional capabilities in feature learning and textual expression, allowing them to identify complex relationships within data without relying on pre-existing assumptions. GenAI present an opportunity to enhance semi-automated peer review systems, potentially addressing the current limitations in the peer review process and improving the efficiency and quality of medical publications. This viewpoint highlights the potential benefits and challenges of integrating GenAI into the peer review and identifies the key issues that need to be addressed.</p>
                </sec>"
10.1007/978-3-031-57675-1_2,Generative AI and the History of Architecture,N/A
10.1108/qae-02-2024-0043,"Generative AI: hopes, controversies and the future of faculty roles in education","<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>Generative artificial intelligence (GAI) has seen exponential growth in recent years due to its capability to generate original content through natural language processing and comprehensive language models. This paper aims to investigate the transformative impact of GAI on higher education, focusing on the evolving roles of faculty in the classroom.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>Using a phenomenological perspective and a process approach, the study involved 25 semi-structured interviews with academicians in higher education.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>The findings reveal that GAI currently creates biased and commercially driven learning environments, challenging traditional pedagogical models. Despite its potential for enhancing education, the autonomous nature of GAI often prioritizes commercial interests over pedagogical goals.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title>
<jats:p>The study is limited to faculty perspectives, suggesting future research should include student viewpoints and diverse educational contexts.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Practical implications</jats:title>
<jats:p>The study highlights the need for higher education institutions to develop comprehensive policies, provide training for faculty and students and design new courses that leverage GAI for personalized learning experiences and enhanced faculty research.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>This paper contributes to the emerging literature on GAI’s impact on education, highlighting its dual nature as both a transformative tool and a potential threat to traditional educational roles and outcomes.</jats:p>
</jats:sec>"
10.14361/9783839474723-004,Bibliographic information published by the Deutsche Nationalbibliothek,N/A
10.1190/tle42110730.1,President's Page: Digital twins in the era of generative AI,"<jats:p> The industry is experiencing significant changes due to artificial intelligence (AI) and the challenges of the energy transition. While some view these changes as threats, recent advances in AI offer unique opportunities, especially in the context of “digital twins” for subsurface monitoring and control. </jats:p>"
10.1016/s0167-8140(24)02491-5,2255: Iterative CT Reconstruction with Generative AI,N/A
10.1109/inocon60754.2024.10511910,Smartserve – Leveraging Generative AI for Multifaceted Services,N/A
10.23919/picmet64035.2024.10653035,Content Creation by Generative AI and Operator Perception A Focus on Operator's Profit-driven Motivation,N/A
10.1016/j.ijinfomgt.2024.102824,Conceptualizing generative AI as style engines: Application archetypes and implications,N/A
10.1007/s43681-022-00215-y,Facial identity protection using deep learning technologies: an application in affective computing,"<jats:title>Abstract</jats:title><jats:p>Automatic prediction of human attributions of valence and arousal using facial recognition technologies can improve human–computer and human–robot interaction. However, data protection has become an issue of great concern in affect recognition using facial images, as the facial identities of people (i.e. recognising who a person is) could be exposed in the process. For instance, malicious individuals could exploit facial images of users to assume their identities and infiltrate biometric authentication systems. Possible solutions to protect the facial identity of users are to: (1) extract anonymised facial features in users’ local machines, namely action units (AU) of facial images, discard their facial images and send the AUs to the developer for processing, and (2) employ a federated learning approach i.e. process users’ facial images in their local machines and only send their locally trained models back to the developer’s machine for augmenting the final model. In this paper, we implement and compare the performance of these privacy-preserving strategies for affect recognition. Results on the popular RECOLA affective datasets show promising affect recognition performance in adopting a federated learning approach to protect users’ identities, with Concordance Correlation Coefficient of 0.426 for valence and 0.390 for arousal.</jats:p>"
10.1007/s43681-023-00363-9,A statistical approach to detect disparity prone features in a group fairness setting,N/A
10.1007/s43681-024-00561-z,Ethical considerations of use of hold-out sets in clinical prediction model management,"<jats:title>Abstract</jats:title><jats:p>Clinical prediction models are statistical or machine learning models used to quantify the risk of a certain health outcome using patient data. These can then inform potential interventions on patients, causing an effect called performative prediction: predictions inform interventions which influence the outcome they were trying to predict, leading to a potential underestimation of risk in some patients if a model is updated on this data. One suggested resolution to this is the use of hold-out sets, in which a set of patients do not receive model derived risk scores, such that a model can be safely retrained. We present an overview of clinical and research ethics regarding potential implementation of hold-out sets for clinical prediction models in health settings. We focus on the ethical principles of beneficence, non-maleficence, autonomy and justice. We also discuss informed consent, clinical equipoise, and truth-telling. We present illustrative cases of potential hold-out set implementations and discuss statistical issues arising from different hold-out set sampling methods. We also discuss differences between hold-out sets and randomised control trials, in terms of ethics and statistical issues. Finally, we give practical recommendations for researchers interested in the use hold-out sets for clinical prediction models.</jats:p>"
10.1108/oxan-es243130,EU seeks leadership on AI ethics rules,"<jats:sec sec-type=""headline"">
               <jats:title content-type=""abstract-subheading"">Headline</jats:title>
               <jats:p>EU: Europe seeks leadership on AI ethics rules</jats:p>
            </jats:sec>"
10.30966/2018.riga.8.6.,AI Ethics: A Strategic Communications Challenge,N/A
10.21428/e90189c8.a16c4bb9,Ethics &amp; Responsible AI in Healthcare,N/A
10.7551/mitpress/12549.003.0014,"It's the Climate, Stupid! On Priorities, the Anthropocene, and Elon Musk's Car in Space",N/A
10.1016/b978-0-12-821442-8.00013-6,"Digital leadership, ethics, and challenges",N/A
10.5040/9781350232143.ch-42,"AI, Ethics, and Digital Humanities",N/A
10.4337/9781802203110.00004,Preface,N/A
10.1007/978-3-030-30062-3_5,Ethics and AI,N/A
10.2139/ssrn.4945566,The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers,N/A
10.1109/mc.2019.2924546,Generative Adversarial Networks in AI-Enabled Safety-Critical Systems: Friend or Foe?,N/A
10.21428/8c225f6e.6c5360d1,The role of ChatGPT in language education: A study of Omani students' perspectives,N/A
10.51265/gfr.2023.4.1.91,Innovation and Risk Factors of Generative AI in the Financial Industry,N/A
10.1609/aaai.v38i21.30512,Diverse Yet Biased: Towards Mitigating Biases in Generative AI (Student Abstract),"<jats:p>Generative Artificial Intelligence (AI) has garnered significant attention for its remarkable ability to generate text, images, and other forms of content. However, an inherent and increasingly concerning issue within generative AI systems is bias. These AI models often exhibit an Anglo-centric bias and tend to overlook the importance of diversity. This can be attributed to their training on extensive datasets sourced from the internet, which inevitably inherit the biases present in those data sources. Employing these datasets leads to AI-generated content that mirrors and perpetuates existing biases, encompassing various aspects such as gender, ethnic and cultural stereotypes. Addressing bias in generative AI is a complex challenge that necessitates substantial efforts. In order to tackle this issue, we propose a methodology for constructing moderately sized datasets with a social inclination. These datasets can be employed to rectify existing imbalances in datasets or to train models to generate socially inclusive material. Additionally, we present preliminary findings derived from training our model on these socially inclined datasets.</jats:p>"
10.36227/techrxiv.171466626.67294030/v1,Global Insights and the Impact of Generative AI-ChatGPT on Multidisciplinary: A Systematic Review and Bibliometric Analysis,N/A
10.12968/s1361-3723(24)70018-9,Generative AI has democratised fraud and cybercrime,"<jats:p> Artificial intelligence has no intrinsic ethical stance. Everything depends on the uses to which it is put. This is leading to an arms race between threat actors, using generative AI and large language models (LLMs) to hone the effectiveness of phishing and other forms of attack, and security professionals looking to conscript AI into the effort to identify and interdict those attackers. What's certain is that no-one can ignore generative AI and LLMs, and it's important to understand both the threat and the potential for enhancing security. </jats:p>"
10.2139/ssrn.4851228,The world’s first effective judgment on copyright infringement by Generative AI Service Providers: Translation of the original Guangzhou Internet Court decision,N/A
10.37648/ijrst.v14i01.004,Enhanced Spark Cluster Recommendation Engine Powered by Generative AI,"<jats:p>Apache Spark, renowned for its proficiency in processing vast datasets, efficiently handles intricate processing tasks. It disperses these tasks across numerous computing instances autonomously or in conjunction with other distributed computing tools. As the volume of data burgeons and machine learning models advance, the imperative for swift and intricate feature engineering and model training intensifies. Clusters comprising multiple compute instances exhibit a noteworthy performance surge compared to individual cases, expediting data processing. However, leveraging such cluster configurations entails substantial costs due to the amalgamation of multiple compute instances (Worker Nodes) overseen by a Controller Node.</jats:p>"
10.62051/ijcsit.v3n2.10,Rapid Review of Generative AI in Smart Medical Applications,"<jats:p>With the continuous advancement of technology, artificial intelligence has significantly impacted various fields, particularly healthcare. Generative models, a key AI technology, have revolutionized medical image generation, data analysis, and diagnosis. This article explores their application in intelligent medical devices. Generative models enhance diagnostic speed and accuracy, improving medical service quality and efficiency while reducing equipment costs. These models show great promise in medical image generation, data analysis, and diagnosis. Additionally, integrating generative models with IoT technology facilitates real-time data analysis and predictions, offering smarter healthcare services and aiding in telemedicine. Challenges include computational demands, ethical concerns, and scenario-specific limitations.</jats:p>"
10.1109/icitee59582.2023.10317774,Generative AI for Industrial Applications: Synthetic Dataset,N/A
10.35542/osf.io/n9w4b,Using Generative AI to Provide Feedback to Adult Tutors in Training and Assess Real-life Performance,"<p>Tutoring remains among the most impactful academic interventions known to improve student achievement. Despite its success, there are few available opportunities for training and professional development for adult tutors. Furthermore, the task of assessing tutor performance during actual tutoring sessions is challenging and time-consuming for human evaluators—enter artificial intelligence (AI). In this work, we harness generative AI, specifi-cally large language models (LLMs) like GPT-4, to introduce an innovative approach for delivering tutors real-time, explanatory feedback while tutors engage in online scenario-based lessons. Hosted within the Personalized Learning Squared (PLUS) tutoring platform, these lessons offer tutors op-portunities to practice responding to common tutoring scenarios. For exam-ple, in the Giving Effective Praise lesson, tutors practice responding to students by providing praise and then receiving immediate and templated feedback generated by LLMs. Beyond demonstrating tutor training contain-ing AI-generated feedback, we report past work using LLMs to assess tutors providing praise, revealing moderate performance, and explaining several prompting methods. While using generative AI shows promise as a low-cost and efficient method for giving adult tutors feedback on their perfor-mance in training and actual tutoring, it is not without limitations. Practical and ethical considerations are discussed. Human tutoring remains a robust and irreplaceable influence on student learning, with generative AI serving as a valuable tool to complement tutors and enhance the impact of tutoring on student learning.</p>"
10.2196/48949,Cocreating an Automated mHealth Apps Systematic Review Process With Generative AI: Design Science Research Approach,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>The use of mobile devices for delivering health-related services (mobile health [mHealth]) has rapidly increased, leading to a demand for summarizing the state of the art and practice through systematic reviews. However, the systematic review process is a resource-intensive and time-consuming process. Generative artificial intelligence (AI) has emerged as a potential solution to automate tedious tasks.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>This study aimed to explore the feasibility of using generative AI tools to automate time-consuming and resource-intensive tasks in a systematic review process and assess the scope and limitations of using such tools.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>We used the design science research methodology. The solution proposed is to use cocreation with a generative AI, such as ChatGPT, to produce software code that automates the process of conducting systematic reviews.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>A triggering prompt was generated, and assistance from the generative AI was used to guide the steps toward developing, executing, and debugging a Python script. Errors in code were solved through conversational exchange with ChatGPT, and a tentative script was created. The code pulled the mHealth solutions from the Google Play Store and searched their descriptions for keywords that hinted toward evidence base. The results were exported to a CSV file, which was compared to the initial outputs of other similar systematic review processes.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>This study demonstrates the potential of using generative AI to automate the time-consuming process of conducting systematic reviews of mHealth apps. This approach could be particularly useful for researchers with limited coding skills. However, the study has limitations related to the design science research methodology, subjectivity bias, and the quality of the search results used to train the language model.</jats:p>
          </jats:sec>"
10.1016/j.fhj.2024.100043,Enhancing communication and clinical reasoning in medical education: Building virtual patients with generative AI,N/A
10.3233/shti240256,The Application of Generative AI for Nursing Education,"<jats:p>Building a nursing education platform through Microsoft Power Apps. The main purpose is to provide convenience, real-time access, and comprehensive knowledge in nursing, as well as to improve work efficiency.</jats:p>"
10.1145/3637864,Generative AI Requires Broad Labor Policy Considerations,<jats:p>Considering how generative artificial intelligence might affect occupations.</jats:p>
10.1515/9783111323749-004,4 Clustering and association algorithm,N/A
10.1080/25741136.2024.2355597,"Generative-AI, the media industries, and the disappearance of human creative labour",N/A
10.1007/979-8-8688-0419-9_1,Introduction,N/A
10.1016/j.apacoust.2023.109425,Generative emotional AI for speech emotion recognition: The case for synthetic emotional speech augmentation,N/A
10.1109/vrw62533.2024.00374,rlty2rlty: Transitioning Between Realities with Generative AI,N/A
10.2139/ssrn.4452670,Capital Market Consequences of Generative AI: Early Evidence from the Ban of ChatGPT in Italy,N/A
10.1145/3653666.3656065,"""I'm Sorry, but I Can't Assist"": Bias in Generative AI",N/A
10.2139/ssrn.4891302,Computer Programming Education in the Age of Generative AI: Insights from Empirical Research,N/A
10.1007/978-1-4842-3679-6_8,Generative Adversarial Networks,N/A
10.1145/3600100.3626262,ArchiGuesser – Teaching Architecture Styles using Generative AI,N/A
10.1126/sciadv.adn5290,Generative AI enhances individual creativity but reduces the collective diversity of novel content,"<jats:p>Creativity is core to being human. Generative artificial intelligence (AI)—including powerful large language models (LLMs)—holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on generative AI ideas. We study the causal impact of generative AI ideas on the production of short stories in an online experiment where some writers obtained story ideas from an LLM. We find that access to generative AI ideas causes stories to be evaluated as more creative, better written, and more enjoyable, especially among less creative writers. However, generative AI–enabled stories are more similar to each other than stories by humans alone. These results point to an increase in individual creativity at the risk of losing collective novelty. This dynamic resembles a social dilemma: With generative AI, writers are individually better off, but collectively a narrower scope of novel content is produced. Our results have implications for researchers, policy-makers, and practitioners interested in bolstering creativity.</jats:p>"
10.1109/icodsa62899.2024.10652178,Estimating Value at Risk for Central Counterparties: A Generative AI Approach,N/A
10.1016/j.drudis.2024.104133,Navigating the frontier of drug-like chemical space with cutting-edge generative AI models,N/A
10.56397/jare.2023.07.02,Integrating Generative AI in Education: How ChatGPT Brings Challenges for Future Learning and Teaching,N/A
10.1093/oxfordhb/9780190067397.013.8,The Role of Professional Norms in the Governance of Artificial Intelligence,"<p>This chapter looks at the role of professional norms in the governance of artificial intelligence (AI). Professional norms of different provenience have the potential to serve as a reservoir for AI governance when contextualized within other governance mechanisms. However, fundamental conceptual issues such as the notion of what constitutes “AI professions,” coupled with a range of empirical questions—including the actual effects of norms on professionals—remain open for further research and discussion. A related challenge relates to the capacity and legitimacy of norms of the profession to deal with fundamental challenges brought forth by AI-based technologies. Nonetheless, the review of the professional norms literature as applied to AI provides elements of an emerging—and certainly evolving—landscape of professional norms in the AI context.</p>"
10.1007/s43681-023-00414-1,"“ChatGPT says no”: agency, trust, and blame in Twitter discourses after the launch of ChatGPT","<jats:title>Abstract</jats:title><jats:p>ChatGPT, a chatbot using the GPT-n series large language model, has surged in popularity by providing conversation, assistance, and entertainment. This has raised questions about its agency and resulting implications on trust and blame, particularly when concerning its portrayal on social media platforms like Twitter. Understanding trust and blame is crucial for gauging public perception, reliance on, and adoption of AI-driven tools like ChatGPT. To explore ChatGPT’s perceived status as an algorithmic social actor and uncover implications for trust and blame through agency and transitivity, we examined 88,058 tweets about ChatGPT, published in a ‘hype period’ between November 2022 and March 2023, using Corpus Linguistics and Critical Discourse Analysis, underpinned by Social Actor Representation. Notably, ChatGPT was presented in tweets as a social actor on 87% of occasions, using personalisation and agency metaphor to emphasise its role in content creation, information dissemination, and influence. However, a dynamic presentation, oscillating between a creative social actor and an information source, reflected users’ uncertainty regarding its capabilities and, thus, blame attribution occurred. On 13% of occasions, ChatGPT was presented passively through backgrounding and exclusion. Here, the emphasis on ChatGPT’s role in informing and influencing underscores interactors’ reliance on it for information, bearing implications for information dissemination and trust in AI-generated content. Therefore, this study contributes to understanding the perceived social agency of decision-making algorithms and their implications on trust and blame, valuable to AI developers and policymakers and relevant in comprehending and dealing with power dynamics in today’s age of AI.</jats:p>"
10.1136/medethics-2022-108243,AI decision-support: a dystopian future of machine paternalism?,N/A
10.1007/s10676-023-09691-0,Responsible reliance concerning development and use of AI in the military domain,N/A
10.1016/j.jemep.2022.100853,"Beyond Covid-19, why AI is revolutionizing the scientific ecosystem",N/A
10.1007/s10551-022-05051-y,Employee Perceptions of the Effective Adoption of AI Principles,N/A
10.1080/09613218.2010.481172,Ethics and the auto-generative design process,N/A
10.1007/978-3-031-48941-9_28,Final Thoughts: Digital Humanities Looking at Generative AI,N/A
10.3389/fnins.2023.1279470,Editorial: Generative AI for brain imaging and brain network construction,N/A
10.21608/jstc.2024.355418,5 Hard Truths About Generative AI for Technology Leaders,N/A
10.1109/ms.2024.3414445,Generative AI Co-Pilot to Support Safety Analyses of Human-Robot Collaborations,N/A
10.1145/3654699,How to Think about Remedies in the Generative AI Copyright Cases,<jats:p>A long-range perspective on generative artificial intelligence litigation.</jats:p>
10.5334/uproc.160,AI-Powered Curricula: Unpacking the Potential and Progress of Generative Technologies in Education,"<jats:p>The aim of this narrative review is to understand how Artificial Intelligence (AI), specifically generative AI (e.g. ChatGPT) is thought to be used to support teachers in content authoring and curriculum production. Our findings indicate that Generative AI is envisioned as a co-design partner in enhancing educational content such as rubrics, lesson plans, interactive exercises, analogies, reflective questions, and case studies tailored to learning goals and instructional needs. The literature suggests that the key issues with the use of ChatGPT in education can be summarised mainly around its accuracy, and reliability, and its potential to lead to plagiarism. The analysis of empirical studies on the use of generative AI in higher education revealed first insights into themes such as ChatGPT's role in administrative tasks, varied perceptions of AI, generational differences in attitudes, multifunctional roles of ChatGPT, concerns about academic integrity and need for AI literacy, and the call for teaching philosophy reform, around assessment.</jats:p>"
10.1016/j.clsr.2023.105898,Generative and AI-powered oracles: “What will they say about you?”,N/A
10.1007/s11528-024-00966-x,Harnessing the Power of Generative AI to Support ALL Learners,"<jats:title>Abstract</jats:title><jats:p>While generative AI such as ChatGPT has important drawbacks, it can potentially unlock new types of learning and support – specially for English learners and students with disabilities – by facilitating teachers’ Universal Design for Learning (UDL) efforts. As generative AI is quickly becoming an integral part of our lives (e.g., being incorporated into existing tools such as Google products), teachers will need to decide how they can use and benefit from this new technology. Our analysis of data collected during a summer teacher institute found that in Summer 2023 the majority of teachers had never used generative AI and were evenly split on whether it was a “friend” or “foe” in their teaching efforts. Overall, 43% of teachers reported, “ChatGPT and other generative AI will help make instruction more accessible for ALL learners,” while 32% of teachers were “undecided” indicating an opportunity to improve teacher perceptions. In this article we discuss strategies for using generative AI for improving UDL to benefit ALL learners brainstormed by the teachers. Practical implications for multilingual learners are discussed.</jats:p>"
10.1002/9781394308286.ch6,"Can
            <scp>ChatGPT</scp>
            Forecast Stock Price Movements?",N/A
10.28925/2663-4023.2022.18.187196,THE INFLUENCE OF NLU AND GENERATIVE AI ON THE DEVELOPMENT OF CYBER DEFENSE SYSTEMS,"<jats:p>The combination of cyber security systems and artificial intelligence is a logical step at this stage of information technology development. Today, many cybersecurity vendors are incorporating machine learning and artificial intelligence into their products or services. However, the effectiveness of investments in advanced machine learning and deep learning technologies in terms of generating meaningful measurable results from these products is a matter of debate. When designing such systems, there are problems with achieving accuracy and scaling. The article considers the classification of artificial intelligence systems, artificial intelligence models used by security products, their capabilities, recommendations that should be taken into account when using generative artificial intelligence technologies for cyber protection systems are given. ChatGPT's NLP capabilities can be used to simplify the configuration of policies in security products. An approach that considers both short-term and long-term metrics to measure progress, differentiation, and customer value through AI is appropriate. The issue of using generative AI based on platform solutions, which allows aggregating various user data, exchanging ideas and experience among a large community, and processing high-quality telemetry data, is also considered. Thanks to the network effect, there is an opportunity to retrain AI models and improve the effectiveness of cyber defense for all users. These benefits lead to a virtual cycle of increased user engagement and improved cyber security outcomes, making platform-based security solutions an attractive choice for businesses and individuals alike. When conducting a cyber security audit of any IT infrastructure using AI, the limits and depth of the audit are established taking into account previous experience.</jats:p>"
10.2139/ssrn.4462976,Examining the Impact of Generative AI on Users’ Voluntary Knowledge Contribution: Evidence from A Natural Experiment on Stack Overflow,N/A
10.2139/ssrn.4693181,Spillover Effects of Generative AI on Human-Generated Content Creation: Evidence from a Crowd-Sourcing Design Platform,N/A
10.2478/amns-2024-0238,Artistic characterization of AI painting based on generative adversarial networks,"<jats:title>Abstract</jats:title>
               <jats:p>Combined with the creation process of AI painting art, it analyzes the artistic design characteristics of AI paintings formed by generative adversarial networks. It utilizes a convolutional neural network to extract the artistic characteristics of AI paintings and combines the error of feature loss to calculate the features, which ensures the stable operation of the generative adversarial network model. To achieve the style migration of AI painting artworks, the Cycle GAN model was designed on this basis. Comparing the features of both AI paintings of generative adversarial networks and paintings of human artists, the perceptual complexity is taken as the dependent variable, and a regression model is established to analyze and calculate the complexity features of AI paintings, as well as to analyze the color matching art of AI paintings by combining the beauty calculation method. According to the comparison results, the AI paintings have a score of 3.71 for inspirational, 3.69 for aesthetic value, 3.52 for compositional rationality, and 3.38 for breakthrough. The AI paintings have a high level of thought and inspirational value.</jats:p>"
10.1145/3631537,NVIDIA at the Center of the Generative AI Ecosystem---For Now,<jats:p>Assessing the ascent of NVIDIA.</jats:p>
10.1111/ele.14397/v2/review2,"Review for ""How widespread use of generative &lt;scp&gt;AI&lt;/scp&gt; for images and video can affect the environment and the science of ecology""",N/A
10.2139/ssrn.4731281,"Gemini or ChatGPT? Capability, Performance, and Selection of Cutting-Edge Generative Artificial Intelligence (AI) in Business Management",N/A
10.22214/ijraset.2024.60599,Handling Unstructured Image using Generative AI and Dev-Ops,"<jats:p>Abstract: This study explores the application of generative artificial intelligence (AI) to convert unstructured medical image data into structured formats. Medical imaging is vital for diagnosis and treatment, yet handling the large volume of unstructured data presents challenges. Utilizing generative AI, particularly convolutional neural networks (CNNs), enables the transformation of raw medical images into structured data representations. By integrating AI techniques, this approach enhances the accessibility and interoperability of medical imaging data, providing valuable insights for healthcare professionals. The proposed methodology aims to streamline the process, optimizing scalability and reproducibility without extensive reliance on DevOps practices. This research signifies a significant step towards leveraging AI for efficient utilization of medical imaging data in healthcare applications</jats:p>"
10.1145/3604237.3626907,A supervised generative optimization approach for tabular data,N/A
10.62051/bejkn640,ChatGPT in Education: Ethical Predicaments of Generative AI,"<jats:p>ChatGPT, which stands for generative artificial intelligence technology, has become the current prominent topic in the realm of education. The innovative practice of artificial intelligence technology in the field of education has laid a technical foundation for ChatGPT to empower college education, and ChatGPT has also been deeply applied in the fields of educational resource recommendation, language learning support, and personalized learning assistance due to its strong text understanding. It brings vast opportunities for the development of high-quality education, whilst also generating ethical risks for its use. Based on the perspective of educational ethics, this study finds that the ethical problems of generative AI education are mainly reflected in the weakening of the stability of the teacher-student relationship, the impact of privacy protection, and academic misconduct caused by improper human-computer interaction. Accordingly, this paper proposes a series of strategies to alleviate educational ethics from multiple perspectives, such as reconstructing the institutional ethics of data governance, improving the policy system, and strengthening the dominant position of students, so as to guide teachers and students in colleges and universities to maintain rational thinking, prudently use new technologies such as ChatGPT, and promote the development of intelligent education in the new era in the direction of standardized and orderly, human-machine coexistence. </jats:p>"
10.24059/olj.v28i3.4632,Using AI-generative tools in tertiary education: Reflections on their effectiveness in improving tertiary students’ English writing abilities,"<jats:p>Artificial intelligence (AI) tools have become a popular topic in the education field. Most of the schools in Hong Kong focus on how to properly use AI software to help students’ learning experience. As this is still a relatively new device, the stance for most of the schools in Hong Kong is skeptical. This study aims to find out whether AL-Generative tools, such as Chatgpt, can help to improve students’ English writing skills in university. Interviews were used to find out students and teachers’ opinions towards using AI in writing. The results indicate that students find AI tools convenient for learning writing skills, but teachers are concerned that the feedback and examples provided by AI are too general and ambiguous. This study provides some interesting opinions from students and teachers about their experience in using AI in learning and writing, and this helps us to understand more about how to use AI effectively in the education sector.</jats:p>"
10.21606/nordes.2023.114,Advent of GAN: How does a generative AI create a moodboard?,N/A
10.1038/s41587-020-0417-3,Reply to ‘Assessing the impact of generative AI on medicinal chemistry’,N/A
10.3390/buildings14010220,Opportunities and Challenges of Generative AI in Construction Industry: Focusing on Adoption of Text-Based Models,"<jats:p>In the last decade, despite rapid advancements in artificial intelligence (AI) transforming many industry practices, construction largely lags in adoption. Recently, the emergence and rapid adoption of advanced large language models (LLMs) like OpenAI’s GPT, Google’s PaLM, and Meta’s Llama have shown great potential and sparked considerable global interest. However, the current surge lacks a study investigating the opportunities and challenges of implementing Generative AI (GenAI) in the construction sector, creating a critical knowledge gap for researchers and practitioners. This underlines the necessity to explore the prospects and complexities of GenAI integration. Bridging this gap is fundamental to optimizing GenAI’s early stage adoption within the construction sector. Given GenAI’s unprecedented capabilities to generate human-like content based on learning from existing content, we reflect on two guiding questions: What will the future bring for GenAI in the construction industry? What are the potential opportunities and challenges in implementing GenAI in the construction industry? This study delves into reflected perception in literature, analyzes the industry perception using programming-based word cloud and frequency analysis, and integrates authors’ opinions to answer these questions. This paper recommends a conceptual GenAI implementation framework, provides practical recommendations, summarizes future research questions, and builds foundational literature to foster subsequent research expansion in GenAI within the construction and its allied architecture and engineering domains.</jats:p>"
10.4018/978-1-7998-8467-5.ch005,Instructing AI Ethics and Human Rights,"<jats:p>This chapter will exploit emerging issues of AI and current literature on AI ethics and human rights teaching. The authors will exploit understanding AI ethics and human rights in daily life and offer teaching methodologies to explain how to teach AI ethics and human rights in K-12 learning environments. Furthermore, the chapter will be devoted to the latest trends and issues on how to teach AI ethics and humans rights teaching in K-12 learning environments. Particular emphasis will be made on a survey of existing ethics teaching methodologies and how to adopt existing teaching strategies into AI ethics teaching in order to improve their understanding on AI ethics and human rights. </jats:p>"
10.15801/je.1.126.201909.91,A Study of the Task of the Ethics Education of Human Nature in the Relationship between Humans and AI Robotics,N/A
10.1007/s11948-024-00500-5,Supporting Trustworthy AI Through Machine Unlearning,"<jats:title>Abstract</jats:title><jats:p>Machine unlearning (MU) is often analyzed in terms of how it can facilitate the “right to be forgotten.” In this commentary, we show that MU can support the OECD’s five principles for trustworthy AI, which are influencing AI development and regulation worldwide. This makes it a promising tool to translate AI principles into practice. We also argue that the implementation of MU is not without ethical risks. To address these concerns and amplify the positive impact of MU, we offer policy recommendations across six categories to encourage the research and uptake of this potentially highly influential new technology.</jats:p>"
10.1109/ntpe.2020.9778175,"The Faustian Bargain — The Promise of AI and the Destruction of Jobs, Part 2",N/A
10.1109/ntpe.2020.9778167,"The Faustian Bargain — The Promise of AI and the Destruction of Jobs, Part 3",N/A
10.1136/jme-2024-110037,Negotiating cultural sensitivity in medical AI,N/A
10.1007/s10676-021-09595-x,Problems with “Friendly AI”,"<jats:title>Abstract</jats:title><jats:p>On virtue ethical grounds, Barbro Fröding and Martin Peterson recently recommended that near-future AIs should be developed as ‘Friendly AI’. AI in social interaction with humans should be programmed such that they mimic aspects of human friendship. While it is a reasonable goal to implement AI systems interacting with humans as Friendly AI, I identify four issues that need to be addressed concerning Friendly AI with Fröding’s and Peterson’s understanding of Friendly AI as a starting point. In a first step, I briefly recapitulate Fröding’s and Peterson’s arguments for Friendly AI. I then highlight some issues with Fröding’s and Peterson’s approach and line of reasoning and identify four problems related to the notion of Friendly AI, which all pertain to the role and need for humans’ moral development. These are that (1) one should consider the moral tendencies and preferences of the humans interacting with a friendly AI, (2) it needs to be considered whether the humans interacting with a Friendly AI are still developing their virtues and character traits, (3) the indirect effects of replacing humans with Friendly AI should be considered with respect to the possibilities for humans to develop their moral virtues and that (4) the question whether the AI is perceived as some form of Artificial General Intelligence cannot be neglected. In conclusion, I argue that all of these four problems are related to humans moral development and that this observation strongly emphasizes the role and need for humans moral development in correlation to the accelerating development of AI-systems.</jats:p>"
10.1017/s0963180123000671,Editorial: The Ethical Implications of Using AI in Medicine,N/A
10.1109/ntpe.2020.9778125,"The Faustian Bargain — The Promise of AI and the Destruction of Jobs, Part 1",N/A
10.2139/ssrn.4385025,Generative Artificial Intelligence (GAI) Ethics Taxonomy- Applying Chat GPT for Robotic Process Automation (GAI-RPA) as Business Case,N/A
10.22492/issn.2187-476x.2022.10,AI Ethics in Next Generation Wireless Networks: A Philosophical Outlook,N/A
10.21428/3d48c34a.6f6c6e57,The Ethics of Reviewing AI Studies by Research Ethics Committees (Recs) in Low and Medium Income Countries (LMICs),N/A
10.4018/978-1-7998-8467-5.ch016,Thinking Machines,"<jats:p>This chapter investigates ethical questions surrounding the possible future emergence of self-aware artificial intelligence (AI). Current research into ethical AI and how this might be applied or extended to future AI is discussed. It is argued that the development of self-aware machines, or their functional equivalents, is possible in principle, and so questions of their ethical status are important. The importance of an objective, reality-based ethics in maintaining human-friendly AI is identified. It is proposed that the conditional nature of life and the value of reason provide the basis of an objective ethics, whose implications include rights to life and liberty, and which apply equally to humans and self-aware machines. Crucial to the development of human-friendly AI will be research on encoding correct rules of reasoning into AI and, using that, validating objective ethics and determining to what extent they will apply to and be followed voluntarily by self-aware machines. </jats:p>"
10.1007/978-3-030-80083-3_7,The Governance of AI and Its Legal Context-Dependency,N/A
10.59728/jaie.2022.1.2.74,A Study on the Use of AI in Interviews,N/A
10.1007/978-3-030-80083-3_6,AI and Its New Winter: From Myths to Realities,N/A
10.25080/majora-212e5952-039,Accelerating Science with the Generative Toolkit for Scientific Discovery (GT4SD),N/A
10.1007/978-3-031-63028-6_31,"AI4LA: An Intelligent Chatbot for Supporting Students with Dyslexia, Based on Generative AI",N/A
10.1145/3375627.3375859,"""The Global South is everywhere, but also always somewhere""",N/A
10.1007/978-3-030-66913-3_9,Ethical Best Practice Applying AI in a Socially Sensible Manner,N/A
10.1007/s00146-022-01430-1,An AI ethics ‘David and Goliath’: value conflicts between large tech companies and their employees,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence ethics requires a united approach from policymakers, AI companies, and individuals, in the development, deployment, and use of these technologies. However, sometimes discussions can become fragmented because of the different levels of governance (Schmitt in AI Ethics 1–12, 2021) or because of different values, stakeholders, and actors involved (Ryan and Stahl in J Inf Commun Ethics Soc 19:61–86, 2021). Recently, these conflicts became very visible, with such examples as the dismissal of AI ethics researcher Dr. Timnit Gebru from Google and the resignation of whistle-blower Frances Haugen from Facebook. Underpinning each debacle was a conflict between the organisation’s economic and business interests and the morals of their employees. This paper will examine tensions between the ethics of AI organisations and the values of their employees, by providing an exploration of the AI ethics literature in this area, and a qualitative analysis of three workshops with AI developers and practitioners. Common ethical and social tensions (such as power asymmetries, mistrust, societal risks, harms, and lack of transparency) will be discussed, along with proposals on how to avoid or reduce these conflicts in practice (e.g., building trust, fair allocation of responsibility, protecting employees’ autonomy, and encouraging ethical training and practice). Altogether, we suggest the following steps to help reduce ethical issues within AI organisations: improved and diverse ethics education and training within businesses; internal and external ethics auditing; the establishment of AI ethics ombudsmen, AI ethics review committees and an AI ethics watchdog; as well as access to trustworthy AI ethics whistle-blower organisations.</jats:p>"
10.1007/s00146-023-01834-7,Evaluating the acceptability of ethical recommendations in industry 4.0: an ethics by design approach,N/A
10.1145/3514094.3539516,Developing Artificial Intelligence for Good: Interdisciplinary Research Collaborations and the Making of Ethical AI,N/A
10.1355/9789815203684-009,THE ASEAN AI GUIDE IS JUST THE START OF AN EVOLVING DEVELOPMENT,N/A
10.4018/979-8-3693-6517-5.ch011,Illuminating Evidence,"<jats:p>The cyber space usage has seen multiple levels and types of cyber-crime. The role of digital forensics to collect criminal evidence and analyze it so as to find a path to the cyber-criminal is equally challenging. In this regard, AI tools and techniques have provided a robust weapon. But the main issue with using AI mechanisms is the understandability; interpretability, of a result, is quite difficult in a court of law. In legal aspects, the AI lacks the features of explainability. In this chapter, the authors shed light on the techniques of explainable artificial intelligence techniques that provide methods to enhance the degree of explainability in tracing a cyber-crime so that the court of law delivers the verdict with complete justice. In the future, XAI-driven digital forensics will go a long way in supporting cyber forensics, and this chapter is a sincere attempt to propel the stakeholders in this domain to give a thought in the same direction.</jats:p>"
10.1007/s00146-021-01286-x,Ethics-based auditing of automated decision-making systems: intervention points and policy implications,"<jats:title>Abstract</jats:title><jats:p>Organisations increasingly use automated decision-making systems (ADMS) to inform decisions that affect humans and their environment. While the use of ADMS can improve the accuracy and efficiency of decision-making processes, it is also coupled with ethical challenges. Unfortunately, the governance mechanisms currently used to oversee human decision-making often fail when applied to ADMS. In previous work, we proposed that ethics-based auditing (EBA)—that is, a structured process by which ADMS are assessed for consistency with relevant principles or norms—can (a) help organisations verify claims about their ADMS and (b) provide decision-subjects with justifications for the outputs produced by ADMS. In this article, we outline the conditions under which EBA procedures can be feasible and effective in practice. First, we argue that EBA is best understood as a ‘soft’ yet ‘formal’ governance mechanism. This implies that the main responsibility of auditors should be to spark ethical deliberation at key intervention points throughout the software development process and ensure that there is sufficient documentation to respond to potential inquiries. Second, we frame AMDS as parts of larger sociotechnical systems to demonstrate that to be feasible and effective, EBA procedures must link to intervention points that span all levels of organisational governance and all phases of the software lifecycle. The main function of EBA should, therefore, be to inform, formalise, assess, and interlink existing governance structures. Finally, we discuss the policy implications of our findings. To support the emergence of feasible and effective EBA procedures, policymakers and regulators could provide standardised reporting formats, facilitate knowledge exchange, provide guidance on how to resolve normative tensions, and create an independent body to oversee EBA of ADMS.</jats:p>"
10.15200/winn.148431.11858,"Science AMA Series: I'm Joanna Bryson, a Professor in Artificial (and Natural) Intelligence. I am being consulted by several governments on AI ethics, particularly on the obligations of AI developers towards AI and society. I'd love to talk – AMA!",N/A
10.1145/3600211.3604745,Multi Value Alignment: four steps for aligning ML/AI development choices with multiple values,N/A
10.1145/3375627.3375862,Joint Optimization of AI Fairness and Utility,N/A
10.1007/s00146-022-01565-1,Bowling alone in the autonomous vehicle: the ethics of well-being in the driverless car,N/A
10.1007/s00146-024-01938-8,"Adaptable robots, ethics, and trust: a qualitative and philosophical exploration of the individual experience of trustworthy AI","<jats:title>Abstract</jats:title><jats:p>Much has been written about the need for trustworthy artificial intelligence (AI), but the underlying meaning of trust and trustworthiness can vary or be used in confusing ways. It is not always clear whether individuals are speaking of a technology’s trustworthiness, a developer’s trustworthiness, or simply of gaining the trust of users by any means. In sociotechnical circles, trustworthiness is often used as a proxy for ‘the good’, illustrating the moral heights to which technologies and developers ought to aspire, at times with a multitude of diverse requirements; or at other times, no specification at all. In philosophical circles, there is doubt that the concept of trust should be applied at all to technologies rather than their human creators. Nevertheless, people continue to intuitively reason about trust in technologies in their everyday language. This qualitative study employed an empirical ethics methodology to address how developers and users define and construct requirements for trust throughout development and use, through a series of interviews. We found that different accounts of trust (rational, affective, credentialist, norms based, relational) served as the basis for individual granting of trust in technologies and operators. Ultimately, the most significant requirement for user trust and assessment of trustworthiness was the accountability of AI developers for the outputs of AI systems, hinging on the identification of accountable moral agents and perceived value alignment between the user and developer’s interests.</jats:p>"
10.1007/s00146-022-01578-w,AI ethics with Chinese characteristics? Concerns and preferred solutions in Chinese academia,"<jats:title>Abstract</jats:title><jats:p>Since Chinese scholars are playing an increasingly important role in shaping the national landscape of discussion on AI ethics, understanding their ethical concerns and preferred solutions is essential for global cooperation on governance of AI. This article, therefore, provides the first elaborated analysis on the discourse on AI ethics in Chinese academia, via a systematic literature review. This article has three main objectives. (1) to identify the most discussed ethical issues of AI in Chinese academia and those being left out (the question of “what”); (2) to analyze the solutions proposed and preferred by Chinese scholars (the question of “how”); and (3) to map out whose voices are dominating and whose are in the marginal (the question of “who”). Findings suggest that in terms of short-term implications, Chinese scholars’ concerns over AI resemble predominantly the content of international ethical guidelines. Yet in terms of long-term implications, there are some significant differences needed to be further addressed in a cultural context. Further, among a wide range of solution proposals, Chinese scholars seem to prefer strong-binding regulations to those weak ethical guidelines. In addition, this article also found that the Chinese academic discourse was dominated by male scholars and those who are from elite universities, which arguably is not a unique phenomenon in China.</jats:p>"
10.1109/snams60348.2023.10375472,Cyber Security Issues and Challenges Related to Generative AI and ChatGPT,N/A
10.2139/ssrn.4888506,Challenging the Machinery of Generative AI with Fact-Checking: Ontology-Driven Biological Graphs for Verifying Human Disease-Gene Links,N/A
10.38124/ijisrt/ijisrt24mar577,Empowering Manufacturing: Generative AI Revolutionizes ERP Application,"<jats:p>This article delves into the transformative implications of integrating state-of-the-art Generative AI technologies into Enterprise Resource Planning (ERP) applications within the manufacturing industry. With the manufacturing landscape experiencing rapid evolution, there is a growing imperative for adaptive and intelligent systems to optimize efficiency, productivity, and decision- making processes. Through the exploration of Generative AI's natural language processing capabilities, this article unveils a new frontier in smart manufacturing, where ERP systems are empowered to redefine conventional paradigms and catalyze innovation. Furthermore, this article examines Generative AI's impact on supply chain management, leveraging its capacity to process extensive textual data for enhanced demand forecasting, inventory optimization, and risk management. This enhances the adaptability and resilience of manufacturing ecosystems, enabling them to navigate dynamic market conditions with agility.</jats:p>"
10.1051/shsconf/202419403003,"Intuitive space texture generation using hand tracking, speech recognition, and generative AI","<jats:p>This research aims to explore new methods of intuitively redesigning room interiors using gesture, speech, and generative AI. This approach represents a new approach to interior design, allowing users to easily customize appearance of a room through voice and hand gestures. This project investigates how hand tracking, speech recognition, and generative AI can be integrated to enable intuitive and user-friendly interior texture customization in virtual spaces. Previous studies on interior design using XR have mainly used augmented reality (AR) to relocate furniture. However, in these methods, the only way to select furniture textures is to search for them in prepared furniture. Our method uses hand-tracking and speech recognition to capture a user’s desired image and employs generative AI to realize these preferences in a VR environment. The process involves scanning real-world furniture and rooms and applying AI-generated textures based on what the user communicates. The system allows users to easily visualize room interiors and modify them according to their preferences. This can enhance the traditional room design process. This method is currently restricted to texture only, but 3D model generation AI could provide additional flexibility. This method also has the potential for collaborative design work by sharing an environment.</jats:p>"
10.1257/jel.20231736,Generative AI for Economic Research: Use Cases and Implications for Economists,"<jats:p> Generative artificial intelligence (AI) has the potential to revolutionize research. I analyze how large language models (LLMs) such as ChatGPT can assist economists by describing dozens of use cases in six areas: ideation and feedback, writing, background research, data analysis, coding, and mathematical derivations. I provide general instructions and demonstrate specific examples of how to take advantage of each of these, classifying the LLM capabilities from experimental to highly useful. I argue that economists can reap significant productivity gains by taking advantage of generative AI to automate micro-tasks. Moreover, these gains will grow as the performance of AI systems continues to improve. I also speculate on the longer-term implications of AI-powered cognitive automation for economic research. The online resources associated with this paper explain how to get started and will provide regular updates on the latest capabilities of generative AI in economics. (JEL A11, C45, D83, I23, O33) </jats:p>"
10.18357/kula.287,Generative AI for Academic Publishing? Some Thoughts About Epistemic Diversity and the Pursuit of Truth,"<jats:p>The uses of generative AI have prompted both positive and negative responses. This short commentary contemplates potential issues concerning bibliodiversity, epistemic diversity, and data surveillance. It also cautions the potential erosion of public trust in academic publishing in the age of generative AI. Invoking the Sokal hoax, the commentary sheds light on what it means when knowledge, experience, expertise, and the pursuit of truth are on the line. 
This commentary is part of the section ""AI and Academic Publishing.""
 </jats:p>"
10.1007/s40319-023-01321-y,ChatGPT and Generative AI Tools: Theft of Intellectual Labor?,N/A
10.1109/ickecs61492.2024.10617388,Transforming Human-Machine Interaction: Generative AI Virtual Asst,N/A
10.34190/eckm.25.1.2770,The Generative AI Solutions for enhancing Knowledge Management:  Literature Review and Roadmap,"<jats:p>This article explores the potential of Generative Artificial Intelligence (GenAI) to improve knowledge management in organizations. Despite growing interest in GenAI, a thorough review of its applications and effectiveness in knowledge management is still needed. GenAI has shown promise in automating content creation, synthesizing information, and improving decision-making processes. However, its integration into knowledge management systems remains fragmented. This study seeks to consolidate existing knowledge and chart a path for future research in this area. The research uses a systematic literature review (SLR) following PRISMA guidelines, complemented by a thematic analysis to ensure a comprehensive assessment of the existing literature. We reviewed articles published between 2020 and 2024, focusing on GenAI applications in knowledge management. A thematic analysis was conducted to identify key themes and trends. This paper contributes to the academic and professional environment by providing a detailed review of GenAI applications in knowledge management, identifying best practices, and offering strategic recommendations for leveraging GenAI to improve organizational processes. The review highlights key areas where GenAI can improve knowledge management: automating the generation of knowledge assets, enhancing knowledge discovery and retrieval, supporting collaboration, and improving decision making through advanced analytics. In addition, it addresses ethical considerations and potential challenges, such as data privacy and the need for human oversight.</jats:p>"
10.1007/979-8-8688-0282-9_9,In-Depth Look at Supportive Visual Algorithms and Computer Vision,N/A
10.54254/2753-7048/53/20240010,Assessing the Copyright Infringement Risk of Generative AI Created Works,"<jats:p>The rapid evolution of the Internet and big data technology has ushered in significant advancements in data production and utilization. However, this progress has brought to the forefront the intricate issue of copyright protection. This is particularly evident with the widespread adoption of generative artificial intelligence (AI), where concerns regarding data source protection, data processing standards, and the boundaries of data application have garnered considerable attention from both academia and industry. Big data mining techniques coupled with generative AI algorithms offer robust support for data collection and processing, but they also introduce risks of data infringement and pose challenges regarding algorithmic compliance and ownership rights over generated works. Addressing these issues is imperative. This paper recommends enhancing legal compliance standards throughout each stage of the generative AI process, recalibrating the scope of acceptable copyright use, and establishing a regulatory framework for generative AI works. These measures are crucial for fostering the sustainable and orderly advancement of the generative AI industry.</jats:p>"
10.1037/neu0000948,Potential cognitive risks of generative transformer-based AI chatbots on higher order executive functions.,N/A
10.30534/ijatcse/2020/58932020,"Deepfake Forensics, an AI-synthesized Detection with Deep Convolutional Generative Adversarial Networks",N/A
10.1145/3635636.3672190,TIME ENOUGH: Generative AI Visions of Climate Change as Cave Paintings of the Future,N/A
10.1109/icivc50857.2020.9177494,AI Illustrator: Art Illustration Generation Based on Generative Adversarial Network,N/A
10.46328/ijte.845,"Generative AI in Education: Pedagogical, Theoretical, and Methodological Perspectives","<jats:p>Recently, ChatGPT, a cutting-edge large language model, has emerged as a powerful Generative Artificial Intelligence (GenAI) tool with the capacity to influence education. ChatGPT provides ample opportunities for learners, researchers, educators, and practitioners to achieve the intended learning outcomes in various disciplines. This special issue examines the diverse applications and implications of GenAI tools including ChatGPT in education, highlighting their potential to enhance teaching and learning across various contexts. Key findings from seventeen studies collected in this special issue demonstrate that GenAI tools can significantly improve educational outcomes by providing personalized feedback, facilitating language learning, and supporting both qualitative and quantitative research methodologies. The findings emphasize GenAI’s capacity to increase learner engagement and motivation, yet also underscore the need for robust ethical guidelines and human oversight due to potential issues with privacy, bias, and accuracy. This special issue also highlights the challenges GenAI faces, such as limitations in contextual understanding and its impact on critical thinking skills. In addition, it provides a foundational framework for exploring effective and responsible GenAI integration, aiming to enrich educational experiences. We conclude that future research should focus on the longitudinal effects of GenAI tools on learning outcomes, developing ethical frameworks for their use, and ensuring their adaptability to diverse learner populations to promote inclusive educational practices.</jats:p>"
10.1080/00131857.2024.2333854,Neuropower and plastic writing: Stiegler and Malabou on generative AI,N/A
10.1057/s41599-024-03540-1,Examining user migration intention from social Q&amp;A communities to generative AI,N/A
10.1080/22041451.2024.2346421,Riding the waves over generative AI in Malaysia: policies and responses,N/A
10.21203/rs.3.rs-3640721/v1,"Pre-service teachers’ Knowledge, Gender Use, and Views about Generative AI in Academic Research","<title>Abstract</title>
        <p>This study explored the knowledge, use, and views of generative artificial intelligence (GAI) tools among pre-service teachers (PSTs) in the context of academic research in Ghana. Adopting a descriptive survey method, data were gathered from 104 PSTs across five institutions, selected for convenience as they were under the supervision of the study's authors. The primary data collection instrument was a five-point Likert-type questionnaire complemented by an open-ended question, analyzed both quantitatively and thematically. The quantitative analysis, including means, frequencies, standard deviations, and percentages, highlighted a strong familiarity with GAI tools such as OpenAI's ChatGPT, DALL·E, and Bard among PSTs. These tools were primarily used during the introduction, literature review and data analysis stages of their research. Sources of GAI tool discovery varied, with personal research, friends, institutions, and social media being the main avenues. The independent samples t-test revealed a gender disparity in the frequency of GAI tool usage, with male PSTs using them more than their female peers. However, both groups acknowledged the benefits of GAI tools, particularly in enhancing confidence and independence in their research activities. Despite the advantages, PSTs indicated potential inaccuracies that arise from GAI tool usage and expressed concerns about overreliance potentially hindering their critical and creative thinking skills. The study suggests that without careful regulation, the originality and thoroughness of PST research could be at risk. Therefore, it is recommended that teacher education programs incorporate GAI tools into their research methodology courses, ensuring that PSTs can effectively leverage these technologies while maintaining academic integrity and innovation in their research.</p>"
10.33422/wcme.2022.12.104,Hybrid Robot 3d-Printed 50 M Long Steel Bridge Realisation - Ai-Ml Assisted Generative Ga/Ea Design Workflows and Optimizations,N/A
10.21608/jstc.2024.361554,The Rise of Generative AI-Driven Design Patterns,N/A
10.1038/s44184-024-00067-w,Behavioral health and generative AI: a perspective on future of therapies and patient care,N/A
10.1515/9783111323749-001,1 Introduction to artificial intelligence,N/A
10.1038/s41433-024-03098-x,OpenAI’s Sora in ophthalmology: revolutionary generative AI in eye health,N/A
10.1038/s44287-024-00053-6,Empowering generative AI through mobile edge computing,N/A
10.36948/ijfmr.2024.v06i03.19697,Case Study of Skilful Use of LLM-Based Generative AI in Management of a Hypothetical Business,"<jats:p>Now that generative AI technology products are already spreading worldwide and are improving their capabilities very fast, it is high time we examine what it can do to decision making, which is the essence of managing businesses.  The present paper has used an innovative methodology where one of the currently available Large Language Model based product, namely ChatGPT, is given a series of questions/prompts to use its own capabilities to give its self-assessment of supporting the different dimensions of managing a business. Multiple conversations illustrated the need for well-structured prompts/questions to get the most out of ChatGPT, otherwise it can get confused and provide illogical response.  The quality of mutual learning improves by constructive feedback between the human interlocutor and generative AI.  Probing it further with multiple scenarios for a hypothetical mid-sized business has verified and confirmed those self-assessments.  The insights and conclusions drawn from those conversations are that skillful use of the LLM-based generative AI products can assist human managers in making better quality decisions at all levels of management in a timely manner.  Senior managers of businesses need to begin an effort to test-drive an available LLM-based generative AI product and plan to leverage it in the ongoing management to become a learning organization in the interest of the business.</jats:p>"
10.1007/978-1-4842-9994-4_5,Google Bard and Beyond,N/A
10.1007/978-3-031-54252-7_6,GenAI Model Security,N/A
10.1007/s11528-024-00990-x,Introduction to the Special Section on Integrating Generative AI in Education,N/A
10.4324/9781032648033-5,What activities can help students use technologies better?,N/A
10.1016/j.clsr.2024.105985,"China's Interim Measures on generative AI: Origin, content and significance",N/A
10.22214/ijraset.2023.56140,The Evolution of Generative AI: Implications for the Media and Film Industry,"<jats:p>Abstract: This research paper scrutinizes and explores the substantial impact of Artificial Intelligence (AI) and Generative AI on the media and film industry. It delves into the continuously evolving applications of AI algorithms and advanced models, emphasizing their profound implications for content creation, production workflows, and distribution strategies. The paper offers comprehensive insights into the operational mechanics of key AI models, underscoring their direct relevance within the domain of media and film. This inquiry provides a timeless and academic perspective on the transformative influence of AI and Generative AI in these industries, facilitating a deeper understanding of their applications and implications</jats:p>"
10.1108/tg-01-2024-0006,Unlocking the power and future potential of generative AI in government transformation,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>This paper aims to investigate whether the implementation of generative artificial intelligence (GAI) impacts government functionality. The study will analyse GAI’s positive attributes across different dimensions to comprehensively understand its value proposition for public organisations. Furthermore, the paper will outline the strategic interventions required to integrate GAI effectively within the organisational context of government transformation.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>This study measures “government functionality” and “GAI implementation” using abstract macro variables as a second-order formative model. It also includes first-order measurable micro-variables to better understand the concept. In addition, the study introduces “organisational context” as a moderating factor to explain the complex dynamics of integrating GAI to improve government functionality. The study proposes a conceptual framework, which was analysed using exploratory data analysis, with primary data collected through questionnaires.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>The study finds a positive correlation between the implementation of GAI and improved government functionality. Furthermore, it found that organisational contextualisation significantly moderates this relationship. All the empirical outcomes align with the prescribed statistical thresholds, concluding that the articulated conceptual framework holds significance.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title>
<jats:p>The study has significant implications for managers, researchers and anyone involved in making, implementing or evaluating decisions related to digital government through GAI. However, the study has limitations, including a limited sample size and contextualisation of the Indian public sector.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>The study contributes to existing knowledge by showing that implementing GAI positively correlates with improving government functionality. It further highlights the significance of GAI implementation according to the specific organisational context.</jats:p>
</jats:sec>"
10.1007/s11528-024-00938-1,"Generative AI, Teacher Knowledge and Educational Research: Bridging Short- and Long-Term Perspectives",N/A
10.1109/hnicem60674.2023.10589199,Generative AI in Education,N/A
10.1921/jpts.v20i3.2192,Artificial intelligence in social work practice education. The potential use of Generative AI for learning,"<jats:p>This article is an account of a social work academic’s first engagement with generative Artificial Intelligence (AI). The aim is to introduce this emerging and fast-growing technology to the social work community in order to promote a dialogue about its potential use for learning in social work practice learning placements. Examples are included to stimulate ideas and motivate educators to use generative AI for teaching and learning. The case is made that students and educators need to understand how to use AI responsibly and skilfully to be agile and equipped for the workplace of the future. Although AI can be useful, it has limitations and cannot replace human interaction which is a fundamental aspect of social work education in the workplace.</jats:p>"
10.1515/9783839469057-006,Addressing the needs and demands of child welfare: A connection between AI Ethics and Ethics of Vulnerability,N/A
10.1186/s12910-022-00871-z,Is there a civic duty to support medical AI development by sharing electronic health records?,"<jats:title>Abstract</jats:title><jats:p>Medical artificial intelligence (AI) is considered to be one of the most important assets for the future of innovative individual and public health care. To develop innovative medical AI, it is necessary to repurpose data that are primarily generated in and for the health care context. Usually, health data can only be put to a secondary use if data subjects provide their informed consent (IC). This regulation, however, is believed to slow down or even prevent vital medical research, including AI development. For this reason, a number of scholars advocate a moral civic duty to share electronic health records (EHRs) that overrides IC requirements in certain contexts. In the medical AI context, the common arguments for such a duty have not been subjected to a comprehensive challenge. This article sheds light on the correlation between two normative discourses concerning informed consent for secondary health record use and the development and use of medical AI. There are three main arguments in favour of a civic duty to support certain developments in medical AI by sharing EHRs: the ‘rule to rescue argument’, the ‘low risks, high benefits argument’, and the ‘property rights argument’. This article critiques all three arguments because they either derive a civic duty from premises that do not apply to the medical AI context, or they rely on inappropriate analogies, or they ignore significant risks entailed by the EHR sharing process and the use of medical AI. Given this result, the article proposes an alternative civic responsibility approach that can attribute different responsibilities to different social groups and individuals and that can contextualise those responsibilities for the purpose of medical AI development.</jats:p>"
10.12781/978-1-907549-14-4-1,India and Appreciative Inquiry: Generative Connection between Ancient Wisdom and Today’s Endeavours in the Field: Introduction,N/A
10.47408/jldhe.vi30.1031,Using social learning theories to explore the role of generative Artificial Intelligence (AI) in collaborative learning,"<jats:p>This opinion piece highlights the integral role of generative Artificial Intelligence (AI) in learning within Higher Education Institutions (HEIs). Employing social learning theories, this opinion piece aims to explore generative AI as a stakeholder in learning. By weaving in social constructivist and learning theories, this opinion paper aims to uncover the capacity of generative AI to facilitate and enhance the learning process. Central to this opinion piece proposition is cultivating a learning community that leverages AI's potential as a new learning stakeholder. This opinion piece aims to contribute to ongoing discussions in the field of learning development by offering a fresh outlook on how AI can be an asset in knowledge co-creation and collaborative learning. The paper does this in the following ways: (1) highlights how generative AI can effectively contribute to learning and knowledge co-creation, and (2) provides some guidance for integrating generative AI in collaborative learning.</jats:p>"
10.1111/jlse.12141,Rising to Meet the Challenge of Generative AI,N/A
10.21608/jstc.2023.297949,Generative AI: What Will Change in 2023,N/A
10.1111/ele.14397/v2/decision1,"Decision letter for ""How widespread use of generative &lt;scp&gt;AI&lt;/scp&gt; for images and video can affect the environment and the science of ecology""",N/A
10.1007/979-8-8688-0473-1_9,"Implementation, Operations, and Maintenance",N/A
10.21203/rs.3.rs-3766549/v1,Expertise-informed Generative AI Enables Ultra-High Data Efficiency for Building Generalist Medical Foundation Model,"<jats:title>Abstract</jats:title>
        <jats:p>Generalist medical foundation models, pre-trained on massive medical datasets, have shown great potential as the next generation of medical artificial intelligence (AI). However, collecting millions of medical data is extremely expensive, time-consuming, and raises concerns over the high-risk leakage of sensitive private patient information. Here, we present a general framework that enables ultra-high data efficiency in building medical foundation models by leveraging expertise-informed generative AI to scale the limited pre-training dataset. Specifically, we follow this framework and propose a new foundation model DERETFound in ophthalmology, using only 16.7% (150,786 images) of the real-world colour fundus photography images required in the latest retinal foundation model RETFound (904,170 images, Y. Zhou et al, Nature 2023). By integrating expert insights into generative AI, we generate approximately one million synthetic data that are consistent with real retinal images in terms of physiological structures and feature distribution. DERETFound achieves comparable or even superior performance to RETFound on nine public datasets across four downstream tasks, including diabetic retinopathy grading, glaucoma diagnosis, age-related macular degeneration grading, multi-disease classification, and challenging external evaluation. In addition, DERETFound demonstrates competitively high label efficiency, saving over 50% of expert-annotated training data compared to RETFound on datasets for diabetic retinopathy grading. Our data-efficient framework challenges the classic view that building medical foundation models requires the collection of large amounts of real-world medical data as a prerequisite. The framework also provides an effective solution for any other diseases that were once discouraged from building foundation models due to limited data, which has profound significance for medical AI.</jats:p>"
10.46827/ejae.v8i4.5154,SCHOOL LEADERS’ PERCEPTIONS ON BARRIERS FOR CREATING INCLUSIVE INTERCULTURAL ENVIRONMENTS AND POTENTIAL GENERATIVE AI BENEFITS,"<jats:p>Greek schools have been progressively challenged by the diversification of student demographics during the past decade. The European refugee crisis after 2015 has triggered immense needs to create inclusive intercultural school environments for disabled students whose first language and cultural backgrounds differ from the dominant Greek middle-class students in mainstream schools. School leaders play a vital role in creating inclusive schοols that respect race, culture, ability, class, family background, and linguistic diversity. Social justice leadership constitutes an integral part of inclusive intercultural education, yet works exploring school leaders’ struggles in engaging in this type of leadership in Greece are scarce. This work gives voice to women educational leaders in Greece and investigates the barriers they encounter in providing appropriate education to migrant students with disabilities. Moreover, it explores school leaders’ perceptions of how generative AI applications can potentially be used to minimize these barriers.&lt;p&gt; &lt;/p&gt;&lt;p&gt;&lt;strong&gt; Article visualizations:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=""/-counters-/soc/0078/a.php"" alt=""Hit counter"" /&gt;&lt;/p&gt;</jats:p>"
10.1007/s10699-024-09957-w,Optimized Skin Lesion Segmentation: Analysing DeepLabV3+ and ASSP Against Generative AI-Based Deep Learning Approach,N/A
10.37022/jiaps.v8i2.464,Empowering Pharmacovigilance: Unleashing the Potential of Generative AI in Drug Safety Monitoring,"<jats:p>Pharmacovigilance plays a crucial role in ensuring drug safety and promoting patient well-being throughout the life cycle of medicinal products. However, this field faces several challenges, including underreporting of adverse events, data quality issues, and the complexity of signal detection in large datasets. To address these challenges and enhance drug safety monitoring, there is a growing interest in harnessing the potential of generative artificial intelligence (AI) techniques. This article explores the applications and implications of generative AI in pharmacovigilance. It provides an overview of popular generative models and their working principles, highlighting their ability to analyse drug databases, medical literature, and real-world data sources to identify drug-drug interactions, adverse events, and potential safety signals. Moreover, it emphasizes the importance of human validation and expert oversight in interpreting and acting on the insights generated by generative AI algorithms. The integration of generative AI with traditional pharmacovigilance methods creates a synergistic approach, combining the computational power of AI with human expertise. This integration can lead to improved signal detection, efficient case report generation, proactive risk assessment, and optimized resource allocation. Additionally, the article addresses challenges related to data quality, interpretability, and model validation in generative AI applications, emphasizing the need for standardized protocols and collaborative efforts among stakeholders. Overall, the potential of generative AI in pharmacovigilance is vast. By leveraging its capabilities, we can enhance drug safety monitoring, facilitate early detection of adverse events, and improve patient outcomes. However, it is crucial to address ethical considerations, ensure data privacy, and maintain human oversight to foster responsible and effective implementation of generative AI in pharmacovigilance practices.</jats:p>"
10.1109/icalt61570.2024.00051,TutorChat: a Chatbot for the Support to Dyslexic Learner’s activity through Generative AI,N/A
10.1108/dlo-05-2024-0141,Harnessing generative AI for self-directed learning: Perspectives from top management,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>The purpose of this paper is to explore the potential of generative AI-driven self-directed learning from the perspective of top management in the Sri Lankan software industry. By applying open innovation theory, the study aims to understand how business leaders perceive the integration of generative AI tools in organizational learning processes. The insights gained are intended to inform and encourage top management to promote generative AI-driven self-directed learning within their organizations.</jats:p>
</jats:sec>
<jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>The research utilized a qualitative approach, conducting semi-structured interviews with eight senior managers from IT companies in Colombo, Sri Lanka. Data was synthesized and analyzed thematically to identify patterns and insights regarding generative AI-driven self-directed learning and its organizational impact.</jats:p>
</jats:sec>
<jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>The study reveals that top management in Sri Lanka's software industry perceives generative AI-driven self-directed learning positively. This perception is rooted in the alignment of such learning with open innovation principles, emphasizing knowledge sharing, collaboration and the integration of external expertise to drive innovation. Generative AI tools empower employees to access diverse knowledge sources, fostering continuous learning and adaptability. Leaders recognize these tools' potential to enhance organizational innovation ecosystems and competitive advantage. The findings suggest that active support from top management, customized training programs and a culture that embraces continuous learning and innovation are crucial for successful implementation.</jats:p>
</jats:sec>
<jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>This paper uniquely explores generative AI-driven self-directed learning through the lens of top management in Sri Lanka's software industry, integrating open innovation theory to highlight its potential in enhancing organizational knowledge, collaboration and competitive advantage.</jats:p>
</jats:sec>"
10.21203/rs.3.rs-3823738/v1,Generative AI-enabled Knowledge Base Fine-tuning: Enhancing Feature Engineering for Customer Churn,"<jats:title>Abstract</jats:title>
        <jats:p>Customers are the most critical component in a business’s success regardless of the industry or product. Companies make significant efforts to acquire and, more importantly, retain their existing customers. Customer churn is a significant challenge for businesses, leading to financial losses. To address this challenge, understanding customer’s cognitive status, behaviors, and early signs of churn is crucial. However, predictive and ML-based analysis, being fed with proper features that are indicative of a customer’s cognitive status or behavior, is extremely helpful in addressing this challenge. Having practical ML-based analysis relies on a well-developed feature engineering process. Previous churn analytical studies mainly applied feature engineering approaches that leveraged demographic, product usage, and revenue features alone, and there is a lack of research on leveraging the information-rich content from interactions between customers and companies. Considering the effectiveness of applying domain knowledge and human expertise in feature engineering, and motivated by our previous work, we propose a Customer Churn-related Knowledge Base (ChurnKB) to enhance the feature engineering process. In the ChurnKB, we leverage textual data mining techniques for extracting churn-related features from texts created by customers, e.g., emails or chat logs with company agents, reviews on the company’s website, and feedback on social media. We use Generative AI (GAI) to enhance and enrich the structure of the ChurnKB regarding features related to customer churn-related cognitive status, feelings, and behaviors. We also leveraged feedback loops and crowdsourcing to enhance and approve the validity of the proposed ChurnKB and apply it to develop a classifier for customer churn problems.</jats:p>"
10.5772/intechopen.112582,Anomaly Detection in Medical Time Series with Generative Adversarial Networks: A Selective Review,"<jats:p>Anomaly detection in medical data is often of critical importance, from diagnosing and potentially localizing disease processes such as epilepsy to detecting and preventing fatal events such as cardiac arrhythmias. Generative adversarial networks (GANs) have since their inception shown promise in various applications and have been shown to be effective in cybersecurity, data denoising, and data augmentation, and have more recently found a potentially important place in the detection of anomalies in medical time series. This chapter provides a selective review of this novel use of GANs, in the process highlighting the nature of anomalies in time series, special challenges related to medical time series, and some general issues in approaching time series anomaly detection with deep learning. We cover the most frequently applied GAN models and briefly detail the current landscape of applying GANs to anomaly detection in two commonly used medical time series, electrocardiography (ECG) and electroencephalography (EEG).</jats:p>"
10.1515/9783111323749-007,7 Methods of cross-validation and bootstrapping,N/A
10.1108/jbs-09-2023-0196,At the crossroads: generative AI and corporate risk management,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>This study aims to identify the risks to corporate reputation presented by deepfakes and how to manage them.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>The approach involves a review of the current literature on deepfakes across different sectors to create a clear picture of the risks that deepfakes entail and how best to deal with them.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>While deepfakes are still mostly easily detectable, their sophistication increases daily, and corporations need both technology and culture shifts to deal with this evolving threat.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>While deepfakes have been the subject of intense interest, to the best of the author’s knowledge, this is the first attempt to look at the problem from the perspective of corporate enterprise risk.</jats:p>
</jats:sec>"
10.5539/hes.v14n1p46,The Game-based Learning (GbL) Platform with Generative AI to Enhance Digital and Technology Literacy Skills,"<jats:p>The GbL platform, or game-based learning platform, with generative AI is a research tool initiated from the combination of game-based learning concepts and generative artificial intelligence technology; thereby, this platform is intended to be used as a guideline for the instruction management, in which learners can respond and interact with the real-time activities by means of gamification. The objectives of this research are (1) to study and synthesize the conceptual framework of the GbL platform with generative AI to enhance digital and technology literacy skills, (2) to develop the architecture of the GbL platform with generative AI to enhance digital and technology literacy skills, and (3) to study the results of the development of the GbL platform with generative AI to enhance digital and technology literacy skills. The results of this research show that (1) the overall elements suitability of the architecture of the GbL platform with generative AI is at the highest level (Mean = 4.51, SD = 0.48), and (2) the overall suitability of the architecture of the GbL platform with generative AI is at the highest level (Mean = 4.59, SD = 0.41). Nevertheless, there are still some research gaps in this study; that is, this study was conducted with quite a small sample group and it focuses mainly on the results of evaluation on the architecture of the GbL platform. Therefore, this research is regarded merely as a pilot study designated for feasibility study to further develop the GbL platform that can be put in practical use in the future.</jats:p>"
10.1109/iccds60734.2024.10560378,Learn Buddy – Transforming Education with Generative AI,N/A
10.1109/sceecs61402.2024.10482119,Automated API Docs Generator using Generative AI,N/A
10.1145/3626253.3633418,AI Enhanced Learning: Powering Curated Videos with Generative Intelligence,N/A
10.18260/1-2--47250,Empowering Students in Emerging Technology: A Framework for Developing Hands-on Competency in Generative AI with Ethical Considerations,N/A
10.36227/techrxiv.172114926.68591276/v1,"Design, Validation, and Risk Assessment of LLM-Based Generative AI Systems Operating in the Legal Sector",N/A
10.1161/circimaging.124.017361,From Gadolinium to Generative AI: The Quest for Contrast-Free Cardiac Imaging,N/A
10.14236/ewic/bcshci2023.13,Drivers of Trust in Generative AI-powered Voice Assistants: The Role of References,N/A
10.1007/s42979-024-03074-y,AI-Lab: A Framework for Introducing Generative Artificial Intelligence Tools in Computer Programming Courses,N/A
10.3399/bjgp24x736605,Generative AI in medical writing: co-author or tool?,N/A
10.4018/979-8-3693-3278-8.ch013,Understanding and Applying Machine Learning Models,"<jats:p>This abstract provides an overview of the fundamental concepts involved in understanding and applying machine learning models. It covers key aspects, including data preparation, training, evaluation, and deployment. The journey begins with a data preprocessing phase, emphasizing the significance of data quality, feature engineering, and addressing challenges such as missing values and outliers. Subsequently, the focus shifts to the diverse landscape of machine learning models, ranging from traditional algorithms to sophisticated architectures. The importance of selecting the most suitable model for a given task, considering factors such as interpretability, scalability, and performance, is emphasized. The process of training ML models is then elucidated, highlighting the crucial role of splitting data into training and testing sets. Essential concepts such as loss functions, optimization algorithms, and hyperparameter tuning are explored in detail. Evaluation metrics, including accuracy, precision, recall, and F1-score, are discussed to assess model performance effectively.</jats:p>"
10.1186/s40561-023-00284-4,An expectancy value theory (EVT) based instrument for measuring student perceptions of generative AI,"<jats:title>Abstract</jats:title><jats:p>This study examines the relationship between student perceptions and their intention to use generative artificial intelligence (GenAI) in higher education. With a sample of 405 students participating in the study, their knowledge, perceived value, and perceived cost of using the technology were measured by an Expectancy-Value Theory (EVT) instrument. The scales were first validated and the correlations between the different components were subsequently estimated. The results indicate a strong positive correlation between perceived value and intention to use generative AI, and a weak negative correlation between perceived cost and intention to use. As we continue to explore the implications of GenAI in education and other domains, it is crucial to carefully consider the potential long-term consequences and the ethical dilemmas that may arise from widespread adoption.</jats:p>"
10.17811/rifie.52.4.2023.401-409,A Systematic Review of Generative AI and (English Medium Instruction) Higher Education,"<jats:p>This systematic review investigates the current state of research on Generative Artificial Intelligence (GenAI) and its implications for (EMI) Higher Education. The study employs a methodology based on an evidence-informed and theoretically credible framework to answer two research questions: (1) What studies of relevance to (EMI) Higher Education have been published thus far, considering the most recent developments of GenAI? and (2) Which key areas are currently lacking in extant literature and in need of further scholarly exploration in this regard in (EMI) Higher Education research? The results of the study reveal a limited number of pertinent publications, indicating a sparse scholarly landscape with a dearth of work on the implications of Generative AI in EMI Higher Education. Based on these findings, preliminary recommendations have been made to guide future research in this area. This study contributes to the literature by highlighting the need for further research on the potential of GenAI to enhance the teaching and learning experience in (EMI) Higher Education and provides a theoretical framework to guide future research. These findings may inform researchers and educators interested in exploring how GenAI may be leveraged from different educational perspectives.</jats:p>"
10.48028/iiprds/ijcsird.v9.i1.03,How Language Gaps Constrain Generative AI Development,"<jats:p>Prompt-based generative artificial intelligence (AI) tools are quickly being deployed for a range of use cases, from writing emails and compiling legal cases to personalizing research essays in a wide range of educational, professional, and vocational disciplines. But language is not monolithic, and opportunities may be missed in developing generative AI tools for non-standard languages and dialects. Current applications often are not optimized for certain populations or communities and, in some instances, may exacerbate social and economic divisions. As noted by the Austrian linguist and philosopher Ludwig Wittgenstein, “The limits of my language mean the limits of my world.” This is especially true today, when the language we speak can change how we engage with technology, and the limits of our online vernacular can constrain the full and fair use of existing and emerging technologies.</jats:p>"
10.1016/j.caeai.2024.100201,Beginning and first-year language teachers’ readiness for the generative AI age,N/A
10.21125/edulearn.2024.0683,SO WE’RE EMBRACING LLMS? NOW WHAT?: A STUDY ON ENHANCING FEEDBACK AND ASSESSMENT IN HIGHER EDUCATION THROUGH GENERATIVE AI,N/A
10.1109/icmisi61517.2024.10580495,Supply Chain Resilience in SMEs: Integration of Generative AI in Decision-Making Framework,N/A
10.1145/3641399.3641403,Accelerating Software Development Using Generative AI: ChatGPT Case Study,N/A
10.2139/ssrn.4603234,"ChatGPT and Similar Generative Artificial Intelligence (AI) for Smart Industry: Role, Challenges and Opportunities for Industry 4.0, Industry 5.0 and Society 5.0",N/A
10.1109/access.2024.3401770,A Generative AI-Driven Method-Level Semantic Clone Detection Based on the Structural and Semantical Comparison of Methods,N/A
10.15323/techart.2023.6.10.2.47,Is Generative Artificial Intelligence (AI) an Opportunity or Threat?: Educational Suggestions for Using ChatGPT,N/A
10.1080/03075079.2024.2332944,Generative AI: is it a paradigm shift for higher education?,N/A
10.1080/08989621.2023.2276169,New collaborative statement by bioethics journal editors on generative AI use,N/A
10.20944/preprints202306.1738.v1,"Current State, Data Requirements and Generative AI Solution for Learning-based Computer Vision in Horticulture","<jats:p>The use of visual signals in horticulture has attracted significant attention and encompassed a wide range of data types such as 2D images, videos, hyperspectral images, and 3D point clouds. These visual signals have proven to be valuable in developing cutting-edge computer vision systems for various applications in horticulture, enabling plant growth monitoring, pest and disease detection, quality and yield estimation, and automated harvesting. However, unlike other sectors, developing deep learning computer vision systems for horticulture encounters unique challenges due to the limited availability of high-quality training and evaluation datasets necessary for deep learning models. This paper investigates the current status of vision systems and available data in order to identify the high-quality data requirements specific to horticultural applications. We analyse the impact of the quality of visual signals on the information content and features that can be extracted from these signals. To address the identified data quality requirements, we explore the usage of a deep learning-based super-resolution model for generative quality enhancement of visual signals. Furthermore, we discuss how these can be applied to meet the growing requirements around data quality for learning-based vision systems. We also present a detailed analysis of the competitive quality generated by the proposed solution compared to cost-intensive hardware-based alternatives. This work aims to guide the development of efficient computer vision models in horticulture by overcoming existing data challenges and paving a pathway forward for contemporary data acquisition.</jats:p>"
10.1016/j.sbi.2023.102769,Generative AI for graph-based drug design: Recent advances and the way forward,N/A
10.1007/s12355-023-01358-w,Generative AI and Its Impact on Sugarcane Industry: An Insight into Modern Agricultural Practices,N/A
10.1111/1468-5973.12532,Higher education crisis: Academic misconduct with generative AI,"<jats:title>Abstract</jats:title><jats:p>Higher educational institutions (HEIs) are facing a significant challenge in maintaining academic integrity due to the technological integration of generative artificial intelligence (AI). The widespread use of AI tools by college students has resulted in an increase in plagiarism and cheating, highlighting the need for effective implementation of this technology. However, there is a lack of research on the best practices for using AI in academic settings. HEIs must take responsibility for addressing these issues, as the majority of institutions do not have formal guidelines for AI use, leading to confusion among students and instructors. To combat academic misconduct, HEIs should establish clear objectives and policies for the equitable, inclusive, and ethical use of AI. Improving AI literacy among students and faculty is crucial, as it ensures that everyone has equal access to technology, preventing a digital divide. Moreover, proactive education on the ethical use of AI is vital for HEIs to prepare students for the AI‐driven future of education and maintain academic integrity.</jats:p>"
10.4324/9781032648033-2,How do we know which skills to teach?,N/A
10.2139/ssrn.4509603,From Quills to Algorithms: Navigating the Impact of Generative AI on Legal Writing Assessments While Preparing Students for the NextGen Bar Exam,N/A
10.12968/s0047-9624(24)60009-x,<b>Ambarella</b> Brings <b>Generative AI</b> Capabilities to Edge Device,N/A
10.21203/rs.3.rs-4150369/v1,Applications of Generative Artificial Intelligence (AI) in Medical Education: A Current Application and Future Prospects in Saudi Arabia,"<title>Abstract</title>
        <p><bold>Background</bold>: The integration of Artificial Intelligence (AI) into medical education has transformative potential, yet systematic evidence regarding its adoption and impact remains limited. This study aims to evaluate the knowledge, attitude, practice and perception related to AI integration among health staff in Saudi Arabian health colleges.
<bold>Methods</bold>: A cross-sectional design was employed, utilizing an online questionnaire distributed to faculty, clinicians, and administrative personnel across Saudi health colleges. The questionnaire, developed based on expert input and literature review, assessed knowledge and perception regarding AI integration. Validity was ensured through expert review, pilot testing, and cognitive interviews. The questionnaire consisted of 6 domains (academic leadership, academic quality, generative AI in curriculum design and development, teaching methods, assessment and evaluation, and ethics, and challenges).
<bold>Results:</bold>  A total of 313 participants were included in this survey, of the respondents, 73.31% were male, with assistant professors comprising nearly half of them (48.55%). Regarding AI use, 37.94% report that they never use general AI, while 30.55% use it sometimes, and 21.86% use it rarely. Significant differences in AI familiarity were observed according to academic rank (p = 0.028). Academic leadership, academic quality, and generative AI received the highest mean scores (4.29±0.64, 4.16±0.70, and 4.16±0.71 respectively), while ethics and challenges scored the lowest (3.80±0.76). Users of AI consistently reported higher mean scores across domains compared to non-users, with statistically significant differences observed for all domains except ethics and challenges.  There was a statistically significant correlation between all domains.
<bold>Conclusions:</bold> The study highlights the importance of strategic AI integration in Saudi Arabian medical education to effectively address healthcare challenges and enhance learning methodologies and practices.</p>"
10.55041/ijsrem33437,"Democratizing Data Visualization and Insights Extraction with Pandas, Generative AI, and CSV Data","<jats:p>Data visualization and insights extraction are crucial components of modern data-driven decision-making processes. However, traditional methods often require extensive coding knowledge, creating barriers for non-technical users. This whitepaper presents a comprehensive solution that integrates the powerful data manipulation capabilities of the Pandas library with cutting-edge Generative AI and natural language processing techniques. By leveraging a fine-tuned GPT-3 model trained on a diverse corpus of data analysis and visualization resources, our approach enables users to upload CSV data files and receive automated insights, default visualizations, and the ability to generate custom visualizations through intuitive natural language prompts. The solution streamlines the workflow, eliminating the need for coding expertise while ensuring data privacy and integrity within a secure execution environment. User studies and benchmarking demonstrate increased productivity, time savings, and high user satisfaction. This solution has the potential to democratize data analysis and visualization, empowering decision-makers across various industries with data-driven insights and informed decision-making processes.</jats:p>"
10.1007/978-1-4842-9994-4_6,Implement LLMs Using Sklearn,N/A
10.22492/issn.2189-1036.2024.17,Automation or Innovation? A Generative AI and Instructional Design Snapshot,N/A
10.1117/12.3001999,"Enhancing inverse design of nanophotonic devices through generative deep learning, Bayesian latent optimization, and transfer learning",N/A
10.1145/3634713.3634732,A Demonstration of End-User Code Customization Using Generative AI,N/A
10.1145/3641399.3641437,Workshop Report on Generative AI-based Software Engineering,N/A
10.30525/978-9934-26-397-2-56,The potential impact of Generative AI on Open Science and reproducibility,N/A
10.1057/s41254-023-00321-6,Diplomatic relationship-building in the age of generative AI: the European Union and China,"<jats:title>Abstract</jats:title><jats:p>This paper examines the impact of generative AI on international diplomacy through the lens of EU–China diplomatic relationship-building. The first section introduces the broader context of AI’s geopolitical impact by distinguishing two different models—the European and Chinese models of regulating and implementing generative AI. The second discuss or explain how the two AI models contrast one another. The third section of the paper focuses on discussing Generative AI and its possible implications for EU–China relations, the extent to which their efforts are likely to strain or facilitate diplomatic relationship-building. The fourth section takes the analysis forward by examining how generative AI could be an unexpected enabling force for EU–China relationship-building. This paper purports that the distinctive EU and Chinese models of generative AI too often belie the opportunities that could potentially enable the EU and China to build their relationship, since generative AI raises shared concerns for the EU and China, and utilising generative AI could make their communications become more efficient, the EU and China may come to reach some kind of shared framework for generative AI development and governance; this could lead to productive talks in other domains such as the trade deficit issue plaguing the EU–China relations or more sensitive cross-strait issues; moreover, in the realm of public diplomacy generative AI could facilitate the EU and China’s public diplomatic efforts towards each other.</jats:p>"
10.1108/tg-01-2024-0022,"Unraveling generative AI in BBC News: application, impact, literacy and governance","<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>This study aims to uncover the ongoing discourse on generative artificial intelligence (AI), literacy and governance while providing nuanced perspectives on stakeholder involvement and recommendations for the effective regulation and utilization of generative AI technologies.</jats:p>
</jats:sec>
<jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>This study chooses generative AI-related online news coverage on BBC News as the case study. Oriented by a case study methodology, this study conducts a qualitative content analysis on 78 news articles related to generative AI.</jats:p>
</jats:sec>
<jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>By analyzing 78 news articles, generative AI is found to be portrayed in the news in the following ways: Generative AI is primarily used in generating texts, images, audio and videos. Generative AI can have both positive and negative impacts on people’s everyday lives. People’s generative AI literacy includes understanding, using and evaluating generative AI and combating generative AI harms. Various stakeholders, encompassing government authorities, industry, organizations/institutions, academia and affected individuals/users, engage in the practice of AI governance concerning generative AI.</jats:p>
</jats:sec>
<jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>Based on the findings, this study constructs a framework of competencies and considerations constituting generative AI literacy. Furthermore, this study underscores the role played by government authorities as coordinators who conduct co-governance with other stakeholders regarding generative AI literacy and who possess the legislative authority to offer robust legal safeguards to protect against harm.</jats:p>
</jats:sec>"
10.3934/steme.2023018,PAIGE: A generative AI-based framework for promoting assignment integrity in higher education,"<jats:p xml:lang=""fr"">&lt;abstract&gt;

&lt;p&gt;The integration of Generative Artificial Intelligence (GAI) tools like ChatGPT, Google Bard, and Bing Chat in higher education shows excellent potential for transformation. However, this integration also raises issues in maintaining academic integrity and preventing plagiarism. In this study, we investigate and analyze practical approaches for efficiently harnessing the potential of GAI while simultaneously ensuring the preservation of assignment integrity. Despite the potential to expedite the learning process and improve accessibility, concerns regarding academic misconduct highlight the necessity for the implementation of novel GAI frameworks for higher education. To effectively tackle these challenges, we propose a conceptual framework, PAIGE (Promoting Assignment Integrity using Generative AI in Education). This framework emphasizes the ethical integration of GAI, promotes active student interaction, and cultivates opportunities for peer learning experiences. Higher education institutions can effectively utilize the PAIGE framework to leverage the promise of GAI while ensuring the preservation of assignment integrity. This approach paves the way for a responsible and thriving future in Generative AI-driven education.&lt;/p&gt;

	      &lt;/abstract&gt;</jats:p>"
10.1111/ele.14397/v1/decision1,"Decision letter for ""How widespread use of generative &lt;scp&gt;AI&lt;/scp&gt; for images and video can affect the environment and the science of ecology""",N/A
10.1093/mam/ozae044.211,Generative AI Enables Label-Free Segmentation for Live Analysis of Supported Nanoparticle Catalysts,N/A
10.1145/3643834.3661588,DesignPrompt: Using Multimodal Interaction for Design Exploration with Generative AI,N/A
10.36287/setsci.18.1.0095,Advancements in Radar Performance through Generative AI: A Sector-Wide Survey,N/A
10.1109/isdfs60797.2024.10527300,AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns,N/A
10.1109/ijcnn60899.2024.10650688,An Open-source Cross-Industry and Cloud-agnostic Generative AI Platform,N/A
10.36227/techrxiv.172114926.68591276/v2,"Design, Validation, and Risk Assessment of LLM-Based Generative AI Systems Operating in the Legal Sector",N/A
10.1371/journal.pdig.0000503,Addressing 6 challenges in generative AI for digital health: A scoping review,"<jats:p>Generative artificial intelligence (AI) can exhibit biases, compromise data privacy, misinterpret prompts that are adversarial attacks, and produce hallucinations. Despite the potential of generative AI for many applications in digital health, practitioners must understand these tools and their limitations. This scoping review pays particular attention to the challenges with generative AI technologies in medical settings and surveys potential solutions. Using PubMed, we identified a total of 120 articles published by March 2024, which reference and evaluate generative AI in medicine, from which we synthesized themes and suggestions for future work. After first discussing general background on generative AI, we focus on collecting and presenting 6 challenges key for digital health practitioners and specific measures that can be taken to mitigate these challenges. Overall, bias, privacy, hallucination, and regulatory compliance were frequently considered, while other concerns around generative AI, such as overreliance on text models, adversarial misprompting, and jailbreaking, are not commonly evaluated in the current literature.</jats:p>"
10.1136/jme-2023-109414,Medical AI: is trust really the issue?,"<jats:p>I discuss an influential argument put forward by Hatherley in the<jats:italic>Journal of Medical Ethics</jats:italic>. Drawing on influential philosophical accounts of interpersonal trust, Hatherley claims that medical artificial intelligence is capable of being reliable, but not trustworthy. Furthermore, Hatherley argues that trust generates moral obligations on behalf of the trustee. For instance, when a patient trusts a clinician, it generates certain moral obligations on behalf of the clinician for her to do what she is entrusted to do. I make three objections to Hatherley’s claims: (1) At least one philosophical account of interagent trust implies that medical AI is capable of being trustworthy. (2) Even if this account should ultimately be rejected, it does not matter much because what we care mostly about is that medical AI is reliable. (3) It is false that trust in itself generates moral obligations on behalf of the trustee.</jats:p>"
10.1016/b978-0-443-18851-0.00015-9,Adverse effects of intelligent support of CSCL—the ethics of conversational agents,N/A
10.13180/icres.2019.29-30.07.015,"Explainable AI, model reconciliation and system-level analysis for safe-control of medical devices",N/A
10.2139/ssrn.4003468,"Moral Distance, AI, and the Ethics of Care",N/A
10.2196/preprints.55368,Proposing a Principle-Based Approach for Teaching AI Ethics in Medical Education (Preprint),"<sec>
                    <title>UNSTRUCTURED</title>
                        <p>The use of artificial intelligence (AI) in medicine, potentially leading to substantial advancements such as improved diagnostics, has been of increasing scientific and societal interest in recent years. However, the use of AI raises new ethical challenges, such as an increased risk of bias and potential discrimination against patients, as well as misdiagnoses potentially leading to over- or underdiagnosis with substantial consequences for patients. Recognizing these challenges, current research underscores the importance of integrating AI ethics into medical education. This viewpoint paper aims to introduce a comprehensive set of ethical principles for teaching AI ethics in medical education. This dynamic and principle-based approach is designed to be adaptive and comprehensive, addressing not only the current but also emerging ethical challenges associated with the use of AI in medicine. This study conducts a theoretical analysis of the current academic discourse on AI ethics in medical education, identifying potential gaps and limitations. The inherent interconnectivity and interdisciplinary nature of these anticipated challenges are illustrated through a focused discussion on “informed consent” in the context of AI in medicine and medical education. This paper proposes a principle-based approach to AI ethics education, building on the 4 principles of medical ethics—autonomy, beneficence, nonmaleficence, and justice—and extending them by integrating 3 public health ethics principles—efficiency, common good orientation, and proportionality. The principle-based approach to teaching AI ethics in medical education proposed in this study offers a foundational framework for addressing the anticipated ethical challenges of using AI in medicine, recommended in the current academic discourse. By incorporating the 3 principles of public health ethics, this principle-based approach ensures that medical ethics education remains relevant and responsive to the dynamic landscape of AI integration in medicine. As the advancement of AI technologies in medicine is expected to increase, medical ethics education must adapt and evolve accordingly. The proposed principle-based approach for teaching AI ethics in medical education provides an important foundation to ensure that future medical professionals are not only aware of the ethical dimensions of AI in medicine but also equipped to make informed ethical decisions in their practice. Future research is required to develop problem-based and competency-oriented learning objectives and educational content for the proposed principle-based approach to teaching AI ethics in medical education.</p>
                </sec>"
10.2139/ssrn.4598223,Preprinting in AI Ethics: Towards a Set of Community Guidelines,N/A
10.5040/9781350374430.ch-004,Techno-Imperialism,N/A
10.13180/icres.2022.18-19.07.p01,The moral issues of AI bias in data ethics,N/A
10.2139/ssrn.4879572,THE LYCEUM PROJECT: AI ETHICS WITH ARISTOTLE&amp;nbsp;,N/A
10.5840/teachphil2021444164,"AI Ethics, by Mark Coeckelbergh",<jats:p/>
10.4337/9781803924021.00030,Index,N/A
10.22554/ijtel.v7i2.132,Asking the Right Questions: The meaning of teaching and learning in the age of generative AI,"<jats:p>As educators grapple with the impact of generative artificial intelligence (AI) and the ease of access to these models by learners, questions of pedagogy and learning have dominated the conversation. This paper explores the concerns of teaching through exploring the contrast between human and machine learning and the implications of both on educators facing an environment where AI is prevalent. Through a prompting process, a position paper was generated by ChatGPT3.5 on these contrasts and implications. The AI output, largely accurate, highlighted strategies for educators while also showcasing the limitations of the tool. Educators and scholars can support themselves and their students through critical reflection of the use of AI tools.</jats:p>"
10.56397/saa.2024.06.28,Governance Prospects for the Development of Generative AI Film Industry from the Perspective of Community Aesthetics,"<jats:p>Generative AI has brought about a new revolution and challenge to the film industry, with artificial intelligence gradually shifting from a creative tool to a creative subject. The issue of symbiosis between AI creation and human creation urgently needs to be addressed. The problems in the AI governance system, laws and regulations, technical means, governance tools, and practical implementation in the film industry are still absent. This paper will trace the origin of generative AI films, reflect on the advantages and limitations of AI in the film industry, and analyze the current development of artificial intelligence governance at home and abroad, thereby promoting the development and governance of the Chinese generative AI film industry, and exploring the ethical relationships between people, society, and oneself under the perspective of community aesthetics.</jats:p>"
10.2139/ssrn.4731283,"Gemini or ChatGPT? Efficiency, Performance, and Adaptability of Cutting-Edge Generative Artificial Intelligence (AI) in Finance and Accounting",N/A
10.1007/s10734-024-01288-w,Generative AI chatbots in higher education: a review of an emerging research area,"<jats:title>Abstract
</jats:title><jats:p>Artificial intelligence (AI) chatbots trained on large language models are an example of generative AI which brings promises and threats to the higher education sector. In this study, we examine the emerging research area of AI chatbots in higher education (HE), focusing specifically on empirical studies conducted since the release of ChatGPT. Our review includes 23 research articles published between December 2022 and December 2023 exploring the use of AI chatbots in HE settings. We take a three-pronged approach to the empirical data. We first examine the state of the emerging field of AI chatbots in HE. Second, we identify the <jats:italic>theories of learning</jats:italic> used in the empirical studies on AI chatbots in HE. Third, we scrutinise the <jats:italic>discourses</jats:italic> of AI in HE framing the latest empirical work on AI chatbots. Our findings contribute to a better understanding of the eclectic state of the nascent research area of AI chatbots in HE, the lack of common conceptual groundings about human learning, and the presence of both dystopian and utopian discourses about the future role of AI chatbots in HE.
</jats:p>"
10.52812/msbd.63,Generative AI for Business Decision-Making: A Case of ChatGPT,"<jats:p>ChatGPT (Generative Pretrained Transformer) is a chatbot using artificial intelligence (AI) launched by OpenAI, which is an AI research and deployment company. The ChatGPT has taken the technology world by storm. The ChatGPT is a trained AI model that can chat almost like a human. The dialog format allows the ChatGPT to answer follow-up questions, admit mistakes, challenge incorrect premises, and reject inappropriate requests. The ChatGPT can be utilized for compiling research, drafting marketing content, brainstorming ideas, delivering aftercare services, increasing customer engagement, and many others. The ChatGPT can provide enormous opportunities for companies leveraging this breakthrough technology strategically. Thus, we evaluate ChatGPT as a tool in common business decision-making cases in the current study. For example, the ChatGPT was asked about the impacts of a hypothetical merging of two supermarket chains in Sweden. In another example, the ChatGPT was asked about recommendations for investment in a Brazilian oil company. Finally, it was asked about the factors that influence online shopping behavior. The results are significant and demonstrate the tremendous potential of the ChatGPT in revolutionizing the corporate world.
 </jats:p>"
10.33545/26648792.2024.v6.i1f.187,Generative AI and its impact on education,N/A
10.2196/preprints.63017,Healthcare Social Robots in the Age of Generative AI: Protocol for a Scoping Review (Preprint),"<sec>
                    <title>BACKGROUND</title>
                        <p>Social Robots (SR), sensorimotor machines designed to interact with humans, can help to respond to the increasing demands in the health care sector (HCS). To ensure a successful use of this technology, acceptance is paramount. Generative Artificial Intelligence (Generative AI) is an emerging technology with the potential to enhance the functionality of SR and promote user acceptance by further improving Human-Robot Interaction.</p>
                </sec>
                                <sec>
                    <title>OBJECTIVE</title>
                        <p>We present a protocol for a Scoping Review of literature on the implementation of Generative AI in SR in the HCS. The aim of this Scoping Review is to map out the intersection of SR and Generative AI in the HCS; to explore if Generative AI is applied in SR in the HCS; to outline which models of Generative AI and SR are used for these implementations; and to explore whether user acceptance is reported as an outcome following these implementations. This Scoping Review supports future research by providing an overview of the state of connectedness of two emerging technologies, SR and Generative AI, and by mapping out potential research gaps</p>
                </sec>
                                <sec>
                    <title>METHODS</title>
                        <p>For this review, we follow the methodological framework developed by Arksey and O'Malley and the recommendations by the Joanna Briggs Institute. Our protocol was drafted using the Preferred Reporting Items for Systematic Reviews and Meta-analyses extension for Scoping Reviews (PRISMA-ScR). We will conduct a systematic literature search of the online databases Medline, Embase, CINAHL, Web of Science, and IEEE Xplore, aiming to retrieve relevant data items via tabular data charting from references meeting specific inclusion criteria. Results will be collated, categorized, summarized by clustering similar publications by classifying the collected data items. The findings will be presented through a series of adequate tables, graphs, visual representations, and corresponding narrative summaries.</p>
                </sec>
                                <sec>
                    <title>RESULTS</title>
                        <p>After conducting a preliminary search and de-duplication, we retrieved 3176 preliminary results. This Scoping Review will be supplemented with the next methodological steps, including retrieving the results in a reference management tool as well as screening titles, abstracts, and full text regarding specific inclusion criteria.</p>
                </sec>
                                <sec>
                    <title>CONCLUSIONS</title>
                        <p>The conducted preliminary search implies that there is a sufficient number of heterogenous references to complete this Scoping Review. To our knowledge, this is the first Scoping Review on the implementation of Generative AI in Healthcare SR.</p>
                </sec>"
10.1177/23294906241249113,Pedagogical Impact of Text-Generative AI and ChatGPT on Business Communication,"<jats:p> The article discusses the impact of text-generative AI in business communication pedagogy. The onset of open AI, such as ChatGPT, has the potential to transform the way faculty and students approach oral and written professional business communication. Through focus group discussions and netnography, the study employs content analysis to evaluate the strengths, weaknesses, opportunities, and threats (SWOT) of integrating AI in the teaching-learning process of business communication in a postgraduate management program. The article strives to reimagine the pedagogical tools and techniques regarding pre-reading assistance, classroom materials, assignments, evaluation, and other learning aids of business communication courses in response to the developments in text-generative AI. </jats:p>"
10.1007/s00266-024-03849-x,Invited Commentary: “Dr. GAI—Significance of Generative AI in Plastic Surgery”,N/A
10.3917/pinc.016.0015,Art(ificial intelligence) imitates life: IP infringement risks presented by Generative AI,"<jats:p>Les technologies d’IA générative telles que Chat GPT-4, DALL.E 2 et Stable Diffusion sont au premier plan des discussions, s’agissant de leur impact sur des secteurs tels que le commerce, l'éducation et la création, en particulier en ce qui concerne les droits de propriété intellectuelle et les risques potentiels de violation de ces derniers. Ces technologies remettent en question les lois existantes sur le droit d’auteur, notamment en termes de droits de reproduction et d’adaptation, créant un équilibre complexe entre l’innovation technologique et les titulaires de droits. La Directive sur le droit d’auteur dans le marché unique numérique de l’Union européenne prévoit certaines exceptions pour l’entraînement de l’IA à travers le minage de textes et de données, mais celles-ci ne s'étendent pas à tous les droits, conduisant à un paysage juridique nuancé. De plus, les spécificités locales en matière de droits de propriété intellectuelle, comme celles de l’UE, du Royaume-Uni, de l’Espagne et de l’Allemagne, compliquent davantage les choses, en particulier en ce qui concerne les bases de données, les photographies et les images de personnes réelles. Cette complexité est accentuée par les débats et les défis juridiques en cours dans différents pays, comme l’UE, le Royaume-Uni et les États-Unis, sur la question de savoir si les œuvres générées par l’IA peuvent être éligibles à la protection du droit d’auteur, reflétant la nature évolutive et incertaine du droit d’auteur à l'ère de l’IA.</jats:p>"
10.1515/9783111323749-003,3 Classification and regression algorithms,N/A
10.18260/1-2--47494,Generative-AI Assisted Feedback Provisioning for Project-Based Learning in CS Courses,N/A
10.21776/rechtjiva.v1n1.1,Bentuk Internalisasi Nilai Etik Mengenai Bias Negatif dan Diskriminasi Dalam Platform generative AI,"<jats:p>This research discusses a form of classification in the realm of Artificial Intelligence, namely generative AI. Generative AI is an AI technology that can create new content in the form of text, images, audio, and others. However, generative AI has the potential to cause problems such as negative bias and unintentional discrimination in the form of negative stereotyping of a racial group. Current regulations are inadequate to address these challenges and determine the liability of parties in the event of harm. This research aims to analyze the urgency of new regulations related to generative AI for anti-bias and discrimination. The approach is normative juridical using statutory, conceptual, and comparative approaches. The results show that current regulations do not adequately protect the public from bias and discrimination by AI. New regulations are needed that include AI ethical principles, AI audits, classification of parties' responsibilities (developers, data providers, regulators), and application of the principle of liability based on fault. These regulations are important to ensure responsible use of generative AI and respect for human rights. In conclusion, current regulations need to be refined and new ones created to address the ethical and liability challenges in the utilization of generative AI in line with anti-discrimination principles.</jats:p>"
10.52153/prj0708008,Editorial: Use of Generative AI in Research – A Cautionary Tale,N/A
10.1145/3649405.3659517,Active Repos: Integrating Generative AI Workflows into GitHub,N/A
10.1016/j.procir.2023.05.002,Reconceptualizing ChatGPT and generative AI as a student-driven innovation in higher education,N/A
10.1016/j.caeai.2023.100184,Integrating generative AI in knowledge building,N/A
10.1145/3632634.3655868,Panel: Using Generative AI in Teaching and Learning.,N/A
10.1145/3613905.3650947,The Social Construction of Generative AI Prompts,N/A
10.1145/3633053.3633057,Incorporating Generative AI into Software Development Education,N/A
10.37622/acst/17.1.2024.39-59,Generative Ai: A New Paradigm for Antibody Design and Development,N/A
10.21203/rs.3.rs-5041554/v1,Leveraging Generative AI and CAD Automation for Efficient Automotive Wheel Design with Limited Data,"<title>Abstract</title>
        <p>This study developed a method that leverages generative artificial intelligence and automated computer-aided design (CAD) technologies for the design of automotive wheel rims. First, numerous two-dimensional (2D) wheel rim designs are automatically generated by inputting prompts into Stable Diffusion, an image generation model, without predefining design parameters and constraints. Subsequently, the reasonableness of the generated 2D designs is examined, and reasonable designs are transformed into three-dimensional CAD models. The proposed method completes model training with a small amount of data and simultaneously establishes a mechanism for verifying the reasonableness of data generation, achieving reliable wheel rim design generation. Compared to existing big data methods, this approach is more easily applicable to industrial applications.</p>"
10.1080/13678868.2024.2337963,Applying generative AI ethically in HRD practice,N/A
10.3389/frobt.2016.00005,Just Imagine! Learning to Emulate and Infer Actions with a Stochastic Generative Architecture,N/A
10.5771/9783987400476-355,Recognizing AI Responses from Generative AI in Knowledge-Based Online Q&amp;A Communities,N/A
10.4018/979-8-3693-1565-1.ch011,Navigating the Legal Landscape of AI-Induced Property Damage,<jats:p>This chapter dissects the proposal for an AI Liability Directive by the European Parliament and the Council. The Directive aims to adapt civil liability rules for artificial intelligence (AI) systems. Two main types of non-contractual liability are scrutinized: fault-based and strict liability. The core of the chapter revolves around the proposed AI Liability Directive. It dissects key provisions and highlights conflicting perspectives from scholars and associations. It also looks at the advantages and disadvantages of these rules and concludes by summarizing its findings and discussing how they might impact future policies related to AI responsibility.</jats:p>
10.4018/979-8-3693-2440-0.ch011,AI-Based Career Counseling Chatbot for Secondary-Level Students,"<jats:p>Making a career selection may be a complex process, and many students struggle to choose a route that fits their interests, skills, and personality. Creating a career path requires careful consideration of these factors, and career coaching done in the old-fashioned manual way is ineffective. This study suggests a paradigm change that would use an AI-driven chatbot career advising system created especially for secondary school pupils. This cutting-edge technology, which uses machine learning, carefully examines users' personalities, interests, and talents in addition to incorporating up-to-date job market data to provide customized career recommendations. Above and beyond simple directions, the chatbot actively assists users in refining their abilities, guaranteeing a smooth transition into the careers of their choice. The ethical aspect of this investigation is essential, as it addresses certain worries and issues related to AI-driven career guidance. User testimonials highlight the system's effectiveness and its critical role in promoting informed career decisions.</jats:p>"
10.3233/faia240229,Operational Criteria of Hybrid Intelligence for Generative AI Virtual Assistants,"<jats:p>The concept of Hybrid Intelligence (HI) is frequently used interchangeably with Human-Centered AI (HCAI) and more broadly as human-in-the-loop. Dellerman et al. [1] outlined three differentiation criteria, emphasizing in particular the need for an evolving continuum of human-AI learning, a concept that has proven challenging to operationalize effectively. Recent efforts aim to expand the definition of HI beyond the domain of human-computer interaction to include application-oriented insights from management science [2]. This broader perspective integrates vital components such as facilitating end-user co-creation through narrative frameworks that foster psychological safety by addressing fears of job displacement [3,4], mitigating risks of deskilling during system deployment and scaling [5], and supporting business process innovation [2]. Additionally, in contrast to HCAI, the name hybrid intelligence conveys the possibly symmetric human-machine relationship and thereby preserves some of the disruptive potential of automated AI rather than relying on purely augmentation of human tasks and intentions [3]. Explicitly, the HI interaction should not only augment the existing, predefined task but also support aspects such as (business) process and business model re-engineering. Despite these considerations, a thorough discussion on which of the many established HCAI concepts and design guidelines form crucial components in achieving the aims of HI has so far been absent in literature. In particular, as it is becoming more and more likely that most knowledge workers will within a short timeframe become operators of complex virtual assistants tapping into LLMs and natural language interfaces, it becomes urgent to ensure that the human-ai interface and associated narrative is constructed to support HI principles and objectives. To initiate this discussion, we formulate explicitly updated HI design criteria in particular for generative AI virtual assistant design and discuss relevant HCAI concept.</jats:p>"
10.2139/ssrn.4432941,"From Ethics to Law: Why, When, and How to Regulate AI",N/A
10.4324/9781003356738-6,Artificial Companion or Companionable AI?,N/A
10.4337/9781800378803.00010,AI and Ethics in Intelligence,N/A
10.47289/aiej20210716-4,Ethical Review in the Age of Artificial Intelligence,"<jats:p>Artificial intelligence (AI), particularly machine learning, has made significant strides in the past decade. Due to the widely applicable nature of this technology, the emergence of increasingly intelligent machines is poised to transform today’s society. Recently, the rate of AI development has aroused significant concerns due to the lack of guiding policy and regulation. Thus, it is integral for the public to recognize the technology and make informed choices regarding the future of AI. This paper serves to acquaint the layperson and other stakeholders involved in AI development with the current progress of AI and the ethical concerns that must be addressed before significant advancements. The subject of discussion is narrowed down to three fields of AI’s most prominent use: (1) the internet; (2) the automotive industry; and (3) the healthcare industry. For each sector, the foundation of the domain-specific AI technique is introduced, the benefits and ethical ramifications are discussed, and a final cost-benefit analysis is provided.</jats:p>"
10.5040/9781350374430.ch-003,Capital Punishment,N/A
10.1201/9781003277330-2,Ethics in AI in Machine Learning,N/A
10.2139/ssrn.4803295,Perception Challenges and Ethics on the Future of Ai as Encountered by Surveyed New Engineers,N/A
10.13180/icres.2022.18-19.07.001,The contribution of neuroscience to AI ethics and morality,N/A
10.1145/3649476.3660360,Enhanced AI for Science using Diffusion-based Generative AI - A Case Study on Ultrasound Computing Tomography,N/A
10.4018/979-8-3693-2440-0.ch021,Exploring Challenges in Online Higher Education for AI Integration Using MICMAC Analysis,"<jats:p>The consequence of Covid-19 has affected the traditional higher education system. Acknowledging the significant role of online education in national development for accessibility and quality education, countries around the world have understood its importance in current digital era. Indian policymakers have been giving due importance to enhancing the education quality, however the progress made by the country in higher education is not adequate. Amidst all the inadequacies of traditional education system, artificial intelligence (AI) technologies are bringing new ray of hope to democratize education system. This chapter is subjected to identify the challenges in online education and suggest specific ways to address each of them. The challenges are categorized into internal and external challenges/barriers. These challenges have been modeled with the expertise of educationalist's opinions and interpretive structural modeling to create a hierarchy of the barriers using MICMAC analysis and categorize these barriers into four clusters.</jats:p>"
10.1002/ail2.63,Generative model‐enhanced human motion prediction,"<jats:title>Abstract</jats:title><jats:p>The task of predicting human motion is complicated by the natural heterogeneity and compositionality of actions, necessitating robustness to distributional shifts as far as out‐of‐distribution (OoD). Here, we formulate a new OoD benchmark based on the Human3.6M and Carnegie Mellon University (CMU) motion capture datasets, and introduce a hybrid framework for hardening discriminative architectures to OoD failure by augmenting them with a generative model. When applied to current state‐of‐the‐art discriminative models, we show that the proposed approach improves OoD robustness without sacrificing in‐distribution performance, and can theoretically facilitate model interpretability. We suggest human motion predictors ought to be constructed with OoD challenges in mind, and provide an extensible general framework for hardening diverse discriminative architectures to extreme distributional shift. The code is available at: <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""https://github.com/bouracha/OoDMotion"">https://github.com/bouracha/OoDMotion</jats:ext-link>.</jats:p>"
10.22318/icls2024.930382,A Comparison Between AI and Human Evaluation with a Focus on Generative AI,N/A
10.1007/s43681-023-00398-y,To be forgotten or to be fair: unveiling fairness implications of machine unlearning methods,"<jats:title>Abstract</jats:title><jats:p>The right to be forgotten (RTBF) allows individuals to request the removal of personal information from online platforms. Researchers have proposed machine unlearning algorithms as a solution for erasing specific data from trained models to support RTBF. However, these methods modify how data are fed into the model and how training is done, which may subsequently compromise AI ethics from the fairness perspective. To help AI practitioners make responsible decisions when adopting these unlearning methods, we present the first study on machine unlearning methods to reveal their fairness implications. We designed and conducted experiments on two typical machine unlearning methods (SISA and AmnesiacML) along with a retraining method (ORTR) as baseline using three fairness datasets under three different deletion strategies. Results show that non-uniform data deletion with the variant of SISA leads to better fairness compared to ORTR and AmnesiacML, while initial training and uniform data deletion do not necessarily affect the fairness of all three methods. This research can help practitioners make informed decisions when implementing RTBF solutions that consider potential trade-offs on fairness.</jats:p>"
10.1007/s43681-021-00111-x,Methodology for integrating artificial intelligence in healthcare systems: learning from COVID-19 to prepare for Disease X,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence and edge devices have been used at an increased rate in managing the COVID-19 pandemic. In this article we review the lessons learned from COVID-19 to postulate possible solutions for a Disease X event. The overall purpose of the study and the research problems investigated is the integration of artificial intelligence function in digital healthcare systems. The basic design of the study includes a systematic state-of-the-art review, followed by an evaluation of different approaches to managing global pandemics. The study design then engages with constructing a new methodology for integrating algorithms in healthcare systems, followed by analysis of the new methodology and a discussion. Action research is applied to review existing state of the art, and a qualitative case study method is used to analyse the knowledge acquired from the COVID-19 pandemic. Major trends found as a result of the study derive from the synthesis of COVID-19 knowledge, presenting new insights in the form of a conceptual methodology—that includes six phases for managing a future Disease X event, resulting with a summary map of various problems, solutions and expected results from integrating functional AI in healthcare systems.</jats:p>"
10.1017/9781009072168.024,AI as Inventor,N/A
10.1163/9789004458109_016,Thinking Ahead: Building a New AI Convention,N/A
10.4337/9781803924021.00029,References,N/A
10.1017/9781108891325.019,Regulating Algorithmic Assemblages: Looking beyond Corporatist AI Ethics,N/A
10.1093/oso/9780192845290.003.0018,Introduction: How to Bridge the Gap from Ethics to Practice,"<jats:title>Abstract</jats:title>
               <jats:p>Moving from ethics to practice means that decision makers will need to take meaningful actions at the levels of team, organization, industry, and government. The recommendations are meant to increase the reliability, safety, and trustworthiness of HCAI systems by way of governance structures: ● software engineering practices within teams ● Safety culture within organizations through business management strategies ● Trustworthy certification through industry-specific independent oversight efforts ● Regulation by government agencies to ensure fair business practices and public safety. The recommendations are a starting point for discussions that can lead to validated practices, which become widely accepted and then evolve to keep up with technology innovations.</jats:p>"
10.18260/1-2--17433,"AI &amp; SciFi:  Teaching Writing, history, Technology, Literature, and Ethics",N/A
10.1109/ntpe.2021.9778139,Emergence of AR and AI in Educational Institutions: A COVID-19 System Transition,N/A
10.18850/jees.2020.55.10,Education of AI Ethics as a Response to Decision Automation,N/A
10.1108/jices-01-2021-0005,The Thailand national AI ethics guideline: an analysis,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>The paper aims to analyze the content of the newly published National AI Ethics Guideline in Thailand. Thailand’s ongoing political struggles and transformation has made it a good case to see how a policy document such as a guideline in AI ethics becomes part of the transformations. Looking at how the two are interrelated will help illuminate the political and cultural dynamics of Thailand as well as how governance of ethics itself is conceptualized.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>The author looks at the history of how the National AI Ethics Guidelines came to be and interprets its content, situating the Guideline within the contemporary history of the country as well as comparing the Guideline with some of the leading existing guidelines.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>It is found that the Guideline represents an ambivalent and paradoxical nature that characterizes Thailand’s attempt at modernization. On the one hand, there is a desire to join the ranks of the more advanced economies, but, on the other hand, there is also a strong desire to maintain its own traditional values. Thailand has not been successful in resolving this tension yet, and this lack of success is shown in the way that content of the AI Ethics Guideline is presented.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Practical implications</jats:title>
<jats:p>The findings of the paper could be useful for further attempts in drafting and revising AI ethics guidelines in the future.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>The paper represents the first attempt, so far as the author is aware, to analyze the content of the Thai AI Ethics Guideline critically.</jats:p>
</jats:sec>"
10.1007/s43681-021-00056-1,The ethical use of high-performance computing and artificial intelligence: fighting COVID-19 at Barcelona Supercomputing Center,N/A
10.55041/ijsrem33863,Design of an AI Chatbot Using Generative AI to Access Local Database,"<jats:p>The ""College Chatbot"" project aims to create an advanced conversational agent tailored for college-related inquiries. This innovative chatbot incorporates a versatile interface, capable of seamlessly processing both text and voice inputs, enhancing user accessibility. Its distinctive feature includes multilingual output capabilities, enabling responses in local languages to cater to a diverse user base. The primary function of the chatbot is to comprehensively address a wide array of college-related queries, providing students and users with accurate and timely information on various aspects of college life, academic affairs, and related topics. By combining a user-friendly interface with linguistic flexibility, the College Chatbot seeks to revolutionize the way students interact with and retrieve information from their educational institutions.           Keywords- AI chatbot, Generative AI, Local database, Natural Language processing, Data Integrarion,User Intraction, Query mechanism, Model training.</jats:p>"
10.1007/979-8-8688-0315-4_15,Real-World Applications of Generative AI For Data Augmentation,N/A
10.1093/oxfordhb/9780190067397.013.15,The Concept of Handoff as a Model for Ethical Analysis and Design,"<p>This chapter introduces the concept of <italic>handoff</italic>, which offers a lens through which to evaluate sociotechnical systems in ethical and political terms. It is particularly tuned to transformations in which system components of one type replace components of another. Of great contemporary interest are handoff instances in which AI take over tasks previously performed by humans, for example, labelling images, processing and producing natural language, controlling other machines, predicting human action (and other events), and make decisions. Grounded in past work in social studies of technology and values in design, the <italic>handoff</italic> analytical model disrupts the idea that if components of a system are modular in functional terms, replacing one with another will leave ethical and political dimensions intact. Instead, the handoff lens highlights different ways that different types of system components operate and interoperate and shows these differences to be relevant to the configuration of values in respective systems. The handoff lens offers a means to make ethically relevant changes salient that might otherwise be overlooked.</p>"
10.1007/s43681-024-00550-2,Ethical concerns of using computer perception technologies among pediatric patients,N/A
10.1101/2024.03.19.24304550,A Systematic Examination of Generative Artificial Intelligence (GAI) Usage Guidelines for Scholarly Publishing in Medical Journals,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>A thorough and in-depth examination of generative artificial intelligence (GAI) usage guidelines in medical journals will inform potential gaps and promote proper GAI usage in scholarly publishing. This study aims to examine the provision and specificity of GAI usage guidelines and their relationships with journal characteristics.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>From the SCImago Journal Rank (SJR) list for medicine in 2022, we selected 98 journals as top journals to represent highly indexed journals and 144 as whole-spectrum sample journals to represent all medical journals. We examined their GAI usage guidelines for scholarly publishing between December 2023 and January 2024.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Compared to whole-spectrum sample journals, the top journals were more likely to provide author guidelines (64.3% vs. 27.8%) and reviewer guidelines (11.2% vs. 0.0%) as well as refer to external guidelines (85.7% vs 74.3%). Probit models showed that SJR score or region was not associated with the provision of these guidelines among top journals. However, among whole-spectrum sample journals, SJR score was positively associated with the provision of author guidelines (0.85, 95% CI 0.49 to 1.25) and references to external guidelines (2.01, 95% CI 1.24 to 3.65). Liner models showed that SJR score was positively associated with the specificity level of author and reviewer guidelines among whole-spectrum sample journals (1.21, 95% CI 0.72 to 1.70), and no such pattern was observed among top journals.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>The provision of GAI usage guidelines is limited across medical journals, especially for reviewer guidelines. The lack of specificity and consistency in existing guidelines highlights areas deserving improvement. These findings suggest that immediate attention is needed to guide GAI usage in scholarly publishing in medical journals.</jats:p></jats:sec><jats:sec id=""s1""><jats:title>Key points</jats:title><jats:sec id=""s1a""><jats:title>Question</jats:title><jats:p>What are the provision and specificity of generative artificial intelligence (GAI) usage guidelines for scholarly publishing in top and whole-spectrum medical journals and their relationships with journal characteristics?</jats:p></jats:sec><jats:sec id=""s1b""><jats:title>Findings</jats:title><jats:p>Author guidelines were more abundant and specific in top journals than in whole-spectrum journals. However, reviewer guidelines were extremely scarce in both groups of journals. Journal ranking score was associated with both provision and specificity of GAI usage guidelines in whole-spectrum journals while no significant relationship was found in top journals.</jats:p></jats:sec><jats:sec id=""s1c""><jats:title>Meaning</jats:title><jats:p>The lack of provision and specificity as well as the inconsistencies in existing guidelines suggest that immediate attention is needed to guide GAI usage in scholarly publishing and safeguard integrity and trust in medical research.</jats:p></jats:sec></jats:sec>"
10.1017/9781009072168.009,AI and Contract Performance,N/A
10.35802/079896,N/A,N/A
10.3102/2111055,Investigating Undergraduate Students’ Perspectives of AI Ethics About ChatGPT (Poster 29),N/A
10.2139/ssrn.4484188,Befriending Chatgpt and Other Superchatbots: An Ai-Integrated Take-Home Assessment to Preserve Writing-Ethics,N/A
10.5040/9781350374430.ch-005,Death-Techniques,N/A
10.4324/9781003105596-19,AI as IA,N/A
10.1016/j.acorp.2023.100066,Generative AI and the end of corpus-assisted data-driven learning? Not so fast!,N/A
10.4324/9781003482918-4,Supporting inclusion in academic integrity in the age of GenAI,N/A
10.5465/amproc.2024.19615abstract,The Regulation of Generative AI: Lessons Learned from ChatGPT’s Suspension in Italy,N/A
10.31234/osf.io/mj46k,Generative AI-Enabled Therapy Support Tool Improves Clinical Outcomes and Patient Engagement in NHS Talking Therapies,"<p>Cognitive behavioural therapy (CBT) is a highly effective treatment for depression and anxiety disorders. Nonetheless, a substantial proportion of patients do not respond to treatment. The lack of engagement with therapeutic materials and exercises between sessions - a necessary component of CBT - is a key determinant of unsuccessful treatment. Here we tested whether the deployment of a generative artificial intelligence (AI) powered personalised therapy support tool supporting patients in between sessions leads to improved treatment success and patient engagement with treatment. We conducted a real-world observational study of 137 patients receiving CBT in five of the UK’s National Health Service (NHS) Talking Therapies services. Ninety-three patients (68%) used the AI-enabled therapy support tool whilst forty-four patients were provided with the standard CBT worksheets. Patients using the AI-enabled therapy support tool exhibited greater attendance at therapy sessions and fewer dropouts from treatment. Furthermore, these patients demonstrated higher reliable improvement, recovery and reliable recovery rates when compared to the control group. Within the intervention group, the degree of usage of the AI-enabled tool was directly related to the number of attended therapy sessions and reduced dropouts. Our results thus show that an AI-enabled, personalised support tool is a promising avenue to improve treatment outcomes and patient adherence.</p>"
10.14429/djlit.44.3.19698,Generative AI Tools for Collaborative Content Creation,"<jats:p>ICT applications streamlined processes across multiple industries and improved efficiency at a lower cost.Although everyone is generally in favor of this, many people view the development of artificial intelligence as both a blessing and a threat. It is unquestionably widely utilized in academic settings for experimentation and concept articulation, but with so many possibilities available, students and researchers find it difficult to select the best one. This study attempts to evaluate seven generative AI content creation tools—Writesonic, Jasper, Grammarly Business, ProWritingAid, Scalenut, Rytr, and TextCortex AI-using data from the GetApp website. Employing content analysis, the seven selected tools were compared on the basis of qualitative data converted to binary format. Grammarly Business led with a score of (76), followed by Scalenut (75), Writesonic (53), ProWritingAid (51), Jasper (48), TextCortex AI (38), and Rytr (36). This study highlights the pivotal role of collaboration in generative AI, showcasing diverse features, from grammar checks to creative content generation. The significance of this study lies in aiding academia and professionals in strategic tool selection, emphasising the need for informed decision-making in collaborative efforts. Integrating Artificial Intelligence and Natural Language Processing enhances scholarly endeavours, underlining the necessity for efficient content creation during digital evolution.</jats:p>"
10.1109/icetci55101.2022.9832398,AI Music Therapist: A Study on Generating Specific Therapeutic Music based on Deep Generative Adversarial Network Approach,N/A
10.1109/cste59648.2023.00065,"Application Status, Problems and Future Prospects of Generative AI in Education",N/A
10.1117/12.3003890,A generative AI approach for interference study on chromogenic triplex images,N/A
10.1109/fie58773.2023.10343517,WIP: Using Generative AI to Assist in Individual Performance Feedback for Engineering Student Teams,N/A
10.1109/mipro60963.2024.10569541,Enhancing Creativity in Sustainable Product Design: The Impact of Generative AI Tools at the Conceptual Stage,N/A
10.1109/csce60160.2023.00068,"Large Language Model (LLM) &amp; GPT, A Monolithic Study in Generative AI",N/A
10.1007/978-3-031-45693-0_4,Generative AI and the Technological Imaginary of Game Design,N/A
10.1109/icc51166.2024.10622524,DroneDefGANt: A Generative AI-Based Approach for Detecting UAS Attacks and Faults,N/A
10.1016/j.clsr.2024.105941,Fraud by generative AI chatbots: On the thin line between deception and negligence,N/A
10.30525/978-9934-26-397-2-34,Generative AI in Teaching and Learning: Prompt Engineering and Towards Digital Equity,N/A
10.1145/3626253.3635595,Generative AI as a Resource for Creativity in Computational Physics,N/A
10.17803/1994-1471.2024.165.8.066-075,Autonomy  of  Generative  AI  Digital Platforms in  Regulating Relations with Users,"<jats:p>The impact of the phenomenon of digital platforms on public relations can be explained, inter alia, by the fact that platform operators are beginning to compete with states for the right to set rules for users, which include almost all participants in civil turnover, inventive and creative industries, consumers of content, etc. Conditional privatization of the regulatory impact on public relations, based on the example of analyzing the rules of generative artificial intelligence platforms for creating various kinds of content, urges the State to respond. With regard to the legal regulation of digital platforms, it seems possible to build such a multi-level system of regulation of public relations that can be based on both general principles of law and codified norms. The developing discussion around the concept of the digital code of the Russian Federation allows us to formulate approaches to identify those areas of relations in the digital environment that are subject to state regulation in regulatory legal acts, as well as those that can be regulated at the level of ""soft law"" and within the framework of contracts between platform operators and users.</jats:p>"
10.31234/osf.io/r7vf5,"Stereotypical Bias Amplification, and Reversal, in an Experimental Model of Human Interaction with Generative AI","<p>Stereotypical biases are readily acquired and expressed by generative AI, causing growing societal concern about these systems amplifying existing human bias. This concern rests on reasonable psychological assumptions, but stereotypical bias amplification during human-AI interaction relative to pre-existing baseline levels hasn’t been demonstrated. Here, we use previous psychological work on gendered character traits to capture and control gender stereotypes expressed in character descriptions generated by Open AI’s ChatGPT. In four experiments (N=782) with a first impressions task we find that unexplained (‘black-box’) character recommendations using stereotypical traits already convey a potent persuasive influence significantly amplifying baseline stereotyping within first impressions. Recommendations that are counter-stereotypical eliminate and effectively reverse human baseline bias, but these stereotype challenging influences propagate less well than reinforcing influences from stereotypical recommendations. Critically, the bias amplification and reversal phenomena occur when ChatGPT elaborates on the core stereotypical content, although ChatGPT’s explanations propagate counter-stereotypical influence more effectively and persuasively than black-box recommendations. Our findings strongly imply that without robust safeguards generative AI will amplify existing bias. But with safeguards, existing bias can be eliminated and even reversed. Our novel approach safely allows such effects to be studied in various contexts where gender and other bias-inducing social stereotypes operate.</p>"
10.1109/csci62032.2023.00168,Sustainability of Nature Parks by Changing Tourist Behavior Using Donations and Generative AI,N/A
10.1016/b978-0-443-18851-0.00007-x,Pitfalls (and advantages) of sophisticated large language models,N/A
10.5040/9781350374430.0006,Select Bibliography,N/A
10.2139/ssrn.4614964,AI Ethics and Ordoliberalism 2.0: Towards A ‘Digital Bill of Rights’,N/A
10.22471/ai.2020.5.2.27,AI Ethics and Privacy Right,N/A
10.31235/osf.io/8jaz4,"What’s Next for AI Ethics, Policy, and Governance? A Global Overview","<p>Since 2016, more than 80 AI ethics documents – including codes, principles, frameworks, and policy strategies – have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents’ creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals.</p>"
10.3390/philosophies9010026,The Rise of Particulars: AI and the Ethics of Care,"<jats:p>Machine learning (ML) trains itself by discovering patterns of correlations that can be applied to new inputs. That is a very powerful form of generalization, but it is also very different from the sort of generalization that the west has valorized as the highest form of truth, such as universal laws in some of the sciences, or ethical principles and frameworks in moral reasoning. Machine learning’s generalizations synthesize the general and the particular in a new way, creating a multidimensional model that often retains more of the complex differentiating patterns it has uncovered in the training process than the human mind can grasp. Particulars speak louder in these models than they do in traditional generalizing frameworks. This creates an odd analogy with recent movements in moral philosophy, particularly the feminist ethics of care which rejects the application of general moral frameworks in favor of caring responses to the particular needs and interests of those affected by a moral decision. This paper suggests that our current wide-spread and justified worries about ML’s inexplicability—primarily arising from its reliance on staggeringly complex patterns of particulars—may be preparing our culture more broadly for a valorizing of particulars as at least as determinative as generalizations, and that this might help further advance the importance of particulars in ideas such as those put forward by the ethics of care.</jats:p>"
10.31219/osf.io/n7y2g,The Debate on the Ethics of AI in Health Care: a Reconstruction and Critical Review,"<p>Healthcare systems across the globe are struggling with increasing costs and worsening outcomes. This presents those responsible for overseeing healthcare with a challenge. Increasingly, policymakers, politicians, clinical entrepreneurs and computer and data scientists argue that a key part of the solution will be ‘Artificial Intelligence’ (AI) – particularly Machine Learning (ML). This argument stems not from the belief that all healthcare needs will soon be taken care of by “robot doctors.” Instead, it is an argument that rests on the classic counterfactual definition of AI as an umbrella term for a range of techniques that can be used to make machines complete tasks in a way that would be considered intelligent were they to be completed by a human. Automation of this nature could offer great opportunities for the improvement of healthcare services and ultimately patients’ health by significantly improving human clinical capabilities in diagnosis, drug discovery, epidemiology, personalised medicine, and operational efficiency. However, if these AI solutions are to be embedded in clinical practice, then at least three issues need to be considered: the technical possibilities and limitations; the ethical, regulatory and legal framework; and the governance framework. In this article, we report on the results of a systematic analysis designed to provide a clear overview of the second of these elements: the ethical, regulatory and legal framework. We find that ethical issues arise at six levels of abstraction (individual, interpersonal, group, institutional, sectoral, and societal) and can be categorised as epistemic, normative, or overarching. We conclude by stressing how important it is that the ethical challenges raised by implementing AI in healthcare settings are tackled proactively rather than reactively and map the key considerations for policymakers to each of the ethical concerns highlighted.</p>"
10.3998/jpe.1175,How AI Can Aid Bioethics,"<jats:p>This paper explores some ways in which artificial intelligence (AI) could be used to improve human moral judgments in bioethics by avoiding some of the most common sources of error in moral judgment, including ignorance, confusion, and bias. It surveys three existing proposals for building human morality into AI: Top-down, bottom-up, and hybrid approaches. Then it proposes a multi-step, hybrid method, using the example of kidney allocations for transplants as a test case. The paper concludes with brief remarks about how to handle several complications, respond to some objections, and extend this novel method to other important moral issues in bioethics and beyond.</jats:p>"
10.1017/9781009072168.010,AI and Corporate Law,N/A
10.2196/56665,Ethics and Governance of Neurotechnology in Africa: Lessons From AI,"<jats:p>As a novel technology frontier, neurotechnology is revolutionizing our perceptions of the brain and nervous system. With growing private and public investments, a thriving ecosystem of direct-to-consumer neurotechnologies has also emerged. These technologies are increasingly being introduced in many parts of the world, including Africa. However, as the use of this technology expands, neuroethics and ethics of emerging technology scholars are bringing attention to the critical concerns it raises. These concerns are largely not new but are uniquely amplified by the novelty of technology. They include ethical and legal issues such as privacy, human rights, human identity, bias, autonomy, and safety, which are part of the artificial intelligence ethics discourse. Most importantly, there is an obvious lack of regulatory oversight and a dearth of literature on the consideration of contextual ethical principles in the design and application of neurotechnology in Africa. This paper highlights lessons African stakeholders need to learn from the ethics and governance of artificial intelligence to ensure the design of ethically responsible and socially acceptable neurotechnology in and for Africa.</jats:p>"
10.4018/979-8-3693-3719-6.ch001,Advances in AI and Machine Learning for Healthcare Informatics,"<jats:p>This study focuses on how medical informatics, bioinformatics and health records are getting rejuvenated by AI and ML, which is leading to a new paradigm in healthcare. It drives home the significance of data-driven methods to address the health problems and enhancing the patient outcomes in healthcare. The study begins with a highlight of AI and ML technologies being used in healthcare. Then it delves deep into the machine learning approaches, appropriate optimisation techniques, methods of disease prediction, and signal processing advances. Besides that, this paper obtains new technological developments such as the improvement of healthcare information systems and the application of artificial intelligence and machine learning to medical signal and image processing. By means of this purpose, one aims to teach scholars, specialists, and decision-makers concerning the transformative capacity of AI and ML in healthcare. Ethical surrounding matters and the future pathways of research and innovation are also discussed.</jats:p>"
10.31178/sc.16.1.03,Perceptions on AI creativity in the pre-generative AI era. Insights from computer scientists and artists,N/A
10.1145/3643834.3660677,Responding to Generative AI Technologies with Research-through-Design: The Ryelands AI Lab as an Exploratory Study,N/A
10.4018/979-8-3693-0831-8.ch012,Screaming Out Loud in the Communication Classroom,"<jats:p>This chapter addresses the role that stereotypes play in the advent of the AI age as specifically evidenced in AI generated images and the implications of it in communication classrooms. AI and human information processing share similar elements: learning, perceiving, and reproducing, and they feed into each other in order to perpetuate stereotypes. In our literature review we draw a comparison and establish a relationship between the fallibility of human cognitive information processing and the production of stereotypical images that AI seems to unabashedly scream back to us. Using several Asians stereotypes as a case study, we experiment with several AI image and video generators: Dall-E, Midjourney, and Pika Art. Different prompts are used to analyze the stereotypical nature of the images generated by these AI platforms. In our discussion section, first, we address the collaborative and hegemonic human-AI interactive problem and highlight two implications of such interactions: the invisibility of historically marginalized groups and harm done to target groups. Second, we discuss the double bind that we have identified through this work, which precludes offering clear cut solutions. Third, by applying what we have analyzed and synthesized in communication classrooms, we argue that communication teachers and students can benefit from image and video generating AI but need to focus on the potential risks of AI in reproducing and spreading harmful stereotypes about less visible, less powerful groups. We conclude that image and video generating AI reveals deeply rooted unconscious biases in people's minds and screams them out loud. Learning how to identify them, and better yet, how to prevent them from the beginning, is critical AI literacy that communication students and future communication and media professionals want to equip themselves with.</jats:p>"
10.1007/s10551-021-05032-7,Are Algorithmic Decisions Legitimate? The Effect of Process and Outcomes on Perceptions of Legitimacy of AI Decisions,"<jats:title>Abstract</jats:title><jats:p>Firms use algorithms to make important business decisions. To date, the algorithmic accountability literature has elided a fundamentally empirical question important to business ethics and management: Under what circumstances, if any, are algorithmic decision-making systems considered <jats:italic>legitimate</jats:italic>? The present study begins to answer this question. Using factorial vignette survey methodology, we explore the impact of decision importance, governance, outcomes, and data inputs on perceptions of the legitimacy of algorithmic decisions made by firms. We find that many of the procedural governance mechanisms in practice today, such as notices and impact statements, do not lead to algorithmic decisions being perceived as more legitimate in general, and, consistent with legitimacy theory, that algorithmic decisions with good outcomes are perceived as more legitimate than bad outcomes. Yet, robust governance, such as offering an appeal process, can create a legitimacy dividend for decisions with bad outcomes. However, when arbitrary or morally dubious factors are used to make decisions, most legitimacy dividends are erased. In other words, companies cannot overcome the legitimacy penalty of using arbitrary or morally dubious factors, such as race or the day of the week, with a good outcome or an appeal process for individuals. These findings add new perspectives to both the literature on legitimacy and policy discussions on algorithmic decision-making in firms.
</jats:p>"
10.1093/oxfordhb/9780190067397.013.40,Artificial Intelligence and Inequality in the Middle East,"<p>This chapter looks at the challenges, opportunities, and tensions facing the equitable development of artificial intelligence (AI) in the MENA region in the aftermath of the Arab Spring. While diverse in their natural and human resource endowments, countries of the region share a commonality in the predominance of a youthful population amid complex political and economic contexts. Rampant unemployment—especially among a growing young population—together with informality, gender, and digital inequalities, will likely shape the impact of AI technologies, especially in the region’s labor-abundant resource-poor countries. The chapter then analyzes issues related to data, legislative environment, infrastructure, and human resources as key inputs to AI technologies which in their current state may exacerbate existing inequalities. Ultimately, the promise for AI technologies for inclusion and helping mitigate inequalities lies in harnessing grounds-up youth entrepreneurship and innovation initiatives driven by data and AI, with a few hopeful signs coming from national policies.</p>"
10.1007/s43681-021-00104-w,Identifying key ethical debates for autonomous robots in agri-food: a research agenda,"<jats:title>Abstract</jats:title><jats:p>Agribusinesses are investing in different forms of AI robots, as there is a lot of hope that these machines will help meet the challenges within the agricultural industry, which is to efficiently produce more food for a growing world population. AI robots are expected to enhance production, while compensating for lack of manpower, reducing production costs, taking over unattractive (risky, heavy, and dirty) jobs and reducing the burden of food production on the environment. In spite of these promises, however, AI robots for agri-food also give rise to ethical questions and concerns, which have been little researched and discussed until now. To fill this gap, we developed a research agenda for future research in this area. To do this, we opened our analysis to focus on ethics AI robots generally to specifically identify which of these issues are most relevant to agro-robots. The question we want to find an answer to is: what are the most relevant ethical questions raised about AI robots for robots developed for the agri-food sector? And which questions are not mentioned in the literature, which are particularly relevant for agro-robots? Our paper will provide an overview over the key issues and areas which deserve further elaboration to come to a more mature ethics of AI agro-robots.</jats:p>"
10.1007/s11948-024-00486-0,AI Through Ethical Lenses: A Discourse Analysis of Guidelines for AI in Healthcare,"<jats:title>Abstract</jats:title><jats:p>While the technologies that enable Artificial Intelligence (AI) continue to advance rapidly, there are increasing promises regarding AI’s beneficial outputs and concerns about the challenges of human–computer interaction in healthcare. To address these concerns, institutions have increasingly resorted to publishing AI guidelines for healthcare, aiming to align AI with ethical practices. However, guidelines as a form of written language can be analyzed to recognize the reciprocal links between its textual communication and underlying societal ideas. From this perspective, we conducted a discourse analysis to understand how these guidelines construct, articulate, and frame ethics for AI in healthcare. We included eight guidelines and identified three prevalent and interwoven discourses: (1) AI is unavoidable and desirable; (2) AI needs to be guided with (some forms of) principles (3) trust in AI is instrumental and primary. These discourses signal an over-spillage of technical ideals to AI ethics, such as over-optimism and resulting hyper-criticism. This research provides insights into the underlying ideas present in AI guidelines and how guidelines influence the practice and alignment of AI with ethical, legal, and societal values expected to shape AI in healthcare.</jats:p>"
10.1007/s43681-021-00107-7,Towards responsible media recommendation,"<jats:title>Abstract</jats:title><jats:p>Reading or viewing recommendations are a common feature on modern media sites. What is shown to consumers as recommendations is nowadays often automatically determined by AI algorithms, typically with the goal of helping consumers discover relevant content more easily. However, the highlighting or filtering of information that comes with such recommendations may lead to undesired effects on consumers or even society, for example, when an algorithm leads to the creation of filter bubbles or amplifies the spread of misinformation. These well-documented phenomena create a need for improved mechanisms for <jats:italic>responsible</jats:italic> media recommendation, which avoid such negative effects of recommender systems. In this research note, we review the threats and challenges that may result from the use of automated media recommendation technology, and we outline possible steps to mitigate such undesired societal effects in the future.</jats:p>"
10.59728/jaie.2022.1.1.72,"Analysis of Trends in ‘AI Ethics’ Research Using Text Mining Techniques: Before and After the Announcement of National Artificial Intelligence Ethics Standards(draft), Focusing on Domestic Journals",N/A
10.35736/jcs.35.2.7,The Study of Consumer Experience and Digital Literacy Effects on Positive Attitudes toward Generative AI Mediated by Digital Self-efficacy,N/A
10.1038/s44319-024-00101-0,Getting real about synthetic data ethics,N/A
10.1007/s00146-024-01875-6,From ethics to epistemology and back again: informativeness and epistemic injustice in explanatory medical machine learning,"<jats:title>Abstract</jats:title><jats:p>In this paper, we discuss epistemic and ethical concerns brought about by machine learning (ML) systems implemented in medicine. We begin by fleshing out the logic underlying a common approach in the specialized literature (which we call the <jats:italic>informativeness account</jats:italic>). We maintain that the informativeness account limits its analysis to the impact of epistemological issues on ethical concerns without assessing the bearings that ethical features have on the epistemological evaluation of ML systems. We argue that according to this methodological approach, epistemological issues are <jats:italic>instrumental</jats:italic> to and <jats:italic>autonomous</jats:italic> of ethical considerations. This means that the informativeness account considers epistemological evaluation uninfluenced and unregulated by an ethical counterpart. Using an example that does not square well into the <jats:italic>informativeness account</jats:italic>, we argue for ethical assessments that have a substantial influence on the epistemological assessment of ML and that such influence should not be understood as merely informative but rather regulatory. Drawing on the case analyzed, we claim that within the theoretical framework of the informativeness approach, forms of epistemic injustice—especially <jats:italic>epistemic objectification</jats:italic>—remain unaddressed. Our analysis should motivate further research investigating the regulatory role that ethical elements play in the epistemology of ML.</jats:p>"
10.1145/3461702.3462608,Unpacking the Expressed Consequences of AI Research in Broader Impact Statements,N/A
10.1007/s00146-021-01277-y,"Morals, ethics, and the technology capabilities and limitations of automated and self-driving vehicles",N/A
10.1145/3306618.3314258,Legible Normativity for AI Alignment,N/A
10.1007/s10676-023-09724-8,The contested role of AI ethics boards in smart societies: a step towards improvement based on board composition by sortition,"<jats:title>Abstract</jats:title><jats:p>The recent proliferation of AI scandals led private and public organisations to implement new ethics guidelines, introduce AI ethics boards, and list ethical principles. Nevertheless, some of these efforts remained a façade not backed by any substantive action. Such behaviour made the public question the legitimacy of the AI industry and prompted scholars to accuse the sector of ethicswashing, machinewashing, and ethics trivialisation—criticisms that spilt over to institutional AI ethics boards. To counter this widespread issue, contributions in the literature have proposed fixes that do not consider its systemic character and are based on a top-down, expert-centric governance. To fill this gap, we propose to make use of <jats:italic>qualified informed lotteries</jats:italic>: a two-step model that transposes the documented benefits of the ancient practice of sortition into the selection of AI ethics boards’ members and combines them with the advantages of a stakeholder-driven, participative, and deliberative bottom-up process typical of Citizens’ Assemblies. The model permits increasing the public’s legitimacy and participation in the decision-making process and its deliverables, curbing the industry’s over-influence and lobbying, and diminishing the instrumentalisation of ethics boards. We suggest that this sortition-based approach may provide a sound base for both public and private organisations in smart societies for constructing a decentralised, bottom-up, participative digital democracy.</jats:p>"
10.4018/979-8-3693-3597-0.ch017,Exploring the Role of Generative Adversarial Networks in Cybersecurity,"<jats:p>To protect the vast amounts of data being stored on computers and transported over networks, cybersecurity is crucial. To prevent increasingly complex assaults in the future, methods for identifying threats must be regularly updated. Experts in security are using GAN to accomplish results in password cracking, anomaly generation, and intrusion detection. The creative use of GANs in cybersecurity is explored in this chapter. An overview of GANs and their importance in strengthening cyber defense mechanisms is presented in the introduction. An extensive literature analysis to clarify the current state of knowledge is provided after a detailed examination of traditional GAN techniques. The chapter clarifies the GAN's architectural subtleties and highlights its applicability to cybersecurity scenarios. Additionally, study of variety of GAN applications in cybersecurity. It classifies different kinds of GANs and frameworks used in their application. It studied the intrusion detection-based GAN model with its parameters results compared with the existing study.</jats:p>"
10.1007/s00146-017-0699-2,The human relationship in the ethics of robotics: a call to Martin Buber’s I and Thou,N/A
10.1145/3600211.3604693,"Democratising AI: Multiple Meanings, Goals, and Methods",N/A
10.1145/3278721.3278737,Software Malpractice in the Age of AI,N/A
10.1007/978-3-030-04468-8_13,AI and Ethics,N/A
10.1108/s2398-601820230000010029,The Explainability and Transparency of AI Monitoring Tools,N/A
10.1017/s0963180123000464,Artificial Intelligence and Human Enhancement: Can AI Technologies Make Us More (Artificially) Intelligent?,"<jats:title>Abstract</jats:title><jats:p>This paper discusses two opposing views about the relation between artificial intelligence (AI) and human intelligence: on the one hand, a worry that heavy reliance on AI technologies might make people less intelligent and, on the other, a hope that AI technologies might serve as a form of cognitive enhancement. The worry relates to the notion that if we hand over too many intelligence-requiring tasks to AI technologies, we might end up with fewer opportunities to train our own intelligence. Concerning AI as a potential form of cognitive enhancement, the paper explores two possibilities: (1) AI as extending—and thereby enhancing—people’s minds, and (2) AI as enabling people to behave in artificially intelligent ways. That is, using AI technologies might enable people to behave as if they have been cognitively enhanced. The paper considers such enhancements both on the level of individuals and on the level of groups.</jats:p>"
10.1109/mts.2019.2915154,AI Ethics in Predictive Policing: From Models of Threat to an Ethics of Care,N/A
10.15801/je.1.115.201709.133,Civic Ethics and the Implication of Moral Education in the period of Robots and Artificial Intelligence -with Emphasis on Utilizing AI Robots-,N/A
10.1007/s11948-021-00348-z,Fairness as Equal Concession: Critical Remarks on Fair AI,N/A
10.1136/medethics-2018-105118,Computer knows best? The need for value-flexibility in medical AI,"<jats:p>Artificial intelligence (AI) is increasingly being developed for use in medicine, including for diagnosis and in treatment decision making. The use of AI in medical treatment raises many ethical issues that are yet to be explored in depth by bioethicists. In this paper, I focus specifically on the relationship between the ethical ideal of shared decision making and AI systems that generate treatment recommendations, using the example of IBM’s Watson for Oncology. I argue that use of this type of system creates both important risks and significant opportunities for promoting shared decision making. If value judgements are fixed and covert in AI systems, then we risk a shift back to more paternalistic medical care. However, if designed and used in an ethically informed way, AI could offer a potentially powerful way of supporting shared decision making. It could be used to incorporate explicit value reflection, promoting patient autonomy. In the context of medical treatment, we need value-flexible AI that can both respond to the values and treatment goals of individual patients and support clinicians to engage in shared decision making.</jats:p>"
10.4018/978-1-6684-9196-6.ch008,Towards an Ethics Framework for Learning Analytics,"<jats:p>This chapter proposed a learning analytics (LA) ethics framework to inform the design and implementation of an ethics-based LA system for tertiary institutions. A background to ethics of LA is provided, ethical approaches discussed, and philosophies explained, followed by an explanation of various ethical dilemmas in LA. A brief overview of ethical framework considerations is given, followed by an overview of three ethical frameworks from practice. An LA maturity measuring instrument is proposed before an LA ethics framework culminates this research. The LA ethics framework can be used towards the development of a specific ethical LA framework for tertiary institutions.</jats:p>"
10.1007/978-3-031-55744-6_3,MAI: A Very Short History and the State of the Art,N/A
10.15801/je.1.114.201709.133,Civic Ethics and the Implication of Moral Education in the period of Robots and Artificial Intelligence -with Emphasis on Utilizing AI Robots-,N/A
10.4018/979-8-3693-3860-5.ch010,Data Ethics and Privacy,"<jats:p>The publicity about information does not appear to decrease nor do the disgraces. Confidentiality openings in the collection, use, and distribution of data have affected all the major technology users, be it Facebook, Google, and AI go beyond the business world with administrations, cities, and educational and health organizations. However, with the speedy growth of social media and advanced technology such as mobile phone apps, various investors gather and use great quantities of data, ignoring ethics and privacy. This chapter explores and discusses the ethical considerations surrounding data collection and use, with a specific focus on privacy concerns related to Large Language Model (LLM) training data. The research further discovers topics such as anonymization, data rights, and consensus, aiming to highlight the importance of ethical practices in conducting data. Additionally, the inclusion of case studies on data misuse and privacy breaches serves to provide real-world examples, emphasizing the need for vigilance and responsible approaches in the realm of data collection and utilization.</jats:p>"
10.1007/978-3-030-81907-1_1,Introduction – The Importance of an Ethics-First Approach to the Development of AI,N/A
10.1089/aipo.2024.29005.cfp,<i>Call for Special Issue Papers:</i> Ethics and Regulation of AI in Precision Oncology,N/A
10.1007/s00146-023-01653-w,No such thing as one-size-fits-all in AI ethics frameworks: a comparative case study,N/A
10.1145/3514094.3534131,A Survey of the Potential Long-term Impacts of AI,N/A
