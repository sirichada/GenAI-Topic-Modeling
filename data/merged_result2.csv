id,label,Title,Abstract,assigned_topic
1,10.70715/jitcai.2024.v1.i1.004,The Impact of GenAI on Student Engagement and Ethics in Higher Education,"<jats:p>The rapid adoption of AI (AI) in higher education is reshaping students’ learning experiences, with tools such as GenAI, Grammarly, and Microsoft Copilot becoming integral to academic work. This study, informed by data from the Digital Education Council Global AI Student Survey 2024, examines the impact of AI on students, focusing on usage patterns, trust in AI-generated content, ethical awareness, and expectations for institutional support. Findings indicate that 86% of students use AI for various academic tasks, with a majority expressing concerns about trust, fairness, and over-reliance on AI. While students value AI’s benefits, only 5% are fully aware of institutional guidelines on AI use, and 72% desire more AI literacy courses, reflecting a significant need for comprehensive support in navigating AI responsibly. The study underscores the importance of clear ethical guidelines, faculty training, and student involvement in AI policy formation to foster responsible AI use and preserve academic integrity. These insights offer valuable guidance for educators and policymakers seeking to integrate AI ethically and effectively into higher education.</jats:p>",1.0
2,10.7256/2454-0625.2024.6.70926,"Socio-cultural risks of multimodal large generative models of ""AI"" (GenAI)","<jats:p>
 The article is devoted to the study of the conditions for ensuring the information security of Russian citizens when using generative ""AI"" technologies in the socio-cultural sphere. The relevance of the topic is due to the modern high rates of development of computer neural networks that generate multimedia content: texts, images, sounds and videos. The developers classify generative technologies as ""AI"", position them as a ""new nuclear project"" capable of radically increasing the productivity of socio-cultural creativity, and receive significant government, corporate and investment financing. The object of the study is modern multimedia generative models, the subject of the study is the possibility of their use in the socio-cultural sphere of creativity and the associated risks of information security. The purpose of the study is to determine the conditions for ensuring the information security of Russian citizens when using multimodal generative technologies in the socio-cultural sphere.  The research materials are scientific publications of recent years (2021–2024) in Russian journals of the HAC list (categories K1, K2) and international Scopus publications (quartiles Q1, Q2) devoted to research and critical analysis of the possibilities of multimodal generative models, associated risks and security tools. The philosophical methodology is applied: theoretical and cultural analysis, synthesis.  The scientific novelty of the article is due to the application of a philosophical theoretical and cultural methodology for a critical comparison of the declarations of developers and the actual potential of applications of multimodal generative technologies. The result of the study is an assessment of how greatly exaggerated the risks predicted based on the positioning of the technologies in question as ""AI"". The real risks are proposed to include: the incompatibility of development costs with the usefulness of the results; lowering the cultural level of professional and amateur creativity and worsening the tastes of the mass audience; use in ""social engineering"", fraud, mass disinformation, fake news, manipulation of public consciousness, ""cancellation culture"", destruction of traditional values and substitution of socio-cultural identity. The means of ensuring the safety of Russian citizens in the development and use of multimedia generative technologies in the socio-cultural sphere are recommended.
	</jats:p>",2.0
3,10.51191/issn.2637-1898.2024.7.12.12,AI: Duality in Applications of GenAI and Assistive AI in Music,"<jats:p>This paper explores the multifaceted role of AI in the field of music, more specifically, examining the positives and negatives of generative and assistive capacities. AI (AI) in music involves the application of computational techniques to various aspects of music creation, production and consumption. In the domain of assistive AI, the concentration is on how machine learning could potentially help musicians in the area of composition and performance to enhance their musical creativity. The paper will discuss an interesting collaborative effort between pure human creativity and computational assistance covering an explanation for a vast number of tools using generative as well as assistive AI models.

In addition, the paper will address the concerns facing the music industry while this technology keeps on improving, the potential drawbacks and ethical considerations. It opens the question of authenticity and emotional depth, and when or if this new technology could be able to replicate it. Further explanation in the paper will consider music examples with a focus on music styles assisted and generated by the use of AI, from pop to classical music.

With a thorough analysis of the aforementioned subject, the paper aims to provide a detailed perspective on the constant evolution of AI tools used in music with highlights on the need for a balanced approach. In providing a detailed perspective on the evolving landscape of AI tools in music, this study adopts a methodological approach that involves comprehensive analysis of both the benefits and challenges associated with these innovative gadgets. The paper contributes to the ongoing discussion on the intersection of technology and artistic expression. By examining the potential benefits and challenges with these innovative models, the paper signifies ongoing discourse on the impact of technology on artistic expression.</jats:p>",2.0
4,10.1007/s43681-022-00176-2,Review of the state of the art in autonomous AI,"<jats:title>Abstract</jats:title><jats:p>This article presents a new design for autonomous AI (AI), based on the state-of-the-art algorithms, and describes a new autonomous AI system called ‘AutoAI’. The methodology is used to assemble the design founded on self-improved algorithms that use new and emerging sources of data (NEFD). The objective of the article is to conceptualise the design of a novel AutoAI algorithm. The conceptual approach is used to advance into building new and improved algorithms. The article integrates and consolidates the findings from existing literature and advances the AutoAI design into (1) using new and emerging sources of data for teaching and training AI algorithms and (2) enabling AI algorithms to use automated tools for training new and improved algorithms. This approach is going beyond the state-of-the-art in AI algorithms and suggests a design that enables autonomous algorithms to self-optimise and self-adapt, and on a higher level, be capable to self-procreate.</jats:p>",-1.0
5,10.1007/s43681-022-00201-4,Assessing the ethical and social concerns of AI in neuroinformatics research: an empirical test of the European Union Assessment List for Trustworthy AI (ALTAI),"<jats:title>Abstract</jats:title><jats:p>Ethical and social concerns are a key obstacle to the adoption of AI (AI) in the life sciences and beyond. The discussion of these issues has intensified in recent years and led to a number of approaches, tools and initiatives. Key amongst them is the idea of ex-ante impact assessments that aim to identify issues at the early stages of development. One prominent example of such ex-ante impact assessment is the European Union's (EU) Assessment list for Trustworthy AI (ALTAI). This article uses the findings of a large-scale application of the ALTAI to a large neuro-informatics project as an exemplar to demonstrate the effectiveness and limitations of the ALTAI in practice. The article shows that ex-ante impact assessment has the potential to help identify and address ethical and social issues. However, they need to be understood as part of a broader socio-technical ecosystem of AI. For ALTAI and related approaches to be useful in bio-medical research, they should be interpreted from a systems theory perspective which allows for their integration into the rich set of tools, legislation and approaches. The paper argues that ex-ante impact assessments have the best chance of being successful if seen applied in conjunction with other approaches in the context of the overall AI ecosystem.</jats:p>",0.0
6,10.7200/esicm.55.333,transformative potential of GenAI  (GenAI) in business,"<jats:p>Objective:This study investigates the transformative potential of GenAI(GenAI) within the business domain and the entrepreneurial activity.Methodology:A comprehensive research design is adopted, integrating text-mining techniques to analysedata obtained from publicly available innovation repositories. A systematic literaturereview (SLR) is developed based on the literature obtained from all databases indexedin Web of Science (WoS), incorporating preprints from arXiv, alongside industry-relatedinnovation data in the form of patents from Google Patents. This method enables the derivationof valuable insights regarding the impact and prospective developments of GenAIacross diverse business sectors and industries by leveraging Natural Language Processing(NLP) and network analysis.Results:The research outcomes highlight the significant potential of GenAI in enabling informeddecision-making, enhancing productivity, and revealing new growth opportunities inthe business landscape. The continuously evolving business environment is examined,emphasising GenAI's role as a catalyst for data-driven innovation. However, there are stillrelevant limitations to overcome.Limitations:The selection of data sources and the study period may have excluded relevant or recentlypublished articles and patents within the scope of the present research. The language ofthe databases analysed is only English.Practical Implications:The practical implications of this study carry significant weight, serving as a valuableresource for decision-makers, researchers, and practitioners navigating the constantlyshifting terrain of business innovation through the lens of GenAI. Understanding thepotential advantages and challenges associated with GenAI adoption equips stakeholdersto make informed decisions and develop future business strategies.</jats:p>",4.0
7,10.1007/s43681-024-00492-9,"AI, the common good, and the democratic deficit in AI governance","<jats:title>Abstract</jats:title><jats:p>There is a broad consensus that AI should contribute to the common good, but it is not clear what is meant by that. This paper discusses this issue and uses it as a lens for analysing what it calls the “democracy deficit” in current AI governance, which includes a tendency to deny the inherently political character of the issue and to take a technocratic shortcut. It indicates what we may agree on and what is and should be up to (further) deliberation when it comes to AI ethics and AI governance. Inspired by the republican tradition in political theory, it also argues for a more active role of citizens and (end-)users: not only as participants in deliberation but also in ensuring, creatively and communicatively, that AI contributes to the common good.</jats:p>",0.0
8,10.23962/ajic.i33.18162,Risks of GenAI (GenAI)-assisted scams on online sharing-economy platforms,"<jats:p>The prevalence of scams proliferating via online platforms has been identified as an emerging societal problem resulting in large-scale financial losses for victims. Online scams typically rely for their success on the generation of fake but convincing user profiles to conceal the identities of the scammers from the people being tricked into parting with their money. The increasing sophistication of GenAI (GenAI), which can produce outputs indistinguishable from real content, thus carries the risk of being adopted by fraudsters to assist in the enactment of online scams. This article considers the risks of the potential uptake and use of GenAI applications by online scammers operating in the sharing economy, with a focus on homestay-marketplace platforms and, in particular, the largest such platform, Airbnb.</jats:p>",-1.0
9,10.1007/s43681-020-00022-3,Management perspective of ethics in AI,"<jats:title>Abstract</jats:title><jats:p>This research addressed the management awareness about the ethical and moral aspects of AI (AI). It is a general trend to speak about AI, and many start-ups and established companies are communicating about the development and implementation of AI solutions. Therefore, it is important to consider different perspectives besides the technology and data as the key elements for AI systems. The way in which societies are interacting and organising themselves will change. Such transformations require diverse perspectives from the society and particularly from AI system developers for shaping the humanity of the future. This research aimed to overcome this barrier with the answers for the question: What kind of awareness does the management of AI companies have about the social impact of its AI product or service? The central research question was divided into five sub-questions that were answered by a fundamental literature review and an empirical research study. This covered the management understanding of the terms moral, ethics, and AI; the internal company prioritization of moral and ethics; and the involved stakeholders in the AI product or service development. It analysed the known and used ethical AI guidelines and principles. In the end, the social responsibility of the management regarding AI systems was analysed and compared.</jats:p>",0.0
10,10.1007/s43681-023-00330-4,Ethics by design for AI,"<jats:title>Abstract</jats:title><jats:p>In this paper, we present an approach for the systematic and comprehensive inclusion of ethical considerations in the design and development process of AI systems, called Ethics by Design for AI (EbD-AI). The approach is the result of a three-year long research effort, and has recently be adopted by the European Commission as part of its ethics review procedure for AI projects. We describe and explain the approach and its different components and its application to the development of AI software and systems. We also compare it to other approaches in AI ethics, and we consider limitations of the approach as well as potential criticisms.</jats:p>",0.0
11,10.1007/s43681-024-00547-x,AI and its ‘slow violence’ to human rights,"<jats:title>Abstract</jats:title><jats:p>Human rights concerns in relation to the impacts brought forth by AI (‘AI’) have revolved around examining how it affects specific rights, such as the right to privacy, non-discrimination and freedom of expression. However, this article argues that the effects go deeper, potentially challenging the foundational assumptions of key concepts and normative justifications of the human rights framework. To unpack this, the article applies the lens of ‘slow violence’, a term borrowed from environmental justice literature, to frame the grinding, gradual, attritional harms of AI towards the human rights framework.</jats:p><jats:p>The article examines the slow violence of AI towards human rights at three different levels. First, the individual as the subject of interest and protection within the human rights framework, is increasingly unable to understand nor seek accountability for harms arising from the deployment of AI systems. This undermines the key premise of the framework which was meant to empower the individual in addressing large power disparities and calling for accountability towards such abuse of power. Secondly, the ‘slow violence’ of AI is also seen through the unravelling of the normative justifications of discrete rights such as the right to privacy, freedom of expression and freedom of thought, upending the reasons and assumptions in which those rights were formulated and formalised in the first place. Finally, the article examines how even the wide interpretations towards the normative foundation of human rights, namely human dignity, is unable to address putative new challenges AI poses towards the concept. It then considers and offers the outline to critical perspectives that can inform a new model of human rights accountability in the age of AI.</jats:p>",0.0
12,10.1007/s43681-024-00427-4,AI (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI,"<jats:title>Abstract</jats:title><jats:p>As AI (AI) rapidly advances and integrates into various domains, cybersecurity emerges as a critical field grappling with both the benefits and pitfalls of AI technologies. This paper explores the multifaceted dimensions of AI-driven cyberattacks, offering insights into their implications, mitigation strategies, underlying motivations, and profound societal impacts. The research centres on developing and presenting the AI Cybersecurity Dimensions (AICD) Framework, a comprehensive, multidimensional schema designed to guide academics, policymakers, and industry professionals in understanding and combating the evolving challenges posed by AI-driven cyber threats. The research unveils the complex dynamics of offensive AI, stressing the need for adaptive defences and ethical considerations. Concurrently, the study highlights adversarial AI threats, calling for proactive measures to address their potential ramifications. Through rigorous textual analyses and extensive literature reviews, the paper underscores the urgency for interdisciplinary approaches to bridge the technology-humanity chasm traditionally observed in cybersecurity discussions. By synthesising these diverse elements, the AICD Framework emerges as an instrumental tool for holistic understanding and practical interventions in the AI-infused cybersecurity landscape. The paper concludes with an urgent call for collaborative efforts in research and practice to navigate the intricate challenges and capitalise on the opportunities borne from the convergence of AI and cybersecurity.</jats:p>",8.0
13,10.1007/s00146-024-02007-w,Intentionality gap and preter-intentionality in GenAI,"<jats:title>Abstract</jats:title><jats:p>The emergence of GenAI, such as large language models and text-to-image models, has had a profound impact on society. The ability of these systems to simulate human capabilities such as text writing and image creation is radically redefining a wide range of practices, from artistic production to education. While there is no doubt that these innovations are beneficial to our lives, the pervasiveness of these technologies should not be underestimated, and raising increasingly pressing ethical questions that require a radical resemantization of certain notions traditionally ascribed to humans alone. Among these notions, that of technological intentionality plays a central role. With regard to this notion, this paper first aims to highlight what we propose to define in terms of the intentionality gap, whereby, insofar as, currently, (1) it is increasingly difficult to assign responsibility for the actions performed by AI systems to humans, as these systems are increasingly autonomous, and (2) it is increasingly complex to reconstruct the reasoning behind the results they produce as we move away from good old fashioned AI; it is now even more difficult to trace the intentionality of AI systems back to the intentions of the developers and end users. This gap between human and technological intentionality requires a revision of the concept of intentionality; to this end, we propose here to assign preter-intentional behavior to GenAI. We use this term to highlight how AI intentionality both incorporates and transcends human intentionality; i.e., it goes beyond (preter) human intentionality while being linked to it. To show the merits of this notion, we first rule out the possibility that such preter-intentionality is merely an unintended consequence and then explore its nature by comparing it with some paradigmatic notions of technological intentionality present in the wider debate on the moral (and technological) status of AI.</jats:p>",2.0
14,10.1386/adch_00088_1,Making the case for introducing GenAI (AI) into design curricula,"<jats:p>The use of GenAI (AI) in higher education design programmes is expanding, yet there is little formalized approach to its integration. Professionally, GenAI is starting to become an indispensable tool for ideation and prototyping, two fundamental skills taught in design’s studio pedagogy. Yet this digital leap into the future risks leaving design educators behind unless they take a proactive approach to its implementation and present its strengths and weaknesses. This study surveyed 74 design students from an Australian university, exploring their current utilization of GenAI and their projections for its future application in design practice. Findings confirm that GenAI is being used in an ad hoc way by students to speed up the ideation process tempered by a sceptical view of its creative output. A list of GenAI training for integration into the design curricula based on current research and survey results is proposed.</jats:p>",1.0
15,10.1007/s43681-022-00206-z,Needs and AI,"<jats:title>Abstract</jats:title><jats:p>Throughout our history, we, Homo sapiens, have used technologies to better satisfy our<jats:italic>needs</jats:italic>. The relation between<jats:italic>needs</jats:italic>and<jats:italic>technology</jats:italic>is so fundamental that the US National Research Council defines the distinguishing characteristic of technology as its goal “to make modifications in the world [in order] to meet human needs” [1]. AI (AI) is one of the most promising emerging technologies of our time. Similar to other technologies, AI is expected by many “to meet [human] needs”. In this article, we reflect on the relationship between<jats:italic>needs</jats:italic>and AI, and call for the realization of<jats:italic>needs-aware</jats:italic>AI systems. We argue that re-thinking<jats:italic>needs</jats:italic><jats:italic>for</jats:italic>,<jats:italic>through</jats:italic>,<jats:italic>by</jats:italic>, and<jats:italic>with</jats:italic>AI can be a very useful means towards the development of realistic approaches for sustainable<jats:italic>H</jats:italic>uman-aware,<jats:italic>A</jats:italic>ccountable,<jats:italic>L</jats:italic>awful, and<jats:italic>E</jats:italic>thical (HALE) AI systems. We discuss some of the most critical gaps, barriers, enablers, and drivers of co-creating future AI-based sociotechnical systems in which [human]<jats:italic>needs</jats:italic>are well considered and met. Finally, we provide an overview of potential challenges and considerations that should be carefully taken into account; and call for joint, immediate, and interdisciplinary efforts and collaborations to start on the path to<jats:italic>needs-aware</jats:italic>AI.</jats:p>",0.0
16,10.1007/s43681-022-00221-0,"Garbage in, toxic data out: a proposal for ethical AI sustainability impact statements","<jats:title>Abstract</jats:title><jats:p>Data and autonomous systems are taking over our lives, from healthcare to smart homes very few aspects of our day to day are not permeated by them. The technological advances enabled by these technologies are limitless. However, with advantages so too come challenges. As these technologies encompass more and more aspects of our lives, we are forgetting the ethical, legal, safety and moral concerns that arise as an outcome of integrating our lives with technology. In this work, we study the lifecycle of AI from data gathering to deployment, providing a structured analytical assessment of the potential ethical, safety and legal concerns. The paper then presents the foundations for the first ethical AI sustainability statement to guide future development of AI in a safe and sustainable manner.</jats:p>",-1.0
17,10.1007/s42001-024-00250-1,GenAI against humanity: nefarious applications of GenAI and large language models,"<jats:title>Abstract</jats:title><jats:p>GenAI (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we’ll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI’s nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.</jats:p>",7.0
18,10.1007/s43681-024-00604-5,"Society in charge: the connection of AI, responsibility, and ethics in German media discourse","<jats:title>Abstract</jats:title><jats:p>AI (AI) is playing an increasingly important role in society, and applications like GenAI and Dall-E, which can produce texts and pictures on their own, are becoming very popular. This development raises questions regarding ethics, values, and responsibility, as AI-generated documents may promote misinformation and erode democracy, while human actors can scarcely be held accountable. AI technology may also support an efficient, rationalized society, which has its advantages and disadvantages. Two main spheres, which influence society’s perspective on the connection between AI, ethics and responsibility, are public media debates and the legal system. Popular newspapers reach broad audiences, so insight is provided into what perspectives on these issues are helping everyday citizens form their opinions. Legal frameworks potentially regulate citizens’ and companies’ dealing with AI technology—and may get included in media discussions on AI. Acknowledging that, this article presents a two-folded analysis. First, the article presents the results of a discourse analysis of 113 articles from German newspapers, ranging from the center-left to the conservative spectrum. The analysis examined how these media frame the connection of AI, ethics, values, and responsibility. The article discusses the discourse analysis together with theoretical assumptions around the question, which actors in society could be counted as accountable in AI regards. Second, a discussion of the European AI legal system is added, to evaluate its connection with the media discourses. The article presents the results of both parts of the analysis together and finally discusses further research perspectives.</jats:p>",-1.0
19,10.4018/jdm.2020040105,AI (AI) Ethics,"<p>AI (AI)-based technology has achieved many great things, such as facial recognition, medical diagnosis, and self-driving cars. AI promises enormous benefits for economic growth, social development, as well as human well-being and safety improvement. However, the low-level of explainability, data biases, data security, data privacy, and ethical problems of AI-based technology pose significant risks for users, developers, humanity, and societies. As AI advances, one critical issue is how to address the ethical and moral challenges associated with AI. Even though the concept of “machine ethics” was proposed around 2006, AI ethics is still in the infancy stage. AI ethics is the field related to the study of ethical issues in AI. To address AI ethics, one needs to consider the ethics of AI and how to build ethical AI. Ethics of AI studies the ethical principles, rules, guidelines, policies, and regulations that are related to AI. Ethical AI is an AI that performs and behaves ethically. One must recognize and understand the potential ethical and moral issues that may be caused by AI to formulate the necessary ethical principles, rules, guidelines, policies, and regulations for AI (i.e., Ethics of AI). With the appropriate ethics of AI, one can then build AI that exhibits ethical behavior (i.e., Ethical AI). This paper will discuss AI ethics by looking at the ethics of AI and ethical AI. What are the perceived ethical and moral issues with AI? What are the general and common ethical principles, rules, guidelines, policies, and regulations that can resolve or at least attenuate these ethical and moral issues with AI? What are some of the necessary features and characteristics of an ethical AI? How to adhere to the ethics of AI to build ethical AI?</p>",0.0
20,10.1007/s11948-024-00507-y,Reconstructing AI Ethics Principles: Rawlsian Ethics of AI,"<jats:title>Abstract</jats:title><jats:p>The popularisation of AI (AI) technologies has sparked discussion about their ethical implications. This development has forced governmental organisations, NGOs, and private companies to react and draft ethics guidelines for future development of ethical AI systems. Whereas many ethics guidelines address values familiar to ethicists, they seem to lack in ethical justifications. Furthermore, most tend to neglect the impact of AI on democracy, governance, and public deliberation. Existing research suggest, however, that AI can threaten key elements of western democracies that are ethically relevant. In this paper, Rawls’s theory of justice is applied to draft a set of guidelines for organisations and policy-makers to guide AI development towards a more ethical direction. The goal is to contribute to the broadening of the discussion on AI ethics by exploring the possibility of constructing AI ethics guidelines that are philosophically justified and take a broader perspective of societal justice. The paper discusses how Rawls’s theory of justice as fairness and its key concepts relate to the ongoing developments in AI ethics and gives a proposition of how principles that offer a foundation for operationalising AI ethics in practice could look like if aligned with Rawls’s theory of justice as fairness.</jats:p>",0.0
21,10.1007/s43681-022-00195-z,Operationalising ethics in AI for healthcare: a framework for AI developers,"<jats:title>Abstract</jats:title><jats:p>AI (AI) offers much promise for improving healthcare. However, it runs the looming risk of causing individual and societal harms; for instance, exacerbating inequalities amongst minority groups, or enabling compromises in the confidentiality of patients’ sensitive data. As such, there is an expanding, unmet need for ensuring AI for healthcare is developed in concordance with human values and ethics. Augmenting “principle-based” guidance that highlight adherence to ethical ideals (without necessarily offering translation into actionable practices), we offer a solution-based framework for operationalising ethics in AI for healthcare. Our framework is built from a scoping review of existing solutions of ethical AI guidelines, frameworks and technical solutions to address human values such as self-direction in healthcare. Our view spans the entire length of the AI lifecycle: data management, model development, deployment and monitoring. Our focus in this paper is to collate actionable solutions (whether technical or non-technical in nature), which can be steps that enable and empower developers in their daily practice to ensuring ethical practices in the broader picture. Our framework is intended to be adopted by AI developers, with recommendations that are accessible and driven by the existing literature. We endorse the recognised need for ‘ethical AI checklists’ co-designed with health AI practitioners, which could further operationalise the technical solutions we have collated. Since the risks to health and wellbeing are so large, we believe a proactive approach is necessary for ensuring human values and ethics are appropriately respected in AI for healthcare.</jats:p>",6.0
22,10.1007/s43681-024-00476-9,The obscure politics of AI: a Marxian socio-technical critique of the AI alignment problem thesis,"<jats:title>Abstract</jats:title><jats:p>There is a growing feeling that AI (AI) is getting out of control. Many AI experts worldwide stress that great care must be taken on the so-called <jats:italic>alignment problem</jats:italic>, broadly understood as the challenge of developing AIs whose actions are in line with human values and goals. The story goes that ever more powerful AI systems are escaping human control and might soon operate in a manner that is no longer guided by human purposes. This is what we call the <jats:italic>AI-out-of-control discourse</jats:italic> which, in this paper, we critically examine and debunk. Drawing on complementary insights from political theory, socio-technical studies and Marxian political economy, we critique the supposed animistic and autonomous nature of AI, and the myth of the uncontrollability of AI. The problem is not that humanity has lost control over AI, but that only a minority of powerful stakeholders are controlling its creation and diffusion, through politically undemocratic processes of decision-making. In these terms, we reframe the alignment problem thesis with an emphasis on citizen engagement and public political participation. We shed light on the existing politics of AI and contemplate alternative political expressions whereby citizens steer AI development or stop it in the first place.</jats:p>",0.0
23,10.1007/s43681-024-00442-5,AI at sentencing: when do algorithms perform well enough to replace humans?,"<jats:title>Abstract</jats:title><jats:p>AI is currently supplanting the work of humans in many societal contexts. The purpose of this article is to consider the question of when algorithmic tools should be regarded as performing sufficiently well to replace human judgements and decision-making at sentencing. More precisely, the question as to which are the ethically plausible criteria for the comparative performance assessments of algorithms and humans is considered with regard to both risk assessment algorithms that are designed to provide predictions of recidivism and sentencing algorithms designed to determine sentences in individual criminal cases. It is argued, first, that the prima facie most obvious assessment criteria do not stand up to ethical scrutiny. Second, that ethically plausible criteria presuppose ethical theory on penal distribution which currently has not been sufficiently developed. And third, that the current lack of assessment criteria has comprehensive implications regarding when algorithmic tools should be implemented in criminal justice practice.</jats:p>",0.0
24,10.1007/s43681-022-00239-4,"Democracy, epistemic agency, and AI: political epistemology in times of AI","<jats:title>Abstract</jats:title><jats:p>Democratic theories assume that citizens have some form of political knowledge in order to vote for representatives or to directly engage in democratic deliberation and participation. However, apart from widespread attention to the phenomenon of fake news and misinformation, less attention has been paid to <jats:italic>how</jats:italic> they are supposed to acquire that knowledge in contexts shaped by AI and related digital technologies. While this topic can also be approached from an empirical angle, this paper contributes to supporting concerns about AI and democracy by looking at the issue through the lens of political epistemology, in particular using the concept of epistemic agency. It argues that AI (AI) endangers democracy since it risks to diminish the epistemic agency of citizens and thereby undermine the relevant kind of political agency in democracy. It shows that next to fake news and manipulation by means of AI analysis of big data, epistemic bubbles and the defaulting of statistical knowledge endanger the epistemic agency of citizens when they form and wish to revise their political beliefs. AI risks to undermine trust in one’s own epistemic capacities and hinder the exercise of those capacities. If we want to protect the knowledge basis of our democracies, we must address these problems in education and technology policy.</jats:p>",0.0
25,10.1007/s43681-021-00091-y,Coarse ethics: how to ethically assess explainable AI,"<jats:title>Abstract</jats:title><jats:p>The integration of AI (AI) into human society mandates that their decision-making process is explicable to users, as exemplified in Asimov’s Three Laws of Robotics. Such human interpretability calls for explainable AI (XAI), of which this paper cites various models. However, the transaction between computable accuracy and human interpretability can be a trade-off, requiring answers to questions about the negotiable conditions and the degrees of AI prediction accuracy that may be sacrificed to enable user-interpretability. The extant research has focussed on technical issues, but it is also desirable to apply a branch of ethics to deal with the trade-off problem. This scholarly domain is labelled<jats:italic>coarse ethics</jats:italic>in this study, which discusses two issues vis-à-vis AI prediction as a type of evaluation. First, which formal conditions would allow trade-offs? The study posits two minimal requisites: adequately high coverage and order-preservation. The second issue concerns conditions that could justify the trade-off between computable accuracy and human interpretability, to which the study suggests two justification methods: impracticability and adjustment of perspective from machine-computable to human-interpretable. This study contributes by connecting ethics to autonomous systems for future regulation by formally assessing the adequacy of AI rationales.</jats:p>",0.0
26,10.61969/jai.1337500,Education in the Era of GenAI (AI): Understanding the Potential Benefits of GenAI in Promoting Teaching and Learning,"<jats:p xml:lang=""en"">Since its maiden release into the public domain on November 30, 2022, GenAI garnered more than one million subscribers within a week. The GenAI tool ⎼GenAI took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of GenAI to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of GenAI in promoting teaching and learning. Benefits of GenAI include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the GenAI such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how GenAI could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving GenAI tools could be used safely and constructively to improve education and support students’ learning.</jats:p>",1.0
27,10.1097/ncm.0000000000000681,AI and Case Management: From AI to Generative Intelligence,<jats:p>Conversations about AI (AI) are impossible to escape since the inception of GenAI in November 2022—whether it is about the end of our jobs or the end of the world! This Editorial talks about AI and its subsets and how this may relate to health care.</jats:p>,3.0
28,10.1007/s43681-023-00260-1,What would qualify an AI for moral standing?,"<jats:title>Abstract</jats:title><jats:p>What criteria must an AI (AI) satisfy to qualify for moral standing? My starting point is that sentient AIs should qualify for moral standing. But future AIs may have unusual combinations of cognitive capacities, such as a high level of cognitive sophistication without sentience. This raises the question of whether sentience is a necessary criterion for moral standing, or merely sufficient. After reviewing nine criteria that have been proposed in the literature, I suggest that there is a strong case for thinking that some non-sentient AIs, such as those that are conscious and have non-valenced preferences and goals, and those that are non-conscious and have sufficiently cognitively complex preferences and goals, should qualify for moral standing. After responding to some challenges, I tentatively argue that taking into account uncertainty about which criteria an entity must satisfy to qualify for moral standing, and strategic considerations such as how such decisions will affect humans and other sentient entities, further supports granting moral standing to some non-sentient AIs. I highlight three implications: that the issue of AI moral standing may be more important, in terms of scale and urgency, than if either sentience or consciousness is necessary; that researchers working on policies designed to be inclusive of sentient AIs should broaden their scope to include all AIs with morally relevant interests; and even those who think AIs cannot be sentient or conscious should take the issue seriously. However, much uncertainty about these considerations remains, making this an important topic for future research.</jats:p>",0.0
29,10.1007/s43681-023-00387-1,Publics’ views on ethical challenges of AI: a scoping review,"<jats:title>Abstract</jats:title><jats:p>This scoping review examines the research landscape about publics’ views on the ethical challenges of AI. To elucidate how the concerns voiced by the publics are translated within the research domain, this study scrutinizes 64 publications sourced from PubMed<jats:sup>®</jats:sup> and Web of Science™. The central inquiry revolves around discerning the motivations, stakeholders, and ethical quandaries that emerge in research on this topic. The analysis reveals that innovation and legitimation stand out as the primary impetuses for engaging the public in deliberations concerning the ethical dilemmas associated with AI technologies. Supplementary motives are rooted in educational endeavors, democratization initiatives, and inspirational pursuits, whereas politicization emerges as a comparatively infrequent incentive. The study participants predominantly comprise the general public and professional groups, followed by AI system developers, industry and business managers, students, scholars, consumers, and policymakers. The ethical dimensions most commonly explored in the literature encompass human agency and oversight, followed by issues centered on privacy and data governance. Conversely, topics related to diversity, nondiscrimination, fairness, societal and environmental well-being, technical robustness, safety, transparency, and accountability receive comparatively less attention. This paper delineates the concrete operationalization of calls for public involvement in AI governance within the research sphere. It underscores the intricate interplay between ethical concerns, public involvement, and societal structures, including political and economic agendas, which serve to bolster technical proficiency and affirm the legitimacy of AI development in accordance with the institutional norms that underlie responsible research practices.</jats:p>",0.0
30,10.1007/s43681-024-00597-1,Introducing the ethical-epistemic matrix: a principle-based tool for evaluating AI in medicine,"<jats:title>Abstract</jats:title><jats:p>While there has been much discussion of the ethical assessment of AI (AI) in medicine, such work has rarely been combined with the parallel body of scholarship analyzing epistemic implications of AI. This paper proposes a method for joint evaluation of AI’s ethical and epistemic implications in medicine that draws on the principle-oriented tradition in bioethics and the consequent ‘ethical matrix’ approach to assessing novel technologies. It first introduces principle-based approaches as specific tools for ethical assessment of AI in medicine and other domains that are contrasted with the lack of comparable epistemic principles that would govern AI evaluation in medicine. In the next section, the ethical matrix is explained as a well-established principle-based tool in applied ethics that has had some limited applications to near-term implications of AI in medicine and elsewhere that can be strengthened, I suggest, using epistemic principles. To this end, the following section looks to the philosophy of science for relevant epistemic principles, identifying ‘accuracy’, ‘consistency’, ‘relevance’, and ‘instrumental efficacy’ as a provisional set for technology evaluation. The next section articulates the relevance of these epistemic principles to AI in medicine by highlighting conventional standards that have already been applied in AI, epistemology, and the medical sciences. Before concluding, the paper then defines and defends the possibility of an ‘ethical-epistemic matrix’ for the application of these epistemic principles alongside established ethical principles to a selection of stakeholder groups: patients, clinicians, developers, and the public.</jats:p>",6.0
31,10.1007/s43681-024-00493-8,The ethics of using AI in scientific research: new guidance needed for a new tool,"<jats:title>Abstract</jats:title><jats:p>Using AI (AI) in research offers many important benefits for science and society but also creates novel and complex ethical issues. While these ethical issues do not necessitate changing established ethical norms of science, they require the scientific community to develop new guidance for the appropriate use of AI. In this article, we briefly introduce AI and explain how it can be used in research, examine some of the ethical issues raised when using it, and offer nine recommendations for responsible use, including: (1) Researchers are responsible for identifying, describing, reducing, and controlling AI-related biases and random errors; (2) Researchers should disclose, describe, and explain their use of AI in research, including its limitations, in language that can be understood by non-experts; (3) Researchers should engage with impacted communities, populations, and other stakeholders concerning the use of AI in research to obtain their advice and assistance and address their interests and concerns, such as issues related to bias; (4) Researchers who use synthetic data should (a) indicate which parts of the data are synthetic; (b) clearly label the synthetic data; (c) describe how the data were generated; and (d) explain how and why the data were used; (5) AI systems should not be named as authors, inventors, or copyright holders but their contributions to research should be disclosed and described; (6) Education and mentoring in responsible conduct of research should include discussion of ethical use of AI.</jats:p>",-1.0
32,10.1007/s43681-022-00133-z,Responsibility assignment won’t solve the moral issues of AI,"<jats:title>Abstract</jats:title><jats:p>Who is responsible for the events and consequences caused by using artificially intelligent tools, and is there a gap between what human agents can be responsible for and what is being done using AI? Both questions presuppose that the term ‘responsibility’ is a good tool for analysing the moral issues surrounding AI. This article will draw this presupposition into doubt and show how reference to responsibility obscures the complexity of moral situations and moral agency, which can be analysed with a more differentiated toolset of moral terminology. It suggests that the impression of responsibility gaps only occurs if we gloss over the complexity of the moral situation in which artificial intelligent tools are employed and if—counterfactually—we ascribe them some kind of pseudo-agential status.</jats:p>",0.0
33,10.1007/s43681-022-00218-9,"Ethics and diversity in AI policies, strategies and initiatives","<jats:title>Abstract</jats:title><jats:p>A burgeoning of AI (AI) technologies in recent years has led to increased discussion about its potential to address many issues considered otherwise intractable, including those highlighted by the United Nations 2030 Agenda for Sustainable Development and associated Sustainable Development Goals. In tandem with this growth in AI is an expanding body of documentation regarding how such advanced technologies should be governed and managed. Issued by a variety of sources and comprising frameworks, policies and guidelines, this body of work encompasses the legal, social, ethical and policy issues around AI. With at least 470 such documents identified, as of May 2021, in the Council of Europe’s tracker of AI initiatives, questions are emerging around the diversity of views expressed, especially regarding the influence of the Global North or Euro-American perspectives. Our previous analysis of a corpus of largely grey literature discovered blind spots regarding both gender representation and perspectives from the Global South. Expanding on that work, this paper examines a significantly extended corpus, with a focus on the role of underrepresented groups in the wider AI discourse. We find that voices from the Global South and consideration of alternative ethical approaches are largely absent from the conversation. In light of the prominence of social, cultural and ethical perspectives from the Global North, this paper explores implications for the development of standards for ethical AI. Concluding by offering approaches to incorporate more diverse ethical viewpoints and beliefs, we call for increased consideration of power structures when developing AI ethics policies and standards within these alternative socio-cultural and socio-economic contexts.</jats:p>",0.0
34,10.1007/s43681-021-00113-9,The double-edged sword of AI: Ethical Adversarial Attacks to counter AI for crime,"<jats:title>Abstract</jats:title><jats:p>AI (AI) has found a myriad of applications in many domains of technology, and more importantly, in improving people’s lives. Sadly, AI solutions have already been utilized for various violations and theft, even receiving the name AI or Crime (AIC). This poses a challenge: are cybersecurity experts thus justified to attack malicious AI algorithms, methods and systems as well, to stop them? Would that be fair and ethical? Furthermore, AI and machine learning algorithms are prone to be fooled or misled by the so-called adversarial attacks. However, adversarial attacks could be used by cybersecurity experts to stop the criminals using AI, and tamper with their systems. The paper argues that this kind of attacks could be named Ethical Adversarial Attacks (EAA), and if used fairly, within the regulations and legal frameworks, they would prove to be a valuable aid in the fight against cybercrime.</jats:p>",8.0
35,10.1007/s43681-024-00521-7,Ethics and the use of GenAI in professional editing,"<jats:title>Abstract</jats:title><jats:p>GenAI (GnAI) has garnered significant attention worldwide across diverse industries, including in book publishing. To date, more attention has been paid to its potential in creative collaboration and less to the editorial possibilities of its application. Interest has accelerated since the breakthrough of a new Large Language Model in late 2022. This paper engages with the ethical and industrial implications of using GnAI in a creative context, namely literary publishing. It raises crucial questions about intellectual property, trust, the author–editor relationship and publishing professionals’ evolving roles in shaping quality literature. Using a published story as a test case, we compare edits using GnAI with those by professional editors over multiple drafts and at different stages of editorial development. We consider the potential ethical implications of the use of GnAI in literary fiction editing, highlighting the principles and practices that underpin professional editing to consider how these may or may not translate in the use of GnAI. This is followed by a discussion of the risks and opportunities in using GnAI in editing literary texts in the trade publishing context.</jats:p>",-1.0
36,10.1007/s43681-021-00093-w,Foundations for the future: institution building for the purpose of AI governance,"<jats:title>Abstract</jats:title><jats:p>Governance efforts for AI (AI) are taking on increasingly more concrete forms, drawing on a variety of approaches and instruments from hard regulation to standardisation efforts, aimed at mitigating challenges from high-risk AI systems. To implement these and other efforts, new institutions will need to be established on a national and international level. This paper sketches a blueprint of such institutions, and conducts in-depth investigations of three key components of any future AI governance institutions, exploring benefits and associated drawbacks: (1) “purpose”, relating to the institution’s overall goals and scope of work or mandate; (2) “geography”, relating to questions of participation and the reach of jurisdiction; and (3) “capacity”, the infrastructural and human make-up of the institution. Subsequently, the paper highlights noteworthy aspects of various institutional roles specifically around questions of institutional purpose, and frames what these could look like in practice, by placing these debates in a European context and proposing different iterations of a European AI Agency. Finally, conclusions and future research directions are proposed.</jats:p>",0.0
37,10.1007/s43681-022-00231-y,"A new control problem? Humanoid robots, AI, and the value of control","<jats:title>Abstract</jats:title><jats:p>The control problem related to robots and AI usually discussed is that we might lose control over advanced technologies. When authors like Nick Bostrom and Stuart Russell discuss this control problem, they write in a way that suggests that having as much control as possible is good while losing control is bad. In life in general, however, not all forms of control are unambiguously positive and unproblematic. Some forms—e.g. control over other persons—are ethically problematic. Other forms of control are positive, and perhaps even intrinsically good. For example, one form of control that many philosophers have argued is intrinsically good and a virtue is self-control. In this paper, I relate these questions about control and its value to different forms of robots and AI more generally. I argue that the more robots are made to resemble human beings, the more problematic it becomes—at least symbolically speaking—to want to exercise full control over these robots. After all, it is unethical for one human being to want to fully control another human being. Accordingly, it might be seen as problematic—viz. as representing something intrinsically bad—to want to create humanoid robots that we exercise complete control over. In contrast, if there are forms of AI such that control over them can be seen as a form of self-control, then this might be seen as a virtuous form of control. The “new control problem”, as I call it, is the question of under what circumstances retaining and exercising complete control over robots and AI is unambiguously ethically good.</jats:p>",0.0
38,10.1007/s43681-022-00139-7,Ethical issues deriving from the delayed adoption of AI in medical imaging,"<jats:title>Abstract</jats:title><jats:p>Medical imaging (MI) has assumed a central role in medicine. AI (AI) has revolutionized computer vision and it is also approaching to impact deeply MI. Fundamental ethical matters have raised and teams of experts around the world are involved in defining ethical borders for AI in MI. However, reading the extremely detailed proposals, it is clear that the treated ethical arguments have been completely redefined and specifically structured for AI in MI. Instead, many of them should be inherited from other technologies already in use in MI. The complete re-definition of ethical principles could produce contradictions and delays for AI adoption in MI, thus arising important ethical concerns. In this paper, potential ethical issues related to AI delay are presented: the objective is to contribute to reuse some concepts from other technologies to streamline the arguments and avoid these concerns.</jats:p>",6.0
39,10.1007/s10551-024-05741-9,Generative Artificial Intelligence as Hypercommons: Ethics of Authorship and Ownership,"<jats:title>Abstract</jats:title><jats:p>In this editorial essay, we argue that GenAI programs (GenAI) draw on what we term a “hypercommons”, involving collectively produced inputs and labour that are largely invisible or untraceable. We argue that automatizing the exploitation of common inputs, in ways that remix and reconfigure them, can lead to a crisis of academic authorship in which the moral agency involved in scholarly production is increasingly eroded. We discuss the relationship between the hypercommons and authorship in terms of moral agency and the ethics of academic production, speculating on different responses to the crisis of authorship as posed by GenAI.</jats:p>",2.0
40,10.1007/s10462-024-10916-x,"Explainable GenAI (GenXAI): a survey, conceptualization, and research agenda","<jats:title>Abstract</jats:title><jats:p>GenAI (GenAI) represents a shift from AI’s ability to “recognize” to its ability to “generate” solutions for a wide range of tasks. As generated solutions and applications grow more complex and multi-faceted, new needs, objectives, and possibilities for explainability (XAI) have emerged. This work elaborates on why XAI has gained importance with the rise of GenAI and the challenges it poses for explainability research. We also highlight new and emerging criteria that explanations should meet, such as verifiability, interactivity, security, and cost considerations. To achieve this, we focus on surveying existing literature. Additionally, we provide a taxonomy of relevant dimensions to better characterize existing XAI mechanisms and methods for GenAI. We explore various approaches to ensure XAI, ranging from training data to prompting. Our paper provides a concise technical background of GenAI for non-technical readers, focusing on text and images to help them understand new or adapted XAI techniques for GenAI. However, due to the extensive body of work on GenAI, we chose not to delve into detailed aspects of XAI related to the evaluation and usage of explanations. Consequently, the manuscript appeals to both technical experts and professionals from other fields, such as social scientists and information systems researchers. Our research roadmap outlines over ten directions for future investigation.</jats:p>",-1.0
41,10.1007/s43681-023-00321-5,Measuring responsible AI (RAI) in banking: a valid and reliable instrument,"<jats:title>Abstract</jats:title><jats:p>Widespread use of AI (AI) and machine learning (ML) in the US banking industry raises red flags with regulators and social groups due to potential risk of data-driven algorithmic bias in credit lending decisions. The absence of a valid and reliable measure of responsible AI (RAI) has stunted the growth of organizational research on RAI (i.e., the organizational balancing act to optimize efficiency and equity). To address this void, we develop a novel measurement instrument to assess RAI maturity in firms. A review of the nascent literature reveals that there is a wide distribution of RAI capabilities. The RAI instrument that we advance is based on the exhaustive review of this dispersed literature. Analyses of data from large US banks show strong evidence of validity and reliability of the RAI maturity instrument.</jats:p>",-1.0
42,10.1007/s43681-022-00253-6,Engineering a social contract: Rawlsian distributive justice through algorithmic game theory and AI,"<jats:title>Abstract</jats:title><jats:p>The potential for AI algorithms and game theory concepts to offer prescriptive and decision-making capability for humankind is increasingly recognized. This derives from the increasing availability of granular, multivariable, well-curated data offering analytical insights for necessarily complex human behaviors and activities. Of the multitude of situations that this decision-making aptitude presents, the application to governmental policy offers a commanding case. This would allow decisions to be made for the benefit of societies and citizens based on rigorous objective information devoid of the traditional approach of choosing policies and societal values based on the opinion of a handful of selected representatives who may be exposed to a lack of comprehensive data analysis capacity and subject to personal biases. There would need to be a critical requirement of wider socially responsible data practices here, beyond those of technical considerations and the incorporation of wider societal fairness approaches. Amongst the schools of political thought particularly acquiescent to the application by this approach would be the egalitarian approach of John Rawls. Here an Original Position’s pre-determination tool of Veil of Ignorance and ensuing Difference Principal presents a method of distributive justice that can be clearly mathematically defined in economics theory through Wald’s Maximin principle. This offers an opportunity to apply algorithmic game theory and AI computational approaches to implement Rawlsian distributive justice that are presented and discussed. The outputs from the algorithmic acquaintance of Rawlsian egalitarianism with applicable state data, protected with appropriate privacy, security, legal, ethical and social governance could in turn lead to automated direct governmental choices and an objective Social Contract for citizens of digitally literate nations.</jats:p>",0.0
43,10.1007/s43681-024-00548-w,On singularity and the Stoics: why Stoicism offers a valuable approach to navigating the risks of AI (AI),"<jats:title>Abstract</jats:title><jats:p>The potential benefits and risks of AI technologies have sparked a wide-ranging debate in both academic and public circles. On one hand, there is an urgent call to address the immediate and avoidable challenges associated with these tools, such as accountability, privacy, bias, understandability, and transparency; on the other hand, prominent figures like Geoffrey Hinton and Elon Musk have voiced concerns over the potential rise of Super AI, whose singularity could pose an existential threat to humanity. Coordinating the efforts of thousands of decentralized entities to prevent such a hypothetical event may seem insurmountable in our intricate and multipolar world. Thus, drawing from both perspectives, this work suggests employing the tools and framework of Stoic philosophy, particularly the concept of the dichotomy of control—focusing on what is within our power. This Stoic principle offers a practical and epistemological approach to managing the complexities of AI, and it encourages individuals to organize their efforts around what they can influence while adapting to the constraints of external factors. Within this framework, the essay found that Stoic wisdom is essential for assessing risks, courage is necessary to face contemporary challenges, and temperance and tranquility are indispensable; and these lessons can inform ongoing public and academic discourse, aiding in the development of more effective policy proposals for aligning Narrow AI and General AI with human values.</jats:p>",0.0
44,10.1007/s43681-024-00519-1,A powerful potion for a potent problem: transformative justice for GenAI in healthcare,"<jats:title>Abstract</jats:title><jats:p>GenAI (AI), as a transformative technology, holds significant promise for applications in healthcare. At the same time, the datafication, AI integration, and commodification of health have opened the floodgates for ethical issues, including those related to fairness, access, beneficence, democracy, solidarity, inclusion, and societal harms. As further the digitalization, innovation, and disruption of healthcare is inevitable, the paper maps out how power, equity, access, identity, participation, and knowledge contribute to creating social injustice issues. It also discusses that current justice approaches—distributive justice, representational justice, restorative justice, and capabilities-centered justice—do not have enough impact to prevent or remedy the many harms and injustices that AI has already created in healthcare or will continue to do so. The paper proposes that a transformative justice approach is needed for GenAI as a transformative technology, focused on (1) peace, emancipation, and eliminating the root causes of injustice, (2) holistic conflict resolution, (3) human rights-based approaches, and (4) the empowerment of agency and actors.</jats:p>",6.0
45,10.1007/s10892-023-09456-3,What Makes Work “Good” in the Age of AI (AI)? Islamic Perspectives on AI-Mediated Work Ethics,"<jats:title>Abstract</jats:title><jats:p>AI (AI) technologies are increasingly creeping into the work sphere, thereby gradually questioning and/or disturbing the long-established moral concepts and norms communities have been using to define what makes work good. Each community, and Muslims make no exception in this regard, has to revisit their moral world to provide well-thought frameworks that can engage with the challenging ethical questions raised by the new phenomenon of AI-mediated work. For a systematic analysis of the broad topic of AI-mediated work ethics from an Islamic perspective, this article focuses on presenting an accessible overview of the “moral world” of work in the Islamic tradition. Three main components of this moral world were selected due to their relevance to the AI context, namely (1) Work is inherently good for humans, (2) Practising a religiously permitted profession and (c) Maintaining good relations with involved stakeholders. Each of these three components is addressed in a distinct section, followed by a sub-section highlighting the relevance of the respective component to the particular context of AI-mediated work. The article argues that there are no unsurmountable barriers in the Islamic tradition against the adoption of AI technologies in work sphere. However, important precautions should be considered to ensure that embracing AI will not be at the cost of work-related moral values. The article also highlights how important lessons can be learnt from the positive historical experience of automata that thrived in the Islamic civilization.</jats:p>",0.0
46,10.1007/s43681-024-00529-z,Securing tomorrow: a comprehensive survey on the synergy of AI and information security,"<jats:title>Abstract</jats:title><jats:p>This survey paper explores the transformative role of AI (AI) in information security. Traditional methods, especially rule-based approaches, faced significant challenges in protecting sensitive data from ever-changing cyber threats, particularly with the rapid increase in data volume. This study thoroughly evaluates AI’s application in information security, discussing its strengths and weaknesses. It provides a detailed review of AI’s impact on information security, examining various AI algorithms used in this field, such as supervised, unsupervised, and reinforcement learning, and highlighting their respective strengths and limitations. The study identifies key areas for future AI research in information security, focusing on improving algorithms, strengthening information security, addressing ethical issues, and exploring safety and security-related concerns. It emphasizes significant security risks, including vulnerability to adversarial attacks, and aims to enhance the robustness and reliability of AI systems in protecting sensitive information by proposing solutions for potential threats. The findings aim to benefit cybersecurity professionals and researchers by offering insights into the intricate relationship between AI, information security, and emerging technologies.</jats:p>",8.0
47,10.1007/s43681-023-00378-2,How AI adopts human biases: the case of cosmetic skincare industry,"<jats:title>Abstract</jats:title><jats:p>The cosmetic skincare industry is a growing market that extends to different regions and customer groups. In addition to scientific advances and technological developments, state-of-the-art digital approaches, including machine learning and other AI (AI)-based techniques, are being applied at different stages of the value chain. The objectives of these efforts include optimizing the supply chain, developing high-quality, effective and safe products and personalization at every step of the customer journey. However, the use of digital technologies comes with risks and undesirable effects. These include a lack of transparency and accountability, compromised fairness and a general deficiency in data governance, all of which are critical at every customer touchpoint. This dark side of digital transformation is recognized by both businesses and governments. In this paper, we explain the concept of bias leading to unfairness for beauty technology applications. Based on published data we identified potential sources of AI bias in the cosmetic skincare industry and/or beauty tech. They were classified by the stage of the AI lifecycle: biases related to target setting, to acquisition and annotation, to modeling, to validation and evaluation, and to deployment and monitoring. We aim to create awareness of such phenomena among readers, whether executives, managers, developers or potential end-users.</jats:p>",0.0
48,10.1007/s43681-023-00279-4,Personality and demographic correlates of support for regulating AI,"<jats:title>Abstract</jats:title><jats:p>The arrival of AI (AI) in our society has sparked many hopes and fears, with people having diverging views on the need to strictly regulate AI. The current study investigates how demographic and personality traits are associated with a desire to strictly regulate AI using a representative sample of adults from New Zealand (<jats:italic>N</jats:italic> = 47,951 participants). Data revealed that support for strict regulation of AI is positively related with agreeableness, neuroticism, and honesty–humility. However, it is negatively related to openness to experiences. A wide range of demographic factors including gender, age, ethnicity, religiosity, neighbourhood level economic deprivation, living rural, relationship status, and parental status were additionally related to support for regulation of AI. However, all these effects were fairly small suggesting that both personality and socio-demographic factors contribute to support for regulating AI, but other factors beyond these characteristics should also be considered for understanding people’s support for regulating AI.</jats:p>",0.0
49,10.1007/s00146-023-01686-1,Ethics of using AI (AI) in veterinary medicine,"<jats:title>Abstract</jats:title><jats:p>This paper provides the first comprehensive analysis of ethical issues raised by AI (AI) in veterinary medicine for companion animals. Veterinary medicine is a socially valued service, which, like human medicine, will likely be significantly affected by AI. Veterinary AI raises some unique ethical issues because of the nature of the client–patient–practitioner relationship, society’s relatively minimal valuation and protection of nonhuman animals and differences in opinion about responsibilities to animal patients and human clients. The paper examines how these distinctive features influence the ethics of AI systems that might benefit clients, veterinarians and animal patients—but also harm them. It offers practical ethical guidance that should interest ethicists, veterinarians, clinic owners, veterinary bodies and regulators, clients, technology developers and AI researchers.</jats:p>",6.0
50,10.1093/pnasnexus/pgae052,"GenAI, human creativity, and art","<jats:title>Abstract</jats:title>
               <jats:p>Recent AI (AI) tools have demonstrated the ability to produce outputs traditionally considered creative. One such system is text-to-image GenAI (e.g. Midjourney, Stable Diffusion, DALL-E), which automates humans’ artistic execution to generate digital artworks. Utilizing a dataset of over 4 million artworks from more than 50,000 unique users, our research shows that over time, text-to-image AI significantly enhances human creative productivity by 25% and increases the value as measured by the likelihood of receiving a favorite per view by 50%. While peak artwork Content Novelty, defined as focal subject matter and relations, increases over time, average Content Novelty declines, suggesting an expanding but inefficient idea space. Additionally, there is a consistent reduction in both peak and average Visual Novelty, captured by pixel-level stylistic elements. Importantly, AI-assisted artists who can successfully explore more novel ideas, regardless of their prior originality, may produce artworks that their peers evaluate more favorably. Lastly, AI adoption decreased value capture (favorites earned) concentration among adopters. The results suggest that ideation and filtering are likely necessary skills in the text-to-image process, thus giving rise to “generative synesthesia”—the harmonious blending of human exploration and AI exploitation to discover new creative workflows.</jats:p>",2.0
51,10.1007/s11948-020-00228-y,"In AI We Trust: Ethics, AI, and Reliability","<jats:title>Abstract</jats:title><jats:p>One of the main difficulties in assessing AI (AI) is the tendency for people to anthropomorphise it. This becomes particularly problematic when we attach human moral activities to AI. For example, the European Commission’s High-level Expert Group on AI (HLEG) have adopted the position that we should establish a relationship of trust with AI and should cultivate trustworthy AI (HLEG AI Ethics guidelines for trustworthy AI, 2019, p. 35). Trust is one of the most important and defining activities in human relationships, so proposing that AI should be trusted, is a very serious claim. This paper will show that AI cannot be something that has the capacity to be trusted according to the most prevalent definitions of trust because it does not possess emotive states or can be held responsible for their actions—requirements of the affective and normative accounts of trust. While AI meets all of the requirements of the rational account of trust, it will be shown that this is not actually a type of trust at all, but is instead, a form of reliance. Ultimately, even complex machines such as AI should not be viewed as trustworthy as this undermines the value of interpersonal trust, anthropomorphises AI, and diverts responsibility from those developing and using them. </jats:p>",0.0
52,10.1007/s43681-023-00268-7,"Symbiosis, not alignment, as the goal for liberal democracies in the transition to artificial general intelligence","<jats:title>Abstract</jats:title><jats:p>A transition to a world with artificial general intelligence (AGI) may occur within the next few decades. This transition may give rise to catastrophic risks from <jats:italic>misaligned</jats:italic> AGI, which have received a significant amount of attention, deservedly. Here I argue that AGI systems that are <jats:italic>intent-aligned</jats:italic>—they always try to do what their operators want them to do—would also create catastrophic risks, mainly due to the power that they concentrate on their operators. With time, that power would almost certainly be catastrophically exploited, potentially resulting in human extinction or permanent dystopia. I suggest that liberal democracies, if they decide to allow the development of AGI, may react to this threat by letting AGI take shape as an <jats:italic>intergenerational social project</jats:italic>, resulting in an arrangement where AGI is not intent-aligned but <jats:italic>symbiotic</jats:italic> with humans. I provide some tentative ideas on what the resulting arrangement may look like and consider what speaks for and what against aiming for intent-aligned AGI as an intermediate step.</jats:p>",0.0
53,10.1007/s43681-021-00074-z,AI in Education (AIEd): a high-level academic and industry note 2021,"<jats:title>Abstract</jats:title><jats:p>In the past few decades, technology has completely transformed the world around us. Indeed, experts believe that the next big digital transformation in how we live, communicate, work, trade and learn will be driven by AI (AI) [83]. This paper presents a high-level industrial and academic overview of AI in Education (AIEd). It presents the focus of latest research in AIEd on reducing teachers’ workload, contextualized learning for students, revolutionizing assessments and developments in intelligent tutoring systems. It also discusses the ethical dimension of AIEd and the potential impact of the Covid-19 pandemic on the future of AIEd’s research and practice. The intended readership of this article is policy makers and institutional leaders who are looking for an introductory state of play in AIEd.</jats:p>",1.0
54,10.3390/ai5030068,Harnessing GenAI for Digital Literacy Innovation: A Comparative Study between Early Childhood Education and Computer Science Undergraduates,"<jats:p>The recent surge of GenAI (AI) in higher education presents a fascinating landscape of opportunities and challenges. AI has the potential to personalize education and create more engaging learning experiences. However, the effectiveness of AI interventions relies on well-considered implementation strategies. The impact of AI platforms in education is largely determined by the particular learning environment and the distinct needs of each student. Consequently, investigating the attitudes of future educators towards this technology is becoming a critical area of research. This study explores the impact of GenAI platforms on students’ learning performance, experience, and satisfaction within higher education. It specifically focuses on students’ experiences with varying levels of technological proficiency. A comparative study was conducted with two groups from different academic contexts undergoing the same experimental condition to design, develop, and implement instructional design projects using various AI platforms to produce multimedia content tailored to their respective subjects. Undergraduates from two disciplines—Early Childhood Education (n = 32) and Computer Science (n = 34)—participated in this study, which examined the integration of GenAI platforms into educational content implementation. Results indicate that both groups demonstrated similar learning performance in designing, developing, and implementing instructional design projects. Regarding user experience, the general outcomes were similar across both groups; however, Early Childhood Education students rated the usefulness of AI multimedia platforms significantly higher. Conversely, Computer Science students reported a slightly higher comfort level with these tools. In terms of overall satisfaction, Early Childhood Education students expressed greater satisfaction with AI software than their counterparts, acknowledging its importance for their future careers. This study contributes to the understanding of how AI platforms affect students from diverse backgrounds, bridging a gap in the knowledge of user experience and learning outcomes. Furthermore, by exploring best practices for integrating AI into educational contexts, it provides valuable insights for educators and scholars seeking to optimize the potential of AI to enhance educational outcomes.</jats:p>",1.0
55,10.1007/s43681-023-00408-z,Assessing deep learning: a work program for the humanities in the age of AI,"<jats:title>Abstract</jats:title><jats:p>Following the success of deep learning (DL) in research, we are now witnessing the fast and widespread adoption of AI (AI) in daily life, influencing the way we act, think, and organize our lives. However, much still remains a mystery when it comes to how these systems achieve such high performance and why they reach the outputs they do. This presents us with an unusual combination: of technical mastery on the one hand, and a striking degree of mystery on the other. This conjunction is not only fascinating, but it also poses considerable risks, which urgently require our attention. Awareness of the need to analyze ethical implications, such as fairness, equality, and sustainability, is growing. However, other dimensions of inquiry receive less attention, including the subtle but pervasive ways in which our dealings with AI shape our way of living and thinking, transforming our culture and human self-understanding. If we want to deploy AI positively in the long term, a broader and more holistic assessment of the technology is vital, involving not only scientific and technical perspectives, but also those from the humanities. To this end, we present outlines of a<jats:italic>work program</jats:italic>for the humanities that aim to contribute to assessing and guiding the potential, opportunities, and risks of further developing and deploying DL systems. This paper contains a thematic introduction (Sect. 1), an introduction to the workings of DL for non-technical readers (Sect. 2), and a main part, containing the outlines of a work program for the humanities (Sect. 3). Readers familiar with DL might want to ignore 2 and instead directly read 3 after 1.</jats:p>",0.0
56,10.1007/s00146-024-01922-2,An elemental ethics for AI: water as resistance within AI’s value chain,"<jats:title>Abstract</jats:title><jats:p>Research and activism have increasingly denounced the problematic environmental record of the infrastructure and value chain underpinning AI (AI). Water-intensive data centres, polluting mineral extraction and e-waste dumping are incontrovertibly part of AI’s footprint. In this article, I turn to areas affected by AI-fuelled environmental harm and identify an ethics of resistance emerging from local activists, which I term ‘elemental ethics’. Elemental ethics interrogates the AI value chain’s problematic relationship with the elements that make up the world, critiques the undermining of local and ancestral approaches to nature and reveals the vital and quotidian harms engendered by so-called intelligent systems. While this ethics is emerging from grassroots and Indigenous groups, it echoes recent calls from environmental philosophy to reconnect with the environment via the elements. In empirical terms, this article looks at groups in Chile resisting a Google data centre project in Santiago and lithium extraction (used for rechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert. As I show, elemental ethics can complement top-down, utilitarian and quantitative approaches to AI ethics and sustainable AI as well as interrogate whose lived experience and well-being counts in debates on AI extinction.</jats:p>",0.0
57,10.1177/17470161241262149,Dual use concerns in AI and the neurosciences: How medical research can end up in war,"<jats:p> Dual Use Research of Concern (DURC) has been well analyzed regarding the life sciences. This article explores the topic of younger fields of medical research and their potential for misuse, especially in the military context. The areas of research considered are AI, neurotechnology, and neuroenhancement. Each of these areas have brought forward highly promising new research. However, in light of the current armed conflicts in Europe and in the Middle East, there is a need to consider what the potential harmful consequences of medical research are. Using the example of war, this article demonstrates various instances of how current medical research could be—or is being—misused and discusses various possible solutions to the dual use dilemma. The main finding is that there needs to be a more concise and international effort to prevent the misuse of research. The raising of awareness in the general medical research community for the topic of DURC is one of the simplest steps that should be undertaken in order to ensure the non-maleficence of global research. Additionally, considering the potentially far-reaching consequences of DURC, it is time to consider the introduction of a new intergovernmental agency to monitor research and establish safeguards in order to cover all fields of research. </jats:p>",6.0
58,10.1007/s43681-023-00273-w,"AI, superefficiency and the end of work: a humanistic perspective on meaning in life","<jats:title>Abstract</jats:title><jats:p>How would it be assessed from an ethical point of view if human wage work were replaced by artificially intelligent systems (AI) in the course of an automation process? An answer to this question has been discussed above all under the aspects of individual well-being and social justice. Although these perspectives are important, in this article, we approach the question from a different perspective: that of leading a meaningful life, as understood in analytical ethics on the basis of the so-called meaning-in-life debate. Our thesis here is that a life without wage work loses specific sources of meaning, but can still be sufficiently meaningful in certain other ways. Our starting point is John Danaher’s claim that ubiquitous automation inevitably leads to an achievement gap. Although we share this diagnosis, we reject his provocative solution according to which game-like virtual realities could be an adequate substitute source of meaning. Subsequently, we outline our own systematic alternative which we regard as a decidedly humanistic perspective. It focuses both on different kinds of social work and on rather passive forms of being related to meaningful contents. Finally, we go into the limits and unresolved points of our argumentation as part of an outlook, but we also try to defend its fundamental persuasiveness against a potential objection.</jats:p>",0.0
59,10.1215/2834703x-11205175,The Ethics of (Generative) AI,"<jats:title>Abstract</jats:title>
               <jats:p>The clamor for AI-based applications involving generative models for text and images has fueled wild speculation about the risks and opportunities for society and humanity at large. The potential “existential” threat as a precursor to artificial general intelligence has provoked wide-ranging debates in the public, politics, and the corporate world involving technologists and ethicists from a range of academic disciplines. This thinkpiece proposes a metaperspective to reflect critically and constructively upon the current state of the field of AI ethics, arguing that scholars working in the domain of ethics should focalize conceptual, substantive, and procedural issues as integral elements of an ethical assessment of given technologies and their applications. It suggests that the ethics of GenAI is conceptually still underexplored and overly propagating technological fixes to problems of all kinds (technosolutionism). Procedurally, it needs to be clarified who can, who ought to, and who ultimately will be considered and heard as an expert on AI ethics, a question of relevance for the trust in, and reliance on, AI.</jats:p>",0.0
60,10.1007/s43681-021-00089-6,Moral exemplars for the virtuous machine: the clinician’s role in ethical AI for healthcare,"<jats:title>Abstract</jats:title><jats:p>AI (AI) continues to pervade several aspects of healthcare with pace and scale. The need for an ethical framework in AI to address this has long been recognized, but to date most efforts have delivered only high-level principles and value statements. Herein, we explain the need for an ethical framework in healthcare AI, the different moral theories that may serve as its basis, the rationale for why we believe this should be built around virtue ethics, and explore this in the context of five key ethical concerns for the introduction of AI in healthcare. Some existing work has suggested that AI may replace clinicians. We argue to the contrary, that the clinician will not be replaced, nor their role attenuated. Rather, they will be integral to the responsible design, deployment, and regulation of AI in healthcare, acting as the moral exemplar for the virtuous machine. We collate relevant points from the literature and formulate our own to present a coherent argument for the central role of clinicians in ethical AI and propose ideas to help advance efforts to employ ML-based solutions within healthcare. Finally, we highlight the responsibility of not only clinicians, but also data scientists, tech companies, ethicists, and regulators to act virtuously in realising the vision of ethical and accountable AI in healthcare.</jats:p>",6.0
61,10.1007/s11948-024-00476-2,AI and Agency: Tie-breaking in AI Decision-Making,"<jats:title>Abstract</jats:title><jats:p>Determining the agency-status of machines and AI has never been more pressing. As we progress into a future where humans and machines more closely co-exist, understanding hallmark features of agency affords us the ability to develop policy and narratives which cater to both humans and machines. This paper maintains that decision-making processes largely underpin agential action, and that in most instances, these processes yield good results in terms of making good choices. However, in some instances, when faced with two (or more) choices, an agent may find themselves with equal reasons to choose either - thus being presented with a tie. This paper argues that in the event of a tie, the ability to create a voluntarist reason is a hallmark feature of agency, and second, that AI, through current tie-breaking mechanisms does not have this ability, and thus fails at this particular feature of agency.</jats:p>",0.0
62,10.59400/cai.v2i1.1378,From bard to Gemini: An investigative exploration journey through Google’s evolution in conversational AI and GenAI,"<jats:p>The advent of AI (AI) has significantly transformed various aspects of human life, particularly in information retrieval and assistance. This research presents a comprehensive evaluation of Gemini, previously known as Google Bard, a state-of-the-art AI chatbot developed by Google. Through a meticulous methodology encompassing both qualitative and quantitative approaches, this research aims to assess Gemini’s performance, usability, integration capabilities, ethical implications. Primary data collection methods, including user surveys and interviews, were utilized to gather towards the qualitative feedback on user experiences with Gemini, supplemented by secondary data analysis using tools such as Google Analytics to capture quantitative metrics. Performance evaluation involved benchmarking against other AI chatbots and technical analysis of Gemini’s architecture and training methods. User experience testing examined usability, engagement, and integration with Google Workspace and third-party services. Ethical considerations regarding data privacy, security, and biases in AI-generated content were also addressed, ensuring compliance with major regulations and promoting ethical AI practices. Acknowledging limitations and challenges inherent in the investigative exploration, data analysis was conducted using thematic and statistical methods to derive insights. The results and findings of this research offer valuable insights into the capabilities and limitations of Gemini, providing implications for future AI development, user interaction design, and ethical AI governance. By contributing to the ongoing discourse on AI advancements and their societal impact, this exploration facilitates informed decision-making and lays the groundwork for future research endeavors in the field of AI-driven conversational agents.</jats:p>",-1.0
63,10.31992/0869-3617-2024-33-2-31-53,Ethics and AI-Plagiarism in an Academic Environment: Students’ Understanding of Compliance with Author’s Ethics and the Problem of Plagiarism in the Process of Interaction with GenAI,"<jats:p>Everyday, AI (AI) is being increasingly integrated into the teaching and learning process at Russian universities. The high level of quality of feedback from AI tools leads to the spread of AI plagiarism – unauthorized borrowing of GenAI materials – among students. The purpose of this study is to: a) highlight aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of plagiarism when interacting with GenAI; b) develop a questionnaire to determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism; c) conduct an online survey of university students, analyze and discuss the results obtained. The paper highlights five aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism when completing educational assignments and preparing research texts: a) students’ general understanding of the issues of compliance with author’s ethics and the problem of plagiarism in an academic environment; b) students’ experience of AI tools for educational purposes; c) students’ understanding of the problem of AI plagiarism and attitude towards borrowing materials from GenAI; d) teachers’ actions to prevent AI plagiarism among students; e) the policy of educational organizations regarding student compliance with ethics and AI plagiarism. An online questionnaire was developed to determine the degree to which students understand the issues of compliance with copyright ethics and the problem of AI plagiarism. 1,599 students from 29 universities of the Russian Federation took part in the survey. The results showed that in general, in the Russian student community, plagiarism is a widespread social phenomenon, many types of which are perceived by young people as a norm of academic behavior. Despite the relatively high awareness of students in the field of AI technologies, the extremely rare use by teachers of specialized subject disciplines of AI tools in the educational process I’d the reason for the current low level of spread of AI plagiarism in the academic environment. At the same time, it is necessary to state that students lack a systematic understanding of exactly how they can “legally” use GenAI materials and what exactly will be considered AI plagiarism. According to students, the importance of understanding the issues of compliance with author ethics and the problem of AI plagiarism will depend, on the one hand, on the actions of teachers to explain to students the rules for using GenAI materials, and on the other hand, the presence in universities of a regulatory framework regulating the field and the extent to which students use AI in the educational process.</jats:p>",1.0
64,10.3390/app13116716,"Exploring the Potential Impact of AI (AI) on International Students in Higher Education: GenAI, Chatbots, Analytics, and International Student Success","<jats:p>International students face unique challenges in pursuing higher education in a foreign country. To address these challenges and enhance their academic experience, higher education institutions are increasingly exploring the use of AI (AI) applications. This research essay aims to investigate the impact of AI on the education of international students. Instead of a traditional literature review, it employs a research approach to examine the potential applications of AI and discuss associated concerns. The research paper explores various AI applications, such as personalized learning experiences, adaptive testing, predictive analytics, and chatbots for learning and research. By analyzing the role of AI in education for international students, this research paper sheds light on how AI can improve learning efficiency and provide customized educational support. Additionally, it identifies significant risks and limitations, including privacy concerns, cultural differences, language proficiency, and ethical implications, which must be effectively addressed. The findings contribute to a better understanding of the potential impact of AI on international students’ educational experiences and offer insights into the integration of AI into educational administration and learning processes.</jats:p>",1.0
65,10.1007/s11846-023-00696-z,The GenAI is out of the bottle: GenAI from a business model innovation perspective,"<jats:title>Abstract</jats:title><jats:p>The introduction of GenAI in November 2022 by OpenAI has stimulated substantial discourse on the implementation of AI (AI) in various domains such as academia, business, and society at large. Although AI has been utilized in numerous areas for several years, the emergence of GenAI (GAI) applications such as GenAI, Jasper, or DALL-E are considered a breakthrough for the acceleration of AI technology due to their ease of use, intuitive interface, and performance. With GAI, it is possible to create a variety of content such as texts, images, audio, code, and even videos. This creates a variety of implications for businesses requiring a deeper examination, including an influence on business model innovation (BMI). Therefore, this study provides a BMI perspective on GAI with two primary contributions: (1) The development of six comprehensive propositions outlining the impact of GAI on businesses, and (2) the discussion of three industry examples, specifically software engineering, healthcare, and financial services. This study employs a qualitative content analysis using a scoping review methodology, drawing from a wide-ranging sample of 513 data points. These include academic publications, company reports, and public information such as press releases, news articles, interviews, and podcasts. The study thus contributes to the growing academic discourse in management research concerning AI's potential impact and offers practical insights into how to utilize this technology to develop new or improve existing business models.</jats:p>",4.0
66,10.1007/s00146-022-01452-9,Cognitive architectures for AI ethics,"<jats:title>Abstract</jats:title><jats:p>As AI (AI) thrives and propagates through modern life, a key question to ask is how to include humans in future AI? Despite human involvement at every stage of the production process from conception and design through to implementation, modern AI is still often criticized for its “black box” characteristics. Sometimes, we do not know what really goes on inside or how and why certain conclusions are met. Future AI will face many dilemmas and ethical issues unforeseen by their creators beyond those commonly discussed (e.g., trolley problems and variants of it) and to which solutions cannot be hard-coded and are often still up for debate. Given the sensitivity of such social and ethical dilemmas and the implications of these for human society at large, when and if our AI make the “wrong” choice we need to understand how they got there in order to make corrections and prevent recurrences. This is particularly true in situations where human livelihoods are at stake (e.g., health, well-being, finance, law) or when major individual or household decisions are taken. Doing so requires opening up the “black box” of AI; especially as they act, interact, and adapt in a human world and how they interact with other AI in this world. In this article, we argue for the application of cognitive architectures for ethical AI. In particular, for their potential contributions to AI transparency, explainability, and accountability. We need to understand how our AI get to the solutions they do, and we should seek to do this on a deeper level in terms of the machine-equivalents of motivations, attitudes, values, and so on. The path to future AI is long and winding but it could arrive faster than we think. In order to harness the positive potential outcomes of AI for humans and society (and avoid the negatives), we need to understand AI more fully in the first place and we expect this will simultaneously contribute towards greater understanding of their human counterparts also.</jats:p>",0.0
67,10.1007/s13347-023-00668-x,AI (AI) in Islamic Ethics: Towards Pluralist Ethical Benchmarking for AI,"<jats:title>Abstract</jats:title><jats:p>This paper explores AI (AI) ethics from an Islamic perspective at a critical time for AI ethical norm-setting. It advocates for a pluralist approach to ethical AI benchmarking. As rapid advancements in AI technologies pose challenges surrounding autonomy, privacy, fairness, and transparency, the prevailing ethical discourse has been predominantly Western or Eurocentric. To address this imbalance, this paper delves into the Islamic ethical traditions to develop a framework that contributes to the global debate on optimal norm setting for designing and using AI technologies.</jats:p><jats:p>The paper outlines Islamic parameters for ethical values and moral actions in the context of AI's ethical uncertainties. It emphasizes the significance of both textual and non-textual Islamic sources in addressing these uncertainties while placing a strong emphasis on the notion of ""good"" or ""<jats:italic>maṣlaḥa</jats:italic>"" as a normative guide for AI's ethical evaluation. Defining <jats:italic>maṣlaḥa</jats:italic> as an ethical state of affairs in harmony with divine will, the paper highlights the coexistence of two interpretations of <jats:italic>maṣlaḥa</jats:italic>: welfarist/utility-based and duty-based. Islamic jurisprudence allows for arguments supporting ethical choices that prioritize building the technical infrastructure for AI to maximize utility. Conversely, it also supports choices that reject consequential utility calculations as the sole measure of value in determining ethical responses to AI advancements.</jats:p>",0.0
68,10.1007/s43681-021-00114-8,AI in research and development for sustainability: the centrality of explicability and research data management,"<jats:title>Abstract</jats:title><jats:p>Sustainability constitutes a focal challenge and objective of our time and requires collaborative efforts. As AI brings forth substantial opportunities for innovations across industry and social contexts, so it provides innovation potential for pursuing sustainability. We argue that (chemical) research and development driven by AI can substantially contribute to sustainability if it is leveraged in an ethical way. Therefore, we propose that the ethical principle <jats:italic>explicability</jats:italic> combined with (open) research data management systems should accompany AI in research and development to foster sustainability in an equitable and collaborative way.</jats:p>",0.0
69,10.1007/s43681-024-00440-7,Engaging the many-hands problem of generative-AI outputs: a framework for attributing credit,"<jats:title>Abstract</jats:title><jats:p>The recent wave of GenAI (GenAI) systems like Stable Diffusion or GenAI that can produce images, text and code from human prompts raises controversial issues about creatorship, originality, creativity and copyright. This paper focuses on creatorship: who creates and should be credited with the outputs made with the help of GenAI? There is currently significant moral, legal and regulatory uncertainty around these questions. We develop a novel framework, called CCC (collective-centered creation), that helps resolve this uncertainty. According to CCC, GenAI outputs are created by collectives in the first instance. Claims to creatorship come in degrees and depend on the nature and significance of individual contributions made by the various agents and entities involved, including users, GenAI systems, developers, producers of training data and others. We demonstrate how CCC can help navigate a range of ongoing controversies around the responsible development and deployment of GenAI technologies and help more accurately attribute credit where it is due.</jats:p>",2.0
70,10.1136/jme-2023-109767,Leveraging AI to detect ethical concerns in medical research: a case study,"<jats:sec><jats:title>Background</jats:title><jats:p>Institutional review boards (IRBs) have been criticised for delays in approvals for research proposals due to inadequate or inexperienced IRB staff. AI (AI), particularly large language models (LLMs), has significant potential to assist IRB members in a prompt and efficient reviewing process.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Four LLMs were evaluated on whether they could identify potential ethical issues in seven validated case studies. The LLMs were prompted with queries related to the proposed eligibility criteria of the study participants, vulnerability issues, information to be disclosed in the informed consent document (ICD), risk–benefit assessment and justification of the use of a placebo. Another query was issued to the LLMs to generate ICDs for these case scenarios.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>All four LLMs were able to provide answers to the queries related to all seven cases. In general, the responses were homogeneous with respect to most elements. LLMs performed suboptimally in identifying the suitability of the placebo arm, risk mitigation strategies and potential risks to study participants in certain case studies with a single prompt. However, multiple prompts led to better outputs in all of these domains. Each of the LLMs included all of the fundamental elements of the ICD for all case scenarios. Use of jargon, understatement of benefits and failure to state potential risks were the key observations in the AI-generated ICD.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>It is likely that LLMs can enhance the identification of potential ethical issues in clinical research, and they can be used as an adjunct tool to prescreen research proposals and enhance the efficiency of an IRB.</jats:p></jats:sec>",6.0
71,10.61969/jai.1512906,"Super AI, GenAI, Narrow AI and Chatbots: An Assessment of AI Technologies for The Public Sector and Public Administration","<jats:p xml:lang=""en"">AI encompasses a wide range of approaches, methodologies, and techniques aimed at mimicking human intelligence in machines. In recent times, the concepts of GenAI (AI), Super AI, and Narrow AI have attracted considerable attention. Undoubtedly, the success of GenAI in capturing all attention has played a significant role in this. AI technology has a profound impact on all sectors, and sector representatives are striving to adapt to this technology more quickly. It is projected that AI could generate an economic size of 13 trillion American dollars by 2030. Developments in AI technologies undoubtedly lead to significant improvements in the functioning of public institutions and access for citizens. AI has the potential to be used in many public services, including security and defense, healthcare services, education, transportation and infrastructure, environmental and natural resource management, law and justice systems, among others. Therefore, evaluating the types of AI, Narrow AI applications, and chatbots for public use is seen as highly beneficial from the perspective of public administration and the public sector. In our study, the topics of super AI, GenAI, narrow AI, and chatbots have been extensively evaluated within the context of the public sector and public administration. Utilizing findings from both Turkish and English literature reviews, the importance and potential impacts of AI within the public sector, along with current trends, have been comprehensively assessed. This research delves into the concepts of AI and its subsets—super AI, GenAI, narrow AI, and chatbots—within the general framework of the public sector. China and the United States are pioneering and leading countries in terms of investment. Although the U.S. stands out in many areas regarding investment, China's integration of AI with national strategies and its policies indicate that it may play a more dominant role in the future. There are four main implementation areas of AI in the public sector: efficiency and automation, service delivery, data-driven governance, and ethical and regulatory challenges. A review of the literature reveals that the ethical, legal, and social implications of implementing AI in the public sector require more careful consideration. The study makes a significant contribution to the field of AI discussions in public administration and the public sector, providing a comprehensive assessment of current discussions on AI in the literature.</jats:p>",-1.0
72,10.1007/s43681-021-00120-w,The social dilemma in AI development and why we have to solve it,"<jats:title>Abstract</jats:title><jats:p>While the demand for ethical AI (AI) systems increases, the number of unethical uses of AI accelerates, even though there is no shortage of ethical guidelines. We argue that a possible underlying cause for this is that AI developers face a social dilemma in AI development ethics, preventing the widespread adaptation of ethical best practices. We define the social dilemma for AI development and describe why the current crisis in AI development ethics cannot be solved without relieving AI developers of their social dilemma. We argue that AI development must be professionalised to overcome the social dilemma, and discuss how medicine can be used as a template in this process.</jats:p>",0.0
73,10.1186/s12910-024-01066-4,Public perceptions of AI in healthcare: ethical concerns and opportunities for patient-centered care,"<jats:title>Abstract</jats:title><jats:sec>
                <jats:title>Background</jats:title>
                <jats:p>In an effort to improve the quality of medical care, the philosophy of patient-centered care has become integrated into almost every aspect of the medical community. Despite its widespread acceptance, among patients and practitioners, there are concerns that rapid advancements in AI may threaten elements of patient-centered care, such as personal relationships with care providers and patient-driven choices. This study explores the extent to which patients are confident in and comfortable with the use of these technologies when it comes to their own individual care and identifies areas that may align with or threaten elements of patient-centered care.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Methods</jats:title>
                <jats:p>An exploratory, mixed-method approach was used to analyze survey data from 600 US-based adults in the State of Florida. The survey was administered through a leading market research provider (August 10–21, 2023), and responses were collected to be representative of the state’s population based on age, gender, race/ethnicity, and political affiliation.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Results</jats:title>
                <jats:p>Respondents were more comfortable with the use of AI in health-related tasks that were not associated with doctor-patient relationships, such as scheduling patient appointments or follow-ups (84.2%). Fear of losing the ‘human touch’ associated with doctors was a common theme within qualitative coding, suggesting a potential conflict between the implementation of AI and patient-centered care. In addition, decision self-efficacy was associated with higher levels of comfort with AI, but there were also concerns about losing decision-making control, workforce changes, and cost concerns. A small majority of participants mentioned that AI could be useful for doctors and lead to more equitable care but only when used within limits.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Conclusion</jats:title>
                <jats:p>The application of AI in medical care is rapidly advancing, but oversight, regulation, and guidance addressing critical aspects of patient-centered care are lacking. While there is no evidence that AI will undermine patient-physician relationships at this time, there is concern on the part of patients regarding the application of AI within medical care and specifically as it relates to their interaction with physicians. Medical guidance on incorporating AI while adhering to the principles of patient-centered care is needed to clarify how AI will augment medical care.</jats:p>
              </jats:sec>",3.0
74,10.1007/s43681-024-00451-4,Safeguarding human values: rethinking US law for GenAI’s societal impacts,"<jats:title>Abstract</jats:title><jats:p>Our interdisciplinary study examines the effectiveness of US law in addressing the complex challenges posed by GenAI systems to fundamental human values, including physical and mental well-being, privacy, autonomy, diversity, and equity. Through the analysis of diverse hypothetical scenarios developed in collaboration with experts, we identified significant shortcomings and ambiguities within the existing legal protections. Constitutional and civil rights law currently struggles to hold AI companies responsible for AI-assisted discriminatory outputs. Moreover, even without considering the liability shield provided by Section 230, existing liability laws may not effectively remedy unintentional and intangible harms caused by AI systems. Demonstrating causal links for liability claims such as defamation or product liability proves exceptionally difficult due to the intricate and opaque nature of these systems. To effectively address these unique and evolving risks posed by GenAI, we propose a “Responsible AI Legal Framework”  that adapts to recognize new threats and utilizes a multi-pronged approach. This framework would enshrine fundamental values in legal frameworks, establish comprehensive safety guidelines, and implement liability models tailored to the complexities of human-AI interactions. By proactively mitigating unforeseen harms like mental health impacts and privacy breaches, this framework aims to create a legal landscape capable of navigating the exciting yet precarious future brought forth by GenAI technologies.</jats:p>",-1.0
75,10.1609/aimag.v40i1.2848,"AI, Robotics, Ethics, and the Military: A Canadian Perspective","<jats:p><jats:italic>Defense and security organizations depend upon science and technology to meet operational needs, predict and counter threats, and meet increasingly complex demands of modern warfare. AI and robotics could provide solutions to a wide range of military gaps and deficiencies. At the same time, the unique and rapidly evolving nature of AI and robotics challenges existing polices, regulations, and values, and introduces complex ethical issues that might impede their development, evaluation, and use by the Canadian Armed Forces (CAF). Early consideration of potential ethical issues raised by military use of emerging AI and robotics technologies in development is critical to their effective implementation. This article presents an ethics assessment framework for emerging AI and robotics technologies. It is designed to help technology developers, policymakers, decision makers, and other stakeholders identify and broadly consider potential ethical issues that might arise with the military use and integration of emerging AI and robotics technologies of interest. We also provide a contextual environment for our framework, as well as an example of how our framework can be applied to a specific technology. Finally, we briefly identify and address several pervasive issues that arose during our research.</jats:italic></jats:p>",0.0
76,10.3390/ai1020012,Cities of the Future? The Potential Impact of AI,"<jats:p>AI (AI), like many revolutionary technologies in human history, will have a profound impact on societies. From this viewpoint, we analyze the combined effects of AI to raise important questions about the future form and function of cities. Combining knowledge from computer science, urban planning, and economics while reflecting on academic and business perspectives, we propose that the future of cities is far from being a determined one and cities may evolve into ghost towns if the deployment of AI is not carefully controlled. This viewpoint presents a fundamentally different argument, because it expresses a real concern over the future of cities in contrast to the many publications who exclusively assume city populations will increase predicated on the neoliberal urban growth paradigm that has for centuries attracted humans to cities in search of work.</jats:p>",0.0
77,10.3389/frai.2024.1497705,The impact of pedagogical beliefs on the adoption of GenAI in higher education: predictive model from UTAUT2,"<jats:p>AI in Education (AIEd) offers advanced tools that can personalize learning experiences and enhance teachers’ research capabilities. This paper explores the beliefs of 425 university teachers regarding the integration of GenAI in educational settings, utilizing the UTAUT2 model to predict their acceptance and usage patterns through the Partial Least Squares (PLS) method. The findings indicate that performance expectations, effort expectancy, social influence, facilitating conditions, and hedonic motivation all positively impact the intention and behavior related to the use of AIEd. Notably, the study reveals that teachers with constructivist pedagogical beliefs are more inclined to adopt AIEd, underscoring the significance of considering teachers’ attitudes and motivations for the effective integration of technology in education. This research provides valuable insights into the factors influencing teachers’ decisions to embrace AIEd, thereby contributing to a deeper understanding of technology integration in educational contexts. Moreover, the study’s results emphasize the critical role of teachers’ pedagogical orientations in their acceptance and utilization of AI technologies. Constructivist educators, who emphasize student-centered learning and active engagement, are shown to be more receptive to incorporating AIEd tools compared to their transmissive counterparts, who focus on direct instruction and information dissemination. This distinction highlights the need for tailored professional development programs that address the specific beliefs and needs of different teaching philosophies. Furthermore, the study’s comprehensive approach, considering various dimensions of the UTAUT2 model, offers a robust framework for analyzing technology acceptance in education.</jats:p>",1.0
78,10.3390/civileng5040049,Optimizing the Utilization of GenAI (AI) in the AEC Industry: GenAI Prompt Engineering and Design,"<jats:p>GenAI (AI) holds significant potential for revolutionizing the Architecture, Engineering, and Construction (AEC) industry by automating complex tasks such as construction scheduling, hazard recognition, resource leveling, information retrieval from BIM, etc. However, realizing this potential requires a strategic approach to ensure effective utilization and maximum benefit. This paper presents guidelines for prompt design and engineering to elicit desired responses from GenAI, a GenAI tool, in AEC applications. Key steps include understanding user intent, leveraging model capabilities, and optimizing prompt structures. By following these guidelines, stakeholders in the AEC industry can harness the power of GenAI to improve construction scheduling processes, increase project efficiency, and ultimately drive innovation and growth in the industry. Several illustrative examples on construction scheduling and hazard recognition are provided to demonstrate the methodology proposed in this research. It is concluded that GenAI, when effectively utilized, significantly enhances project scheduling and hazard recognition capability in the AEC industry with minimal error.</jats:p>",4.0
79,10.1007/s43681-023-00359-5,Exploring the status of AI for healthcare research in Africa: a bibliometric and thematic analysis,"<jats:title>Abstract</jats:title><jats:p>This paper explores the status of AI (AI) for healthcare research in Africa. The aim was to use bibliometric and thematic analysis methods to determine the publication counts, leading authors, top journals and publishers, most active institutions and countries, most cited institutions, funding bodies, top subject areas, co-occurrence of keywords and co-authorship. Bibliographic data were collected on April 9 2022, through the Lens database, based on the critical areas of authorship studies, such as authorship pattern, number of authors, etc. The findings showed that several channels were used to disseminate the publications, including articles, conference papers, reviews, and others. Publications on computer science topped the list of documented subject categories. The Annals of Tropical Medicine and Public Health is the top journal, where articles on AI have been published. One of the top nations that published AI research was the United Kingdom. With 143 publications, Harvard University was the higher education institution that produced the most in terms of affiliation. It was discovered that the Medical Research Council was one of the funding organizations that supported research, resulting in the publication of articles in AI. By summarizing the current research themes and trends, this work serves as a valuable resource for researchers, practitioners, and funding organizations interested in AI for healthcare research in Africa.</jats:p>",-1.0
80,10.1007/s43681-024-00497-4,Anticipating impacts: using large-scale scenario-writing to explore diverse implications of GenAI in the news environment,"<jats:title>Abstract</jats:title><jats:p>The tremendous rise of GenAI has reached every part of society—including the news environment. There are many concerns about the individual and societal impact of the increasing use of GenAI, including issues such as disinformation and misinformation, discrimination, and the promotion of social tensions. However, research on anticipating the impact of GenAI is still in its infancy and mostly limited to the views of technology developers and/or researchers. In this paper, we aim to broaden the perspective and capture the expectations of three stakeholder groups (news consumers; technology developers; content creators) about the potential negative impacts of GenAI, as well as mitigation strategies to address these. Methodologically, we apply scenario-writing and use participatory foresight in the context of a survey (n = 119) to delve into cognitively diverse imaginations of the future. We qualitatively analyze the scenarios using thematic analysis to systematically map potential impacts of GenAI on the news environment, potential mitigation strategies, and the role of stakeholders in causing and mitigating these impacts. In addition, we measure respondents' opinions on a specific mitigation strategy, namely transparency obligations as suggested in Article 52 of the draft EU AI Act. We compare the results across different stakeholder groups and elaborate on different expected impacts across these groups. We conclude by discussing the usefulness of scenario-writing and participatory foresight as a toolbox for GenAI impact assessment.</jats:p>",-1.0
81,10.1007/s10462-024-10768-5,Revolutionizing personalized medicine with GenAI: a systematic review,"<jats:title>Abstract</jats:title><jats:sec>
                <jats:title>Background</jats:title>
                <jats:p>Precision medicine, targeting treatments to individual genetic and clinical profiles, faces challenges in data collection, costs, and privacy. GenAI offers a promising solution by creating realistic, privacy-preserving patient data, potentially revolutionizing patient-centric healthcare.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Objective</jats:title>
                <jats:p>This review examines the role of deep generative models (DGMs) in clinical informatics, medical imaging, bioinformatics, and early diagnostics, showcasing their impact on precision medicine.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Methods</jats:title>
                <jats:p>Adhering to PRISMA guidelines, the review analyzes studies from databases such as Scopus and PubMed, focusing on AI's impact in precision medicine and DGMs' applications in synthetic data generation.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Results</jats:title>
                <jats:p>DGMs, particularly Generative Adversarial Networks (GANs), have improved synthetic data generation, enhancing accuracy and privacy. However, limitations exist, especially in the accuracy of foundation models like Large Language Models (LLMs) in digital diagnostics.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Conclusion</jats:title>
                <jats:p>Overcoming data scarcity and ensuring realistic, privacy-safe synthetic data generation are crucial for advancing personalized medicine. Further development of LLMs is essential for improving diagnostic precision. The application of GenAI in personalized medicine is emerging, highlighting the need for more interdisciplinary research to advance this field.</jats:p>
              </jats:sec>",5.0
82,10.1108/jeim-02-2024-0095,Assessing the impact of digital service innovation (DSI) on business performance: the mediating effect of Artificial Intelligence (AI),"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>The research aims to explore the dynamic relationship between digital service innovation (DSI), AI (AI) and business performance (BPer) in service-based models with a focus on how AI-enhanced insights from service use and customer feedback can strengthen business strategies. The aims are to show that DSI and AI are key to driving growth and efficiency in the digital economy and to underscore AI’s role in utilizing contextual data to improve decision-making and business outcomes.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>The study uses general structural equation modeling to analyze Spanish manufacturing firms, focusing on medium-sized enterprises and including both business-to-business and business-to-consumer orientations. Data are drawn from the Iberian Balance Analysis System [Sistema de Análisis de Balances Ibéricos (SABI)] database, complemented by a Qualtrics survey to assess the integration of AI in decision-making processes. The methodology is designed to evaluate the interplay between DSI, AI and BPer, with the aim of identifying actionable insights for service-based business orientations.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p> The study clarifies the relationships between DSI, AI and BPer, providing new theoretical and empirical insights. The findings confirm DSI's direct positive impact on performance and suggest AI’s nuanced mediating role, emphasizing the need for strategic DSI-AI integration in manufacturing firms for enhanced performance.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title><jats:p> The research explains the synergistic bond between DSI and AI in boosting BPer and discovering how by-product data can be transformed into strategic insights.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Practical implications</jats:title><jats:p> This study advises manufacturing sector leaders to integrate DSI and AI for enhanced performance and competitive advantage, emphasizing the value of high-quality, contextual data for AI learning and decision-making.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p> Researchers will observe that the study confirms the positive impact of DSI on BPer, while also highlighting the significant role of AI in enhancing this effect.</jats:p></jats:sec>",4.0
83,10.1007/s43681-024-00424-7,"E-coaching systems and social justice: ethical concerns about inequality, coercion, and stigmatization","<jats:title>Abstract</jats:title><jats:p>Poor self-regulation has been linked to various behaviors that contribute to pressing societal issues, including rising household debt, inefficient use of sustainable resources, and increasing healthcare demands. In light of this observation, the prospect of individuals receiving automated, tailored support by “e-coaching systems” to scaffold and improve their self-regulation is thought to hold promise for making society-wide progress in addressing such issues. Though there may be legitimate reasons for promoting the use of such systems, and individuals might welcome the support, our aim in the present article is to contribute to the ethics of e-coaching by showing how societal pressures towards the widespread adoption of automated e-coaching systems raise concerns in relation to three distinct aspects of social justice. We argue that societal inequalities may be introduced or exacerbated by (1) unequal access to the technologies, (2) unequally distributed restrictions to liberty and subjection to coercion, and (3) the potentially disparate impact of the use of e-coaching technologies on (self-)stigmatizing perceptions of competence. The article offers a research agenda for studying and addressing these concerns.</jats:p>",0.0
84,10.1007/s43681-022-00186-0,Ethical concerns with replacing human relations with humanoid robots: an ubuntu perspective,"<jats:title>Abstract</jats:title><jats:p>This paper considers ethical concerns with regard to replacing human relations with humanoid robots. Many have written about the impact that certain types of relations with robots may have on us, and why we should be concerned about robots replacing human relations. There has, however, been no consideration of this issue from an African philosophical perspective. Ubuntu philosophy provides a novel perspective on how relations with robots may impact our own moral character and moral development. This paper first discusses what humanoid robots are, why and how humans tend to anthropomorphise them, and what the literature says about robots crowding out human relations. It then explains the ideal of becoming “fully human”, which pertains to being particularly moral in character. In ubuntu philosophy, we are not only biologically human, but must strive to become better, more moral versions of ourselves, to become fully human. We can become fully human by having other regarding traits or characteristics within the context of interdependent, or humane, relationships (such as by exhibiting human equality, reciprocity, or solidarity). This concept of becoming fully human is important in ubuntu philosophy. Having explained that idea, the main argument of the paper is then put forward: that treating humanoid robots as if they are human is morally concerning if they crowd out human relations, because such relations prevent us from becoming fully human. This is because we cannot experience human equality, solidarity, and reciprocity with robots, which can be seen to characterise interdependent, or humane, relations with human beings.</jats:p>",0.0
85,10.1007/s43681-021-00131-7,The ethical issues of the application of AI in healthcare: a systematic scoping review,"<jats:title>Abstract</jats:title><jats:p>AI (AI) is being increasingly applied in healthcare. The expansion of AI in healthcare necessitates AI-related ethical issues to be studied and addressed. This systematic scoping review was conducted to identify the ethical issues of AI application in healthcare, to highlight gaps, and to propose steps to move towards an evidence-informed approach for addressing them. A systematic search was conducted to retrieve all articles examining the ethical aspects of AI application in healthcare from Medline (PubMed) and Embase (OVID), published between 2010 and July 21, 2020. The search terms were “AI” or “machine learning” or “deep learning” in combination with “ethics” or “bioethics”. The studies were selected utilizing a PRISMA flowchart and predefined inclusion criteria. Ethical principles of respect for human autonomy, prevention of harm, fairness, explicability, and privacy were charted. The search yielded 2166 articles, of which 18 articles were selected for data charting on the basis of the predefined inclusion criteria. The focus of many articles was a general discussion about ethics and AI. Nevertheless, there was limited examination of ethical principles in terms of consideration for design or deployment of AI in most retrieved studies. In the few instances where ethical principles were considered, fairness, preservation of human autonomy, explicability and privacy were equally discussed. The principle of prevention of harm was the least explored topic. Practical tools for testing and upholding ethical requirements across the lifecycle of AI-based technologies are largely absent from the body of reported evidence. In addition, the perspective of different stakeholders is largely missing.</jats:p>",6.0
86,10.1609/aimag.v41i2.5294,DARPA's Impact on AI,"<jats:p><jats:italic>The Defense Advanced Research Project Agency's (DARPA) mission is to make pivotal investments leading to research breakthroughs that support national security. DARPA AI (AI) programs have emphasized the need for machines to perceive and interact with the world around them; to frame problems and to arrive at solutions and decisions based on reasoning; to implement those decisions, perhaps through consultation with a human or another machine; to learn; to explain the rationale for decisions; to adhere to rules of ethical behavior defined for humans; to adapt to dynamic environments; and, to do all of this in real‐time. In short, DARPA has always been interested in AI frameworks that integrate AI and computer science technologies, and the application of those frameworks to DARPA‐hard problems. In this article, we describe the significant role that DARPA has played in the establishment of AI, and introduce six articles that explore DARPA's Three Waves of AI.</jats:italic></jats:p>",8.0
87,10.1007/s43681-021-00111-x,Methodology for integrating AI in healthcare systems: learning from COVID-19 to prepare for Disease X,"<jats:title>Abstract</jats:title><jats:p>AI and edge devices have been used at an increased rate in managing the COVID-19 pandemic. In this article we review the lessons learned from COVID-19 to postulate possible solutions for a Disease X event. The overall purpose of the study and the research problems investigated is the integration of AI function in digital healthcare systems. The basic design of the study includes a systematic state-of-the-art review, followed by an evaluation of different approaches to managing global pandemics. The study design then engages with constructing a new methodology for integrating algorithms in healthcare systems, followed by analysis of the new methodology and a discussion. Action research is applied to review existing state of the art, and a qualitative case study method is used to analyse the knowledge acquired from the COVID-19 pandemic. Major trends found as a result of the study derive from the synthesis of COVID-19 knowledge, presenting new insights in the form of a conceptual methodology—that includes six phases for managing a future Disease X event, resulting with a summary map of various problems, solutions and expected results from integrating functional AI in healthcare systems.</jats:p>",3.0
88,10.1007/s43681-024-00443-4,AI hype as a cyber security risk: the moral responsibility of implementing GenAI in business,"<jats:title>Abstract</jats:title><jats:p>This paper examines the ethical obligations companies have when implementing GenAI (AI). We point to the potential cyber security risks companies are exposed to when rushing to adopt GenAI solutions or buying into “AI hype”. While the benefits of implementing GenAI solutions for business have been widely touted, the inherent risks associated have been less well publicised. There are growing concerns that the race to integrate GenAI is not being accompanied by adequate safety measures. The rush to buy into the hype of GenAI and not fall behind the competition is potentially exposing companies to broad and possibly catastrophic cyber-attacks or breaches. In this paper, we outline significant cyber security threats GenAI models pose, including potential ‘backdoors’ in AI models that could compromise user data or the risk of ‘poisoned’ AI models producing false results. In light of these the cyber security concerns, we discuss the moral obligations of implementing GenAI into business by considering the ethical principles of beneficence, non-maleficence, autonomy, justice, and explicability. We identify two examples of ethical concern, <jats:italic>overreliance</jats:italic> and <jats:italic>over-trust</jats:italic> in GenAI, both of which can negatively influence business decisions, leaving companies vulnerable to cyber security threats. This paper concludes by recommending a set of checklists for ethical implementation of GenAI in business environment to minimise cyber security risk based on the discussed moral responsibilities and ethical concern.</jats:p>",8.0
89,10.1007/s10551-023-05339-7,The Ethical Implications of AI (AI) For Meaningful Work,"<jats:title>Abstract</jats:title><jats:p>The increasing workplace use of artificially intelligent (AI) technologies has implications for the experience of meaningful human work. Meaningful work refers to the perception that one’s work has worth, significance, or a higher purpose. The development and organisational deployment of AI is accelerating, but the ways in which this will support or diminish opportunities for meaningful work and the ethical implications of these changes remain under-explored. This conceptual paper is positioned at the intersection of the meaningful work and ethical AI literatures and offers a detailed assessment of the ways in which the deployment of AI can enhance or diminish employees’ experiences of meaningful work. We first outline the nature of meaningful work and draw on philosophical and business ethics accounts to establish its ethical importance. We then explore the impacts of three paths of AI deployment (replacing some tasks, ‘tending the machine’, and amplifying human skills) across five dimensions constituting a holistic account of meaningful work, and finally assess the ethical implications. In doing so we help to contextualise the meaningful work literature for the era of AI, extend the ethical AI literature into the workplace, and conclude with a range of practical implications and future research directions.
</jats:p>",-1.0
90,10.1007/s43681-021-00077-w,"The ethics of facial recognition technologies, surveillance, and accountability in an age of AI: a comparative analysis of US, EU, and UK regulatory frameworks","<jats:title>Abstract</jats:title><jats:p>The rapid development of facial recognition technologies (FRT) has led to complex ethical choices in terms of balancing individual privacy rights versus delivering societal safety. Within this space, increasingly commonplace use of these technologies by law enforcement agencies has presented a particular lens for probing this complex landscape, its application, and the acceptable extent of citizen surveillance. This analysis focuses on the regulatory contexts and recent case law in the United States (USA), United Kingdom (UK), and European Union (EU) in terms of the use and misuse of FRT by law enforcement agencies. In the case of the USA, it is one of the main global regions in which the technology is being rapidly evolved, and yet, it has a patchwork of legislation with less emphasis on data protection and privacy. Within the context of the EU and the UK, there has been a critical focus on the development of accountability requirements particularly when considered in the context of the EU’s General Data Protection Regulation (GDPR) and the legal focus on Privacy by Design (PbD). However, globally, there is no standardised human rights framework and regulatory requirements that can be easily applied to FRT rollout. This article contains a discursive discussion considering the complexity of the ethical and regulatory dimensions at play in these spaces including considering data protection and human rights frameworks. It concludes that data protection impact assessments (DPIA) and human rights impact assessments together with greater transparency, regulation, audit and explanation of FRT use, and application in individual contexts would improve FRT deployments. In addition, it sets out ten critical questions which it suggests need to be answered for the successful development and deployment of FRT and AI more broadly. It is suggested that these should be answered by lawmakers, policy makers, AI developers, and adopters.</jats:p>",0.0
91,10.1177/00986283241287203,"Using GenAI to Promote Psychological, Feedback, and AI Literacies in Undergraduate Psychology","<jats:sec><jats:title>Background</jats:title><jats:p> With the arrival of GenAI (genAI) tools, psychology educators are rethinking their assessment practices. </jats:p></jats:sec><jats:sec><jats:title>Objective</jats:title><jats:p> This paper describes one approach to integrating genAI into an assessment designed to promote psychological literacy. </jats:p></jats:sec><jats:sec><jats:title>Method</jats:title><jats:p> Students used GenAI to generate a media release about a published article and then wrote a critique. We evaluated whether students were able to use the marking rubric to assess the GenAI output, and whether working with the rubric early in the assessment process had benefits for their grades on subsequent tasks. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> The results show that students accurately assessed the GenAI output against the marking rubric, judging the output to be stylistically good but lacking in accurate coverage of the aims, methods, and results of the research. Working with genAI and the marking rubric early in the assessment process had benefits for performance, relative to cohorts that had engaged in peer review. </jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p> By allowing students to use genAI and scaffolding the process of critiquing and revising, students gained competencies in psychological, feedback, and AI literacies. </jats:p></jats:sec><jats:sec><jats:title>Teaching implications</jats:title><jats:p> Integrating genAI presents opportunities for learning, if educators can think beyond the artifact and design assessment that allows our students to showcase their learning process. </jats:p></jats:sec>",1.0
92,10.3389/frai.2024.1474019,A GenAI-driven interactive listening assessment task,"<jats:sec><jats:title>Introduction</jats:title><jats:p>Assessments of interactional competence have traditionally been limited in large-scale language assessments. The listening portion suffers from construct underrepresentation, whereas the speaking portion suffers from limited task formats such as in-person interviews or role plays. Human-delivered tasks are challenging to administer at large scales, while automated assessments are typically very narrow in their assessment of the construct because they have carried over the limitations of traditional paper-based tasks to digital formats. However, computer-based assessments do allow for more interactive, automatically administered tasks, but come with increased complexity in task creation. Large language models present new opportunities for enhanced automated item generation (AIG) processes that can create complex content types and tasks at scale that support richer assessments.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>This paper describes the use of such methods to generate content at scale for an interactive listening measure of interactional competence for the Duolingo English Test (DET), a large-scale, high-stakes test of English proficiency. The Interactive Listening task assesses test takers’ ability to participate in a full conversation, resulting in a more authentic assessment of interactive listening ability than prior automated assessments by positing comprehension and interaction as purposes of listening.</jats:p></jats:sec><jats:sec><jats:title>Results and discussion</jats:title><jats:p>The results of a pilot of 713 tasks with hundreds of responses per task, along with the results of human review, demonstrate the feasibility of a human-in-the-loop, GenAI-driven approach for automatic creation of complex educational assessments at scale.</jats:p></jats:sec>",1.0
93,10.3389/frai.2024.1437315,GenAI with WGAN-GP for boosting seizure detection accuracy,"<jats:sec><jats:title>Background</jats:title><jats:p>Imbalanced datasets pose challenges for developing accurate seizure detection systems based on electroencephalogram (EEG) data. GenAI techniques may help augment minority class data to facilitate automatic epileptic seizure detection.</jats:p></jats:sec><jats:sec><jats:title>New method</jats:title><jats:p>This study investigates the impact of various data augmentation (DA) approaches, including Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), Vanilla GAN, Conditional GAN (CGAN), and Cramer GAN, on classification performance with Random Forest models. The best-performing GAN variant, WGAN-GP, was then integrated with a bidirectional Long Short-Term Memory (LSTM) architecture and compared against traditional and synthetic oversampling methods.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The evaluation of different GAN variants for data augmentation with Random Forest classifiers identified WGAN-GP as the most effective approach. The integration of WGAN-GP with bidirectional LSTM yielded substantial performance improvements, outperforming traditional oversampling methods and achieving an accuracy of 91.73% on the augmented data, compared to 86% accuracy on real data without augmentation.</jats:p></jats:sec><jats:sec><jats:title>Comparison with existing methods</jats:title><jats:p>The proposed GenAI approach combining WGAN-GP and recurrent neural network models outperforms comparative synthetic oversampling methods on metrics relevant for reliable seizure detection from imbalanced EEG datasets.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Incorporating the WGAN-GP GenAI technique for data augmentation and integrating it with bidirectional LSTM elevates seizure detection accuracy for imbalanced EEG datasets, surpassing the performance of traditional oversampling and class weight adjustment methods. This approach shows promise for improving epilepsy monitoring and management through enhanced automated detection system effectiveness.</jats:p></jats:sec>",5.0
94,10.1007/s43681-024-00553-z,Eudemonia of a machine,"<jats:title>Abstract</jats:title><jats:p>Henry Ford once said, “For most purposes, a man with a machine is better than a man without a machine.” To this, engineers today propose an addendum – “and a man that <jats:italic>is</jats:italic> a machine is best of all” – which they have made their goal. The world over, engineers are working to make the ultimate machine, “the holy grail of AI,” a <jats:italic>conscious</jats:italic> humanoid. On the one hand, such a “machine” will be capable of relieving us of all our burdens. On the other hand, in so doing, will we not have “birthed,” as it were, a new class of slaves? In this essay I seek to summarize the various arguments made in this debate, bring to bear moral positions from the philosophy of technology, philosophy of law and philosophy of religion, as well as demonstrate the moral impropriety of such an endeavor from each of the classic moral approaches (i.e., Virtue Ethics, Consequentialism, Kantian Deontology). Finally, given that the debate centers around what is the “good life” for human or humanoid, I expand upon Aristotle’s Eudemonia and Maimonides’ <jats:italic>Summum Bonum</jats:italic> to argue that life is precious in its affordance to allow conscious beings, human or humanoid, to aspire to the best life possible.</jats:p>",0.0
95,10.1108/jices-03-2020-0034,An exploration of the impact of AI (AI) and automation for communication professionals,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>AI (AI) and automation are currently changing human life with a great implication in the communication field. This research focusses on understanding the current and growing impact of AI and automation in the role of communication professionals to identify what skills and training are needed to face its impacts leading to a recommendation.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>The research involves methodological triangulation, analysing and comparing data gathered from consulting with experts using the Delphi method, focus group with communication students, and literature review.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>Findings show that the likely impacts are on the one hand the enhancing of efficiency and productivity, as well as freeing communication professionals to focus on the creative side, strategy and analytical thinking, on the other hand, repetitive and low-level jobs could be lost, being higher position jobs or those involving creativity and decision making harder to automate. Two types of training are needed: to gather experience with the current AI and automated tools, and to focus on developing human qualities that AI cannot replicate.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>The outcomes of this research are valuable to help current and future communication practitioners, as well as organisations, to be one step ahead and survive the age AI and automation, being aware of its current and near-future impacts. The paper offers a list of recommended soft and technical skills, as well as training needed, categorizing them in low, medium and high priority.</jats:p>
</jats:sec>",-1.0
96,10.21272/bel.6(2).6-11.2022,Ethical Concerns While Using AI in Recruitment of Employees,"<jats:p>AI has evolved as an alternative to human intelligence. It affects the lives of billions of people. It mimics humans by solving problems and understanding the task. These AI technologies must have some moral values and ethics incorporated within itself. The usage of AI is growing worldwide, posing more ethical issues to consider. In recent years, many companies have used various AI tools such as chatbots and face recognition software for fulfilling their hiring needs. This research work will focus on such devices that help manage one of the important functions of human resources: recruitment. It will identify various challenges and ethical issues that a firm faces while assimilating AI tools in the process of Recruitment. The hiring companies need to make the job seekers realize that AI-powered tools would be free from discrimination and safeguard privacy. The purpose of the study is to identify the ethical issues while incorporating AI into hiring needs. The study will be based on reviews and features of applications. The study mentions various applications whose features might be unethical for job seekers. Findings reveal that the significant unethical issues faced by the hiring companies are Data privacy and unconscious biasness. The biasness is due to the algorithm that works according to the inputs fed to build it, and the programmer might have subconscious biasness in his mind. AI has restored concerns regarding privacy and data protection. According to a report by UNESCO, Women make up only 22% of all AI professionals. Gender prejudices and stereotyping are perpetuated in AI technologies due to their underrepresentation in the sector. Virtual personal assistants like Siri, Alexa, and Cortana are “female” by default, which is no accident. The submissiveness they display is an illustration of how AI (AI) might continue to support and extend gender bias in our society.</jats:p>",0.0
97,10.3390/su14031730,AI (AI) and Sustainable Development Goals (SDGs): Exploring the Impact of AI on Politics and Society,"<jats:p>In popular discourse, AI (AI) has turned into one of the most inexplicable issues It has turned into an artifact that people do not dare to question [...]</jats:p>",0.0
98,10.15407/jai2024.03.032,"AI: free will, self-consciousness and ethics.","<jats:p>The article discusses key aspects of AI creation, including issues of free will, self-awareness and ethics. The focus is on the neurobiological basis of consciousness, in particular the structure and functions of the new cerebral cortex, as well as the mechanisms of recognition, memory and prediction, which are important for modelling cognitive processes in artificial systems. The paper discusses the role of neural networks in reproducing cognitive functions, such as perception and decision making, and presents modern approaches to training neural networks. A separate part of the paper is devoted to the issue of modelling self-awareness and subjective experience in AI and how realistic it is to create self-aware machines. Ethical issues of AI creation are at the centre of the discussion, including the topics of the rights of self-aware machines, their responsibilities and their role in society. The article considers the possible social consequences of the emergence of artificial personalities, the need to develop new legal frameworks and legal protections for such beings. It also discusses the problem of free will in the context of both biological and artificial systems, citing experiments and philosophical theories that question free will as a phenomenon. It concludes that the creation of AI has great potential, but requires careful ethical and legal analysis to ensure the harmonious integration of artificial persons into social and legal structures.</jats:p>",0.0
99,10.3389/frai.2024.1377011,Ethics and responsible AI deployment,"<jats:p>As AI (AI) becomes more prevalent, protecting personal privacy is a critical ethical issue that must be addressed. This article explores the need for ethical AI systems that safeguard individual privacy while complying with ethical standards. By taking a multidisciplinary approach, the research examines innovative algorithmic techniques such as differential privacy, homomorphic encryption, federated learning, international regulatory frameworks, and ethical guidelines. The study concludes that these algorithms effectively enhance privacy protection while balancing the utility of AI with the need to protect personal data. The article emphasises the importance of a comprehensive approach that combines technological innovation with ethical and regulatory strategies to harness the power of AI in a way that respects and protects individual privacy.</jats:p>",0.0
100,10.1007/s13347-022-00498-3,Images of AI: a Blind Spot in AI Ethics,"<jats:title>Abstract</jats:title><jats:p>This paper argues that the AI ethics has generally neglected the issues related to the science communication of AI. In particular, the article focuses on visual communication about AI and, more specifically, on the use of certain stock images in science communication about AI — in particular, those characterized by an excessive use of blue color and recurrent subjects, such as androgyne faces, half-flesh and half-circuit brains, and variations on Michelangelo’s <jats:italic>The Creation of Adam</jats:italic>. In the first section, the author refers to a “referentialist” ethics of science communication for an ethical assessment of these images. From this perspective, these images are unethical. While the ethics of science communication generally promotes virtues like modesty and humility, similar images are arrogant and overconfident. In the second section, the author uses French philosopher Jacques Rancière’s concepts of “distribution of the sensible,” “disagreement,” and “pensive image.” Rancière’s thought paves the way to a deeper critique of these images of AI. The problem with similar images is not their lack of reference to the “things themselves.” It rather lies in the way they stifle any possible forms of disagreement about AI. However, the author argues that stock images and other popular images of AI are not a problem per se, and they can also be a resource. This depends on the real possibility for these images to support forms of pensiveness. In the conclusion, the question is asked whether the kind of ethics or politics of AI images proposed in this article can be applied to AI ethics tout court.</jats:p>",2.0
101,10.1007/s43681-024-00488-5,AI art and public literacy: the miseducation of Ai-Da the robot,"<jats:title>Abstract</jats:title><jats:p>This article examines the implications of the artworks and public performances of the robot artist Ai-Da. While the project claims to advance AI public literacy and foster critical debate around intelligent systems, it instead ends up perpetuating popular misunderstandings about AI creativity, agency, and consciousness. Built in 2019, Ai-Da is a humanoid robot capable of creating drawings, paintings, and composing poetry. However, the project often conceals or miscommunicates the technical aspects of Ai-Da’s capabilities in a manner that encourages the public to misattribute human-like traits to the robot. This lack of transparency in the presentation of Ai-Da’s abilities and the creative processes involved risks reinforcing existing misconceptions about AI, rather than promoting a more nuanced understanding. By employing discourse analysis and drawing on scholarship on machine and computational creativity, anthropomorphism in social robots, and posthuman embodiment, this article uses the Ai-Da project as a case study to illustrate how the dangers of AI hype can be obscured when presented through the lens of public art. The analysis examines how the Ai-Da project, despite its stated goals of advancing AI literacy, fails to effectively challenge and may even exacerbate public misperceptions about the nature of AI-generated art and creativity.</jats:p>",2.0
102,10.32609/0042-8736-2024-10-110-127,The impact of AI on creative industries: Freelancers’ anxieties and concerns,"<jats:p>The article examines the impact of the rapid development of AI (AI) technologies on the creative industries and the concerns of workers in this field regarding the potential deterioration of their working conditions and displacement from the labor market. The aim of the study is to identify the degree of concern among freelancers engaged in intellectual and creative professions regarding competition with AI and to assess their perception of AI’s current capabilities in making creative content. The empirical basis was provided by online survey data of 778 Russian freelancers receiving jobs through the Freelance.ru digital platform, conducted in spring 2024. It was found that many respondents are already actively using AI in their work. The majority of freelancers note AI’s high current capabilities in creating texts, images, translation, and other areas, and more than a third believe that in the coming years AI will be able to do their typical work as well or even better than they do it themselves. Those who were least likely to experience concerns about their future were individuals who had been trained in AI, used it to perform job tasks, satisfied with their work, and had a high level of income, i.e., generally had a stable position in the labor market. Despite the concerns of some workers, the development of AI opens up new opportunities for the creative industries; however, regular monitoring of the situation is required to develop measures to adapt the labor market.</jats:p>",-1.0
103,10.15407/jai2023.02.010,The impact of AI on modern education: prospects and challenges,"<jats:p>This paper examines the intricate links between AI (AI) and education, delving into both theoretical and practical aspects while evaluating possible ramifications for labor market dynamics, professional activity, and wider educational paradigms. Our research methodology involved analyzing relevant scientific literature, classifying data, consulting with subject matter experts, and synthesizing the results. Our research suggests that AI has the ability to greatly improve pedagogical processes, personalize learning experiences to meet individual student needs, and successfully address the time and financial limitations that are inherent in traditional educational models. However, our study also reveals challenges related to data confidentiality, potential plagiarism and fraud associated with AI use, and socioeconomic disparities resulting from unequal technology access. Additionally, we identified a significant gap in current AI usage standards legislation. It is essential for researchers, educators, and policymakers to recognize the potential risks of AI implementation in educational settings and proactively develop strategies that prioritize ethics, safety, and effectiveness. With labor market trends favoring specialists knowledgeable in utilizing AI tools, a consequent change in curricula is expected. In response to our findings, we recommend the creation of new academic disciplines that concentrate on the cultivation of AI expertise; the establishment of comprehensive national AI strategies; the crafting of retraining roadmaps for those who may be affected by AI automation; the inclusion of online AI courses in existing educational programs; and the promotion of grant funding for future AI research. Our future research will concentrate on reducing the potential negative impacts of integrating AI into educational systems.</jats:p>",1.0
104,10.1386/sfs_00082_1,AI in Indian films: Anukul and AI ethics,"<jats:p>In recent years, AI (AI) has emerged as an important theme in Indian cinema. Taking note of Indian filmmakers’ interest in AI, this article examines the Hindi-language short film <jats:italic>Anukul</jats:italic> directed by Sujoy Ghosh. Based on a short story by the legendary auteur Satyajit Ray, the film draws us to a posthuman world order in which machines have assumed rights and threaten to push humans out of the loop. The AI–human interface in <jats:italic>Anukul</jats:italic> provokes a number of ethical questions and this article aims to think through some of the ethical and legal concerns raised in the film.</jats:p>",0.0
105,10.3390/educsci14101062,University Students’ Insights of GenAI (AI) Writing Tools,"<jats:p>The current study examined university students’ insights into GenAI writing tools regarding their familiarity with, perceived concerns about, and perceived benefits of these tools in their academic work. The study used a cross-sectional descriptive research design, and data were collected using a questionnaire instrument. The participants were ninety-five undergraduate and graduate students from a College of Education at a university in Jordan. The results show that university students show moderate familiarity with GenAI writing tools (M = 3.14, SD = 0.81), especially in engagement but lacking technical knowledge. They also have moderate concerns (M = 3.35, SD = 0.85), particularly about misinformation and data security. Despite these concerns, students recognize the benefits (M = 3.62, SD = 0.81), especially regarding the capabilities of these tools in simulating creativity and fostering innovation. In addition, the results showed that gender and educational level appear to have little effect on familiarity, concerns, and perceived benefits regarding these tools. Based on the findings, the study recommends enhancing students’ familiarity with GenAI tools through providing technical training, hands-on opportunities, and ethical discussions. In addition, the study recommends addressing students’ concerns regarding GenAI writing tools by improving data security related to GenAI, providing ethical guidelines regarding the use of these tools, and boosting AI literacy. Finally, it is recommended to enhance students’ perceptions of the benefits of GenAI writing tools by highlighting the creative potential of these tools within the educational setting, using these tools to offer personalized learning experiences that adapt to individual learning styles, and promoting collaboration through GenAI writing tools.</jats:p>",1.0
106,10.3389/fsoc.2024.1417766,AI and real decisions: predictive systems and GenAI vs. emotive-cognitive legal deliberations,"<jats:p>The use of AI in law represents one of the biggest challenges across different legal systems. Supporters of predictive systems believe that decisionmaking could be more efficient, consistent and predictable by using AI. European legislation and legal scholars, however, identify areas where AI developments are at high risk or too dangerous to be used in judicial proceedings. In this article, we contribute to this debate by problematizing predictive systems based on previous judgments and the growing use of GenAI in judicial proceedings. Through illustrations from real criminal cases in Italian courts and prosecution offices, we show misalignments between the functions of AI systems and the essential features of legal decision-making and identify possible legitimate usages. We argue that current predictive systems and GenAI crunch the complexity of judicial proceedings, the dynamics of fact-finding and legal encoding. They reduce the delivery of justice to statistical connections between data or metadata, cutting off the emotive-cognitive process that lies at the core of legal decision-making.</jats:p>",0.0
107,10.1007/s43681-023-00325-1,Responsible AI in human resources management: a review of the empirical literature,"<jats:title>Abstract</jats:title><jats:p>As it is the case for many business processes and activities disciplines, AI (AI) is increasingly integrated in human resources management (HRM). While AI has great potential to augment the HRM activities in organizations, automating the management of humans is not without risks and limitations. The identification of these risks is fundamental to promote responsible use of AI in HRM. We thus conducted a review of the empirical academic literature across disciplines on the affordances and responsible principles of AI in HRM. This is the first review of responsible AI in HRM that focuses solely on studies containing observations, measurements, and tests about this phenomenon. The multi-domain and multidisciplinary approach and empirical focus provides a better understanding of the reality of the development, study, and deployment of AI in HRM and sheds light on how these are conducted responsibly. We conclude with a call for research based on what we identified as the most needed and promising avenues.</jats:p>",-1.0
108,10.1007/s10506-024-09412-y,"An interdisciplinary account of the terminological choices by EU policymakers ahead of the final agreement on the AI Act: AI system, general purpose AI system, foundation model, and GenAI","<jats:title>Abstract</jats:title><jats:p>The European Union’s AI Act (AI Act) is a groundbreaking regulatory framework that integrates technical concepts and terminology from the rapidly evolving ecosystems of AI research and innovation into the legal domain. Precise definitions accessible to both AI experts and lawyers are crucial for the legislation to be effective. This paper provides an interdisciplinary analysis of the concepts of <jats:italic>AI system</jats:italic>, <jats:italic>general purpose AI system</jats:italic>, <jats:italic>foundation model</jats:italic> and <jats:italic>GenAI</jats:italic> across the different versions of the legal text (Commission proposal, Parliament position and Council General Approach) before the final political agreement. The goal is to help bridge the understanding of these key terms between the technical and legal communities and contribute to a proper implementation of the AI Act. We provide an analysis of the concept of <jats:italic>AI system</jats:italic> considering its scientific foundation and the crucial role that it plays in the regulation, which requires a sound definition both from legal and technical standpoints. We connect the outcomes of this discussion with the analysis of the concept of <jats:italic>general purpose AI system</jats:italic> and its evolution during the negotiations. We also address the distinct conceptual meanings of <jats:italic>AI system</jats:italic> vs <jats:italic>AI model</jats:italic> and explore the technical nuances of the term <jats:italic>foundation model</jats:italic>. We conclude that rooting the definition of <jats:italic>foundation model</jats:italic> to its general purpose capabilities following standardised evaluation methodologies appears to be most appropriate approach. Lastly, we tackle the concept of <jats:italic>GenAI</jats:italic>, arguing that definitions of <jats:italic>AI system</jats:italic> that include “content” as one of the system’s outputs already captures it, and concluding that not all <jats:italic>GenAI</jats:italic> is based on <jats:italic>foundation models</jats:italic>.</jats:p>",0.0
109,10.15407/jai2023.02.094,Resonance diagnostics of production space of generative systems of AI,"<jats:p>The development of AI generative systems (AIGS) in the modern world requires addressing issues related to the quality, stability, and efficiency of the generated content. In this context, resonance diagnostics become of paramount importance. The purpose of this study is to explore the possibilities of applying resonance diagnostics for detecting, analyzing, and resolving problems in AI generative systems. To achieve the set goal, the following tasks were identified: analysis of the theoretical foundations of resonance diagnostics; investigation of the potential of using resonance signals to adjust AIGS learning parameters; studying the impact of resonance diagnostics on the stability and adaptation of AIGS to changing operating conditions. The study conducted an analysis of resonance diagnostics in the context of AIGS and revealed its powerful influence on addressing issues related to system quality and productivity. The research demonstrated that resonance diagnostics can be used to achieve realism, diversity, and quality of generated content. Additionally, it was determined that it can contribute to enhancing the stability and adaptation of systems to varying operational conditions</jats:p>",2.0
110,10.1007/s43681-024-00536-0,Minimum levels of interpretability for artificial moral agents,"<jats:title>Abstract</jats:title><jats:p>As AI (AI) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. For models involved in moral decision-making (MDM), also known as artificial moral agents (AMA), interpretability provides a way to trust and understand the agent’s internal reasoning mechanisms for effective use and error correction. In this paper, we bridge the technical approaches to interpretability with construction of AMAs to establish minimal safety requirements for deployed AMAs. We begin by providing an overview of AI interpretability in the context of MDM, thereby framing different levels of interpretability (or transparency) in relation to the different ways of constructing AMAs. Introducing the concept of the Minimum Level of Interpretability (MLI) and drawing on examples from the field, we explore two overarching questions: whether a lack of model transparency prevents trust and whether model transparency helps us sufficiently understand AMAs. Finally, we conclude by recommending specific MLIs for various types of agent constructions, aiming to facilitate their safe deployment in real-world scenarios.</jats:p>",0.0
111,10.1007/s43681-023-00296-3,AI’s right to life,"<jats:title>Abstract</jats:title><jats:p>The right to life is fundamental and primary and is a precondition for exercising other rights (Ramcharan in Ramcharan (ed), The right to life in International Law, Martinus Nijhoff Publishers, Dordrecht, 1985). Its universal recognition in the arena of international law is associated with the concept of a human being endowed with inherent and inalienable dignity. Categorization of the circle of entities covered with the right to life today seems obvious and indisputable. Intense development of AI, also the fact that it has passed the Turing test which checks AI’s thinking ability in a way similar to human reasoning, inspires a reflection on AI’s future legal status. This study will investigate a thesis of whether AI may be entitled to the right to life. The analysis will be carried out around an exploratory question: what are the requirements for being afforded protection of the right to life?</jats:p>",0.0
112,10.1007/s10676-024-09745-x,Ethics of GenAI and manipulation: a design-oriented research agenda,"<jats:title>Abstract</jats:title><jats:p>GenAI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around GenAI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of GenAI technologies.</jats:p>",2.0
113,10.1177/02666669231200628,AI in developing countries: The impact of GenAI (AI) technologies for development,"<jats:p> This paper explores the potential impact of GenAI (GenAI) on developing countries, considering both positive and negative effects across various domains of information, culture, and industry. GenAI refers to AI (AI) systems that generate content, such as text, audio, or video, aiming to produce novel and creative outputs based on training data. Compared to conversational AI, GenAI systems have the unique capability of not only providing replies but also generating the content of those responses. Recent advancements in AI during the Fourth Industrial Revolution, exemplified by tools like GenAI, have gained popularity and reshaped content production and creation. However, the benefits of GenAI are not equally accessible to all, especially in developing countries, where limited access to cutting-edge technologies and inadequate infrastructure pose challenges. This paper seeks to understand the potential impact of GenAI technologies on developing countries, considering economic growth, access to technology, and the potential paradigm shift in education, healthcare, and the environment. The findings emphasize the importance of providing the necessary support and infrastructure to ensure that GenAI contributes to inclusive development rather than deepening existing inequalities. The study highlights the significance of integrating GenAI into the context of the Fourth Industrial Revolution in developing countries, where technological change is a crucial determinant of progress and equitable growth. </jats:p>",-1.0
114,10.1017/s0963180123000464,AI and Human Enhancement: Can AI Technologies Make Us More (Artificially) Intelligent?,"<jats:title>Abstract</jats:title><jats:p>This paper discusses two opposing views about the relation between AI (AI) and human intelligence: on the one hand, a worry that heavy reliance on AI technologies might make people less intelligent and, on the other, a hope that AI technologies might serve as a form of cognitive enhancement. The worry relates to the notion that if we hand over too many intelligence-requiring tasks to AI technologies, we might end up with fewer opportunities to train our own intelligence. Concerning AI as a potential form of cognitive enhancement, the paper explores two possibilities: (1) AI as extending—and thereby enhancing—people’s minds, and (2) AI as enabling people to behave in artificially intelligent ways. That is, using AI technologies might enable people to behave as if they have been cognitively enhanced. The paper considers such enhancements both on the level of individuals and on the level of groups.</jats:p>",0.0
115,10.3390/ai5030080,Perspectives for GenAI-Assisted Art Therapy for Melanoma Patients,"<jats:p>Digital technologies are making their mark in medicine, and especially also in art therapy, offering innovative therapeutic interventions for patients, including those with melanoma skin cancer. However, the integration of novel technologies, such as AI-generated art, brings along ethical, psychological, and technical challenges that are viewed differently among therapists. We aim to gauge art therapists’ views on the ethical, application, and challenge facets of utilizing AI-generated art from medical images in therapy. The focus is on assessing its applicability and limitations for melanoma patients. Art therapists were surveyed via a questionnaire focusing on their experience, digital tool familiarity, and views on AI in therapy, encompassing ethics, benefits, challenges, and applicability for melanoma. Art therapists have already implemented digital technologies and acknowledged potential therapeutic benefits of creating personalized artworks with GenAI. Attention needs to be given to technological hurdles and the necessity for supplementary interventions. Views on the method’s adaptability varied, underscoring a need for tailored, patient-focused applications. Art therapists are welcoming AI-generated art as a promising creative therapeutic tool and acknowledge potential therapeutic benefits. There are ethical, technical, and psychological challenges that must be addressed for application in therapeutic sessions. Therapists should navigate AI integration with sensitivity, adhering to ethical norms around consent and privacy. Future studies should show the therapeutic benefit in practice with emphasis on equipping therapists to manage the technical complexities effectively. Furthermore, it is important to ensure that patients can influence the AI output, allowing for creative moments in the process.</jats:p>",2.0
116,10.1007/s43681-022-00246-5,Artificial moral experts: asking for ethical advice to artificial intelligent assistants,"<jats:title>Abstract</jats:title><jats:p>In most domains of human life, we are willing to accept that there are experts with greater knowledge and competencies that distinguish them from non-experts or laypeople. Despite this fact, the very recognition of expertise curiously becomes more controversial in the case of “moral experts”. Do moral experts exist? And, if they indeed do, are there ethical reasons for us to follow their advice? Likewise, can emerging technological developments broaden our very concept of moral expertise? In this article, we begin by arguing that the objections that have tried to deny the existence (and convenience) of moral expertise are unsatisfactory. After that, we show that people have ethical reasons to ask for a piece of moral advice in daily life situations. Then, we argue that some AI (AI) systems can play an increasing role in human morality by becoming moral experts. Some AI-based moral assistants can qualify as artificial moral experts and we would have good ethical reasons to use them.</jats:p>",0.0
117,10.1007/s00146-022-01578-w,AI ethics with Chinese characteristics? Concerns and preferred solutions in Chinese academia,"<jats:title>Abstract</jats:title><jats:p>Since Chinese scholars are playing an increasingly important role in shaping the national landscape of discussion on AI ethics, understanding their ethical concerns and preferred solutions is essential for global cooperation on governance of AI. This article, therefore, provides the first elaborated analysis on the discourse on AI ethics in Chinese academia, via a systematic literature review. This article has three main objectives. (1) to identify the most discussed ethical issues of AI in Chinese academia and those being left out (the question of “what”); (2) to analyze the solutions proposed and preferred by Chinese scholars (the question of “how”); and (3) to map out whose voices are dominating and whose are in the marginal (the question of “who”). Findings suggest that in terms of short-term implications, Chinese scholars’ concerns over AI resemble predominantly the content of international ethical guidelines. Yet in terms of long-term implications, there are some significant differences needed to be further addressed in a cultural context. Further, among a wide range of solution proposals, Chinese scholars seem to prefer strong-binding regulations to those weak ethical guidelines. In addition, this article also found that the Chinese academic discourse was dominated by male scholars and those who are from elite universities, which arguably is not a unique phenomenon in China.</jats:p>",0.0
118,10.3390/ai4010003,Ethics &amp; AI: A Systematic Review on Ethical Concerns and Related Strategies for Designing with AI in Healthcare,"<jats:p>In modern life, the application of AI (AI) has promoted the implementation of data-driven algorithms in high-stakes domains, such as healthcare. However, it is becoming increasingly challenging for humans to understand the working and reasoning of these complex and opaque algorithms. For AI to support essential decisions in these domains, specific ethical issues need to be addressed to prevent the misinterpretation of AI, which may have severe consequences for humans. However, little research has been published on guidelines that systematically addresses ethical issues when AI techniques are applied in healthcare. In this systematic literature review, we aimed to provide an overview of ethical concerns and related strategies that are currently identified when applying AI in healthcare. The review, which followed the PRISMA guidelines, revealed 12 main ethical issues: justice and fairness, freedom and autonomy, privacy, transparency, patient safety and cyber security, trust, beneficence, responsibility, solidarity, sustainability, dignity, and conflicts. In addition to these 12 main ethical issues, we derived 19 ethical sub-issues and associated strategies from the literature.</jats:p>",6.0
119,10.1108/techs-12-2022-0047,Cognitive morality and AI (AI): a proposed classification of AI systems using Kohlberg's theory of cognitive ethics,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>The widespread usage of AI (AI) is prompting a number of ethical issues, including those involving concerns for fairness, surveillance, transparency, neutrality and human rights. The purpose of this manuscript is to explore possibility of developing cognitive morality in AI systems.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>This is explorative research. The manuscript investigates the likelihood of cognitive moral development in AI systems as well as potential pathways for such development. Concurrently, it proposes a novel idea for the characterization and development of ethically conscious and artificially intelligent robotic machines.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>This manuscript explores the possibility of categorizing AI machines according to the level of cognitive morality they embody, and while doing so, it makes use of Lawrence Kohlberg's study related to cognitive moral development in humans. The manuscript further suggests that by providing appropriate inputs to AI machines in accordance with the proposed concept, humans may assist in the development of an ideal AI creature that would be morally more responsible and act as moral agents, capable of meeting the demands of morality.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title><jats:p>This manuscript has some restrictions because it focuses exclusively on Kohlberg's perspective. This theory is not flawless. Carol Gilligan, one of Kohlberg's former doctoral students, said that Kohlberg's proposal was unfair and sexist because it didn't take into account the views and experiences of women. Even if one follows the law, they may still be engaging in immoral behaviour, as Kohlberg argues, because laws and social norms are not perfect. This study makes it easier for future research in the field to look at how the ideas of people like Joao Freire and Carl Rogers can be used in AI systems.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>It is an original research that derives inspiration from the cognitive moral development theory of American Professor named Lawrence Kohlberg. The authors present a fresh way of thinking about how to classify AI systems, which should make it easier to give robots cognitive morality.</jats:p></jats:sec>",0.0
120,10.1007/s00146-021-01309-7,"Online public discourse on AI and ethics in China: context, content, and implications","<jats:title>Abstract</jats:title><jats:p>The societal and ethical implications of AI (AI) have sparked discussions among academics, policymakers and the public around the world. What has gone unnoticed so far are the likewise vibrant discussions in China. We analyzed a large sample of discussions about AI ethics on two Chinese social media platforms. Findings suggest that participants were diverse, and included scholars, IT industry actors, journalists, and members of the general public. They addressed a broad range of concerns associated with the application of AI in various fields. Some even gave recommendations on how to tackle these issues. We argue that these discussions are a valuable source for understanding the future trajectory of AI development in China as well as implications for global dialogue on AI governance.</jats:p>",0.0
121,10.1007/s43681-023-00411-4,Should we develop AGI? Artificial suffering and the moral development of humans,"<jats:title>Abstract</jats:title><jats:p>Recent research papers and tests in real life point in the direction that machines in the future may develop some form of possibly rudimentary inner life. Philosophers have warned and emphasized that the possibility of artificial suffering or the possibility of machines as moral patients should not be ruled out. In this paper, I reflect on the consequences for moral development of striving for AGI. In the introduction, I present examples which point into the direction of the future possibility of artificial suffering and highlight the increasing similarity between, for example, machine–human and human–human interaction. Next, I present and discuss responses to the possibility of artificial suffering supporting a cautious attitude for the sake of the machines. From a virtue ethical perspective and the development of human virtues, I subsequently argue that humans should not pursue the path of developing and creating AGI, not merely for the sake of possible suffering in machines, but also due to machine–human interaction becoming more alike to human–human interaction and for the sake of the human’s own moral development. Thus, for several reasons, humanity, as a whole, should be extremely cautious about pursuing the path of developing AGI—Artificial General Intelligence.</jats:p>",0.0
122,10.18034/ajhal.v5i2.659,Investigating the Prospects of GenAI,"<jats:p>In this exploratory work, we investigate cutting-edge techniques in machine learning known as GenAI (GenAI). The costs of trial and error during product development can be significantly reduced if faster, more affordable, and more accurate multi-scale materials simulations powered by fully GenAI are available. Engineers have spent decades attempting to develop humanoid robots that are both practical and resemble people in appearance and behavior. Because it enables us to circumvent the inherent dimensionality of this obstacle, GenAI has the potential to be a beneficial instrument for the current creation process. Moreover, the research underlines that GenAI, capable of producing media such as text, images, and audio in response to prompts, appears to improve daily. In addition, numerous technological companies are currently building and releasing their competing systems.</jats:p>",2.0
123,10.1007/s43681-022-00155-7,"Adopting smart glasses responsibly: potential benefits, ethical, and privacy concerns with Ray-Ban stories","<jats:title>Abstract</jats:title><jats:p>The adoption of innovative wearable technologies is potentially increasing as a new trend. Jumping into the augmented reality (AR) and Metaverse, Facebook (now known as Meta) launched smart glasses partnering with Ray-Ban sunglasses brand’s parent company EssilorLuxottica. Ray-Ban stories has several technical features for entertainment and socializing; more importantly, these features can be adopted in the future for more advanced wearable. However, these smart glasses also came with many ethical and privacy concerns along with their potential benefits. Furthermore, the unbridled deployment of these smart glasses brought several challenging questions for public social interaction when we will have more such devices in our lives. This short article has discussed the Ray-Ban stories’ ethical and privacy issues for social interaction and public places.</jats:p>",0.0
124,10.1177/26349795231221699,A multimodal grammar of AI: Measuring the gains and losses in GenAI,"<jats:p> This paper analyzes the scope of AI (AI) from the perspective of a multimodal grammar. Its focal point is GenAI, a technology that puts so-called Large Language Models to work. The first part of the paper analyzes GenAI, based as it is on the statistical probability of one token (a word or part of a word) following another. If the relation of tokens is meaningful, this is circumstantial and no more, because its mechanisms of statistical analysis eschew any theory of meaning. This is the case not only for the written text that GenAI leverages, but by extension image and multimodal forms of meaning that it can generate. The AI can only work with non-textual forms of meaning after applying language labels, and to that extent is captive not only to the limits of probabilistic statistics but the limits of written language as well. While acknowledging gains arising from the brute statistical power of GenAI, in its second part the paper goes on to map what is lost in its statistical and text-bound approaches to multimodal meaning-making. Our measure of these gains and losses is guided by the concept of grammar, defined here as a theory of the elemental patterns of meaning in the world—not just written text and speech, but also image, space, object, body, and sound. Ironically, a good deal of what is lost by GenAI is computable. The third and final part of the paper briefly discusses educational applications of GenAI. Given both its power and intrinsic limitations, we have been experimenting with the application of GenAI in educational settings and the ways it might be put to pedagogical use. How does a grammatical analysis help us to identify the scope of worthwhile application? Finally, if more of human experience is computable than can be captured in text-bound AI, how might it be possible at the level of code to create a synthesis in which grammatical and multimodal approaches complement GenAI? </jats:p>",7.0
125,10.34135/mmidentity-2023-36,AI as a creator of journalistic content,"<jats:p>The advent of AI (AI) is gradually reshaping society and impacting more and more areas, not excluding media. However, if we want to exploit the potential of this new technological phenomenon as effectively as possible, and in order to maintain established norms of media production, it is necessary to examine the capabilities of AI tools and compare them with human creations. One popular AI tool is GenAI. In media production, it is used, for example, in the creation of journalistic texts, as it is specialized to generate texts in a ‘human way’ based on input data. Therefore, the research paper aims to assess this AI tool in the context of the criteria of press release production as a basic journalistic genre. The object of investigation is to assess and evaluate the fulfilment of the basic criteria set for a quality press release, for two groups of media content – human-generated and GenAI-generated. In the evaluation, the authors will emphasize the fulfillment of the specific criteria formulated in the assignment, noting also the AI’s ability to learn and improve its creations. In its theoretical framework and analytical argumentation, the paper draws on existing knowledge from the literature, including scholarly studies by experts such as H. Pravdová (2016), T. Rončáková (2011), B. Jones et al. (2022), A. Tušer (2010), J. Mistrík (1989), M. Richter (2023), A. Kellerman (2023), B. Dhiman (2023), D. Zagorulko (2023) and others.</jats:p>",-1.0
126,10.1007/s43681-024-00452-3,The digital divide in action: how experiences of digital technology shape future relationships with AI,"<jats:title>Abstract</jats:title><jats:p>The digital divide remains an ongoing societal concern, with digital exclusion shown to have a significantly detrimental impact on people’s quality of life. AI (AI), the latest wave of digitalisation, is being integrated into the fabric of society at an accelerated rate, the speed of which has prompted ethical concerns. Without addressing the digital divide, the AI revolution risks exacerbating the existing consequences of digital exclusion and limiting the potential for all people to reap the benefits provided by AI. To understand the factors that might contribute to experiences of AI, and how these might be related to digital exclusion, we surveyed a diverse online community sample (<jats:italic>N</jats:italic> = 303). We created a novel measure of digital confidence capturing individual levels of awareness, familiarity, and sense of competence with digital technology. Results indicated that measures of digital confidence were predicted by structural, behavioural, and psychological differences, such that women, older people, those on lower salaries, people with less digital access, and those with lower digital well-being, reported significantly less digital confidence. Furthermore, digital confidence significantly moderated the relationship between people’s experiences with everyday AI technologies and their general attitudes towards AI. This understanding of the spill-over effects of digital exclusion onto experiences of AI is fundamental to the articulation and delivery of inclusive AI.</jats:p>",-1.0
127,10.1007/s43681-020-00029-w,The interrelation between data and AI ethics in the context of impact assessments,"<jats:title>Abstract</jats:title><jats:p>In the growing literature on AI (AI) impact assessments, the literature on data protection impact assessments is heavily referenced. Given the relative maturity of the data protection debate and that it has translated into legal codification, it is indeed a natural place to start for AI. In this article, we anticipate directions in what we believe will become a dominant and impactful forthcoming debate, namely, how to conceptualise the relationship between data protection and AI impact. We begin by discussing the value canvas i.e. the ethical principles that underpin data and AI ethics, and discuss how these are instantiated in the context of value trade-offs when the ethics are applied. Following this, we map three kinds of relationships that can be envisioned between data and AI ethics, and then close with a discussion of asymmetry in value trade-offs when privacy and fairness are concerned.</jats:p>",0.0
128,10.3389/frai.2024.1167137,Analyzing global utilization and missed opportunities in debt-for-nature swaps with GenAI,"<jats:p>We deploy a prompt-augmented GPT-4 model to distill comprehensive datasets on the global application of debt-for-nature swaps (DNS), a pivotal financial tool for environmental conservation. Our analysis includes 195 nations and identifies 21 countries that have not yet used DNS before as prime candidates for DNS. A significant proportion demonstrates consistent commitments to conservation finance (0.86 accuracy as compared to historical swaps records). Conversely, 35 countries previously active in DNS before 2010 have since been identified as unsuitable. Notably, Argentina, grappling with soaring inflation and a substantial sovereign debt crisis, and Poland, which has achieved economic stability and gained access to alternative EU conservation funds, exemplify the shifting suitability landscape. The study's outcomes illuminate the fragility of DNS as a conservation strategy amid economic and political volatility.</jats:p>",0.0
129,10.1007/s10676-024-09752-y,Design culture for Sustainable urban AI: Bruno Latour and the search for a different AI urbanism,"<jats:title>Abstract</jats:title><jats:p>The aim of this paper is to investigate the relationship between AI urbanism and sustainability by drawing upon some key concepts of Bruno Latour’s philosophy. The idea of a sustainable AI urbanism - often understood as the juxtaposition of smart and eco urbanism - is here critiqued through a reconstruction of the conceptual sources of these two urban paradigms. Some key ideas of smart and eco urbanism are indicated as incompatible and therefore the fusion of these two paradigms is assessed as an unstable basis for shaping sustainable AI urbanism. The concepts in question - modernity, science and nature – are subsequently redefined following Latour’s philosophical perspective, in an attempt to define a different theoretical basis for a sustainable AI urbanism in the Anthropocene. Finally, the principles of a design philosophy shaped by Latour are used to change the design culture that informs AI urbanism towards a more sustainable practice. This paper constructs and promotes a dialogue between the disciplines of philosophy and urban theory with urban design in the conviction that the principles produced by the former and the practices carried out by the latter must start a biunivocal relationship. The paper reveals that in order to change design culture in the field of AI urbanism, it is necessary to rethink some of the key ideas that inform the Western and modern worldview through novel philosophical reflections.</jats:p>",0.0
130,10.3390/philosophies9030080,"Africa, GenAI, and GenAI Systems: Ethical Benefits, Concerns, and the Need for Governance","<jats:p>This paper examines the impact and implications of GenAI and other GenAI technologies within the African context while looking at the ethical benefits and concerns that are particularly pertinent to the continent. Through a robust analysis of GenAI and other GenAI systems using established approaches for analysing the ethics of emerging technologies, this paper provides unique ethical benefits and concerns for these systems in the African context. This analysis combined approaches such as anticipatory technology ethics (ATE), ethical impact assessment (EIA), and ethical issues of emerging ICT applications with AI (ETICA) with specific issues from the literature. The findings show that GenAI and other GenAI systems raise unique ethical concerns such as bias, intergenerational justice, exploitation of labour and cultural diversity in Africa but also have significant ethical benefits. These ethical concerns and benefits are considered crucial in shaping the design and deployment of GenAI and similar technologies responsibly. It further explores the potential applications of GenAI in critical domain areas such as education, agriculture, and healthcare, thereby demonstrating the transformative possibilities that these technologies can have on Africa. This paper underscores the critical role of AI governance as Africa increasingly adopts GenAI and similar AI systems. It argues that a comprehensive understanding of AI governance is essential not only for maximising the benefits of GenAI systems but also for facilitating a global dialogue. This dialogue aims to foster shared knowledge and insights between the Global North and the Global South, which is important for the development and creation of inclusive and equitable AI policies and practices that can be beneficial for all regions.</jats:p>",-1.0
131,10.1142/s2705078524400022,From Value Realism to Inclusive Ethics: A New Path for Human Rights and AI Development,"<jats:p> This paper critically examines Carlos Montemayor’s human rights-based approach to AI ethics, as presented in his 2023 book The Prospect of a Humanitarian AI. Montemayor proposes that the concept of human rights, grounded in the cognitive needs of human beings, should guide AI development. This paper challenges Montemayor’s moral value-realism (the belief in objective moral values) by arguing for a more inclusive ethical framework that reconciles moral value-realism with skepticism. By proposing an agnostic stance towards moral truths, it suggests a new framework for AI ethics based on subjective intrinsic valuations and shared moral commitments, rather than on objective moral truths. In this proposed framework, without either denying or asserting the existence of absolute values and moral truths, human rights in AI ethics are grounded in collective moral commitments to value human beings intrinsically, rather than in inherent moral truths. This facilitates a more pragmatic and inclusive solution that allows for a diversity of ethical and metaethical perspectives while aligning with the complex nature of AI’s societal integration. The paper concludes that while Montemayor’s work is foundational, it invites further adaptation to develop AI systems that are ethically sound and in harmony with a broad spectrum of human values. </jats:p>",0.0
132,10.3390/healthcare12111082,The Impact of AI (AI) on Midwifery Education: A Scoping Review,"<jats:p>As in other healthcare professions, AI will influence midwifery education. To prepare midwifes for a future where AI plays a significant role in healthcare, educational requirements need to be adapted. This scoping review aims to outline the current state of research regarding the impact of AI on midwifery education. The review follows the framework of Arksey and O’Malley and the PRISMA-ScR. Two databases (Academic Search Premier and PubMed) were searched for different search strings, following defined inclusion criteria, and six articles were included. The results indicate that midwifery practice and education is faced with several challenges as well as opportunities when integrating AI. All articles see the urgent need to implement AI technologies into midwifery education for midwives to actively participate in AI initiatives and research. Midwifery educators need to be trained and supported to use and teach AI technologies in midwifery. In conclusion, the integration of AI in midwifery education is still at an early stage. There is a need for multidisciplinary research. The analysed literature indicates that midwifery curricula should integrate AI at different levels for graduates to be prepared for their future in healthcare.</jats:p>",-1.0
133,10.1108/md-10-2023-1968,GenAI (AI) tools in innovation management: a study on the appropriation of GenAI by innovation managers,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>Using AI to strengthen creativity and problem-solving capabilities of professionals involved in innovation management holds huge potential for improving organizational decision-making. However, there is a lack of research on the use of AI technologies by innovation managers. The study uses the theory of appropriation to explore how specific factors – agile leadership (AL), innovation orientation (IO) and individual creativity (IC) – impact innovation managers' use of GenAI tools, such as GenAI (CGA).</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>The research model is tested through a large-scale survey of 222 Italian innovation managers. Data have been analyzed using structural equation modeling following a two-step approach. First, the measurement model was assessed to ensure the constructs reliability. Subsequently, the structural model was analyzed to draw the conclusions on theorized model relationships and their statistical significance.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>The research findings reveal positive associations between IO and IC with CGA, demonstrating that innovation managers who exhibit strong innovation orientations and higher Individual Creativity are more likely to adopt and personalize GenAI. However, the study did not confirm a significant association between AL and CGA.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>Our findings have important implications for organizations seeking to maximize the potential of GenAI in innovation management. Understanding the factors that drive the adoption and customization of GenAI tools can inform strategies for better integration into the innovation process, thereby leading to enhanced innovation outcomes and improved decision-making processes.</jats:p></jats:sec>",4.0
134,10.3390/educsci14111209,"Modeling Teachers’ Acceptance of GenAI Use in Higher Education: The Role of AI Literacy, Intelligent TPACK, and Perceived Trust","<jats:p>This study delves into the factors that drive teachers’ adoption of GenAI (GenAI) technologies in higher education. Anchored by the technology acceptance model (TAM), the research expands its inquiry by integrating the constructs of intelligent technological pedagogical content knowledge (TPACK), AI literacy, and perceived trust. Data were gathered from a sample of 237 university teachers through a structured questionnaire. The study employed structural equation modeling (SEM) to determine the relationships among the constructs. The results revealed that both AI literacy and perceived ease were the most influential factors affecting teachers’ acceptance of GenAI. Notably, intelligent TPACK and perceived trust were found to be pivotal mediators in this relationship. The findings underscore the importance of fostering AI literacy and adapting intelligent TPACK frameworks to better equip educators in the age of AI. Furthermore, there is a clear need for targeted professional development initiatives focusing on practical training that enhances AI literacy. These programs should provide hands-on experience with GenAI tools, boosting educators’ confidence and ability to integrate them into their teaching practices.</jats:p>",1.0
135,10.1002/aaai.12150,Institute for AI and Fundamental Interactions (IAIFI): Infusing physics intelligence into AI,"<jats:title>Abstract</jats:title><jats:p>The NSF AI Institute for AI and Fundamental Interactions (IAIFI, pronounced /aI‐faI/) is one of the inaugural NSF AI research institutes (<jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""https://iaifi.org"">https://iaifi.org</jats:ext-link>). The IAIFI is enabling physics discoveries and advancing foundational AI through the development of novel AI approaches that incorporate first principles from fundamental physics. By combining state‐of‐the‐art research with early career talent and a growing AI + physics community in the Boston area and beyond, the IAIFI is enabling researchers to develop AI technologies to tackle some of the most challenging problems in physics, and transfer these technologies to the broader AI community. Since trustworthy AI is as important for physics discovery as it is for other applications of AI in society, IAIFI researchers are applying physics principles to develop more robust AI tools and to illuminate existing AI technologies. To cultivate human intelligence, the IAIFI promotes training, education, and public engagement at the intersection of physics and AI. In these ways, the IAIFI is fusing deep learning with deep thinking to gain a deeper understanding of our universe and AI.</jats:p>",0.0
136,10.1007/s10462-024-10973-2,Digital deception: GenAI in social engineering and phishing,"<jats:title>Abstract</jats:title><jats:p>The advancement of AI (AI) and Machine Learning (ML) has profound implications for both the utility and security of our digital interactions. This paper investigates the transformative role of GenAI in Social Engineering (SE) attacks. We conduct a systematic review of social engineering and AI capabilities and use a theory of social engineering to identify three pillars where GenAI amplifies the impact of SE attacks: Realistic Content Creation, Advanced Targeting and Personalization, and Automated Attack Infrastructure. We integrate these elements into a conceptual model designed to investigate the complex nature of AI-driven SE attacks—the GenAI Social Engineering Framework. We further explore human implications and potential countermeasures to mitigate these risks. Our study aims to foster a deeper understanding of the risks, human implications, and countermeasures associated with this emerging paradigm, thereby contributing to a more secure and trustworthy human-computer interaction.</jats:p>",8.0
137,10.3390/socsci13050250,Perpetuation of Gender Bias in Visual Representation of Professions in the GenAI Tools DALL·E and Bing Image Creator,"<jats:p>AI (AI)-based generative imaging systems such as DALL·E, Midjourney, Stable Diffusion, and Adobe Firefly, which work by transforming natural language descriptions into images, are revolutionizing computer vision. In this exploratory and qualitative research, we have replicated requests for images of women in different professions by comparing these representations in previous studies with DALL·E, observing that this model continues to provide in its last version, DALL·E 3, inequitable results in terms of gender. In addition, Bing Image Creator, Microsoft’s free tool that is widely used among the population and runs under DALL·E, has been tested for the first time. It also presents a sexualization of women and stereotypical children’s representations. The results reveal the following: 1. A slight improvement in terms of the presence of women in professions previously shown only with men. 2. They continue to offer biased results in terms of the objectification of women by showing sexualized women. 3. The representation of children highlights another level of gender bias, reinforcing traditional stereotypes associated with gender roles from childhood, which can impact future decisions regarding studies and occupations.</jats:p>",-1.0
138,10.1007/s43681-024-00444-3,The moral decision machine: a challenge for artificial moral agency based on moral deference,"<jats:title>Abstract</jats:title><jats:p>Humans are responsible moral agents in part because they can competently respond to moral reasons. Several philosophers have argued that artificial agents cannot do this and therefore cannot be responsible moral agents. I present a counterexample to these arguments: the ‘Moral Decision Machine’. I argue that the ‘Moral Decision Machine’ responds to moral reasons just as competently as humans do. However, I suggest that, while a hopeful development, this does not warrant strong optimism about ‘artificial moral agency’. The ‘Moral Decision Machine’ (and similar agents) can only respond to moral reasons by deferring to others, and there are good reasons to think this is incompatible with responsible moral agency. While the challenge to artificial moral agency based on moral reasons-responsiveness can be satisfactorily addressed; the challenge based on moral deference remains an open question. The right way to understand the challenge, I argue, is as a route to the claim that artificial agents are unlikely to be responsible moral agents because they cannot be authentic.</jats:p>",0.0
139,10.1007/s44163-022-00043-3,The tech industry hijacking of the AI ethics research agenda and why we should reclaim it,"<jats:title>Abstract</jats:title><jats:p>This paper reflects on the tech industry’s colonization of the AI ethics research field and addresses conflicts of interest in public policymaking concerning AI. The AI ethics research community faces two intertwined challenges: In the first place, we have a tech industry heavily influencing the AI ethics research agenda. Secondly, cleaning up after the tech industry has implied that we have turned to value-driven design methods to bring ethics to AI design. But by framing research questions relevant to a technical practice, we have facilitated the technological solutionism behind the tech industry’s business model. Therefore, this paper takes the first steps to reshape the AI ethics research agenda by suggesting moving toward an emancipatory framework that brings politics to design while, at the same time, bearing in mind that AI is not to be treated as an inevitability. As a research community, we must focus on the repressive power dynamics exacerbated by AI and address challenges facing the vulnerable groups seldom heard, despite the fact that they are the ones most negatively affected by AI initiatives.</jats:p>",0.0
140,10.1017/s1049096524000167,Surveying the Impact of GenAI on Political Science Education,"<jats:title>ABSTRACT</jats:title>
	  <jats:p>Recent applications of new innovations in AI have brought up questions about how this new technology will change the landscape and practices in a wide range of industries and sectors. This article focuses on the impact of generative large language models on teaching, learning, and academic assessment in political science education by analyzing two novel surveys administered by the discipline’s major professional body, the American Political Science Association. We present the results of these surveys and conclude with recommendations.</jats:p>",1.0
141,10.1007/s43681-022-00230-z,Breaking bad news in the era of AI and algorithmic medicine: an exploration of disclosure and its ethical justification using the hedonic calculus,"<jats:title>Abstract</jats:title><jats:p>An appropriate ethical framework around the use of AI (AI) in healthcare has become a key desirable with the increasingly widespread deployment of this technology. Advances in AI hold the promise of improving the precision of outcome prediction at the level of the individual. However, the addition of these technologies to patient–clinician interactions, as with any complex human interaction, has potential pitfalls. While physicians have always had to carefully consider the ethical background and implications of their actions, detailed deliberations around fast-moving technological progress may not have kept up. We use a common but key challenge in healthcare interactions, the disclosure of bad news (likely imminent death), to illustrate how the philosophical framework of the 'Felicific Calculus' developed in the eighteenth century by Jeremy Bentham, may have a timely quasi-quantitative application in the age of AI. We show how this ethical algorithm can be used to assess, across seven mutually exclusive and exhaustive domains, whether an AI-supported action can be morally justified.</jats:p>",6.0
142,10.3390/children10081372,Generative Adversarial Network (GenAI) in Pediatric Radiology: A Systematic Review,"<jats:p>GenAI, especially with regard to the generative adversarial network (GAN), is an important research area in radiology as evidenced by a number of literature reviews on the role of GAN in radiology published in the last few years. However, no review article about GAN in pediatric radiology has been published yet. The purpose of this paper is to systematically review applications of GAN in pediatric radiology, their performances, and methods for their performance evaluation. Electronic databases were used for a literature search on 6 April 2023. Thirty-seven papers met the selection criteria and were included. This review reveals that the GAN can be applied to magnetic resonance imaging, X-ray, computed tomography, ultrasound and positron emission tomography for image translation, segmentation, reconstruction, quality assessment, synthesis and data augmentation, and disease diagnosis. About 80% of the included studies compared their GAN model performances with those of other approaches and indicated that their GAN models outperformed the others by 0.1–158.6%. However, these study findings should be used with caution because of a number of methodological weaknesses. For future GAN studies, more robust methods will be essential for addressing these issues. Otherwise, this would affect the clinical adoption of the GAN-based applications in pediatric radiology and the potential advantages of GAN could not be realized widely.</jats:p>",5.0
143,10.1007/s44244-023-00012-4,The role of GenAI (GAI) in customer personalisation (CP) development in SMEs: a theoretical framework and research propositions,"<jats:title>Abstract</jats:title><jats:p>Based on the dynamic capabilities (DC) theory, the aim of this study is to investigate the contribution of GenAI (GAI) to the development of customer personalisation (CP) within business organisations, particularly SMEs. This paper also explores how the function of GAI in the development of CP is supported by technological advancements like deep learning (DL), smart data (SD), and the Internet of Things (IoT). Using a theoretical framework based on DC theory and an analysis of the literature on GAI, DL, SD, IoT, and CP, the relationship between GAI and CP is theoretically studied. The dependent variable in this theoretical framework is CP, and the independent variable is GAI. Furthermore, while DL and SD just mediate the connection between GAI and CP, IoT moderates the relationship between GAI and SD. Figure 1 presents the theoretical framework and research propositions. On the basis of the constructs in this study, research propositions were developed and discussed. Eight significant research propositions on the relationship between GAI and CP development were developed using the theoretical framework used in this study. According to the suggested theoretical framework and research propositions, context-oriented CP can be created by GAI using DL and SD in conjunction with IoT when high-level customer attributes are retrieved in a structured, accurate, and real-time manner. Additionally, it results in important marketing outcomes including interactive marketing, value co-creation, and consumer loyalty. This study develops a theoretical framework and research propositions that theorise the relationship between GAI and CP which is rooted in literature and also based on DC perspective. The mediating roles of DL and SD on the relationship between GAI and CP, and the moderating role of IoT on the relationship between GAI and SD, provide support to GAI in the development of CP. This study also provides insight into SMEs’ adoption of GAI to generate context-oriented CP that may impact on their marketing development in areas such as interactive marketing, value co-creation, better targeting and customer loyalty.</jats:p>",4.0
144,10.1007/s00146-023-01854-3,"Artists or art thieves? media use, media messages, and public opinion about AI image generators","<jats:title>Abstract</jats:title><jats:p>This study investigates how patterns of media use and exposure to media messages are related to attitudes about AI (AI) image generators. In doing so, it builds on theoretical accounts of media framing and public opinion about science and technology topics, including AI. The analyses draw on data from a survey of the US public (<jats:italic>N</jats:italic> = 1,035) that included an experimental manipulation of exposure to tweets framing AI image generators in terms of real art, artists’ concerns, artists’ outrage, or competing interpretations. The results show that technology news use and science fiction viewing predicted support for AI art but also predicted belief that AI image generators will take jobs and steal art styles from human artists. In addition, the experimental results demonstrate that exposure to specific media messages can influence these responses. The findings carry implications for understanding the future adoption, use, and regulation of AI image generators.</jats:p>",2.0
145,10.1007/s10551-022-05056-7,AI and Declined Guilt: Retailing Morality Comparison Between Human and AI,"<jats:title>Abstract</jats:title><jats:p>Several technological developments, such as self-service technologies and AI (AI), are disrupting the retailing industry by changing consumption and purchase habits and the overall retail experience. Although AI represents extraordinary opportunities for businesses, companies must avoid the dangers and risks associated with the adoption of such systems. Integrating perspectives from emerging research on AI, morality of machines, and norm activation, we examine how individuals morally behave toward AI agents and self-service machines. Across three studies, we demonstrate that consumers’ moral concerns and behaviors differ when interacting with technologies versus humans. We show that moral intention (intention to report an error) is less likely to emerge for AI checkout and self-checkout machines compared with human checkout. In addition, moral intention decreases as people consider the machine less humanlike. We further document that the decline in morality is caused by less guilt displayed toward new technologies. The non-human nature of the interaction evokes a decreased feeling of guilt and ultimately reduces moral behavior. These findings offer insights into how technological developments influence consumer behaviors and provide guidance for businesses and retailers in understanding moral intentions related to the different types of interactions in a shopping environment.</jats:p>",0.0
146,10.2196/42936,Strategies to Improve the Impact of AI on Health Equity: Scoping Review,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>Emerging AI (AI) applications have the potential to improve health, but they may also perpetuate or exacerbate inequities.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>This review aims to provide a comprehensive overview of the health equity issues related to the use of AI applications and identify strategies proposed to address them.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>We searched PubMed, Web of Science, the IEEE (Institute of Electrical and Electronics Engineers) Xplore Digital Library, ProQuest U.S. Newsstream, Academic Search Complete, the Food and Drug Administration (FDA) website, and ClinicalTrials.gov to identify academic and gray literature related to AI and health equity that were published between 2014 and 2021 and additional literature related to AI and health equity during the COVID-19 pandemic from 2020 and 2021. Literature was eligible for inclusion in our review if it identified at least one equity issue and a corresponding strategy to address it. To organize and synthesize equity issues, we adopted a 4-step AI application framework: Background Context, Data Characteristics, Model Design, and Deployment. We then created a many-to-many mapping of the links between issues and strategies.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>In 660 documents, we identified 18 equity issues and 15 strategies to address them. Equity issues related to Data Characteristics and Model Design were the most common. The most common strategies recommended to improve equity were improving the quantity and quality of data, evaluating the disparities introduced by an application, increasing model reporting and transparency, involving the broader community in AI application development, and improving governance.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>Stakeholders should review our many-to-many mapping of equity issues and strategies when planning, developing, and implementing AI applications in health care so that they can make appropriate plans to ensure equity for populations affected by their products. AI application developers should consider adopting equity-focused checklists, and regulators such as the FDA should consider requiring them. Given that our review was limited to documents published online, developers may have unpublished knowledge of additional issues and strategies that we were unable to identify.</jats:p>
          </jats:sec>",3.0
147,10.3390/computation12050101,Unveiling AI-Generated Financial Text: A Computational Approach Using Natural Language Processing and GenAI,"<jats:p>This study is an in-depth exploration of the nascent field of Natural Language Processing (NLP) and GenAI (AI), and it concentrates on the vital task of distinguishing between human-generated text and content that has been produced by AI models. Particularly, this research pioneers the identification of financial text derived from AI models such as GenAI and paraphrasing tools like QuillBot. While our primary focus is on financial content, we have also pinpointed texts generated by paragraph rewriting tools and utilized GenAI for various contexts this multiclass identification was missing in previous studies. In this paper, we use a comprehensive feature extraction methodology that combines TF–IDF with Word2Vec, along with individual feature extraction methods. Importantly, combining a Random Forest model with Word2Vec results in impressive outcomes. Moreover, this study investigates the significance of the window size parameters in the Word2Vec approach, revealing that a window size of one produces outstanding scores across various metrics, including accuracy, precision, recall and the F1 measure, all reaching a notable value of 0.74. In addition to this, our developed model performs well in classification, attaining AUC values of 0.94 for the ‘GPT’ class; 0.77 for the ‘Quil’ class; and 0.89 for the ‘Real’ class. We also achieved an accuracy of 0.72, precision of 0.71, recall of 0.72, and F1 of 0.71 for our extended prepared dataset. This study contributes significantly to the evolving landscape of AI text identification, providing valuable insights and promising directions for future research.</jats:p>",7.0
148,10.54337/jbm.v12i1.8402,Using AI (AI) Generative Technologies For Business Model Design with IDEATe Process: A Speculative Viewpoint,"<jats:p>PurposeAI (AI) and the more recent generative technologies are disrupting many activities related to strategy and operations within organizations. Business model design is no exception. We define business model design as an iterative process involving a combination of creativity, decisions, and tests, consisting of envisioning and creating a business model (for a brand-new activity) or a new business model (for an existing activity), to change an existing situation into a preferred one. In this paper, we discuss the potential impact of generative technologies on the business model design process, highlighting the opportunities and challenges that these technologies present and suggesting some methods for using generative technologies for business model design.
Design/Methodology/ApproachWe build on knowledge about business model design and on documentation from forums, social networks, and media about generative technologies. We also used GenAI platforms to test dozens of prompts related to business model design.
FindingsWe propose the IDEATe process for business model design and identify six major changes in the process or the outcome of business model design that generative technologies can trigger. We also discuss blind spots and risks associated with the use of generative technologies for business model design. Finally, we advance some functions of generative technologies that may support this process.
Originality/ValueInstead of focusing on how generative technologies could change business models, we investigate how these technologies could impact the design of business models. We make propositions to use these technologies properly for business model design.</jats:p>",4.0
149,10.1017/neu.2024.50,Use of GenAI (AI) in psychiatry and mental health care: a systematic review,"<jats:title>Abstract</jats:title>
	  <jats:sec id=""S0924270824000504_as1"">
	    <jats:title>Objectives:</jats:title>
	    <jats:p>Tools based on GenAI (AI) such as GenAI have the potential to transform modern society, including the field of medicine. Due to the prominent role of language in psychiatry, e.g., for diagnostic assessment and psychotherapy, these tools may be particularly useful within this medical field. Therefore, the aim of this study was to systematically review the literature on GenAI applications in psychiatry and mental health.</jats:p>
	  </jats:sec>
	  <jats:sec id=""S0924270824000504_as2"">
	    <jats:title>Methods:</jats:title>
	    <jats:p>We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. The search was conducted across three databases, and the resulting articles were screened independently by two researchers. The content, themes, and findings of the articles were qualitatively assessed.</jats:p>
	  </jats:sec>
	  <jats:sec id=""S0924270824000504_as3"">
	    <jats:title>Results:</jats:title>
	    <jats:p>The search and screening process resulted in the inclusion of 40 studies. The median year of publication was 2023. The themes covered in the articles were mainly mental health and well-being in general – with less emphasis on specific mental disorders (substance use disorder being the most prevalent). The majority of studies were conducted as prompt experiments, with the remaining studies comprising surveys, pilot studies, and case reports. Most studies focused on models that generate language, GenAI in particular.</jats:p>
	  </jats:sec>
	  <jats:sec id=""S0924270824000504_as4"">
	    <jats:title>Conclusions:</jats:title>
	    <jats:p>GenAI in psychiatry and mental health is a nascent but quickly expanding field. The literature mainly focuses on applications of GenAI, and finds that GenAI performs well, but notes that it is limited by significant safety and ethical concerns. Future research should strive to enhance transparency of methods, use experimental designs, ensure clinical relevance, and involve users/patients in the design phase.</jats:p>
	  </jats:sec>",-1.0
150,10.3389/fpubh.2023.1142062,The ethics of advancing AI in healthcare: analyzing ethical considerations for Japan’s innovative AI hospital system,"<jats:p>Public and private investments into developing digital health technologies—including AI (AI)—are intensifying globally. Japan is a key case study given major governmental investments, in part through a Cross-Ministerial Strategic Innovation Promotion Program (SIP) for an “Innovative AI Hospital System.” Yet, there has been little critical examination of the SIP Research Plan, particularly from an ethics approach. This paper reports on an analysis of the Plan to identify the extent to which it addressed ethical considerations set out in the World Health Organization’s 2021 Guidance on the Ethics and Governance of AI for Health. A coding framework was created based on the six ethical principles proposed in the Guidance and was used as the basis for a content analysis. 101 references to aspects of the framework were identified in the Plan, but attention to the ethical principles was found to be uneven, ranging from the strongest focus on the potential benefits of AI to healthcare professionals and patients (<jats:italic>n</jats:italic> = 44; Principle 2), to no consideration of the need for responsive or sustainable AI (<jats:italic>n</jats:italic> = 0; Principle 6). Ultimately, the findings show that the Plan reflects insufficient consideration of the ethical issues that arise from developing and implementing AI for healthcare purposes. This case study is used to argue that, given the ethical complexity of the use of digital health technologies, consideration of the full range of ethical concerns put forward by the WHO must urgently be made visible in future plans for AI in healthcare.</jats:p>",6.0
151,10.62962/001c.74354,Examining the Impact and Challenges of AI (AI) in Healthcare,"<jats:p>AI (AI) is revolutionizing the healthcare sector in various ways, from boosting diagnostic performance and patient management to speeding up clinical operations and lowering medical costs. Medical imaging analysis, medication discovery, patient monitoring, and patient communication and assistance are just a few of the uses for AI-powered solutions. The influence of AI in healthcare is broad, with the potential to change healthcare delivery. Implementing AI in healthcare would enhance patient outcomes, safety, and experience and empower healthcare organizations to function efficiently and effectively. Although there are some reservations regarding the ethical and privacy implications of AI in healthcare, the benefits are evident as technology evolves and progresses. As AI develops extensively and is incorporated into the healthcare business, we may expect to see even more significant and positive outcomes in the coming years. The purpose of this paper is to examine the effects of AI on healthcare and to investigate its potential benefits and challenges.</jats:p>",3.0
152,10.1007/s43681-021-00110-y,"Disruption, technology and the question of (artificial) identity","<jats:title>Abstract</jats:title><jats:p>The current state of human–machine interaction has set forth a process of hybridization of human identity. Technology—and most notably AI—is used as an effective cognitive extender, which enables the extension of human personhood to include artificial elements, leading to the emergence of artificial identity. Discussing—and accommodating—anthropomorphization in human–machine interaction should no longer be the primary focus. Rather, the scope and quality of frameworks in which the hybridization of human identity occurs and evolves has significant ethical implications that pose very pragmatic challenges to users, the industry, and regulators. This paper puts forth a few main principles upon which such a discussion should evolve. We illustrate why disruptiveness can easily turn into human harm when the frameworks facilitating it overlook the human vulnerabilities that arise from hybrid identity, notably the asymmetric and asynchronous relationship between the human and artificial counterparts. Finally, we claim that these new types of vulnerabilities, to which a person is exposed due to the intimate degree of pairing with technology, justifies introducing and protecting artificial identity as well.</jats:p>",8.0
153,10.59982/18294359-24.15-is-09,IMPACT OF AI (AI) ON ECONOMICS AND MANAGEMENT,"<jats:p>The scientific article presents the results of the research on the peculiarities of AI technology’s impact on the transformation of business process management systems, management decision-making, and organizations’ management improvement in all sectors of the economy. The relevance of the work on the selected issue is determined by the formation of a digital economy model in which technology, including AI, contributes to improving the efficiency of management and the economic activities of business entities. The object of the study is AI (AI) technologies. The subject of the study is the impact of AI technologies on management practices and economic activities of commercial organizations. Within the framework of the scientific article, the theoretical and practical aspects of the concept of “AI” are considered. The factors that caused the development of AI technologies in modern economics and management are identified. The main directions of influence of AI technology in the management activities of organizations are analyzed. The prospects and possibilities of AI technology in economics and management are addressed. The actual problems that create threats and barriers to the introduction and effective use of AI technology with the improvement of the management system at the enterprise are identified. The conclusions of the article establish that the impact of AI on the economy and management forms a sustainable foundation for successful digital transformation by increasing the economic efficiency of economic activity and management.</jats:p>",-1.0
154,10.32782/2707-8019/2024-2-9,THE INNOVATIVE IMPACT OF GenAI ON DIGITAL BUSINESS TRANSFORMATION,"<jats:p>In the context of intensifying competition and evolving market dynamics, the deployment of cutting-edge technologies has become not merely a discretionary choice, but an indispensable imperative for any enterprise aspiring to achieve successful growth. GenAI, with its substantial potential for automation, personalisation and optimisation of business processes, is emerging as a highly promising avenue of digital transformation. This study is dedicated to investigating approaches and delineating strategies for aligning GenAI with the requirements of digital business transformation. The research examines the development of AI, with a focus on symbolic AI, machine learning, deep learning and GenAI. In addition, it considers the impact of these developments on business processes. The article identifies the potential benefits and challenges associated with the adaptation of GenAI to the needs of modern business, in the areas of marketing, sales and data analysis. The utilisation of diverse methodologies and techniques, including prompts, fine-tuning, and the incorporation of interactive guidance systems, can enhance the efficacy and precision of GenAI in a business setting, thereby facilitating optimal outcomes in a multitude of tasks. The authors put forth the proposition of employing GenAI technology in conjunction with Retrieval-Augmented Generation, with the objective of enhancing the quality and relevance of responses to user queries. Additionally, they advocate for the utilisation of agents or orchestration tools to provide guidance to models. The successful implementation of GenAI hinges on three key factors: the clear definition of objectives, the selection of suitable tools and technologies, and the assurance of managerial and staff support. The implementation of GenAI will contribute to increased efficiency through the automation of routine tasks, enhanced competitiveness through personalisation and innovation, optimised cost structures that increase profitability, and expanded opportunities for research and development.</jats:p>",4.0
155,10.36548/jtcsst.2023.4.003,GenAI (AI) Educational Pedagogy Development: Conversational AI with User-Centric GenAI4,"<jats:p>In terms of language models, GenAI (GenAI), and more specifically GenAI, offer a significant technological achievement as a revolutionary tool for natural language processing (NLP) and a transformative educational business tool. GenAI users' suggestions have the ability to optimize teaching and learning, thereby having a substantial impact on the educational environment of the twenty-first century. Educational robots are getting easier to access for a number of reasons. The human-robot cooperation that has advanced scientifically in industry 5.0 extreme digital automation, will also probably become a regular aspect of life in the days to come. This study examines the prospective uses of GenAI for NLP synthesis as well as its potential role as a conversational agent in the classroom business. GenAI's capacity to understand and produce language that is human-like by employing NLP to generate semantics was essential to its ability to replicate the most advanced human technology through comprehensive assumptions of  patterns and structures it learns from its training data. With the rise of AI (AI) driven conversational agents, prompt engineering has become an important aspect of digital learning. It is essential to get ready for an AI-dominated future when general and educational technologies combine. The study demonstrated how society may impact and contribute to the development of AI pedagogic learning using an instructional robotics application driven by AI, emphasizing the responsibility of humans as producers to reduce any potential misfortunes. The study   highlights that since GenAI technologies have the potential to drastically change teaching and learning approaches and necessitate new ways of thinking, more research on organizational robotics, with a focus on human collaboration and education, will emerge from the technological concerns raised in this study.</jats:p>",1.0
156,10.53469/jrve.2024.6(06).10,The Impact of the Development of AI with Generative Ability on Education,"<jats:p>One kind of AI technology called GenAI is used to create new text, picture, audio, and video material. It may be used for many different things in education, such creating material, enhancing data, personalizing learning, simulating situations, and providing training. It also raises moral questions about prejudices, veracity, false information, intellectual property, loss of employment, and potential future developments like more realism and responsiveness. Content creation, personalized learning, administrative work automation, interactive learning environments, feedback and evaluation, natural language processing (NLP), forecasting and prediction, and collaborative learning are some of the educational applications of GenAI approaches. These technological advancements are intended to improve educational opportunities, streamline administrative duties, and provide individualized course materials. But there are still issues to be resolved, like protecting data privacy, managing human - AI interaction well, and preventing biases in information produced by AI. The process of creating a GenAI system for teaching includes gathering data, choosing a model, training it, and deploying it. Although scalable, personalized, and engaging learning solutions offered by GenAI hold great promise for revolutionizing education, there are a number of drawbacks that may restrict the technology's applicability and prevent it from being widely used. The difficulty of maintaining and upgrading these systems, ethical and privacy problems, and the caliber and bias of the produced information are examples of technical constraints. The applications, legal frameworks, and social consequences of GenAI will be shaped by its technological limits. To fully realize the benefits of AI in education, issues including data privacy breaches, possible bias in AI systems, and the digital divide must be resolved.</jats:p>",1.0
157,10.1007/s43681-021-00055-2,Perspectives about artificial moral agents,"<jats:title>Abstract</jats:title><jats:p>The pursuit of AMAs is complicated. Disputes about the development, design, moral agency, and future projections for these systems have been reported in the literature. This empirical study explores these controversial matters by surveying (AI) Ethics scholars with the aim of establishing a more coherent and informed debate. Using Q-methodology, we show the wide breadth of viewpoints and approaches to artificial morality. Five main perspectives about AMAs emerged from our data and were subsequently interpreted and discussed: (i) Machine Ethics: The Way Forward; (ii) Ethical Verification: Safe and Sufficient; (iii) Morally Uncertain Machines: Human Values to Avoid Moral Dystopia; (iv) Human Exceptionalism: Machines Cannot Moralize; and (v) Machine Objectivism: Machines as Superior Moral Agents. A potential source of these differing perspectives is the failure of Machine Ethics to be widely observed or explored as an applied ethic and more than a futuristic end. Our study helps improve the foundations for an informed debate about AMAs, where contrasting views and agreements are disclosed and appreciated. Such debate is crucial to realize an interdisciplinary approach to artificial morality, which allows us to gain insights into morality while also engaging practitioners.</jats:p>",0.0
158,10.1001/jama.2023.9630,GenAI in Health Care and Liability Risks for Physicians and Safety Concerns for Patients,"<jats:p>This Viewpoint discusses the potential use of GenAI (AI) in medical care and the liability risks for physicians using the technology, as well as offers suggestions for safeguards to protect patients.</jats:p>",3.0
159,10.1007/s00146-020-00992-2,"The Chinese approach to AI: an analysis of policy, ethics, and regulation","<jats:title>Abstract</jats:title><jats:p>In July 2017, China’s State Council released the country’s strategy for developing AI (AI), entitled ‘New Generation AI Development Plan’ (新一代人工智能发展规划). This strategy outlined China’s aims to become the world leader in AI by 2030, to monetise AI into a trillion-yuan (ca. 150 billion dollars) industry, and to emerge as the driving force in defining ethical norms and standards for AI. Several reports have analysed specific aspects of China’s AI policies or have assessed the country’s technical capabilities. Instead, in this article, we focus on the socio-political background and policy debates that are shaping China’s AI strategy. In particular, we analyse the main strategic areas in which China is investing in AI and the concurrent ethical debates that are delimiting its use. By focusing on the policy backdrop, we seek to provide a more comprehensive and critical understanding of China’s AI policy by bringing together debates and analyses of a wide array of policy documents.</jats:p>",0.0
160,10.1093/bjrai/ubae012,"Clinical applications of GenAI in radiology: image translation, synthesis, and text generation","<jats:title>Abstract</jats:title>
               <jats:p>GenAI (AI) has enabled tasks in radiology, including tools for improving image quality. Recently, new hotspots have emerged, such as intra- or inter-modal image translation, task-specific image synthesis, and text generation. Advances in GenAI have facilitated the move towards low-dose, cost-effective, and high-quality radiological image acquisition. Large language models can aid radiologists by generating professional answers and facilitating patient-physician communications. However, radiologists must be aware of potential inaccuracies in the generated content and should only use such tools after rigorous validation of their performance.</jats:p>",-1.0
161,10.70315/uloap.ulahu.2024.0102007,Explainability Imperative of GenAI Navigating the Moral Dilemma of AI in Nigeria and Charting a Path for the Future,"<jats:p>This paper explores the explanability imperative in the context of GenAI (GAI) and its crucial role in addressing the concerns posed by AI technology in Nigeria. This underscores the ethical necessity for AI systems, especially generative ones to provide clear and understandable explanations for their decisions and actions. Although the advent of GenAI undoubtedly heralds the future and however, has also exposed Nigerian society to new vulnerabilities that seemingly are detrimental to our epistemic agency and peaceful political settings. Employing the phenomenological method of philosophical inquiry here, we discovered that this new technology has posed big threats to the future world, and that Nigeria falls amongst this new technology users. To navigate the moral dilemma caused by GenAI, this paper suggests many proactive approaches like the development of localized AI explainability standards, the regulatory frameworks, and educational initiatives to promote awareness and understanding of AI systems in Nigeria. By prioritizing the Explanability Imperative, Nigeria can chart a path towards a future whereby AI technologies aligned with societal values, upholds standard education, and as well contributes positively to the nation’s development. This paper encapsulates the importance of AI explainability in Nigeria’s AI landscape and its potential to shape a more ethically responsible and transparent AI future.</jats:p>",-1.0
162,10.1017/s089267941900011x,The Future Impact of AI on Humans and Human Rights,"<jats:title>Abstract</jats:title><jats:p>What are the implications of AI (AI) on human rights in the next three decades? Precise answers to this question are made difficult by the rapid rate of innovation in AI research and by the effects of human practices on the adaption of new technologies. Precise answers are also challenged by imprecise usages of the term “AI.” There are several types of research that all fall under this general term. We begin by clarifying what we mean by AI. Most of our attention is then focused on the implications of artificial general intelligence (AGI), which entail that an algorithm or group of algorithms will achieve something like superintelligence. While acknowledging that the feasibility of superintelligence is contested, we consider the moral and ethical implications of such a potential development. What do machines owe humans and what do humans owe superintelligent machines?</jats:p>",0.0
163,10.1177/14614448241234040,Impact of misinformation from GenAI on user information processing: How people understand misinformation from GenAI,"<jats:p> This study examines the impact of AI (AI) on the ways in which users process and respond to misinformation in GenAI (GenAI) contexts. Drawing on the heuristic–systematic model and the concept of diagnosticity, our approach examines a cognitive model for processing misinformation in GenAI. The study’s findings revealed that users with a high-heuristic processing mechanism, which affects positive diagnostic perception, were more likely to proactively discern misinformation than users with low-heuristic processing and low-perceived diagnosticity. When exposed to misinformation from GenAI, users’ perceived diagnosticity of misinformation can be accurately predicted by the ways in which they perform heuristic systematic evaluations. With this focus on misinformation processing, this study provides theoretical insights and relevant recommendations for firms to be more resilient in protecting users from the detrimental impacts of misinformation. </jats:p>",-1.0
164,10.1142/s2705078524400034,Decolonizing AI: Implementing <i>Humanitarian AI</i>,"<jats:p> In this article, we critically examine the implementation of what Carlos Montemayor calls Humanitarian AI from a decolonizing perspective. We highlight the dichotomy of optimism and fear surrounding AI, elucidating its potential to address fundamental human problems and the risks of monopolistic control. We critique Montemayor’s proposal to align AI with a human rights framework, arguing that it insufficiently addresses global inequalities. Our tripartite analysis focuses on the distribution of AI resources, language inclusion, and content diversity to ensure AI benefits all humanity. We emphasize the need for equitable access to AI, linguistic diversity in AI training data, and the preservation of marginalized epistemologies. We advocate for strategies to mitigate environmental impacts and avoid cultural imperialism disguised as altruism, calling for a balanced approach between private sector innovation and state regulation to foster a truly humanitarian AI. </jats:p>",0.0
165,10.1186/s12910-023-00917-w,Trustworthy AI and ethical design: public perceptions of trustworthiness of an AI-based decision-support tool in the context of intrapartum care,"<jats:title>Abstract</jats:title><jats:sec>
                <jats:title>Background</jats:title>
                <jats:p>Despite the recognition that developing AI (AI) that is trustworthy is necessary for public acceptability and the successful implementation of AI in healthcare contexts, perspectives from key stakeholders are often absent from discourse on the ethical design, development, and deployment of AI. This study explores the perspectives of birth parents and mothers on the introduction of AI-based cardiotocography (CTG) in the context of intrapartum care, focusing on issues pertaining to trust and trustworthiness.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Methods</jats:title>
                <jats:p>Seventeen semi-structured interviews were conducted with birth parents and mothers based on a speculative case study. Interviewees were based in England and were pregnant and/or had given birth in the last two years. Thematic analysis was used to analyze transcribed interviews with the use of NVivo. Major recurring themes acted as the basis for identifying the values most important to this population group for evaluating the trustworthiness of AI.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Results</jats:title>
                <jats:p>Three themes pertaining to the perceived trustworthiness of AI emerged from interviews: (1) trustworthy AI-developing institutions, (2) trustworthy data from which AI is built, and (3) trustworthy decisions made with the assistance of AI. We found that birth parents and mothers trusted public institutions over private companies to develop AI, that they evaluated the trustworthiness of data by how representative it is of all population groups, and that they perceived trustworthy decisions as being mediated by humans even when supported by AI.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Conclusions</jats:title>
                <jats:p>The ethical values that underscore birth parents and mothers’ perceptions of trustworthy AI include fairness and reliability, as well as practices like patient-centered care, the promotion of publicly funded healthcare, holistic care, and personalized medicine. Ultimately, these are also the ethical values that people want to protect in the healthcare system. Therefore, trustworthy AI is best understood not as a list of design features but in relation to how it undermines or promotes the ethical values that matter most to its end users. An ethical commitment to these values when creating AI in healthcare contexts opens up new challenges and possibilities for the design and deployment of AI.</jats:p>
              </jats:sec>",6.0
166,10.3389/frai.2024.1415782,Adolescents’ use and perceived usefulness of GenAI for schoolwork: exploring their relationships with executive functioning and academic achievement,"<jats:p>In this study, we aimed to explore the frequency of use and perceived usefulness of LLM GenAI chatbots (e.g., GenAI) for schoolwork, particularly in relation to adolescents’ executive functioning (EF), which includes critical cognitive processes like planning, inhibition, and cognitive flexibility essential for academic success. Two studies were conducted, encompassing both younger (Study 1: <jats:italic>N</jats:italic> = 385, 46% girls, mean age 14 years) and older (Study 2: <jats:italic>N</jats:italic> = 359, 67% girls, mean age 17 years) adolescents, to comprehensively examine these associations across different age groups. In Study 1, approximately 14.8% of participants reported using GenAI, while in Study 2, the adoption rate among older students was 52.6%, with GenAI emerging as the preferred tool among adolescents in both studies. Consistently across both studies, we found that adolescents facing more EF challenges perceived GenAI as more useful for schoolwork, particularly in completing assignments. Notably, academic achievement showed no significant associations with AI usage or usefulness, as revealed in Study 1. This study represents the first exploration into how individual characteristics, such as EF, relate to the frequency and perceived usefulness of LLM GenAI chatbots for schoolwork among adolescents. Given the early stage of GenAI chatbots during the survey, future research should validate these findings and delve deeper into the utilization and integration of GenAI into educational settings. It is crucial to adopt a proactive approach to address the potential challenges and opportunities associated with these emerging technologies in education.</jats:p>",1.0
167,10.36548/jaicn.2024.3.003,Empowering Education by Developing and Evaluating GenAI-Powered Tutoring System for Enhanced Student Learning,"<jats:p>Personalized learning has always been a dream for schools, educators, and students but until recently, educators didn’t have the time or resources to implement it on a large scale. With the advancements in AI, GenAI can automate many of a teacher’s core tasks, such as creating lesson resources. providing lesson structures and key talking points, designing infographics, creating slideshows, and converting text into videos and images. This study details the development and evaluation of an AI-powered tutoring system designed to enhance student learning experiences. Motivated by the transformative potential of AI in education, the research aims to utilize large language models, including OpenAI, to create a personalized and adaptive learning environment. The research is a two-phase approach, involving a comprehensive literature review, problem definition, and AI integration in the Research Phase, followed by design, prototyping, and testing in the Design and Development Phase. The course creation workflow emphasizes the collaborative efforts of human tutors and AI algorithms using the GPT-3.5-Turbo model. The study identified the potential improvement in education where the course has been created by AI including the image generated by DALLE-3 and contributing to the evolving landscape of AI-assisted education using the text-to-voice, an automatic speech recognition system by Whisper, offering an innovative and transformative learning experience for students and tutors. The course content has question-answering chatbots where the students can ask any questions related to the topic while learning.</jats:p>",1.0
168,10.3390/heritage7030070,Will AI Affect How Cultural Heritage Will Be Managed in the Future? Responses Generated by Four genAI Models,"<jats:p>GenAI (genAI) language models have become firmly embedded in public consciousness. Their abilities to extract and summarise information from a wide range of sources in their training data have attracted the attention of many scholars. This paper examines how four genAI large language models (GenAI, GPT4, DeepAI, and Google Bard) responded to prompts, asking (i) whether AI would affect how cultural heritage will be managed in the future (with examples requested) and (ii) what dangers might emerge when relying heavily on genAI to guide cultural heritage professionals in their actions. The genAI systems provided a range of examples, commonly drawing on and extending the status quo. Without a doubt, AI tools will revolutionise the execution of repetitive and mundane tasks, such as the classification of some classes of artifacts, or allow for the predictive modelling of the decay of objects. Important examples were used to assess the purported power of genAI tools to extract, aggregate, and synthesize large volumes of data from multiple sources, as well as their ability to recognise patterns and connections that people may miss. An inherent risk in the ‘results’ presented by genAI systems is that the presented connections are ‘artifacts’ of the system rather than being genuine. Since present genAI tools are unable to purposively generate creative or innovative thoughts, it is left to the reader to determine whether any text that is provided by genAI that is out of the ordinary is meaningful or nonsensical. Additional risks identified by the genAI systems were that some cultural heritage professionals might use AI systems without the required level of AI literacy and that overreliance on genAI systems might lead to a deskilling of general heritage practitioners.</jats:p>",7.0
169,10.37867/te140127,IMPACT OF AI (AI) ON EDUCATION: CHANGING PARADIGMS AND APPROACHES,"<jats:p>AI (AI) technology is to make human life easy and trouble-free and contribute to the advancement of human development. AI is a driving technological force of the twenty-first century and it has been a centre of discussion in technological innovations for its unlimited potential to alter the scenario of social interaction through resolving social challenges and virtually transform every industry. Education is the top priority of present society because it is a fundamental human right that builds peace and drives sustainable development across the world. The integration and application of AI in the classrooms will make teaching and learning effective by supporting teachers and learners in the process through the usage of robotic technology and sensors. AI-based technology facilitates inclusive and equitable quality education along with ensuring universal access to life-long learning for all across the world. The technology of AI has been advanced and sophisticated that can recognize the gesture of the students and understand their mood and ease during the lecture even it can read facial expressions and posture of the students to understand difficulties and problems they are facing in the lecture and recommends altering the lesson. AI technology-based assessment system can be used to assess students' knowledge, understanding, skills such as collaboration and persistence and characteristics such as confidence and motivation etc. AI technology has developed speech-to-text transcription, predictive text and facial recognition promising an inclusive future for all learners.</jats:p>",1.0
170,10.17816/dd430356,AI ethics code in healthcare. Sustainability of AI systems: Why do we talk about their impact on the environment?,"<jats:p>Environmental problems have a tremendous impact on the entire world population, particularly on human health, which plays a leading role in individual well-being. Environmental pollution, according to some estimates, kills approximately 9 million people every year. The introduction of AI (AI) systems in many areas has enormous potential in reducing human impact on the environment; however, such systems have negative effects. The potential of AI systems to improve healthcare is inextricably linked to the ethical challenges posed by the complexity of these systems and their impact on the lives and health of communities, patients, and staff. In addition to aspects that relate directly to the algorithms, data, and clinical application of AI systems, long-term risks exist that are not obvious at first glance. One of these risks is the negative impact of AI systems on the environment, which may harm human health indirectly. AI systems are more than software, having physical components that are necessary for their functioning, such as processors, memory, and sensors. The manufacture and the energy consumption of the components has a profound effect on the environment. One study showed that when a single AI algorithm is trained, carbon emissions may reach values corresponding to the total carbon emissions from five cars lifetime.&#x0D;
This study analyzes existing literature linking the development of AI systems, especially in healthcare, to their effects on the environment. The study is intended to complement the emerging AI Ethics Code for healthcare, specifically the principles of sustainability that will be included in this code.&#x0D;
The study concludes that the environmental impact of AI systems should be considered when formulating ethical standards for AI in healthcare. These standards must be considered during the development, testing, and application phases of AI systems. All the people involved in the creation and use of AI systems (developers, physicians, and regulators) must monitor the environmental impact and minimize the environmental consequences of such systems at all stages of their existence. This principle calls for minimizing negative impacts, improving the energy efficiency, and disposing physical components in strict compliance with current legislation. Moreover, the rapid development of AI systems and the ethical dilemmas require that solutions be proposed jointly and ethical standards be developed in a manner that is consistent and sensitive to emerging technologies.</jats:p>",-1.0
171,10.7717/peerj-cs.1099,AI-SPedia: a novel ontology to evaluate the impact of research in the field of AI,"<jats:sec>
               <jats:title>Background</jats:title>
               <jats:p>Sharing knowledge such as resources, research results, and scholarly documents, is of key importance to improving collaboration between researchers worldwide. Research results from the field of AI (AI) are vital to share because of the extensive applicability of AI to several other fields of research. This has led to a significant increase in the number of AI publications over the past decade. The metadata of AI publications, including bibliometrics and altmetrics indicators, can be accessed by searching familiar bibliographical databases such as Web of Science (WoS), which enables the impact of research to be evaluated and identify rising researchers and trending topics in the field of AI.</jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title>Problem description</jats:title>
               <jats:p>In general, bibliographical databases have two limitations in terms of the type and form of metadata we aim to improve. First, most bibliographical databases, such as WoS, are more concerned with bibliometric indicators and do not offer a wide range of altmetric indicators to complement traditional bibliometric indicators. Second, the traditional format in which data is downloaded from bibliographical databases limits users to keyword-based searches without considering the semantics of the data.</jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title>Proposed solution</jats:title>
               <jats:p>To overcome these limitations, we developed a repository, named AI-SPedia. The repository contains semantic knowledge of scientific publications concerned with AI and considers both the bibliometric and altmetric indicators. Moreover, it uses semantic web technology to produce and store data to enable semantic-based searches. Furthermore, we devised related competency questions to be answered by posing smart queries against the AI-SPedia datasets.</jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title>Results</jats:title>
               <jats:p>The results revealed that AI-SPedia can evaluate the impact of AI research by exploiting knowledge that is not explicitly mentioned but extracted using the power of semantics. Moreover, a simple analysis was performed based on the answered questions to help make research policy decisions in the AI domain. The end product, AI-SPedia, is considered the first attempt to evaluate the impacts of AI scientific publications using both bibliometric and altmetric indicators and the power of semantic web technology.</jats:p>
            </jats:sec>",-1.0
172,10.3390/fintech2030024,Developing an Ethical Framework for Responsible AI (AI) and Machine Learning (ML) Applications in Cryptocurrency Trading: A Consequentialism Ethics Analysis,"<jats:p>The rise in AI (AI) and machine learning (ML) in cryptocurrency trading has precipitated complex ethical considerations, demanding a thorough exploration of responsible regulatory approaches. This research expands upon this need by employing a consequentialist theoretical framework, emphasizing the outcomes of AI and ML’s deployment within the sector and its effects on stakeholders. Drawing on critical case studies, such as SBF and FTX, and conducting an extensive review of relevant literature, this study explores the ethical implications of AI and ML in the context of cryptocurrency trading. It investigates the necessity for novel regulatory methods that address the unique characteristics of digital assets alongside existing legalities, such as those about fraud and insider trading. The author proposes a typology framework for AI and ML trading by comparing consequentialism to other ethical theories applicable to AI and ML use in cryptocurrency trading. By applying a consequentialist lens, this study underscores the significance of balancing AI and ML’s transformative potential with ethical considerations to ensure market integrity, investor protection, and overall well-being in cryptocurrency trading.</jats:p>",-1.0
173,10.1007/s11023-024-09694-w,Mapping the Ethics of GenAI: A Comprehensive Scoping Review,"<jats:title>Abstract</jats:title><jats:p>The advent of GenAI and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks. These risks often differ from those associated with traditional discriminative machine learning. To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of GenAI, including especially large language models and text-to-image models. Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature. The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding fairness, safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others. We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios.</jats:p>",-1.0
174,10.1007/s00146-024-02020-z,Drawing the full picture on diverging findings: adjusting the view on the perception of art created by AI,"<jats:title>Abstract</jats:title><jats:p>AI is becoming increasingly prevalent in creative fields that were thought to be exclusively human. Thus, it is non-surprising that a negative bias toward AI-generated artwork has been proclaimed. However, results are mixed. Studies that have presented AI-generated and human-created images simultaneously have detected a bias, but most studies in which participants saw either AI-generated or human-created images have not. Therefore, we propose that the bias arises foremost in a competitive situation between AI and humans. In a sample of <jats:italic>N</jats:italic> = 952 participants, we show that different evaluations emerge only when AI-generated and human-created pieces of art are presented simultaneously. Importantly, we demonstrate that AI art is not devalued, but rather, human art is upvalued, indicating the existence of a positive bias toward humans, rather than a negative bias. Further, we show that attitudes toward AI and empathy partially explain the different valuations of AI and human art in competitive situations.</jats:p>",2.0
175,10.1609/aimag.v41i4.5296,AI for Social Impact: Learning and Planning in the Data‐to‐Deployment Pipeline,"<jats:p><jats:italic>With the maturing of AI (AI) and multiagent systems research, we have a tremendous opportunity to direct these advances toward addressing complex societal problems. In pursuit of this goal of AI for social impact, we as AI researchers must go beyond improvements in computational methodology; it is important to step out in the field to demonstrate social impact. To this end, we focus on the problems of public safety and security, wildlife conservation, and public health in low‐resource communities, and present research advances in multiagent systems to address one key cross‐cutting challenge: how to effectively deploy our limited intervention resources in these problem domains. We present case studies from our deployments around the world as well as lessons learned that we hope are of use to researchers who are interested in AI for social impact. In pushing this research agenda, we believe AI can indeed play an important role in fighting social injustice and improving society.</jats:italic></jats:p>",-1.0
176,10.3390/ai5040095,"Digital Technologies Impact on Healthcare Delivery: A Systematic Review of AI (AI) and Machine-Learning (ML) Adoption, Challenges, and Opportunities","<jats:p>Recent significant advances in the healthcare industry due to AI (AI) and machine learning (ML) have been shown to revolutionize healthcare delivery by improving efficiency, accuracy, and patient outcomes. However, these technologies can face significant challenges and ethical considerations. This systematic review aimed to gather and synthesize the current knowledge on the impact of AI and ML adoption in healthcare delivery, with its associated challenges and opportunities. This study adhered to the PRISMA guidelines. Articles from 2014 to 2024 were selected from various databases using specific keywords. Eligible studies were included after rigorous screening and quality assessment using checklist tools. Themes were identified through data analysis and thematic analysis. From 4981 articles screened, a data synthesis of nine eligible studies revealed themes, including productivity enhancement, improved patient care through decision support and precision medicine, legal and policy challenges, technological considerations, organizational and managerial aspects, ethical concerns, data challenges, and socioeconomic implications. There exist significant opportunities, as well as substantial challenges and ethical concerns, associated with integrating AI and ML into healthcare delivery. Implementation strategies must be carefully designed, considering technical, ethical, and social factors.</jats:p>",3.0
177,10.3390/ai5020028,AI in Healthcare: GenAI and Beyond,"<jats:p>AI (AI), the simulation of human intelligence processes by machines, is having a growing impact on healthcare [...]</jats:p>",3.0
178,10.1007/s11569-024-00457-6,"An Integrated Embodiment Concept Combines Neuroethics and AI Ethics – Relational Perspectives on AI, Emerging Neurotechnologies and the Future of Work","<jats:title>Abstract</jats:title><jats:p>Applications of AI (AI) bear great transformative potential in the economic, technological and social sectors, impacting especially future work environments. Ethical regulation of AI requires a relational understanding of the technology by relevant stakeholder groups such as researchers, developers, politicians, civil servants, affected workers or other users applying AI in their work processes. The purpose of this paper is to support relational AI discourse for an improved ethical framing and regulation of the technology. The argumentation emphasizes a widespread reembodied understanding of AI technology as critical requirement for capable ethical and regulatory frameworks. A sociotechnical perspective encourages the material interpretation of AI as reembodied adaptation of biological intelligence. Reviewing Cartesian dualism as motivating the disembodiment of human intelligence for its transfer to machines, the argumentation develops an integrated embodiment concept of AI in its mechanistic, naturalistic, combined AI and neuroethical, and relational contexts. This concept is discussed in relation to basic phenomenological and postphenomenological assumptions, and is applied to the example of AI-based neurotechnology potentially disrupting future work processes. Strengthening a human-centered approach, the presented concept for a reembodied understanding of AI technology enables better integrated ethical and regulatory debates, and improves social discourse and human agency in developing and regulating AI technology.</jats:p>",0.0
179,10.54105/ijainn.b1084.04020224,Bias in Text Generative Open AI,"<jats:p>The rise of text generation models, especially those powered by advanced deep learning architectures like Open AI’s GPT-3, has unquestionably transformed various natural language processing applications. However, these models have recently faced examination due to their inherent biases, often evident in the generated text. This paper critically examines the issue of bias in text generation models, exploring the challenges posed, the ethical implications it entails, and the potential strategies to mitigate bias. Firstly, we go through the causes of the origin of the bias, ways to minimize it, and mathematical representation of Bias.</jats:p>",7.0
180,10.3389/frai.2024.1460651,Opportunities and challenges of using GenAI to personalize educational assessment,"<jats:p>In line with the positive effects of personalized learning, personalized assessments are expected to maximize learner motivation and engagement, allowing learners to show what they truly know and can do. Considering the advances in GenAI (GenAI), in this perspective article, we elaborate on the opportunities of integrating GenAI into personalized educational assessments to maximize learner engagement, performance, and access. We also draw attention to the challenges of integrating GenAI into personalized educational assessments regarding its potential risks to the assessment’s core values of validity, reliability, and fairness. Finally, we discuss possible solutions and future directions.</jats:p>",1.0
181,10.34104/ijma.023.0074090,The Impact of AI (AI) on Customer Relationship Management: A Qualitative Study,"<jats:p>Ever since the commercialization of the Internet in the '90s, technology has been evolving faster than ever with the advent of cloud computing, social media, ubiquitous mobile devices, the Internet of Things (IoT), blockchain, and more. A staggering number of three billion internet users, five billion mobile users, and six billion devices are now connected through this massive global network of networks, facilitating customer information exchange and interaction never before seen in history. Driven by recent technological advances in computing power, big data, high-speed internet connection, and easier access to models built with advanced algorithms, AI (AI) is the next wave of innovation, which has already come into widespread awareness in the consumer world with the emergence of virtual assistants and chatbots (e.g., Amazon's Alexa, Apple's Siri, Google's Assistant), image recognition (e.g., Facebook Photos, Google ImageNet), personalized recommendations (e.g., Netflix, Amazon) and autonomous driving (e.g., Tesla, Google Waymo). This qualitative research study intends to learn about the impact of AI on customer relationship management (CRM), specifically in the area of customer service of problem resolution. Most prior research focuses on the AI technologies leveraged in CRM systems, such as machine learning, natural language processing, voice recognition, chatbots, data analytics, and cloud infrastructure. Few extant studies have used a qualitative research methodology to gather data from industry experts to truly understand the impact of AI technologies on customer relationship management, especially in the area of customer service and problem resolution. This study aims to fill this research gap. This research contributes to the literature on AI in the context of CRM and is of value to both academics and practitioners as it provides a detailed analysis and documentation of the impact of AI on the customer service domain.</jats:p>",4.0
182,10.3390/ijfs8030045,Industry 4.0 in Finance: The Impact of AI (AI) on Digital Financial Inclusion,"<jats:p>This study sought to investigate the impact of AI on digital financial inclusion. Digital financial inclusion is becoming central in the debate on how to ensure that people who are at the lower levels of the pyramid become financially active. Fintech companies are using AI and its various applications to ensure that the goal of digital financial inclusion is realized that is to ensure that low-income earners, the poor, women, youths, small businesses participate in the mainstream financial market. This study used conceptual and documentary analysis of peer-reviewed journals, reports and other authoritative documents on AI and digital financial inclusion to assess the impact of AI on digital financial inclusion. The present study discovered that AI has a strong influence on digital financial inclusion in areas related to risk detection, measurement and management, addressing the problem of information asymmetry, availing customer support and helpdesk through chatbots and fraud detection and cybersecurity. Therefore, it is recommended that financial institutions and non-financial institutions and governments across the world adopt and scale up the use of AI tools and applications as they present benefits in the quest to ensure that the vulnerable groups of people who are not financially active do participate in the formal financial market with minimum challenges and maximum benefits.</jats:p>",-1.0
183,10.35295/osls.iisl/0000-0000-0000-1273,The ethics of AI: An analysis of ethical frameworks disciplining AI in justice and other contexts of application,"<jats:p>The recent introduction of AI tools in the justice sector poses several ethical implications as risks for judges’ independence and for procedural transparency, and discrimination biases. By developing ethical frameworks governing AI application, private and public agents have been increasingly dealing with risks pertaining to the use of AI. By inventorying and analyzing a set of ethical documents through content analysis, this study highlights the ethical implications involved in the application of AI. Moreover, by investigating the CEPEJ Charter (European Commission for the Effectiveness of Justice of the Council of Europe), the unique ethical document focusing on AI in justice, we were able to clarify potential differences between justice and other contexts of AI application with respect to risks prospected and the protection of ethical principles. The analysis confirms that the discipline of AI is a complex subject that involves very different aspects and therefore needs a broad focus on all contexts of application.</jats:p>",0.0
184,10.1142/s2705078522500060,Artificial Agential Intelligence,"<jats:p> Since AI (AI) emerged in the mid-20th century, it has incurred many theoretical criticisms (Dreyfus, H. [1972] What Computers Can’t Do (MIT Press, New York); Dreyfus, H. [1992] What Computers Still Can’t Do (MIT Press, New York); Searle, J. [1980] Minds, brains and programs, Behav. Brain Sci. 3, 417–457; Searle, J. [1984] Minds, Brains and Sciences (Harvard University Press, Cambridge, MA); Searle, J. [1992] The Rediscovery of the Mind (MIT Press, Cambridge, MA); Fodor, J. [2002] The Mind Doesn’t Work that Way: The Scope and Limits of Computational Psychology (MIT Press, Cambridge, MA).). The technical improvements of machine learning and deep learning, though, have been continuing and many breakthroughs have occurred recently. This makes theoretical considerations urgent again: can this new wave of AI fare better than its precursors in emulating or even having human-like minds? I propose a cautious yet positive hypothesis: current AI might create human-like mind, but only if it incorporates certain conceptual rewiring: it needs to shift from a task-based to an agent-based framework, which can be dubbed “Artificial Agential Intelligence” (AAI). It comprises practical reason (McDowell, J. [1979] Virtue and reason, Monist 62(3), 331–350; McDowell, J. [1996] Mind and World (Harvard University Press, Cambridge, MA)), imaginative understanding (Campbell, J. [2020] Causation in Psychology (Harvard University Press, Cambridge, MA)), and animal knowledge (Sosa, E. [2007] A Virtue Epistemology: Apt Belief and Reflective Knowledge, volume 1 (Oxford University Press, Oxford, UK); Sosa, E. [2015] Judgment and Agency (Oxford University Press, Cambridge, MA)). Moreover, I will explore whether and in what way neuroscience-inspired AI and predictive coding (Hassabis, D., Kumaran, D., Summerfield, C., &amp; Botvinick, M. [2017] Neuroscience-inspired AI, Neuron 95(2), 245–258) can help carry out this project. </jats:p>",0.0
185,10.51191/issn.2637-1898.2019.2.2.24,"AI and Aesthetics of Music: Intelligent Anarchy.
diagramic notebook on: AI-IA relations",<jats:p>An experimental diagramic article on the relationship between AI and the aesthetic of music.</jats:p>,2.0
186,10.61643/c16282,Emotion and Inspiration through GenAI Art,"<jats:p>Art inspires emotion. The process of creation invigorates the senses and keeps the researcher on fire. To create artistic masterpieces using words through natural language processing further stimulates the mind and helps to overcome writer’s block. An AI tool interprets the words and transforms them into artistic expressions that feel like magic. Yet, AI art is not without consequences. Critics challenge the digital rights of AI-generated images, noting that they are derivative works. This paper examines the joy of creation as well as the copyright challenges creators face when writing detailed prompts to generate amazing artwork. The role of affective computing illustrates the relationship between the designer and the AI tool while ethical concerns remind designers to use caution in their prompts.  The article examines the architecture of a prompt, the characteristics to enhance it, and examples of GenAI art. Additional content includes references to handouts, art catalogs, and legal opinions regarding the copyright debate. The article concludes with a recommendation with respect to the intellectual property rights of artists and their creative work.</jats:p>",2.0
187,10.2196/48392,The GenAI (GenAI) Revolution Has Made AI Approachable for Medical Professionals,"<jats:p>In November 2022, OpenAI publicly launched its large language model (LLM), GenAI, and reached the milestone of having over 100 million users in only 2 months. LLMs have been shown to be useful in a myriad of health care–related tasks and processes. In this paper, I argue that attention to, public access to, and debate about LLMs have initiated a wave of products and services using GenAI (AI), which had previously found it hard to attract physicians. This paper describes what AI tools have become available since the beginning of the GenAI revolution and contemplates how it they might change physicians’ perceptions about this breakthrough technology.</jats:p>",3.0
188,10.33847/2712-8148.4.1_4,Unlocking the Black Box: Explainable AI (XAI) for Trust and Transparency in AI Systems,"<jats:p>Explainable AI (XAI) has emerged as a critical field in AI research, addressing the lack of transparency and interpretability in complex AI models. This conceptual review explores the significance of XAI in promoting trust and transparency in AI systems. The paper analyzes existing literature on XAI, identifies patterns and gaps, and presents a coherent conceptual framework. Various XAI techniques, such as saliency maps, attention mechanisms, rule-based explanations, and model-agnostic approaches, are discussed to enhance interpretability. The paper highlights the challenges posed by black-box AI models, explores the role of XAI in enhancing trust and transparency, and examines the ethical considerations and responsible deployment of XAI. By promoting transparency and interpretability, this review aims to build trust, encourage accountable AI systems, and contribute to the ongoing discourse on XAI.</jats:p>",-1.0
189,10.5209/aris.83808,"Arguments for the Rise of AI Art: Does AI Art Have Creativity, Motivation, Self-awareness and Emotion?","<jats:p>With the advent of AI (AI), 'AI art' was born, and the concept of 'AI aesthetics' was derived. Despite the emergence of this new concept in art theory, the question of whether artworks created by AI have artistic and aesthetic value still needs to be debated in academia. While new concepts related to AI art are emerging, the discussion of whether a sustainable and critical theory system can be constructed in the field of computers and art, which is most closely related to it, ought to be focused on AI itself to explore whether it possesses similar characteristics of creativity and emotion as traditional art creation processes. This paper will first analyze the origins and possibilities of AI art and then explore the enormous impact of the rise of AI art on current and future human society in 4 dimensions: creativity, motivation, self-awareness, and emotion.</jats:p>",2.0
190,10.3389/frai.2024.1426761,Ethics dumping in AI,"<jats:p>AI (AI) systems encode not just statistical models and complex algorithms designed to process and analyze data, but also significant normative baggage. This ethical dimension, derived from the underlying code and training data, shapes the recommendations given, behaviors exhibited, and perceptions had by AI. These factors influence how AI is regulated, used, misused, and impacts end-users. The multifaceted nature of AI’s influence has sparked extensive discussions across disciplines like Science and Technology Studies (STS), Ethical, Legal and Social Implications (ELSI) studies, public policy analysis, and responsible innovation—underscoring the need to examine AI’s ethical ramifications. While the initial wave of AI ethics focused on articulating principles and guidelines, recent scholarship increasingly emphasizes the practical implementation of ethical principles, regulatory oversight, and mitigating unforeseen negative consequences. Drawing from the concept of “ethics dumping” in research ethics, this paper argues that practices surrounding AI development and deployment can, unduly and in a very concerning way, offload ethical responsibilities from developers and regulators to ill-equipped users and host environments. Four key trends illustrating such ethics dumping are identified: (1) AI developers embedding ethics through coded value assumptions, (2) AI ethics guidelines promoting broad or unactionable principles disconnected from local contexts, (3) institutions implementing AI systems without evaluating ethical implications, and (4) decision-makers enacting ethical governance frameworks disconnected from practice. Mitigating AI ethics dumping requires empowering users, fostering stakeholder engagement in norm-setting, harmonizing ethical guidelines while allowing flexibility for local variation, and establishing clear accountability mechanisms across the AI ecosystem.</jats:p>",0.0
191,10.1007/s44163-023-00072-6,Shall androids dream of genocides? How GenAI can change the future of memorialization of mass atrocities,"<jats:title>Abstract</jats:title><jats:p>The memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. Digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. At the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. The emergence of generative forms of AI (AI), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. AI can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. The use of GenAI in this context raises numerous questions: For example, can the paucity of training data on mass atrocities distort how AI interprets some atrocity-related inquiries? How important is the ability to differentiate between human- and AI-made content concerning mass atrocities? Can AI-made content be used to promote false information concerning atrocities? This article addresses these and other questions by examining the opportunities and risks associated with using GenAIs for memorializing mass atrocities. It also discusses recommendations for AIs integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.</jats:p>",7.0
192,10.3390/ai1020008,AI (AI) or Intelligence Augmentation (IA): What Is the Future?,"<jats:p>AI (AI) is a rapidly growing technological phenomenon that all industries wish to exploit to benefit from efficiency gains and cost reductions. At the macrolevel, AI appears to be capable of replacing humans by undertaking intelligent tasks that were once limited to the human mind. However, another school of thought suggests that instead of being a replacement for the human mind, AI can be used for intelligence augmentation (IA). Accordingly, our research seeks to address these different views, their implications, and potential risks in an age of increased artificial awareness. We show that the ultimate goal of humankind is to achieve IA through the exploitation of AI. Moreover, we articulate the urgent need for ethical frameworks that define how AI should be used to trigger the next level of IA.</jats:p>",0.0
193,10.3390/ai4010012,AI-Enhanced UUV Actuator Control,"<jats:p>This manuscript compares deterministic AI to a model-following control applied to DC motor control, including an evaluation of the threshold computation rate to let unmanned underwater vehicles correctly follow the challenging discontinuous square wave command signal. The approaches presented in the main text are validated by simulations in MATLAB®, where the motor process is discretized at multiple step sizes, which is inversely proportional to the computation rate. Performance is compared to canonical benchmarks that are evaluated by the error mean and standard deviation. With a large step size, discrete deterministic AI shows a larger error mean than the model-following self-turning regulator approach (the selected benchmark). However, the performance improves with a decreasing step size. The error mean is close to the continuous deterministic AI when the step size is reduced to 0.2 s, which means that the computation rate and the sampling period restrict discrete deterministic AI. In that case, continuous deterministic AI is the most feasible and reliable selection for future applications on unmanned underwater vehicles, since it is superior to all the approaches investigated at multiple computation rates.</jats:p>",-1.0
194,10.1007/s43681-021-00039-2,AI auditing and impact assessment: according to the UK information commissioner’s office,"<jats:title>Abstract</jats:title><jats:p>As the use of data and AI systems becomes crucial to core services and business, it increasingly demands a multi-stakeholder and complex governance approach. The Information Commissioner's Office’s ‘Guidance on the AI auditing framework: Draft guidance for consultation’ is a move forward in AI governance. The aim of this initiative is toward producing guidance that encompasses both technical (e.g. system impact assessments) and non-engineering (e.g. human oversight) components to governance and represents a significant milestone in the movement towards standardising AI governance. This paper will summarise and critically evaluate the ICO effort and try to anticipate future debates and present some general recommendations.</jats:p>",0.0
195,10.2196/48123,Effect of Benign Biopsy Findings on an AI–Based Cancer Detector in Screening Mammography: Retrospective Case-Control Study,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>AI (AI)–based cancer detectors (CAD) for mammography are starting to be used for breast cancer screening in radiology departments. It is important to understand how AI CAD systems react to benign lesions, especially those that have been subjected to biopsy.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>Our goal was to corroborate the hypothesis that women with previous benign biopsy and cytology assessments would subsequently present increased AI CAD abnormality scores even though they remained healthy.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>This is a retrospective study applying a commercial AI CAD system (Insight MMG, version 1.1.4.3; Lunit Inc) to a cancer-enriched mammography screening data set of 10,889 women (median age 56, range 40-74 years). The AI CAD generated a continuous prediction score for tumor suspicion between 0.00 and 1.00, where 1.00 represented the highest level of suspicion. A binary read (flagged or not flagged) was defined on the basis of a predetermined cutoff threshold (0.40). The flagged median and proportion of AI scores were calculated for women who were healthy, those who had a benign biopsy finding, and those who were diagnosed with breast cancer. For women with a benign biopsy finding, the interval between mammography and the biopsy was used for stratification of AI scores. The effect of increasing age was examined using subgroup analysis and regression modeling.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>Of a total of 10,889 women, 234 had a benign biopsy finding before or after screening. The proportions of flagged healthy women were 3.5%, 11%, and 84% for healthy women without a benign biopsy finding, those with a benign biopsy finding, and women with breast cancer, respectively (P&lt;.001). For the 8307 women with complete information, radiologist 1, radiologist 2, and the AI CAD system flagged 8.5%, 6.8%, and 8.5% of examinations of women who had a prior benign biopsy finding. The AI score correlated only with increasing age of the women in the cancer group (P=.01).</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>Compared to healthy women without a biopsy, the examined AI CAD system flagged a much larger proportion of women who had or would have a benign biopsy finding based on a radiologist’s decision. However, the flagging rate was not higher than that for radiologists. Further research should be focused on training the AI CAD system taking prior biopsy information into account.</jats:p>
          </jats:sec>",3.0
196,10.1145/2685328.2685334,"Cryptocurrencies, smart contracts, and AI","<jats:p>Recent developments in ""cryptocurrencies"" and ""smart contracts"" are creating new opportunities for applying AI techniques. These economic technologies would benefit from greater real world knowledge and reasoning as they become integrated with everyday commerce. Cryptocurrencies and smart contracts may also provide an infrastructure for ensuring that AI systems follow specified legal and safety regulations as they become more integrated into human society.</jats:p>",0.0
197,10.1145/3465074.3465080,Applied AI matters: AI4Code,"<jats:p>The marriage of AI (AI) techniques to problems surrounding the generation, maintenance, and use of source code has come to the fore in recent years as an important AI application area1. A large chunk of this recent attention can be attributed to contemporaneous advancements in Natural Language Processing (NLP) techniques and sub-fields. The naturalness hypothesis, which states that ""software is a form of human communication"" and that code exhibits patterns that are similar to (human) natural languages (Devanbu, 2015; Hindle, Barr, Gabel, Su, &amp; Devanbu, 2016), has allowed for the application of many of these NLP advances to code-centric usecases. This development has contributed to a spate of work in the community --- much of it captured in a survey by Allamanis, Barr, Devanbu, and Sutton (2018) that focuses on classifying these approaches by the type of probabilistic model applied to source code.</jats:p>
          <jats:p>This increase in the variety of AI techniques applied to source code has found various manifestations in the industry at large. Code and software form the backbone that underpins almost all modern technical advancements: it is thus natural that breakthroughs in this area should reflect in the emergence of real world deployments.</jats:p>",1.0
198,10.1007/s43681-024-00560-0,Ensuring fundamental rights compliance and trustworthiness of law enforcement AI systems: the ALIGNER Fundamental Rights Impact Assessment,"<jats:title>Abstract</jats:title><jats:p>AI systems can expand the capabilities and enhance the efficiency of law enforcement agencies preventing, investigating, detecting, and prosecuting criminal offences in the European Union. At the same time, the deployment of AI in the security domain often raises numerous legal and ethical concerns. The ALIGNER Fundamental Rights Impact Assessment is an operational tool, rooted in fundamental rights and in the principles of AI ethics, ready to be integrated in the AI governance measures of European law enforcement agencies to inform their decision-making processes and ensure compliance with the recently adopted AI Act. This paper first introduces the main tensions between law enforcement AI and fundamental rights, as enshrined in the Charter of Fundamental Rights of the European Union; then, it gives an overview of the main developments and best practices in AI governance and their relationship with fundamental rights as well as AI ethics; and finally, it describes the structure of the ALIGNER Fundamental Rights Impact Assessment.</jats:p>",0.0
199,10.1007/s00146-024-02004-z,Risk and artificial general intelligence,"<jats:title>Abstract</jats:title><jats:p>Artificial General Intelligence (AGI) is said to pose many risks, be they catastrophic, existential and otherwise. This paper discusses whether the notion of risk can apply to AGI, both descriptively and in the current regulatory framework. The paper argues that current definitions of risk are ill-suited to capture supposed AGI existential risks, and that the risk-based framework of the EU AI Act is inadequate to deal with truly general, agential systems.</jats:p>",0.0
200,10.1007/s00146-022-01474-3,Exposing implicit biases and stereotypes in human and AI: state of the art and challenges with a focus on gender,"<jats:title>Abstract</jats:title><jats:p>Biases in cognition are ubiquitous. Social psychologists suggested biases and stereotypes serve a multifarious set of cognitive goals, while at the same time stressing their potential harmfulness. Recently, biases and stereotypes became the purview of heated debates in the machine learning community too. Researchers and developers are becoming increasingly aware of the fact that some biases, like gender and race biases, are entrenched in the algorithms some AI applications rely upon. Here, taking into account several existing approaches that address the problem of implicit biases and stereotypes, we propose that a strategy to cope with this phenomenon is to unmask those found in AI systems by understanding their cognitive dimension, rather than simply trying to correct algorithms. To this extent, we present a discussion bridging together findings from cognitive science and insights from machine learning that can be integrated in a state-of-the-art semantic network. Remarkably, this resource can be of assistance to scholars (e.g., cognitive and computer scientists) while at the same time contributing to refine AI regulations affecting social life. We show how only through a thorough understanding of the cognitive processes leading to biases, and through an interdisciplinary effort, we can make the best of AI technology.</jats:p>",-1.0
201,10.46392/kjge.2023.17.6.333,Analysis of Professors’ Experiences with GenAI and the Concerns of Classroom Use : Application of the Concerns-Based Adoption Model (CBAM),"<jats:p>The rise of GenAI, such as GenAI, poses challenges for the education sector. To explore strategies for addressing GenAI issues and identifying educational applications, it is essential for us to understand the concerns and attitudes of educators, who are the key implementers in education. In this context, this study investigated the experiences and concerns of university professors regarding the educational use of GenAI by utilizing the Concerns-Based Adoption Model (CBAM). Data was collected from 100 professors representing various disciplines at University A in Gyeonggi-do. The majority of respondents had some experience with GenAI, but its educational utilization was limited. Concerns included the provision of incorrect or biased information, ethical issues, and users' lack of ability to use GenAI. However, 61% of respondents expressed their intention to apply GenAI in their courses in the upcoming semester. Concern levels aligned with the non-user profile, with lower concerns in the consequence stage. There were no significant differences in concern levels based on the professors' disciplines, but differences were observed based on their experience and intention to use GenAI in their classes. Professors who had already used GenAI in their classes showed higher concerns regarding the consequence, collaboration, and refocusing stages. Based on these findings, implications for the educational use of GenAI in higher education were discussed.</jats:p>",1.0
202,10.1002/ail2.100,Developing and Deploying End‐to‐End Machine Learning Systems for Social Impact: A Rubric and Practical AI Case Studies From African Contexts,"<jats:title>ABSTRACT</jats:title><jats:p>AI (AI) and machine learning have demonstrated the potential to provide solutions to societal challenges, for example, automated crop diagnostics for smallholder farmers, environmental pollution modelling and prediction for cities and machine translation systems for languages that enable information access and communication for segments of the population who are unable to speak or write official languages, among others. Despite the potential of AI, the practical and technical issues related to its development and deployment in the African context are the least documented and understood. The development and deployment of AI for social impact systems in the developing world present new intricacies and requirements emanating from the unique technology and social ecosystems in these settings. This paper provides a rubric for developing and deploying AI systems for social impact with a focus on the African context. The rubric is derived from the analysis of a series of selected real‐world case studies of AI applications in Africa. We assessed the selected AI case studies against the proposed rubric. The rubric and examples of AI applications presented in this paper are expected to contribute to the development and application of AI systems in other African contexts.</jats:p>",-1.0
203,10.3390/educsci14020172,A Primer on GenAI,"<jats:p>Many educators and professionals in different industries may need to become more familiar with the basic concepts of AI (AI) and GenAI (Gen-AI). Therefore, this paper aims to introduce some of the basic concepts of AI and Gen-AI. The approach of this explanatory paper is first to introduce some of the underlying concepts, such as AI, machine learning, deep learning, artificial neural networks, and large language models (LLMs), that would allow the reader to better understand GenAI. The paper also discusses some of the applications and implications of GenAI on businesses and education, followed by the current challenges associated with GenAI.</jats:p>",4.0
204,10.1002/cae.22806,"Impact of basic AI (AI) course on understanding concepts, literacy, and empowerment in the field of AI among students","<jats:title>Abstract</jats:title><jats:p>With the development of information technologies and information processing methods, it is important to provide high‐quality education in the field of AI (AI). The study aims to investigate the impact of an educational course on AI on the comprehension of concepts, literacy, and empowerment in the field of AI among students of higher educational institutions. The experiment involved 125 students from Hohai University in China. As a result of taking the training course, students were able to improve their understanding of concepts (increasing their average score from 6.33 to 9.69), literacy (from 2.94 to 3.99), and empowerment (from 3.90 to 4.04) in AI. The resulting data statistically confirmed the effectiveness of the developed course for improving confidence in the field of AI. The training module can be applied to improve confidence in the field of AI for students in various careers, as information competence is important these days and increases the success of graduates in employment. When it comes to further research, the encouraging results of this study suggest opportunities for promoting this training program among a diverse group of participants. To confirm the effectiveness of the developed course, it can be conducted among students in schools and other educational institutions, reducing it to even more basic if necessary.</jats:p>",1.0
205,10.3390/ai4030034,Explainable AI (XAI): Concepts and Challenges in Healthcare,"<jats:p>AI (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (“clinical decision support”), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a “black box”, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable AI (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.</jats:p>",3.0
206,10.3389/frai.2023.1239466,Does AI kill employment growth: the missing link of corporate AI posture,"<jats:sec><jats:title>Introduction</jats:title><jats:p>An intense debate has been on-going about how AI (AI) technology investments have an impact on employment. The debate has often focused on the potential of AI for human task automation, omitting the strategic incentive for firms to cooperate with their workers as to exploit AI technologies for the most relevant benefit of new product and service innovation.</jats:p></jats:sec><jats:sec><jats:title>Method</jats:title><jats:p>We calibrate an empirical probit regression model of how changes in employment relate to AI diffusion, based on formalizing a game-theoretical model of a firm exploiting the twin role of AI innovation and AI automation for both absolute and competitive advantage.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The theoretical game-theory prediction is that employment following AI technology adoption is not negative, and ultimately depends on how AI leads to new success in innovation, competition which defines the competitive reward of innovation and profit sharing between workers and firms. Our estimation, is based on a global survey of 3,000 large companies across 10 countries, demonstrates that a firm employment growth depends on two strategic postures, that is, the firm relative maturity of AI adoption as well as its relative bias toward AI-based product innovation.</jats:p></jats:sec><jats:sec><jats:title>Discussion</jats:title><jats:p>The contribution of this research is to highlight the twin role of firm and workers in shaping how technology will affect employment. AI in particular marries the potential of task automation with even more potential for expansion.</jats:p></jats:sec>",-1.0
207,10.1007/s44223-024-00066-z,Adaptive interior design method for different MBTI personality types based on GenAI,"<jats:title>Abstract</jats:title><jats:p>Accurately predicting homeowners’ aesthetic preferences is crucial in interior design. This study develops a fine-tuning model (LORA) for interior design styles corresponding to different MBTI personality types, leveraging the Stable Diffusion Web UI platform and integrating it into a GenAI framework. Subsequently, personalized aesthetic preference architectural interior renderings are recommended based on homeowners’ personality traits, aiming to achieve an adaptive interior design approach. To achieve more precise adaptive solutions, this research surveys the style and color tendencies of respondents with different MBTI personality types and adds style description prompts to assist in image generation. The study finds that this method can better predict the interior design styles favored by certain MBTI personality types. This research contributes to addressing aesthetic biases between designers and homeowners, bringing innovative ideas and methods to interior design, and is expected to enhance homeowners’ satisfaction.</jats:p>",2.0
208,10.1111/nup.12306,AI and Robotics in Nursing: Ethics of Caring as a Guide to Dividing Tasks Between AI and Humans,"<jats:title>Abstract</jats:title><jats:p>Nurses have traditionally been regarded as clinicians that deliver compassionate, safe, and empathetic health care (Nurses again outpace other professions for honesty &amp; ethics, 2018). Caring is a fundamental characteristic, expectation, and moral obligation of the nursing and caregiving professions (Nursing: Scope and standards of practice, American Nurses Association, Silver Spring, MD, 2015). Along with caring, nurses are expected to undertake ever‐expanding duties and complex tasks. In part because of the growing physical, intellectual and emotional demandingness, of nursing as well as technological advances, AI (AI) and AI care robots are rapidly changing the healthcare landscape. As technology becomes more advanced, efficient, and economical, opportunities and pressure to introduce AI into nursing care will only increase. In the first part of the article, we review recent and existing applications of AI in nursing and speculate on future use. Second, situate our project within the recent literature on the ethics of nursing and AI. Third, we explore three dominant theories of caring and the two paradigmatic expressions of caring (touch and presence) and conclude that AI—at least for the foreseeable future—is incapable of caring in the sense central to nursing and caregiving ethics. We conclude that for AI to be implemented ethically, it cannot transgress the core values of nursing, usurp aspects of caring that can only meaningfully be carried out by human beings, and it must support, open, or improve opportunities for nurses to provide the uniquely human aspects of care.</jats:p>",6.0
209,10.1145/3375637.3375643,Cosmology of AI project,"<jats:p>AI (AI) has transcended beyond buzzwords, keywords or trends. The ubiquity of AI is so profound that it has managed to seep into popular culture. It breeds throughout social media platforms. It dominates the airwaves. It is impossible to watch television without mention of the acronym, the word, and how everyone is using it. However, if we look a little closer, take a deep dive into the AI pool where everyone seems to be swimming, we began to learn that AI has been misinterpreted, misrepresented, and incorrectly defined. ""In the Laws of Thought and Thinking Machines,"" Hughes and Hughes (2019) talk about how uncovering the truth of the definition, meaning, applications, and implications of AI would be a noteworthy goal. Today we are pursuing that goal.</jats:p>",0.0
210,10.3390/ijerph20043772,When AI Voices Human Concerns: The Paradoxical Effects of AI Voice on Climate Risk Perception and Pro-Environmental Behavioral Intention,"<jats:p>AI (AI)-enabled text-to-speech transformation has been widely employed to deliver online information in various fields. However, few studies have investigated the effect of the AI voice in environmental risk communication, especially in the field of climate change, an issue that poses a severe threat to global public health. To address this gap, the current study examines how the AI voice impacts the persuasive outcome of climate-related information and the potential mechanism that underlies this process. Based on the social and affect heuristics of voice, we propose a serial mediation model to test the effect of climate-related information delivered by different voice types (AI voice vs. human voice) in eliciting risk perception and motivating pro-environmental behavioral intention. Through an online auditory experiment (N = 397), we found the following. First, the AI voice was as effective as the human voice in eliciting risk perception and motivating pro-environmental behavioral intention. Second, compared with human voice, the AI voice yielded a listener’s lower level of perceived identity oneness with the speaker, which decreased risk perception and subsequently inhibited pro-environmental behavioral intention. Third, compared with human voice, the AI voice produced a higher level of auditory fear, which increased risk perception and thereby led to stronger pro-environmental behavioral intention. The paradoxical role of the AI voice and its wise use in environmental risk communication for promoting global public health are discussed.</jats:p>",-1.0
211,10.54097/hset.v68i.12112,Research on the application of AI generated AI technology in new media art,"<jats:p>With the rapid development of information society and computer science, new media art has not made a qualitative leap in the past decade. The reason is that in big data and machine learning, the rapid development of AI technology has brought computer science up to a higher level, while art is still stuck in aesthetics, acoustics, vision, psychology and other aspects, lack of logic, intelligence and integration. Therefore, the new media art needs the support of the computer technology, especially the new technology.In the design stage of new media art, technologies such as AI, machine learning and big data mining are integrated to improve the molding speed of new media art through mathematical modeling.Through machine learning, quickly understand user experience and user needs, further optimize new media art works, use AI AI technology to simulate new media art, and simulate the whole process of user experience and interaction is the main content of this paper.This makes new media art become a typical application in the field of AICG (AI production content). GenAI will greatly reduce the marginal cost of creation and knowledge work, greatly improve labor productivity and economic value, and realize the generation of new media art original content at one tenth of the cost and thousands times production speed.</jats:p>",2.0
212,10.1097/acm.0000000000005439,GenAI and GenAI for Medical Education: Potential Impact and Opportunity,"<jats:title>Abstract</jats:title>
          <jats:p>GenAI has ushered in a new era of AI (AI) that already has significant consequences for many industries, including health care and education. GenAI tools, such as GenAI, refer to AI that is designed to create or generate new content, such as text, images, or music, from their trained parameters. With free access online and an easy-to-use conversational interface, GenAI quickly accumulated more than 100 million users within the first few months of its launch. Recent headlines in the popular press have ignited concerns relevant to medical education over the possible implications of cheating and plagiarism in assessments as well as excitement over new opportunities for learning, assessment, and research. In this Scholarly Perspective, the authors offer insights and recommendations about GenAI for medical educators based on literature review, including the AI literacy framework. The authors provide a definition of GenAI, introduce an AI literacy framework and competencies, and offer considerations for potential impacts and opportunities to optimize integration of GenAI for admissions, learning, assessment, and medical education research to help medical educators navigate and start planning for this new environment. As GenAI tools continue to expand, educators need to increase their AI literacy through education and vigilance around new advances in the technology and serve as stewards of AI literacy to foster social responsibility and ethical awareness around the use of AI.</jats:p>",3.0
213,10.21141/pjp.2023.08,"Chatbots, GenAI, and Scholarly Manuscripts","<jats:p>This statement revises our earlier “WAME Recommendations on GenAI and Chatbots in Relation to Scholarly Publications” (January 20, 2023). The revision reflects the proliferation of chatbots and their expanding use in scholarly publishing over the last few months, as well as emerging concerns regarding lack of authenticity of content when using chatbots. These Recommendations are intended to inform editors and help them develop policies for the use of chatbots in papers published in their journals. They aim to help authors and reviewers understand how best to attribute the use of chatbots in their work, and to address the need for all journal editors to have access to manuscript screening tools. In this rapidly evolving field, we will continue to modify these recommendations as the software and its applications develop.</jats:p>",-1.0
214,10.70715/jitcai.2024.v1.i1.004,The Impact of GenAI on Student Engagement and Ethics in Higher Education,"<jats:p>The rapid adoption of AI (AI) in higher education is reshaping students’ learning experiences, with tools such as GenAI, Grammarly, and Microsoft Copilot becoming integral to academic work. This study, informed by data from the Digital Education Council Global AI Student Survey 2024, examines the impact of AI on students, focusing on usage patterns, trust in AI-generated content, ethical awareness, and expectations for institutional support. Findings indicate that 86% of students use AI for various academic tasks, with a majority expressing concerns about trust, fairness, and over-reliance on AI. While students value AI’s benefits, only 5% are fully aware of institutional guidelines on AI use, and 72% desire more AI literacy courses, reflecting a significant need for comprehensive support in navigating AI responsibly. The study underscores the importance of clear ethical guidelines, faculty training, and student involvement in AI policy formation to foster responsible AI use and preserve academic integrity. These insights offer valuable guidance for educators and policymakers seeking to integrate AI ethically and effectively into higher education.</jats:p>",1.0
215,10.7256/2454-0625.2024.6.70926,"Socio-cultural risks of multimodal large generative models of ""AI"" (GenAI)","<jats:p>
 The article is devoted to the study of the conditions for ensuring the information security of Russian citizens when using generative ""AI"" technologies in the socio-cultural sphere. The relevance of the topic is due to the modern high rates of development of computer neural networks that generate multimedia content: texts, images, sounds and videos. The developers classify generative technologies as ""AI"", position them as a ""new nuclear project"" capable of radically increasing the productivity of socio-cultural creativity, and receive significant government, corporate and investment financing. The object of the study is modern multimedia generative models, the subject of the study is the possibility of their use in the socio-cultural sphere of creativity and the associated risks of information security. The purpose of the study is to determine the conditions for ensuring the information security of Russian citizens when using multimodal generative technologies in the socio-cultural sphere.  The research materials are scientific publications of recent years (2021–2024) in Russian journals of the HAC list (categories K1, K2) and international Scopus publications (quartiles Q1, Q2) devoted to research and critical analysis of the possibilities of multimodal generative models, associated risks and security tools. The philosophical methodology is applied: theoretical and cultural analysis, synthesis.  The scientific novelty of the article is due to the application of a philosophical theoretical and cultural methodology for a critical comparison of the declarations of developers and the actual potential of applications of multimodal generative technologies. The result of the study is an assessment of how greatly exaggerated the risks predicted based on the positioning of the technologies in question as ""AI"". The real risks are proposed to include: the incompatibility of development costs with the usefulness of the results; lowering the cultural level of professional and amateur creativity and worsening the tastes of the mass audience; use in ""social engineering"", fraud, mass disinformation, fake news, manipulation of public consciousness, ""cancellation culture"", destruction of traditional values and substitution of socio-cultural identity. The means of ensuring the safety of Russian citizens in the development and use of multimedia generative technologies in the socio-cultural sphere are recommended.
	</jats:p>",2.0
216,10.51191/issn.2637-1898.2024.7.12.12,AI: Duality in Applications of GenAI and Assistive AI in Music,"<jats:p>This paper explores the multifaceted role of AI in the field of music, more specifically, examining the positives and negatives of generative and assistive capacities. AI (AI) in music involves the application of computational techniques to various aspects of music creation, production and consumption. In the domain of assistive AI, the concentration is on how machine learning could potentially help musicians in the area of composition and performance to enhance their musical creativity. The paper will discuss an interesting collaborative effort between pure human creativity and computational assistance covering an explanation for a vast number of tools using generative as well as assistive AI models.

In addition, the paper will address the concerns facing the music industry while this technology keeps on improving, the potential drawbacks and ethical considerations. It opens the question of authenticity and emotional depth, and when or if this new technology could be able to replicate it. Further explanation in the paper will consider music examples with a focus on music styles assisted and generated by the use of AI, from pop to classical music.

With a thorough analysis of the aforementioned subject, the paper aims to provide a detailed perspective on the constant evolution of AI tools used in music with highlights on the need for a balanced approach. In providing a detailed perspective on the evolving landscape of AI tools in music, this study adopts a methodological approach that involves comprehensive analysis of both the benefits and challenges associated with these innovative gadgets. The paper contributes to the ongoing discussion on the intersection of technology and artistic expression. By examining the potential benefits and challenges with these innovative models, the paper signifies ongoing discourse on the impact of technology on artistic expression.</jats:p>",2.0
217,10.1007/s43681-022-00176-2,Review of the state of the art in autonomous AI,"<jats:title>Abstract</jats:title><jats:p>This article presents a new design for autonomous AI (AI), based on the state-of-the-art algorithms, and describes a new autonomous AI system called ‘AutoAI’. The methodology is used to assemble the design founded on self-improved algorithms that use new and emerging sources of data (NEFD). The objective of the article is to conceptualise the design of a novel AutoAI algorithm. The conceptual approach is used to advance into building new and improved algorithms. The article integrates and consolidates the findings from existing literature and advances the AutoAI design into (1) using new and emerging sources of data for teaching and training AI algorithms and (2) enabling AI algorithms to use automated tools for training new and improved algorithms. This approach is going beyond the state-of-the-art in AI algorithms and suggests a design that enables autonomous algorithms to self-optimise and self-adapt, and on a higher level, be capable to self-procreate.</jats:p>",-1.0
218,10.1007/s43681-022-00201-4,Assessing the ethical and social concerns of AI in neuroinformatics research: an empirical test of the European Union Assessment List for Trustworthy AI (ALTAI),"<jats:title>Abstract</jats:title><jats:p>Ethical and social concerns are a key obstacle to the adoption of AI (AI) in the life sciences and beyond. The discussion of these issues has intensified in recent years and led to a number of approaches, tools and initiatives. Key amongst them is the idea of ex-ante impact assessments that aim to identify issues at the early stages of development. One prominent example of such ex-ante impact assessment is the European Union's (EU) Assessment list for Trustworthy AI (ALTAI). This article uses the findings of a large-scale application of the ALTAI to a large neuro-informatics project as an exemplar to demonstrate the effectiveness and limitations of the ALTAI in practice. The article shows that ex-ante impact assessment has the potential to help identify and address ethical and social issues. However, they need to be understood as part of a broader socio-technical ecosystem of AI. For ALTAI and related approaches to be useful in bio-medical research, they should be interpreted from a systems theory perspective which allows for their integration into the rich set of tools, legislation and approaches. The paper argues that ex-ante impact assessments have the best chance of being successful if seen applied in conjunction with other approaches in the context of the overall AI ecosystem.</jats:p>",0.0
219,10.7200/esicm.55.333,transformative potential of GenAI  (GenAI) in business,"<jats:p>Objective:This study investigates the transformative potential of GenAI(GenAI) within the business domain and the entrepreneurial activity.Methodology:A comprehensive research design is adopted, integrating text-mining techniques to analysedata obtained from publicly available innovation repositories. A systematic literaturereview (SLR) is developed based on the literature obtained from all databases indexedin Web of Science (WoS), incorporating preprints from arXiv, alongside industry-relatedinnovation data in the form of patents from Google Patents. This method enables the derivationof valuable insights regarding the impact and prospective developments of GenAIacross diverse business sectors and industries by leveraging Natural Language Processing(NLP) and network analysis.Results:The research outcomes highlight the significant potential of GenAI in enabling informeddecision-making, enhancing productivity, and revealing new growth opportunities inthe business landscape. The continuously evolving business environment is examined,emphasising GenAI's role as a catalyst for data-driven innovation. However, there are stillrelevant limitations to overcome.Limitations:The selection of data sources and the study period may have excluded relevant or recentlypublished articles and patents within the scope of the present research. The language ofthe databases analysed is only English.Practical Implications:The practical implications of this study carry significant weight, serving as a valuableresource for decision-makers, researchers, and practitioners navigating the constantlyshifting terrain of business innovation through the lens of GenAI. Understanding thepotential advantages and challenges associated with GenAI adoption equips stakeholdersto make informed decisions and develop future business strategies.</jats:p>",4.0
220,10.1007/s43681-024-00492-9,"AI, the common good, and the democratic deficit in AI governance","<jats:title>Abstract</jats:title><jats:p>There is a broad consensus that AI should contribute to the common good, but it is not clear what is meant by that. This paper discusses this issue and uses it as a lens for analysing what it calls the “democracy deficit” in current AI governance, which includes a tendency to deny the inherently political character of the issue and to take a technocratic shortcut. It indicates what we may agree on and what is and should be up to (further) deliberation when it comes to AI ethics and AI governance. Inspired by the republican tradition in political theory, it also argues for a more active role of citizens and (end-)users: not only as participants in deliberation but also in ensuring, creatively and communicatively, that AI contributes to the common good.</jats:p>",0.0
221,10.23962/ajic.i33.18162,Risks of GenAI (GenAI)-assisted scams on online sharing-economy platforms,"<jats:p>The prevalence of scams proliferating via online platforms has been identified as an emerging societal problem resulting in large-scale financial losses for victims. Online scams typically rely for their success on the generation of fake but convincing user profiles to conceal the identities of the scammers from the people being tricked into parting with their money. The increasing sophistication of GenAI (GenAI), which can produce outputs indistinguishable from real content, thus carries the risk of being adopted by fraudsters to assist in the enactment of online scams. This article considers the risks of the potential uptake and use of GenAI applications by online scammers operating in the sharing economy, with a focus on homestay-marketplace platforms and, in particular, the largest such platform, Airbnb.</jats:p>",-1.0
222,10.1007/s43681-020-00022-3,Management perspective of ethics in AI,"<jats:title>Abstract</jats:title><jats:p>This research addressed the management awareness about the ethical and moral aspects of AI (AI). It is a general trend to speak about AI, and many start-ups and established companies are communicating about the development and implementation of AI solutions. Therefore, it is important to consider different perspectives besides the technology and data as the key elements for AI systems. The way in which societies are interacting and organising themselves will change. Such transformations require diverse perspectives from the society and particularly from AI system developers for shaping the humanity of the future. This research aimed to overcome this barrier with the answers for the question: What kind of awareness does the management of AI companies have about the social impact of its AI product or service? The central research question was divided into five sub-questions that were answered by a fundamental literature review and an empirical research study. This covered the management understanding of the terms moral, ethics, and AI; the internal company prioritization of moral and ethics; and the involved stakeholders in the AI product or service development. It analysed the known and used ethical AI guidelines and principles. In the end, the social responsibility of the management regarding AI systems was analysed and compared.</jats:p>",0.0
223,10.1007/s43681-023-00330-4,Ethics by design for AI,"<jats:title>Abstract</jats:title><jats:p>In this paper, we present an approach for the systematic and comprehensive inclusion of ethical considerations in the design and development process of AI systems, called Ethics by Design for AI (EbD-AI). The approach is the result of a three-year long research effort, and has recently be adopted by the European Commission as part of its ethics review procedure for AI projects. We describe and explain the approach and its different components and its application to the development of AI software and systems. We also compare it to other approaches in AI ethics, and we consider limitations of the approach as well as potential criticisms.</jats:p>",0.0
224,10.1007/s43681-024-00547-x,AI and its ‘slow violence’ to human rights,"<jats:title>Abstract</jats:title><jats:p>Human rights concerns in relation to the impacts brought forth by AI (‘AI’) have revolved around examining how it affects specific rights, such as the right to privacy, non-discrimination and freedom of expression. However, this article argues that the effects go deeper, potentially challenging the foundational assumptions of key concepts and normative justifications of the human rights framework. To unpack this, the article applies the lens of ‘slow violence’, a term borrowed from environmental justice literature, to frame the grinding, gradual, attritional harms of AI towards the human rights framework.</jats:p><jats:p>The article examines the slow violence of AI towards human rights at three different levels. First, the individual as the subject of interest and protection within the human rights framework, is increasingly unable to understand nor seek accountability for harms arising from the deployment of AI systems. This undermines the key premise of the framework which was meant to empower the individual in addressing large power disparities and calling for accountability towards such abuse of power. Secondly, the ‘slow violence’ of AI is also seen through the unravelling of the normative justifications of discrete rights such as the right to privacy, freedom of expression and freedom of thought, upending the reasons and assumptions in which those rights were formulated and formalised in the first place. Finally, the article examines how even the wide interpretations towards the normative foundation of human rights, namely human dignity, is unable to address putative new challenges AI poses towards the concept. It then considers and offers the outline to critical perspectives that can inform a new model of human rights accountability in the age of AI.</jats:p>",0.0
225,10.1007/s43681-024-00427-4,AI (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI,"<jats:title>Abstract</jats:title><jats:p>As AI (AI) rapidly advances and integrates into various domains, cybersecurity emerges as a critical field grappling with both the benefits and pitfalls of AI technologies. This paper explores the multifaceted dimensions of AI-driven cyberattacks, offering insights into their implications, mitigation strategies, underlying motivations, and profound societal impacts. The research centres on developing and presenting the AI Cybersecurity Dimensions (AICD) Framework, a comprehensive, multidimensional schema designed to guide academics, policymakers, and industry professionals in understanding and combating the evolving challenges posed by AI-driven cyber threats. The research unveils the complex dynamics of offensive AI, stressing the need for adaptive defences and ethical considerations. Concurrently, the study highlights adversarial AI threats, calling for proactive measures to address their potential ramifications. Through rigorous textual analyses and extensive literature reviews, the paper underscores the urgency for interdisciplinary approaches to bridge the technology-humanity chasm traditionally observed in cybersecurity discussions. By synthesising these diverse elements, the AICD Framework emerges as an instrumental tool for holistic understanding and practical interventions in the AI-infused cybersecurity landscape. The paper concludes with an urgent call for collaborative efforts in research and practice to navigate the intricate challenges and capitalise on the opportunities borne from the convergence of AI and cybersecurity.</jats:p>",8.0
226,10.1007/s00146-024-02007-w,Intentionality gap and preter-intentionality in GenAI,"<jats:title>Abstract</jats:title><jats:p>The emergence of GenAI, such as large language models and text-to-image models, has had a profound impact on society. The ability of these systems to simulate human capabilities such as text writing and image creation is radically redefining a wide range of practices, from artistic production to education. While there is no doubt that these innovations are beneficial to our lives, the pervasiveness of these technologies should not be underestimated, and raising increasingly pressing ethical questions that require a radical resemantization of certain notions traditionally ascribed to humans alone. Among these notions, that of technological intentionality plays a central role. With regard to this notion, this paper first aims to highlight what we propose to define in terms of the intentionality gap, whereby, insofar as, currently, (1) it is increasingly difficult to assign responsibility for the actions performed by AI systems to humans, as these systems are increasingly autonomous, and (2) it is increasingly complex to reconstruct the reasoning behind the results they produce as we move away from good old fashioned AI; it is now even more difficult to trace the intentionality of AI systems back to the intentions of the developers and end users. This gap between human and technological intentionality requires a revision of the concept of intentionality; to this end, we propose here to assign preter-intentional behavior to GenAI. We use this term to highlight how AI intentionality both incorporates and transcends human intentionality; i.e., it goes beyond (preter) human intentionality while being linked to it. To show the merits of this notion, we first rule out the possibility that such preter-intentionality is merely an unintended consequence and then explore its nature by comparing it with some paradigmatic notions of technological intentionality present in the wider debate on the moral (and technological) status of AI.</jats:p>",2.0
227,10.1386/adch_00088_1,Making the case for introducing GenAI (AI) into design curricula,"<jats:p>The use of GenAI (AI) in higher education design programmes is expanding, yet there is little formalized approach to its integration. Professionally, GenAI is starting to become an indispensable tool for ideation and prototyping, two fundamental skills taught in design’s studio pedagogy. Yet this digital leap into the future risks leaving design educators behind unless they take a proactive approach to its implementation and present its strengths and weaknesses. This study surveyed 74 design students from an Australian university, exploring their current utilization of GenAI and their projections for its future application in design practice. Findings confirm that GenAI is being used in an ad hoc way by students to speed up the ideation process tempered by a sceptical view of its creative output. A list of GenAI training for integration into the design curricula based on current research and survey results is proposed.</jats:p>",1.0
228,10.1007/s43681-022-00206-z,Needs and AI,"<jats:title>Abstract</jats:title><jats:p>Throughout our history, we, Homo sapiens, have used technologies to better satisfy our<jats:italic>needs</jats:italic>. The relation between<jats:italic>needs</jats:italic>and<jats:italic>technology</jats:italic>is so fundamental that the US National Research Council defines the distinguishing characteristic of technology as its goal “to make modifications in the world [in order] to meet human needs” [1]. AI (AI) is one of the most promising emerging technologies of our time. Similar to other technologies, AI is expected by many “to meet [human] needs”. In this article, we reflect on the relationship between<jats:italic>needs</jats:italic>and AI, and call for the realization of<jats:italic>needs-aware</jats:italic>AI systems. We argue that re-thinking<jats:italic>needs</jats:italic><jats:italic>for</jats:italic>,<jats:italic>through</jats:italic>,<jats:italic>by</jats:italic>, and<jats:italic>with</jats:italic>AI can be a very useful means towards the development of realistic approaches for sustainable<jats:italic>H</jats:italic>uman-aware,<jats:italic>A</jats:italic>ccountable,<jats:italic>L</jats:italic>awful, and<jats:italic>E</jats:italic>thical (HALE) AI systems. We discuss some of the most critical gaps, barriers, enablers, and drivers of co-creating future AI-based sociotechnical systems in which [human]<jats:italic>needs</jats:italic>are well considered and met. Finally, we provide an overview of potential challenges and considerations that should be carefully taken into account; and call for joint, immediate, and interdisciplinary efforts and collaborations to start on the path to<jats:italic>needs-aware</jats:italic>AI.</jats:p>",0.0
229,10.1007/s43681-022-00221-0,"Garbage in, toxic data out: a proposal for ethical AI sustainability impact statements","<jats:title>Abstract</jats:title><jats:p>Data and autonomous systems are taking over our lives, from healthcare to smart homes very few aspects of our day to day are not permeated by them. The technological advances enabled by these technologies are limitless. However, with advantages so too come challenges. As these technologies encompass more and more aspects of our lives, we are forgetting the ethical, legal, safety and moral concerns that arise as an outcome of integrating our lives with technology. In this work, we study the lifecycle of AI from data gathering to deployment, providing a structured analytical assessment of the potential ethical, safety and legal concerns. The paper then presents the foundations for the first ethical AI sustainability statement to guide future development of AI in a safe and sustainable manner.</jats:p>",-1.0
230,10.1007/s42001-024-00250-1,GenAI against humanity: nefarious applications of GenAI and large language models,"<jats:title>Abstract</jats:title><jats:p>GenAI (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we’ll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI’s nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.</jats:p>",7.0
231,10.1007/s43681-024-00604-5,"Society in charge: the connection of AI, responsibility, and ethics in German media discourse","<jats:title>Abstract</jats:title><jats:p>AI (AI) is playing an increasingly important role in society, and applications like GenAI and Dall-E, which can produce texts and pictures on their own, are becoming very popular. This development raises questions regarding ethics, values, and responsibility, as AI-generated documents may promote misinformation and erode democracy, while human actors can scarcely be held accountable. AI technology may also support an efficient, rationalized society, which has its advantages and disadvantages. Two main spheres, which influence society’s perspective on the connection between AI, ethics and responsibility, are public media debates and the legal system. Popular newspapers reach broad audiences, so insight is provided into what perspectives on these issues are helping everyday citizens form their opinions. Legal frameworks potentially regulate citizens’ and companies’ dealing with AI technology—and may get included in media discussions on AI. Acknowledging that, this article presents a two-folded analysis. First, the article presents the results of a discourse analysis of 113 articles from German newspapers, ranging from the center-left to the conservative spectrum. The analysis examined how these media frame the connection of AI, ethics, values, and responsibility. The article discusses the discourse analysis together with theoretical assumptions around the question, which actors in society could be counted as accountable in AI regards. Second, a discussion of the European AI legal system is added, to evaluate its connection with the media discourses. The article presents the results of both parts of the analysis together and finally discusses further research perspectives.</jats:p>",-1.0
232,10.4018/jdm.2020040105,AI (AI) Ethics,"<p>AI (AI)-based technology has achieved many great things, such as facial recognition, medical diagnosis, and self-driving cars. AI promises enormous benefits for economic growth, social development, as well as human well-being and safety improvement. However, the low-level of explainability, data biases, data security, data privacy, and ethical problems of AI-based technology pose significant risks for users, developers, humanity, and societies. As AI advances, one critical issue is how to address the ethical and moral challenges associated with AI. Even though the concept of “machine ethics” was proposed around 2006, AI ethics is still in the infancy stage. AI ethics is the field related to the study of ethical issues in AI. To address AI ethics, one needs to consider the ethics of AI and how to build ethical AI. Ethics of AI studies the ethical principles, rules, guidelines, policies, and regulations that are related to AI. Ethical AI is an AI that performs and behaves ethically. One must recognize and understand the potential ethical and moral issues that may be caused by AI to formulate the necessary ethical principles, rules, guidelines, policies, and regulations for AI (i.e., Ethics of AI). With the appropriate ethics of AI, one can then build AI that exhibits ethical behavior (i.e., Ethical AI). This paper will discuss AI ethics by looking at the ethics of AI and ethical AI. What are the perceived ethical and moral issues with AI? What are the general and common ethical principles, rules, guidelines, policies, and regulations that can resolve or at least attenuate these ethical and moral issues with AI? What are some of the necessary features and characteristics of an ethical AI? How to adhere to the ethics of AI to build ethical AI?</p>",0.0
233,10.1007/s11948-024-00507-y,Reconstructing AI Ethics Principles: Rawlsian Ethics of AI,"<jats:title>Abstract</jats:title><jats:p>The popularisation of AI (AI) technologies has sparked discussion about their ethical implications. This development has forced governmental organisations, NGOs, and private companies to react and draft ethics guidelines for future development of ethical AI systems. Whereas many ethics guidelines address values familiar to ethicists, they seem to lack in ethical justifications. Furthermore, most tend to neglect the impact of AI on democracy, governance, and public deliberation. Existing research suggest, however, that AI can threaten key elements of western democracies that are ethically relevant. In this paper, Rawls’s theory of justice is applied to draft a set of guidelines for organisations and policy-makers to guide AI development towards a more ethical direction. The goal is to contribute to the broadening of the discussion on AI ethics by exploring the possibility of constructing AI ethics guidelines that are philosophically justified and take a broader perspective of societal justice. The paper discusses how Rawls’s theory of justice as fairness and its key concepts relate to the ongoing developments in AI ethics and gives a proposition of how principles that offer a foundation for operationalising AI ethics in practice could look like if aligned with Rawls’s theory of justice as fairness.</jats:p>",0.0
234,10.1007/s43681-022-00195-z,Operationalising ethics in AI for healthcare: a framework for AI developers,"<jats:title>Abstract</jats:title><jats:p>AI (AI) offers much promise for improving healthcare. However, it runs the looming risk of causing individual and societal harms; for instance, exacerbating inequalities amongst minority groups, or enabling compromises in the confidentiality of patients’ sensitive data. As such, there is an expanding, unmet need for ensuring AI for healthcare is developed in concordance with human values and ethics. Augmenting “principle-based” guidance that highlight adherence to ethical ideals (without necessarily offering translation into actionable practices), we offer a solution-based framework for operationalising ethics in AI for healthcare. Our framework is built from a scoping review of existing solutions of ethical AI guidelines, frameworks and technical solutions to address human values such as self-direction in healthcare. Our view spans the entire length of the AI lifecycle: data management, model development, deployment and monitoring. Our focus in this paper is to collate actionable solutions (whether technical or non-technical in nature), which can be steps that enable and empower developers in their daily practice to ensuring ethical practices in the broader picture. Our framework is intended to be adopted by AI developers, with recommendations that are accessible and driven by the existing literature. We endorse the recognised need for ‘ethical AI checklists’ co-designed with health AI practitioners, which could further operationalise the technical solutions we have collated. Since the risks to health and wellbeing are so large, we believe a proactive approach is necessary for ensuring human values and ethics are appropriately respected in AI for healthcare.</jats:p>",6.0
235,10.1007/s43681-024-00476-9,The obscure politics of AI: a Marxian socio-technical critique of the AI alignment problem thesis,"<jats:title>Abstract</jats:title><jats:p>There is a growing feeling that AI (AI) is getting out of control. Many AI experts worldwide stress that great care must be taken on the so-called <jats:italic>alignment problem</jats:italic>, broadly understood as the challenge of developing AIs whose actions are in line with human values and goals. The story goes that ever more powerful AI systems are escaping human control and might soon operate in a manner that is no longer guided by human purposes. This is what we call the <jats:italic>AI-out-of-control discourse</jats:italic> which, in this paper, we critically examine and debunk. Drawing on complementary insights from political theory, socio-technical studies and Marxian political economy, we critique the supposed animistic and autonomous nature of AI, and the myth of the uncontrollability of AI. The problem is not that humanity has lost control over AI, but that only a minority of powerful stakeholders are controlling its creation and diffusion, through politically undemocratic processes of decision-making. In these terms, we reframe the alignment problem thesis with an emphasis on citizen engagement and public political participation. We shed light on the existing politics of AI and contemplate alternative political expressions whereby citizens steer AI development or stop it in the first place.</jats:p>",0.0
236,10.1007/s43681-024-00442-5,AI at sentencing: when do algorithms perform well enough to replace humans?,"<jats:title>Abstract</jats:title><jats:p>AI is currently supplanting the work of humans in many societal contexts. The purpose of this article is to consider the question of when algorithmic tools should be regarded as performing sufficiently well to replace human judgements and decision-making at sentencing. More precisely, the question as to which are the ethically plausible criteria for the comparative performance assessments of algorithms and humans is considered with regard to both risk assessment algorithms that are designed to provide predictions of recidivism and sentencing algorithms designed to determine sentences in individual criminal cases. It is argued, first, that the prima facie most obvious assessment criteria do not stand up to ethical scrutiny. Second, that ethically plausible criteria presuppose ethical theory on penal distribution which currently has not been sufficiently developed. And third, that the current lack of assessment criteria has comprehensive implications regarding when algorithmic tools should be implemented in criminal justice practice.</jats:p>",0.0
237,10.1007/s43681-022-00239-4,"Democracy, epistemic agency, and AI: political epistemology in times of AI","<jats:title>Abstract</jats:title><jats:p>Democratic theories assume that citizens have some form of political knowledge in order to vote for representatives or to directly engage in democratic deliberation and participation. However, apart from widespread attention to the phenomenon of fake news and misinformation, less attention has been paid to <jats:italic>how</jats:italic> they are supposed to acquire that knowledge in contexts shaped by AI and related digital technologies. While this topic can also be approached from an empirical angle, this paper contributes to supporting concerns about AI and democracy by looking at the issue through the lens of political epistemology, in particular using the concept of epistemic agency. It argues that AI (AI) endangers democracy since it risks to diminish the epistemic agency of citizens and thereby undermine the relevant kind of political agency in democracy. It shows that next to fake news and manipulation by means of AI analysis of big data, epistemic bubbles and the defaulting of statistical knowledge endanger the epistemic agency of citizens when they form and wish to revise their political beliefs. AI risks to undermine trust in one’s own epistemic capacities and hinder the exercise of those capacities. If we want to protect the knowledge basis of our democracies, we must address these problems in education and technology policy.</jats:p>",0.0
238,10.1007/s43681-021-00091-y,Coarse ethics: how to ethically assess explainable AI,"<jats:title>Abstract</jats:title><jats:p>The integration of AI (AI) into human society mandates that their decision-making process is explicable to users, as exemplified in Asimov’s Three Laws of Robotics. Such human interpretability calls for explainable AI (XAI), of which this paper cites various models. However, the transaction between computable accuracy and human interpretability can be a trade-off, requiring answers to questions about the negotiable conditions and the degrees of AI prediction accuracy that may be sacrificed to enable user-interpretability. The extant research has focussed on technical issues, but it is also desirable to apply a branch of ethics to deal with the trade-off problem. This scholarly domain is labelled<jats:italic>coarse ethics</jats:italic>in this study, which discusses two issues vis-à-vis AI prediction as a type of evaluation. First, which formal conditions would allow trade-offs? The study posits two minimal requisites: adequately high coverage and order-preservation. The second issue concerns conditions that could justify the trade-off between computable accuracy and human interpretability, to which the study suggests two justification methods: impracticability and adjustment of perspective from machine-computable to human-interpretable. This study contributes by connecting ethics to autonomous systems for future regulation by formally assessing the adequacy of AI rationales.</jats:p>",0.0
239,10.61969/jai.1337500,Education in the Era of GenAI (AI): Understanding the Potential Benefits of GenAI in Promoting Teaching and Learning,"<jats:p xml:lang=""en"">Since its maiden release into the public domain on November 30, 2022, GenAI garnered more than one million subscribers within a week. The GenAI tool ⎼GenAI took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of GenAI to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of GenAI in promoting teaching and learning. Benefits of GenAI include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the GenAI such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how GenAI could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving GenAI tools could be used safely and constructively to improve education and support students’ learning.</jats:p>",1.0
240,10.1097/ncm.0000000000000681,AI and Case Management: From AI to Generative Intelligence,<jats:p>Conversations about AI (AI) are impossible to escape since the inception of GenAI in November 2022—whether it is about the end of our jobs or the end of the world! This Editorial talks about AI and its subsets and how this may relate to health care.</jats:p>,3.0
241,10.1007/s43681-023-00260-1,What would qualify an AI for moral standing?,"<jats:title>Abstract</jats:title><jats:p>What criteria must an AI (AI) satisfy to qualify for moral standing? My starting point is that sentient AIs should qualify for moral standing. But future AIs may have unusual combinations of cognitive capacities, such as a high level of cognitive sophistication without sentience. This raises the question of whether sentience is a necessary criterion for moral standing, or merely sufficient. After reviewing nine criteria that have been proposed in the literature, I suggest that there is a strong case for thinking that some non-sentient AIs, such as those that are conscious and have non-valenced preferences and goals, and those that are non-conscious and have sufficiently cognitively complex preferences and goals, should qualify for moral standing. After responding to some challenges, I tentatively argue that taking into account uncertainty about which criteria an entity must satisfy to qualify for moral standing, and strategic considerations such as how such decisions will affect humans and other sentient entities, further supports granting moral standing to some non-sentient AIs. I highlight three implications: that the issue of AI moral standing may be more important, in terms of scale and urgency, than if either sentience or consciousness is necessary; that researchers working on policies designed to be inclusive of sentient AIs should broaden their scope to include all AIs with morally relevant interests; and even those who think AIs cannot be sentient or conscious should take the issue seriously. However, much uncertainty about these considerations remains, making this an important topic for future research.</jats:p>",0.0
242,10.1007/s43681-023-00387-1,Publics’ views on ethical challenges of AI: a scoping review,"<jats:title>Abstract</jats:title><jats:p>This scoping review examines the research landscape about publics’ views on the ethical challenges of AI. To elucidate how the concerns voiced by the publics are translated within the research domain, this study scrutinizes 64 publications sourced from PubMed<jats:sup>®</jats:sup> and Web of Science™. The central inquiry revolves around discerning the motivations, stakeholders, and ethical quandaries that emerge in research on this topic. The analysis reveals that innovation and legitimation stand out as the primary impetuses for engaging the public in deliberations concerning the ethical dilemmas associated with AI technologies. Supplementary motives are rooted in educational endeavors, democratization initiatives, and inspirational pursuits, whereas politicization emerges as a comparatively infrequent incentive. The study participants predominantly comprise the general public and professional groups, followed by AI system developers, industry and business managers, students, scholars, consumers, and policymakers. The ethical dimensions most commonly explored in the literature encompass human agency and oversight, followed by issues centered on privacy and data governance. Conversely, topics related to diversity, nondiscrimination, fairness, societal and environmental well-being, technical robustness, safety, transparency, and accountability receive comparatively less attention. This paper delineates the concrete operationalization of calls for public involvement in AI governance within the research sphere. It underscores the intricate interplay between ethical concerns, public involvement, and societal structures, including political and economic agendas, which serve to bolster technical proficiency and affirm the legitimacy of AI development in accordance with the institutional norms that underlie responsible research practices.</jats:p>",0.0
243,10.1007/s43681-024-00597-1,Introducing the ethical-epistemic matrix: a principle-based tool for evaluating AI in medicine,"<jats:title>Abstract</jats:title><jats:p>While there has been much discussion of the ethical assessment of AI (AI) in medicine, such work has rarely been combined with the parallel body of scholarship analyzing epistemic implications of AI. This paper proposes a method for joint evaluation of AI’s ethical and epistemic implications in medicine that draws on the principle-oriented tradition in bioethics and the consequent ‘ethical matrix’ approach to assessing novel technologies. It first introduces principle-based approaches as specific tools for ethical assessment of AI in medicine and other domains that are contrasted with the lack of comparable epistemic principles that would govern AI evaluation in medicine. In the next section, the ethical matrix is explained as a well-established principle-based tool in applied ethics that has had some limited applications to near-term implications of AI in medicine and elsewhere that can be strengthened, I suggest, using epistemic principles. To this end, the following section looks to the philosophy of science for relevant epistemic principles, identifying ‘accuracy’, ‘consistency’, ‘relevance’, and ‘instrumental efficacy’ as a provisional set for technology evaluation. The next section articulates the relevance of these epistemic principles to AI in medicine by highlighting conventional standards that have already been applied in AI, epistemology, and the medical sciences. Before concluding, the paper then defines and defends the possibility of an ‘ethical-epistemic matrix’ for the application of these epistemic principles alongside established ethical principles to a selection of stakeholder groups: patients, clinicians, developers, and the public.</jats:p>",6.0
244,10.1007/s43681-024-00493-8,The ethics of using AI in scientific research: new guidance needed for a new tool,"<jats:title>Abstract</jats:title><jats:p>Using AI (AI) in research offers many important benefits for science and society but also creates novel and complex ethical issues. While these ethical issues do not necessitate changing established ethical norms of science, they require the scientific community to develop new guidance for the appropriate use of AI. In this article, we briefly introduce AI and explain how it can be used in research, examine some of the ethical issues raised when using it, and offer nine recommendations for responsible use, including: (1) Researchers are responsible for identifying, describing, reducing, and controlling AI-related biases and random errors; (2) Researchers should disclose, describe, and explain their use of AI in research, including its limitations, in language that can be understood by non-experts; (3) Researchers should engage with impacted communities, populations, and other stakeholders concerning the use of AI in research to obtain their advice and assistance and address their interests and concerns, such as issues related to bias; (4) Researchers who use synthetic data should (a) indicate which parts of the data are synthetic; (b) clearly label the synthetic data; (c) describe how the data were generated; and (d) explain how and why the data were used; (5) AI systems should not be named as authors, inventors, or copyright holders but their contributions to research should be disclosed and described; (6) Education and mentoring in responsible conduct of research should include discussion of ethical use of AI.</jats:p>",-1.0
245,10.1007/s43681-022-00133-z,Responsibility assignment won’t solve the moral issues of AI,"<jats:title>Abstract</jats:title><jats:p>Who is responsible for the events and consequences caused by using artificially intelligent tools, and is there a gap between what human agents can be responsible for and what is being done using AI? Both questions presuppose that the term ‘responsibility’ is a good tool for analysing the moral issues surrounding AI. This article will draw this presupposition into doubt and show how reference to responsibility obscures the complexity of moral situations and moral agency, which can be analysed with a more differentiated toolset of moral terminology. It suggests that the impression of responsibility gaps only occurs if we gloss over the complexity of the moral situation in which artificial intelligent tools are employed and if—counterfactually—we ascribe them some kind of pseudo-agential status.</jats:p>",0.0
246,10.1007/s43681-022-00218-9,"Ethics and diversity in AI policies, strategies and initiatives","<jats:title>Abstract</jats:title><jats:p>A burgeoning of AI (AI) technologies in recent years has led to increased discussion about its potential to address many issues considered otherwise intractable, including those highlighted by the United Nations 2030 Agenda for Sustainable Development and associated Sustainable Development Goals. In tandem with this growth in AI is an expanding body of documentation regarding how such advanced technologies should be governed and managed. Issued by a variety of sources and comprising frameworks, policies and guidelines, this body of work encompasses the legal, social, ethical and policy issues around AI. With at least 470 such documents identified, as of May 2021, in the Council of Europe’s tracker of AI initiatives, questions are emerging around the diversity of views expressed, especially regarding the influence of the Global North or Euro-American perspectives. Our previous analysis of a corpus of largely grey literature discovered blind spots regarding both gender representation and perspectives from the Global South. Expanding on that work, this paper examines a significantly extended corpus, with a focus on the role of underrepresented groups in the wider AI discourse. We find that voices from the Global South and consideration of alternative ethical approaches are largely absent from the conversation. In light of the prominence of social, cultural and ethical perspectives from the Global North, this paper explores implications for the development of standards for ethical AI. Concluding by offering approaches to incorporate more diverse ethical viewpoints and beliefs, we call for increased consideration of power structures when developing AI ethics policies and standards within these alternative socio-cultural and socio-economic contexts.</jats:p>",0.0
247,10.1007/s43681-021-00113-9,The double-edged sword of AI: Ethical Adversarial Attacks to counter AI for crime,"<jats:title>Abstract</jats:title><jats:p>AI (AI) has found a myriad of applications in many domains of technology, and more importantly, in improving people’s lives. Sadly, AI solutions have already been utilized for various violations and theft, even receiving the name AI or Crime (AIC). This poses a challenge: are cybersecurity experts thus justified to attack malicious AI algorithms, methods and systems as well, to stop them? Would that be fair and ethical? Furthermore, AI and machine learning algorithms are prone to be fooled or misled by the so-called adversarial attacks. However, adversarial attacks could be used by cybersecurity experts to stop the criminals using AI, and tamper with their systems. The paper argues that this kind of attacks could be named Ethical Adversarial Attacks (EAA), and if used fairly, within the regulations and legal frameworks, they would prove to be a valuable aid in the fight against cybercrime.</jats:p>",8.0
248,10.1007/s43681-024-00521-7,Ethics and the use of GenAI in professional editing,"<jats:title>Abstract</jats:title><jats:p>GenAI (GnAI) has garnered significant attention worldwide across diverse industries, including in book publishing. To date, more attention has been paid to its potential in creative collaboration and less to the editorial possibilities of its application. Interest has accelerated since the breakthrough of a new Large Language Model in late 2022. This paper engages with the ethical and industrial implications of using GnAI in a creative context, namely literary publishing. It raises crucial questions about intellectual property, trust, the author–editor relationship and publishing professionals’ evolving roles in shaping quality literature. Using a published story as a test case, we compare edits using GnAI with those by professional editors over multiple drafts and at different stages of editorial development. We consider the potential ethical implications of the use of GnAI in literary fiction editing, highlighting the principles and practices that underpin professional editing to consider how these may or may not translate in the use of GnAI. This is followed by a discussion of the risks and opportunities in using GnAI in editing literary texts in the trade publishing context.</jats:p>",-1.0
249,10.1007/s43681-021-00093-w,Foundations for the future: institution building for the purpose of AI governance,"<jats:title>Abstract</jats:title><jats:p>Governance efforts for AI (AI) are taking on increasingly more concrete forms, drawing on a variety of approaches and instruments from hard regulation to standardisation efforts, aimed at mitigating challenges from high-risk AI systems. To implement these and other efforts, new institutions will need to be established on a national and international level. This paper sketches a blueprint of such institutions, and conducts in-depth investigations of three key components of any future AI governance institutions, exploring benefits and associated drawbacks: (1) “purpose”, relating to the institution’s overall goals and scope of work or mandate; (2) “geography”, relating to questions of participation and the reach of jurisdiction; and (3) “capacity”, the infrastructural and human make-up of the institution. Subsequently, the paper highlights noteworthy aspects of various institutional roles specifically around questions of institutional purpose, and frames what these could look like in practice, by placing these debates in a European context and proposing different iterations of a European AI Agency. Finally, conclusions and future research directions are proposed.</jats:p>",0.0
250,10.1007/s43681-022-00231-y,"A new control problem? Humanoid robots, AI, and the value of control","<jats:title>Abstract</jats:title><jats:p>The control problem related to robots and AI usually discussed is that we might lose control over advanced technologies. When authors like Nick Bostrom and Stuart Russell discuss this control problem, they write in a way that suggests that having as much control as possible is good while losing control is bad. In life in general, however, not all forms of control are unambiguously positive and unproblematic. Some forms—e.g. control over other persons—are ethically problematic. Other forms of control are positive, and perhaps even intrinsically good. For example, one form of control that many philosophers have argued is intrinsically good and a virtue is self-control. In this paper, I relate these questions about control and its value to different forms of robots and AI more generally. I argue that the more robots are made to resemble human beings, the more problematic it becomes—at least symbolically speaking—to want to exercise full control over these robots. After all, it is unethical for one human being to want to fully control another human being. Accordingly, it might be seen as problematic—viz. as representing something intrinsically bad—to want to create humanoid robots that we exercise complete control over. In contrast, if there are forms of AI such that control over them can be seen as a form of self-control, then this might be seen as a virtuous form of control. The “new control problem”, as I call it, is the question of under what circumstances retaining and exercising complete control over robots and AI is unambiguously ethically good.</jats:p>",0.0
251,10.1007/s43681-022-00139-7,Ethical issues deriving from the delayed adoption of AI in medical imaging,"<jats:title>Abstract</jats:title><jats:p>Medical imaging (MI) has assumed a central role in medicine. AI (AI) has revolutionized computer vision and it is also approaching to impact deeply MI. Fundamental ethical matters have raised and teams of experts around the world are involved in defining ethical borders for AI in MI. However, reading the extremely detailed proposals, it is clear that the treated ethical arguments have been completely redefined and specifically structured for AI in MI. Instead, many of them should be inherited from other technologies already in use in MI. The complete re-definition of ethical principles could produce contradictions and delays for AI adoption in MI, thus arising important ethical concerns. In this paper, potential ethical issues related to AI delay are presented: the objective is to contribute to reuse some concepts from other technologies to streamline the arguments and avoid these concerns.</jats:p>",6.0
252,10.1007/s10551-024-05741-9,Generative Artificial Intelligence as Hypercommons: Ethics of Authorship and Ownership,"<jats:title>Abstract</jats:title><jats:p>In this editorial essay, we argue that GenAI programs (GenAI) draw on what we term a “hypercommons”, involving collectively produced inputs and labour that are largely invisible or untraceable. We argue that automatizing the exploitation of common inputs, in ways that remix and reconfigure them, can lead to a crisis of academic authorship in which the moral agency involved in scholarly production is increasingly eroded. We discuss the relationship between the hypercommons and authorship in terms of moral agency and the ethics of academic production, speculating on different responses to the crisis of authorship as posed by GenAI.</jats:p>",2.0
253,10.1007/s10462-024-10916-x,"Explainable GenAI (GenXAI): a survey, conceptualization, and research agenda","<jats:title>Abstract</jats:title><jats:p>GenAI (GenAI) represents a shift from AI’s ability to “recognize” to its ability to “generate” solutions for a wide range of tasks. As generated solutions and applications grow more complex and multi-faceted, new needs, objectives, and possibilities for explainability (XAI) have emerged. This work elaborates on why XAI has gained importance with the rise of GenAI and the challenges it poses for explainability research. We also highlight new and emerging criteria that explanations should meet, such as verifiability, interactivity, security, and cost considerations. To achieve this, we focus on surveying existing literature. Additionally, we provide a taxonomy of relevant dimensions to better characterize existing XAI mechanisms and methods for GenAI. We explore various approaches to ensure XAI, ranging from training data to prompting. Our paper provides a concise technical background of GenAI for non-technical readers, focusing on text and images to help them understand new or adapted XAI techniques for GenAI. However, due to the extensive body of work on GenAI, we chose not to delve into detailed aspects of XAI related to the evaluation and usage of explanations. Consequently, the manuscript appeals to both technical experts and professionals from other fields, such as social scientists and information systems researchers. Our research roadmap outlines over ten directions for future investigation.</jats:p>",-1.0
254,10.1007/s43681-023-00321-5,Measuring responsible AI (RAI) in banking: a valid and reliable instrument,"<jats:title>Abstract</jats:title><jats:p>Widespread use of AI (AI) and machine learning (ML) in the US banking industry raises red flags with regulators and social groups due to potential risk of data-driven algorithmic bias in credit lending decisions. The absence of a valid and reliable measure of responsible AI (RAI) has stunted the growth of organizational research on RAI (i.e., the organizational balancing act to optimize efficiency and equity). To address this void, we develop a novel measurement instrument to assess RAI maturity in firms. A review of the nascent literature reveals that there is a wide distribution of RAI capabilities. The RAI instrument that we advance is based on the exhaustive review of this dispersed literature. Analyses of data from large US banks show strong evidence of validity and reliability of the RAI maturity instrument.</jats:p>",-1.0
255,10.1007/s43681-022-00253-6,Engineering a social contract: Rawlsian distributive justice through algorithmic game theory and AI,"<jats:title>Abstract</jats:title><jats:p>The potential for AI algorithms and game theory concepts to offer prescriptive and decision-making capability for humankind is increasingly recognized. This derives from the increasing availability of granular, multivariable, well-curated data offering analytical insights for necessarily complex human behaviors and activities. Of the multitude of situations that this decision-making aptitude presents, the application to governmental policy offers a commanding case. This would allow decisions to be made for the benefit of societies and citizens based on rigorous objective information devoid of the traditional approach of choosing policies and societal values based on the opinion of a handful of selected representatives who may be exposed to a lack of comprehensive data analysis capacity and subject to personal biases. There would need to be a critical requirement of wider socially responsible data practices here, beyond those of technical considerations and the incorporation of wider societal fairness approaches. Amongst the schools of political thought particularly acquiescent to the application by this approach would be the egalitarian approach of John Rawls. Here an Original Position’s pre-determination tool of Veil of Ignorance and ensuing Difference Principal presents a method of distributive justice that can be clearly mathematically defined in economics theory through Wald’s Maximin principle. This offers an opportunity to apply algorithmic game theory and AI computational approaches to implement Rawlsian distributive justice that are presented and discussed. The outputs from the algorithmic acquaintance of Rawlsian egalitarianism with applicable state data, protected with appropriate privacy, security, legal, ethical and social governance could in turn lead to automated direct governmental choices and an objective Social Contract for citizens of digitally literate nations.</jats:p>",0.0
256,10.1007/s43681-024-00548-w,On singularity and the Stoics: why Stoicism offers a valuable approach to navigating the risks of AI (AI),"<jats:title>Abstract</jats:title><jats:p>The potential benefits and risks of AI technologies have sparked a wide-ranging debate in both academic and public circles. On one hand, there is an urgent call to address the immediate and avoidable challenges associated with these tools, such as accountability, privacy, bias, understandability, and transparency; on the other hand, prominent figures like Geoffrey Hinton and Elon Musk have voiced concerns over the potential rise of Super AI, whose singularity could pose an existential threat to humanity. Coordinating the efforts of thousands of decentralized entities to prevent such a hypothetical event may seem insurmountable in our intricate and multipolar world. Thus, drawing from both perspectives, this work suggests employing the tools and framework of Stoic philosophy, particularly the concept of the dichotomy of control—focusing on what is within our power. This Stoic principle offers a practical and epistemological approach to managing the complexities of AI, and it encourages individuals to organize their efforts around what they can influence while adapting to the constraints of external factors. Within this framework, the essay found that Stoic wisdom is essential for assessing risks, courage is necessary to face contemporary challenges, and temperance and tranquility are indispensable; and these lessons can inform ongoing public and academic discourse, aiding in the development of more effective policy proposals for aligning Narrow AI and General AI with human values.</jats:p>",0.0
257,10.1007/s43681-024-00519-1,A powerful potion for a potent problem: transformative justice for GenAI in healthcare,"<jats:title>Abstract</jats:title><jats:p>GenAI (AI), as a transformative technology, holds significant promise for applications in healthcare. At the same time, the datafication, AI integration, and commodification of health have opened the floodgates for ethical issues, including those related to fairness, access, beneficence, democracy, solidarity, inclusion, and societal harms. As further the digitalization, innovation, and disruption of healthcare is inevitable, the paper maps out how power, equity, access, identity, participation, and knowledge contribute to creating social injustice issues. It also discusses that current justice approaches—distributive justice, representational justice, restorative justice, and capabilities-centered justice—do not have enough impact to prevent or remedy the many harms and injustices that AI has already created in healthcare or will continue to do so. The paper proposes that a transformative justice approach is needed for GenAI as a transformative technology, focused on (1) peace, emancipation, and eliminating the root causes of injustice, (2) holistic conflict resolution, (3) human rights-based approaches, and (4) the empowerment of agency and actors.</jats:p>",6.0
258,10.1007/s10892-023-09456-3,What Makes Work “Good” in the Age of AI (AI)? Islamic Perspectives on AI-Mediated Work Ethics,"<jats:title>Abstract</jats:title><jats:p>AI (AI) technologies are increasingly creeping into the work sphere, thereby gradually questioning and/or disturbing the long-established moral concepts and norms communities have been using to define what makes work good. Each community, and Muslims make no exception in this regard, has to revisit their moral world to provide well-thought frameworks that can engage with the challenging ethical questions raised by the new phenomenon of AI-mediated work. For a systematic analysis of the broad topic of AI-mediated work ethics from an Islamic perspective, this article focuses on presenting an accessible overview of the “moral world” of work in the Islamic tradition. Three main components of this moral world were selected due to their relevance to the AI context, namely (1) Work is inherently good for humans, (2) Practising a religiously permitted profession and (c) Maintaining good relations with involved stakeholders. Each of these three components is addressed in a distinct section, followed by a sub-section highlighting the relevance of the respective component to the particular context of AI-mediated work. The article argues that there are no unsurmountable barriers in the Islamic tradition against the adoption of AI technologies in work sphere. However, important precautions should be considered to ensure that embracing AI will not be at the cost of work-related moral values. The article also highlights how important lessons can be learnt from the positive historical experience of automata that thrived in the Islamic civilization.</jats:p>",0.0
259,10.1007/s43681-024-00529-z,Securing tomorrow: a comprehensive survey on the synergy of AI and information security,"<jats:title>Abstract</jats:title><jats:p>This survey paper explores the transformative role of AI (AI) in information security. Traditional methods, especially rule-based approaches, faced significant challenges in protecting sensitive data from ever-changing cyber threats, particularly with the rapid increase in data volume. This study thoroughly evaluates AI’s application in information security, discussing its strengths and weaknesses. It provides a detailed review of AI’s impact on information security, examining various AI algorithms used in this field, such as supervised, unsupervised, and reinforcement learning, and highlighting their respective strengths and limitations. The study identifies key areas for future AI research in information security, focusing on improving algorithms, strengthening information security, addressing ethical issues, and exploring safety and security-related concerns. It emphasizes significant security risks, including vulnerability to adversarial attacks, and aims to enhance the robustness and reliability of AI systems in protecting sensitive information by proposing solutions for potential threats. The findings aim to benefit cybersecurity professionals and researchers by offering insights into the intricate relationship between AI, information security, and emerging technologies.</jats:p>",8.0
260,10.1007/s43681-023-00378-2,How AI adopts human biases: the case of cosmetic skincare industry,"<jats:title>Abstract</jats:title><jats:p>The cosmetic skincare industry is a growing market that extends to different regions and customer groups. In addition to scientific advances and technological developments, state-of-the-art digital approaches, including machine learning and other AI (AI)-based techniques, are being applied at different stages of the value chain. The objectives of these efforts include optimizing the supply chain, developing high-quality, effective and safe products and personalization at every step of the customer journey. However, the use of digital technologies comes with risks and undesirable effects. These include a lack of transparency and accountability, compromised fairness and a general deficiency in data governance, all of which are critical at every customer touchpoint. This dark side of digital transformation is recognized by both businesses and governments. In this paper, we explain the concept of bias leading to unfairness for beauty technology applications. Based on published data we identified potential sources of AI bias in the cosmetic skincare industry and/or beauty tech. They were classified by the stage of the AI lifecycle: biases related to target setting, to acquisition and annotation, to modeling, to validation and evaluation, and to deployment and monitoring. We aim to create awareness of such phenomena among readers, whether executives, managers, developers or potential end-users.</jats:p>",0.0
261,10.1007/s43681-023-00279-4,Personality and demographic correlates of support for regulating AI,"<jats:title>Abstract</jats:title><jats:p>The arrival of AI (AI) in our society has sparked many hopes and fears, with people having diverging views on the need to strictly regulate AI. The current study investigates how demographic and personality traits are associated with a desire to strictly regulate AI using a representative sample of adults from New Zealand (<jats:italic>N</jats:italic> = 47,951 participants). Data revealed that support for strict regulation of AI is positively related with agreeableness, neuroticism, and honesty–humility. However, it is negatively related to openness to experiences. A wide range of demographic factors including gender, age, ethnicity, religiosity, neighbourhood level economic deprivation, living rural, relationship status, and parental status were additionally related to support for regulation of AI. However, all these effects were fairly small suggesting that both personality and socio-demographic factors contribute to support for regulating AI, but other factors beyond these characteristics should also be considered for understanding people’s support for regulating AI.</jats:p>",0.0
262,10.1007/s00146-023-01686-1,Ethics of using AI (AI) in veterinary medicine,"<jats:title>Abstract</jats:title><jats:p>This paper provides the first comprehensive analysis of ethical issues raised by AI (AI) in veterinary medicine for companion animals. Veterinary medicine is a socially valued service, which, like human medicine, will likely be significantly affected by AI. Veterinary AI raises some unique ethical issues because of the nature of the client–patient–practitioner relationship, society’s relatively minimal valuation and protection of nonhuman animals and differences in opinion about responsibilities to animal patients and human clients. The paper examines how these distinctive features influence the ethics of AI systems that might benefit clients, veterinarians and animal patients—but also harm them. It offers practical ethical guidance that should interest ethicists, veterinarians, clinic owners, veterinary bodies and regulators, clients, technology developers and AI researchers.</jats:p>",6.0
263,10.1093/pnasnexus/pgae052,"GenAI, human creativity, and art","<jats:title>Abstract</jats:title>
               <jats:p>Recent AI (AI) tools have demonstrated the ability to produce outputs traditionally considered creative. One such system is text-to-image GenAI (e.g. Midjourney, Stable Diffusion, DALL-E), which automates humans’ artistic execution to generate digital artworks. Utilizing a dataset of over 4 million artworks from more than 50,000 unique users, our research shows that over time, text-to-image AI significantly enhances human creative productivity by 25% and increases the value as measured by the likelihood of receiving a favorite per view by 50%. While peak artwork Content Novelty, defined as focal subject matter and relations, increases over time, average Content Novelty declines, suggesting an expanding but inefficient idea space. Additionally, there is a consistent reduction in both peak and average Visual Novelty, captured by pixel-level stylistic elements. Importantly, AI-assisted artists who can successfully explore more novel ideas, regardless of their prior originality, may produce artworks that their peers evaluate more favorably. Lastly, AI adoption decreased value capture (favorites earned) concentration among adopters. The results suggest that ideation and filtering are likely necessary skills in the text-to-image process, thus giving rise to “generative synesthesia”—the harmonious blending of human exploration and AI exploitation to discover new creative workflows.</jats:p>",2.0
264,10.1007/s11948-020-00228-y,"In AI We Trust: Ethics, AI, and Reliability","<jats:title>Abstract</jats:title><jats:p>One of the main difficulties in assessing AI (AI) is the tendency for people to anthropomorphise it. This becomes particularly problematic when we attach human moral activities to AI. For example, the European Commission’s High-level Expert Group on AI (HLEG) have adopted the position that we should establish a relationship of trust with AI and should cultivate trustworthy AI (HLEG AI Ethics guidelines for trustworthy AI, 2019, p. 35). Trust is one of the most important and defining activities in human relationships, so proposing that AI should be trusted, is a very serious claim. This paper will show that AI cannot be something that has the capacity to be trusted according to the most prevalent definitions of trust because it does not possess emotive states or can be held responsible for their actions—requirements of the affective and normative accounts of trust. While AI meets all of the requirements of the rational account of trust, it will be shown that this is not actually a type of trust at all, but is instead, a form of reliance. Ultimately, even complex machines such as AI should not be viewed as trustworthy as this undermines the value of interpersonal trust, anthropomorphises AI, and diverts responsibility from those developing and using them. </jats:p>",0.0
265,10.1007/s43681-023-00268-7,"Symbiosis, not alignment, as the goal for liberal democracies in the transition to artificial general intelligence","<jats:title>Abstract</jats:title><jats:p>A transition to a world with artificial general intelligence (AGI) may occur within the next few decades. This transition may give rise to catastrophic risks from <jats:italic>misaligned</jats:italic> AGI, which have received a significant amount of attention, deservedly. Here I argue that AGI systems that are <jats:italic>intent-aligned</jats:italic>—they always try to do what their operators want them to do—would also create catastrophic risks, mainly due to the power that they concentrate on their operators. With time, that power would almost certainly be catastrophically exploited, potentially resulting in human extinction or permanent dystopia. I suggest that liberal democracies, if they decide to allow the development of AGI, may react to this threat by letting AGI take shape as an <jats:italic>intergenerational social project</jats:italic>, resulting in an arrangement where AGI is not intent-aligned but <jats:italic>symbiotic</jats:italic> with humans. I provide some tentative ideas on what the resulting arrangement may look like and consider what speaks for and what against aiming for intent-aligned AGI as an intermediate step.</jats:p>",0.0
266,10.1007/s43681-021-00074-z,AI in Education (AIEd): a high-level academic and industry note 2021,"<jats:title>Abstract</jats:title><jats:p>In the past few decades, technology has completely transformed the world around us. Indeed, experts believe that the next big digital transformation in how we live, communicate, work, trade and learn will be driven by AI (AI) [83]. This paper presents a high-level industrial and academic overview of AI in Education (AIEd). It presents the focus of latest research in AIEd on reducing teachers’ workload, contextualized learning for students, revolutionizing assessments and developments in intelligent tutoring systems. It also discusses the ethical dimension of AIEd and the potential impact of the Covid-19 pandemic on the future of AIEd’s research and practice. The intended readership of this article is policy makers and institutional leaders who are looking for an introductory state of play in AIEd.</jats:p>",1.0
267,10.3390/ai5030068,Harnessing GenAI for Digital Literacy Innovation: A Comparative Study between Early Childhood Education and Computer Science Undergraduates,"<jats:p>The recent surge of GenAI (AI) in higher education presents a fascinating landscape of opportunities and challenges. AI has the potential to personalize education and create more engaging learning experiences. However, the effectiveness of AI interventions relies on well-considered implementation strategies. The impact of AI platforms in education is largely determined by the particular learning environment and the distinct needs of each student. Consequently, investigating the attitudes of future educators towards this technology is becoming a critical area of research. This study explores the impact of GenAI platforms on students’ learning performance, experience, and satisfaction within higher education. It specifically focuses on students’ experiences with varying levels of technological proficiency. A comparative study was conducted with two groups from different academic contexts undergoing the same experimental condition to design, develop, and implement instructional design projects using various AI platforms to produce multimedia content tailored to their respective subjects. Undergraduates from two disciplines—Early Childhood Education (n = 32) and Computer Science (n = 34)—participated in this study, which examined the integration of GenAI platforms into educational content implementation. Results indicate that both groups demonstrated similar learning performance in designing, developing, and implementing instructional design projects. Regarding user experience, the general outcomes were similar across both groups; however, Early Childhood Education students rated the usefulness of AI multimedia platforms significantly higher. Conversely, Computer Science students reported a slightly higher comfort level with these tools. In terms of overall satisfaction, Early Childhood Education students expressed greater satisfaction with AI software than their counterparts, acknowledging its importance for their future careers. This study contributes to the understanding of how AI platforms affect students from diverse backgrounds, bridging a gap in the knowledge of user experience and learning outcomes. Furthermore, by exploring best practices for integrating AI into educational contexts, it provides valuable insights for educators and scholars seeking to optimize the potential of AI to enhance educational outcomes.</jats:p>",1.0
268,10.1007/s43681-023-00408-z,Assessing deep learning: a work program for the humanities in the age of AI,"<jats:title>Abstract</jats:title><jats:p>Following the success of deep learning (DL) in research, we are now witnessing the fast and widespread adoption of AI (AI) in daily life, influencing the way we act, think, and organize our lives. However, much still remains a mystery when it comes to how these systems achieve such high performance and why they reach the outputs they do. This presents us with an unusual combination: of technical mastery on the one hand, and a striking degree of mystery on the other. This conjunction is not only fascinating, but it also poses considerable risks, which urgently require our attention. Awareness of the need to analyze ethical implications, such as fairness, equality, and sustainability, is growing. However, other dimensions of inquiry receive less attention, including the subtle but pervasive ways in which our dealings with AI shape our way of living and thinking, transforming our culture and human self-understanding. If we want to deploy AI positively in the long term, a broader and more holistic assessment of the technology is vital, involving not only scientific and technical perspectives, but also those from the humanities. To this end, we present outlines of a<jats:italic>work program</jats:italic>for the humanities that aim to contribute to assessing and guiding the potential, opportunities, and risks of further developing and deploying DL systems. This paper contains a thematic introduction (Sect. 1), an introduction to the workings of DL for non-technical readers (Sect. 2), and a main part, containing the outlines of a work program for the humanities (Sect. 3). Readers familiar with DL might want to ignore 2 and instead directly read 3 after 1.</jats:p>",0.0
269,10.1007/s00146-024-01922-2,An elemental ethics for AI: water as resistance within AI’s value chain,"<jats:title>Abstract</jats:title><jats:p>Research and activism have increasingly denounced the problematic environmental record of the infrastructure and value chain underpinning AI (AI). Water-intensive data centres, polluting mineral extraction and e-waste dumping are incontrovertibly part of AI’s footprint. In this article, I turn to areas affected by AI-fuelled environmental harm and identify an ethics of resistance emerging from local activists, which I term ‘elemental ethics’. Elemental ethics interrogates the AI value chain’s problematic relationship with the elements that make up the world, critiques the undermining of local and ancestral approaches to nature and reveals the vital and quotidian harms engendered by so-called intelligent systems. While this ethics is emerging from grassroots and Indigenous groups, it echoes recent calls from environmental philosophy to reconnect with the environment via the elements. In empirical terms, this article looks at groups in Chile resisting a Google data centre project in Santiago and lithium extraction (used for rechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert. As I show, elemental ethics can complement top-down, utilitarian and quantitative approaches to AI ethics and sustainable AI as well as interrogate whose lived experience and well-being counts in debates on AI extinction.</jats:p>",0.0
270,10.1177/17470161241262149,Dual use concerns in AI and the neurosciences: How medical research can end up in war,"<jats:p> Dual Use Research of Concern (DURC) has been well analyzed regarding the life sciences. This article explores the topic of younger fields of medical research and their potential for misuse, especially in the military context. The areas of research considered are AI, neurotechnology, and neuroenhancement. Each of these areas have brought forward highly promising new research. However, in light of the current armed conflicts in Europe and in the Middle East, there is a need to consider what the potential harmful consequences of medical research are. Using the example of war, this article demonstrates various instances of how current medical research could be—or is being—misused and discusses various possible solutions to the dual use dilemma. The main finding is that there needs to be a more concise and international effort to prevent the misuse of research. The raising of awareness in the general medical research community for the topic of DURC is one of the simplest steps that should be undertaken in order to ensure the non-maleficence of global research. Additionally, considering the potentially far-reaching consequences of DURC, it is time to consider the introduction of a new intergovernmental agency to monitor research and establish safeguards in order to cover all fields of research. </jats:p>",6.0
271,10.1007/s43681-023-00273-w,"AI, superefficiency and the end of work: a humanistic perspective on meaning in life","<jats:title>Abstract</jats:title><jats:p>How would it be assessed from an ethical point of view if human wage work were replaced by artificially intelligent systems (AI) in the course of an automation process? An answer to this question has been discussed above all under the aspects of individual well-being and social justice. Although these perspectives are important, in this article, we approach the question from a different perspective: that of leading a meaningful life, as understood in analytical ethics on the basis of the so-called meaning-in-life debate. Our thesis here is that a life without wage work loses specific sources of meaning, but can still be sufficiently meaningful in certain other ways. Our starting point is John Danaher’s claim that ubiquitous automation inevitably leads to an achievement gap. Although we share this diagnosis, we reject his provocative solution according to which game-like virtual realities could be an adequate substitute source of meaning. Subsequently, we outline our own systematic alternative which we regard as a decidedly humanistic perspective. It focuses both on different kinds of social work and on rather passive forms of being related to meaningful contents. Finally, we go into the limits and unresolved points of our argumentation as part of an outlook, but we also try to defend its fundamental persuasiveness against a potential objection.</jats:p>",0.0
272,10.1215/2834703x-11205175,The Ethics of (Generative) AI,"<jats:title>Abstract</jats:title>
               <jats:p>The clamor for AI-based applications involving generative models for text and images has fueled wild speculation about the risks and opportunities for society and humanity at large. The potential “existential” threat as a precursor to artificial general intelligence has provoked wide-ranging debates in the public, politics, and the corporate world involving technologists and ethicists from a range of academic disciplines. This thinkpiece proposes a metaperspective to reflect critically and constructively upon the current state of the field of AI ethics, arguing that scholars working in the domain of ethics should focalize conceptual, substantive, and procedural issues as integral elements of an ethical assessment of given technologies and their applications. It suggests that the ethics of GenAI is conceptually still underexplored and overly propagating technological fixes to problems of all kinds (technosolutionism). Procedurally, it needs to be clarified who can, who ought to, and who ultimately will be considered and heard as an expert on AI ethics, a question of relevance for the trust in, and reliance on, AI.</jats:p>",0.0
273,10.1007/s43681-021-00089-6,Moral exemplars for the virtuous machine: the clinician’s role in ethical AI for healthcare,"<jats:title>Abstract</jats:title><jats:p>AI (AI) continues to pervade several aspects of healthcare with pace and scale. The need for an ethical framework in AI to address this has long been recognized, but to date most efforts have delivered only high-level principles and value statements. Herein, we explain the need for an ethical framework in healthcare AI, the different moral theories that may serve as its basis, the rationale for why we believe this should be built around virtue ethics, and explore this in the context of five key ethical concerns for the introduction of AI in healthcare. Some existing work has suggested that AI may replace clinicians. We argue to the contrary, that the clinician will not be replaced, nor their role attenuated. Rather, they will be integral to the responsible design, deployment, and regulation of AI in healthcare, acting as the moral exemplar for the virtuous machine. We collate relevant points from the literature and formulate our own to present a coherent argument for the central role of clinicians in ethical AI and propose ideas to help advance efforts to employ ML-based solutions within healthcare. Finally, we highlight the responsibility of not only clinicians, but also data scientists, tech companies, ethicists, and regulators to act virtuously in realising the vision of ethical and accountable AI in healthcare.</jats:p>",6.0
274,10.1007/s11948-024-00476-2,AI and Agency: Tie-breaking in AI Decision-Making,"<jats:title>Abstract</jats:title><jats:p>Determining the agency-status of machines and AI has never been more pressing. As we progress into a future where humans and machines more closely co-exist, understanding hallmark features of agency affords us the ability to develop policy and narratives which cater to both humans and machines. This paper maintains that decision-making processes largely underpin agential action, and that in most instances, these processes yield good results in terms of making good choices. However, in some instances, when faced with two (or more) choices, an agent may find themselves with equal reasons to choose either - thus being presented with a tie. This paper argues that in the event of a tie, the ability to create a voluntarist reason is a hallmark feature of agency, and second, that AI, through current tie-breaking mechanisms does not have this ability, and thus fails at this particular feature of agency.</jats:p>",0.0
275,10.59400/cai.v2i1.1378,From bard to Gemini: An investigative exploration journey through Google’s evolution in conversational AI and GenAI,"<jats:p>The advent of AI (AI) has significantly transformed various aspects of human life, particularly in information retrieval and assistance. This research presents a comprehensive evaluation of Gemini, previously known as Google Bard, a state-of-the-art AI chatbot developed by Google. Through a meticulous methodology encompassing both qualitative and quantitative approaches, this research aims to assess Gemini’s performance, usability, integration capabilities, ethical implications. Primary data collection methods, including user surveys and interviews, were utilized to gather towards the qualitative feedback on user experiences with Gemini, supplemented by secondary data analysis using tools such as Google Analytics to capture quantitative metrics. Performance evaluation involved benchmarking against other AI chatbots and technical analysis of Gemini’s architecture and training methods. User experience testing examined usability, engagement, and integration with Google Workspace and third-party services. Ethical considerations regarding data privacy, security, and biases in AI-generated content were also addressed, ensuring compliance with major regulations and promoting ethical AI practices. Acknowledging limitations and challenges inherent in the investigative exploration, data analysis was conducted using thematic and statistical methods to derive insights. The results and findings of this research offer valuable insights into the capabilities and limitations of Gemini, providing implications for future AI development, user interaction design, and ethical AI governance. By contributing to the ongoing discourse on AI advancements and their societal impact, this exploration facilitates informed decision-making and lays the groundwork for future research endeavors in the field of AI-driven conversational agents.</jats:p>",-1.0
276,10.31992/0869-3617-2024-33-2-31-53,Ethics and AI-Plagiarism in an Academic Environment: Students’ Understanding of Compliance with Author’s Ethics and the Problem of Plagiarism in the Process of Interaction with GenAI,"<jats:p>Everyday, AI (AI) is being increasingly integrated into the teaching and learning process at Russian universities. The high level of quality of feedback from AI tools leads to the spread of AI plagiarism – unauthorized borrowing of GenAI materials – among students. The purpose of this study is to: a) highlight aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of plagiarism when interacting with GenAI; b) develop a questionnaire to determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism; c) conduct an online survey of university students, analyze and discuss the results obtained. The paper highlights five aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism when completing educational assignments and preparing research texts: a) students’ general understanding of the issues of compliance with author’s ethics and the problem of plagiarism in an academic environment; b) students’ experience of AI tools for educational purposes; c) students’ understanding of the problem of AI plagiarism and attitude towards borrowing materials from GenAI; d) teachers’ actions to prevent AI plagiarism among students; e) the policy of educational organizations regarding student compliance with ethics and AI plagiarism. An online questionnaire was developed to determine the degree to which students understand the issues of compliance with copyright ethics and the problem of AI plagiarism. 1,599 students from 29 universities of the Russian Federation took part in the survey. The results showed that in general, in the Russian student community, plagiarism is a widespread social phenomenon, many types of which are perceived by young people as a norm of academic behavior. Despite the relatively high awareness of students in the field of AI technologies, the extremely rare use by teachers of specialized subject disciplines of AI tools in the educational process I’d the reason for the current low level of spread of AI plagiarism in the academic environment. At the same time, it is necessary to state that students lack a systematic understanding of exactly how they can “legally” use GenAI materials and what exactly will be considered AI plagiarism. According to students, the importance of understanding the issues of compliance with author ethics and the problem of AI plagiarism will depend, on the one hand, on the actions of teachers to explain to students the rules for using GenAI materials, and on the other hand, the presence in universities of a regulatory framework regulating the field and the extent to which students use AI in the educational process.</jats:p>",1.0
277,10.3390/app13116716,"Exploring the Potential Impact of AI (AI) on International Students in Higher Education: GenAI, Chatbots, Analytics, and International Student Success","<jats:p>International students face unique challenges in pursuing higher education in a foreign country. To address these challenges and enhance their academic experience, higher education institutions are increasingly exploring the use of AI (AI) applications. This research essay aims to investigate the impact of AI on the education of international students. Instead of a traditional literature review, it employs a research approach to examine the potential applications of AI and discuss associated concerns. The research paper explores various AI applications, such as personalized learning experiences, adaptive testing, predictive analytics, and chatbots for learning and research. By analyzing the role of AI in education for international students, this research paper sheds light on how AI can improve learning efficiency and provide customized educational support. Additionally, it identifies significant risks and limitations, including privacy concerns, cultural differences, language proficiency, and ethical implications, which must be effectively addressed. The findings contribute to a better understanding of the potential impact of AI on international students’ educational experiences and offer insights into the integration of AI into educational administration and learning processes.</jats:p>",1.0
278,10.1007/s11846-023-00696-z,The GenAI is out of the bottle: GenAI from a business model innovation perspective,"<jats:title>Abstract</jats:title><jats:p>The introduction of GenAI in November 2022 by OpenAI has stimulated substantial discourse on the implementation of AI (AI) in various domains such as academia, business, and society at large. Although AI has been utilized in numerous areas for several years, the emergence of GenAI (GAI) applications such as GenAI, Jasper, or DALL-E are considered a breakthrough for the acceleration of AI technology due to their ease of use, intuitive interface, and performance. With GAI, it is possible to create a variety of content such as texts, images, audio, code, and even videos. This creates a variety of implications for businesses requiring a deeper examination, including an influence on business model innovation (BMI). Therefore, this study provides a BMI perspective on GAI with two primary contributions: (1) The development of six comprehensive propositions outlining the impact of GAI on businesses, and (2) the discussion of three industry examples, specifically software engineering, healthcare, and financial services. This study employs a qualitative content analysis using a scoping review methodology, drawing from a wide-ranging sample of 513 data points. These include academic publications, company reports, and public information such as press releases, news articles, interviews, and podcasts. The study thus contributes to the growing academic discourse in management research concerning AI's potential impact and offers practical insights into how to utilize this technology to develop new or improve existing business models.</jats:p>",4.0
279,10.1007/s00146-022-01452-9,Cognitive architectures for AI ethics,"<jats:title>Abstract</jats:title><jats:p>As AI (AI) thrives and propagates through modern life, a key question to ask is how to include humans in future AI? Despite human involvement at every stage of the production process from conception and design through to implementation, modern AI is still often criticized for its “black box” characteristics. Sometimes, we do not know what really goes on inside or how and why certain conclusions are met. Future AI will face many dilemmas and ethical issues unforeseen by their creators beyond those commonly discussed (e.g., trolley problems and variants of it) and to which solutions cannot be hard-coded and are often still up for debate. Given the sensitivity of such social and ethical dilemmas and the implications of these for human society at large, when and if our AI make the “wrong” choice we need to understand how they got there in order to make corrections and prevent recurrences. This is particularly true in situations where human livelihoods are at stake (e.g., health, well-being, finance, law) or when major individual or household decisions are taken. Doing so requires opening up the “black box” of AI; especially as they act, interact, and adapt in a human world and how they interact with other AI in this world. In this article, we argue for the application of cognitive architectures for ethical AI. In particular, for their potential contributions to AI transparency, explainability, and accountability. We need to understand how our AI get to the solutions they do, and we should seek to do this on a deeper level in terms of the machine-equivalents of motivations, attitudes, values, and so on. The path to future AI is long and winding but it could arrive faster than we think. In order to harness the positive potential outcomes of AI for humans and society (and avoid the negatives), we need to understand AI more fully in the first place and we expect this will simultaneously contribute towards greater understanding of their human counterparts also.</jats:p>",0.0
280,10.1007/s13347-023-00668-x,AI (AI) in Islamic Ethics: Towards Pluralist Ethical Benchmarking for AI,"<jats:title>Abstract</jats:title><jats:p>This paper explores AI (AI) ethics from an Islamic perspective at a critical time for AI ethical norm-setting. It advocates for a pluralist approach to ethical AI benchmarking. As rapid advancements in AI technologies pose challenges surrounding autonomy, privacy, fairness, and transparency, the prevailing ethical discourse has been predominantly Western or Eurocentric. To address this imbalance, this paper delves into the Islamic ethical traditions to develop a framework that contributes to the global debate on optimal norm setting for designing and using AI technologies.</jats:p><jats:p>The paper outlines Islamic parameters for ethical values and moral actions in the context of AI's ethical uncertainties. It emphasizes the significance of both textual and non-textual Islamic sources in addressing these uncertainties while placing a strong emphasis on the notion of ""good"" or ""<jats:italic>maṣlaḥa</jats:italic>"" as a normative guide for AI's ethical evaluation. Defining <jats:italic>maṣlaḥa</jats:italic> as an ethical state of affairs in harmony with divine will, the paper highlights the coexistence of two interpretations of <jats:italic>maṣlaḥa</jats:italic>: welfarist/utility-based and duty-based. Islamic jurisprudence allows for arguments supporting ethical choices that prioritize building the technical infrastructure for AI to maximize utility. Conversely, it also supports choices that reject consequential utility calculations as the sole measure of value in determining ethical responses to AI advancements.</jats:p>",0.0
281,10.1007/s43681-021-00114-8,AI in research and development for sustainability: the centrality of explicability and research data management,"<jats:title>Abstract</jats:title><jats:p>Sustainability constitutes a focal challenge and objective of our time and requires collaborative efforts. As AI brings forth substantial opportunities for innovations across industry and social contexts, so it provides innovation potential for pursuing sustainability. We argue that (chemical) research and development driven by AI can substantially contribute to sustainability if it is leveraged in an ethical way. Therefore, we propose that the ethical principle <jats:italic>explicability</jats:italic> combined with (open) research data management systems should accompany AI in research and development to foster sustainability in an equitable and collaborative way.</jats:p>",0.0
282,10.1007/s43681-024-00440-7,Engaging the many-hands problem of generative-AI outputs: a framework for attributing credit,"<jats:title>Abstract</jats:title><jats:p>The recent wave of GenAI (GenAI) systems like Stable Diffusion or GenAI that can produce images, text and code from human prompts raises controversial issues about creatorship, originality, creativity and copyright. This paper focuses on creatorship: who creates and should be credited with the outputs made with the help of GenAI? There is currently significant moral, legal and regulatory uncertainty around these questions. We develop a novel framework, called CCC (collective-centered creation), that helps resolve this uncertainty. According to CCC, GenAI outputs are created by collectives in the first instance. Claims to creatorship come in degrees and depend on the nature and significance of individual contributions made by the various agents and entities involved, including users, GenAI systems, developers, producers of training data and others. We demonstrate how CCC can help navigate a range of ongoing controversies around the responsible development and deployment of GenAI technologies and help more accurately attribute credit where it is due.</jats:p>",2.0
283,10.1136/jme-2023-109767,Leveraging AI to detect ethical concerns in medical research: a case study,"<jats:sec><jats:title>Background</jats:title><jats:p>Institutional review boards (IRBs) have been criticised for delays in approvals for research proposals due to inadequate or inexperienced IRB staff. AI (AI), particularly large language models (LLMs), has significant potential to assist IRB members in a prompt and efficient reviewing process.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Four LLMs were evaluated on whether they could identify potential ethical issues in seven validated case studies. The LLMs were prompted with queries related to the proposed eligibility criteria of the study participants, vulnerability issues, information to be disclosed in the informed consent document (ICD), risk–benefit assessment and justification of the use of a placebo. Another query was issued to the LLMs to generate ICDs for these case scenarios.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>All four LLMs were able to provide answers to the queries related to all seven cases. In general, the responses were homogeneous with respect to most elements. LLMs performed suboptimally in identifying the suitability of the placebo arm, risk mitigation strategies and potential risks to study participants in certain case studies with a single prompt. However, multiple prompts led to better outputs in all of these domains. Each of the LLMs included all of the fundamental elements of the ICD for all case scenarios. Use of jargon, understatement of benefits and failure to state potential risks were the key observations in the AI-generated ICD.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>It is likely that LLMs can enhance the identification of potential ethical issues in clinical research, and they can be used as an adjunct tool to prescreen research proposals and enhance the efficiency of an IRB.</jats:p></jats:sec>",6.0
284,10.61969/jai.1512906,"Super AI, GenAI, Narrow AI and Chatbots: An Assessment of AI Technologies for The Public Sector and Public Administration","<jats:p xml:lang=""en"">AI encompasses a wide range of approaches, methodologies, and techniques aimed at mimicking human intelligence in machines. In recent times, the concepts of GenAI (AI), Super AI, and Narrow AI have attracted considerable attention. Undoubtedly, the success of GenAI in capturing all attention has played a significant role in this. AI technology has a profound impact on all sectors, and sector representatives are striving to adapt to this technology more quickly. It is projected that AI could generate an economic size of 13 trillion American dollars by 2030. Developments in AI technologies undoubtedly lead to significant improvements in the functioning of public institutions and access for citizens. AI has the potential to be used in many public services, including security and defense, healthcare services, education, transportation and infrastructure, environmental and natural resource management, law and justice systems, among others. Therefore, evaluating the types of AI, Narrow AI applications, and chatbots for public use is seen as highly beneficial from the perspective of public administration and the public sector. In our study, the topics of super AI, GenAI, narrow AI, and chatbots have been extensively evaluated within the context of the public sector and public administration. Utilizing findings from both Turkish and English literature reviews, the importance and potential impacts of AI within the public sector, along with current trends, have been comprehensively assessed. This research delves into the concepts of AI and its subsets—super AI, GenAI, narrow AI, and chatbots—within the general framework of the public sector. China and the United States are pioneering and leading countries in terms of investment. Although the U.S. stands out in many areas regarding investment, China's integration of AI with national strategies and its policies indicate that it may play a more dominant role in the future. There are four main implementation areas of AI in the public sector: efficiency and automation, service delivery, data-driven governance, and ethical and regulatory challenges. A review of the literature reveals that the ethical, legal, and social implications of implementing AI in the public sector require more careful consideration. The study makes a significant contribution to the field of AI discussions in public administration and the public sector, providing a comprehensive assessment of current discussions on AI in the literature.</jats:p>",-1.0
285,10.1007/s43681-021-00120-w,The social dilemma in AI development and why we have to solve it,"<jats:title>Abstract</jats:title><jats:p>While the demand for ethical AI (AI) systems increases, the number of unethical uses of AI accelerates, even though there is no shortage of ethical guidelines. We argue that a possible underlying cause for this is that AI developers face a social dilemma in AI development ethics, preventing the widespread adaptation of ethical best practices. We define the social dilemma for AI development and describe why the current crisis in AI development ethics cannot be solved without relieving AI developers of their social dilemma. We argue that AI development must be professionalised to overcome the social dilemma, and discuss how medicine can be used as a template in this process.</jats:p>",0.0
286,10.1186/s12910-024-01066-4,Public perceptions of AI in healthcare: ethical concerns and opportunities for patient-centered care,"<jats:title>Abstract</jats:title><jats:sec>
                <jats:title>Background</jats:title>
                <jats:p>In an effort to improve the quality of medical care, the philosophy of patient-centered care has become integrated into almost every aspect of the medical community. Despite its widespread acceptance, among patients and practitioners, there are concerns that rapid advancements in AI may threaten elements of patient-centered care, such as personal relationships with care providers and patient-driven choices. This study explores the extent to which patients are confident in and comfortable with the use of these technologies when it comes to their own individual care and identifies areas that may align with or threaten elements of patient-centered care.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Methods</jats:title>
                <jats:p>An exploratory, mixed-method approach was used to analyze survey data from 600 US-based adults in the State of Florida. The survey was administered through a leading market research provider (August 10–21, 2023), and responses were collected to be representative of the state’s population based on age, gender, race/ethnicity, and political affiliation.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Results</jats:title>
                <jats:p>Respondents were more comfortable with the use of AI in health-related tasks that were not associated with doctor-patient relationships, such as scheduling patient appointments or follow-ups (84.2%). Fear of losing the ‘human touch’ associated with doctors was a common theme within qualitative coding, suggesting a potential conflict between the implementation of AI and patient-centered care. In addition, decision self-efficacy was associated with higher levels of comfort with AI, but there were also concerns about losing decision-making control, workforce changes, and cost concerns. A small majority of participants mentioned that AI could be useful for doctors and lead to more equitable care but only when used within limits.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Conclusion</jats:title>
                <jats:p>The application of AI in medical care is rapidly advancing, but oversight, regulation, and guidance addressing critical aspects of patient-centered care are lacking. While there is no evidence that AI will undermine patient-physician relationships at this time, there is concern on the part of patients regarding the application of AI within medical care and specifically as it relates to their interaction with physicians. Medical guidance on incorporating AI while adhering to the principles of patient-centered care is needed to clarify how AI will augment medical care.</jats:p>
              </jats:sec>",3.0
287,10.1007/s43681-024-00451-4,Safeguarding human values: rethinking US law for GenAI’s societal impacts,"<jats:title>Abstract</jats:title><jats:p>Our interdisciplinary study examines the effectiveness of US law in addressing the complex challenges posed by GenAI systems to fundamental human values, including physical and mental well-being, privacy, autonomy, diversity, and equity. Through the analysis of diverse hypothetical scenarios developed in collaboration with experts, we identified significant shortcomings and ambiguities within the existing legal protections. Constitutional and civil rights law currently struggles to hold AI companies responsible for AI-assisted discriminatory outputs. Moreover, even without considering the liability shield provided by Section 230, existing liability laws may not effectively remedy unintentional and intangible harms caused by AI systems. Demonstrating causal links for liability claims such as defamation or product liability proves exceptionally difficult due to the intricate and opaque nature of these systems. To effectively address these unique and evolving risks posed by GenAI, we propose a “Responsible AI Legal Framework”  that adapts to recognize new threats and utilizes a multi-pronged approach. This framework would enshrine fundamental values in legal frameworks, establish comprehensive safety guidelines, and implement liability models tailored to the complexities of human-AI interactions. By proactively mitigating unforeseen harms like mental health impacts and privacy breaches, this framework aims to create a legal landscape capable of navigating the exciting yet precarious future brought forth by GenAI technologies.</jats:p>",-1.0
288,10.1609/aimag.v40i1.2848,"AI, Robotics, Ethics, and the Military: A Canadian Perspective","<jats:p><jats:italic>Defense and security organizations depend upon science and technology to meet operational needs, predict and counter threats, and meet increasingly complex demands of modern warfare. AI and robotics could provide solutions to a wide range of military gaps and deficiencies. At the same time, the unique and rapidly evolving nature of AI and robotics challenges existing polices, regulations, and values, and introduces complex ethical issues that might impede their development, evaluation, and use by the Canadian Armed Forces (CAF). Early consideration of potential ethical issues raised by military use of emerging AI and robotics technologies in development is critical to their effective implementation. This article presents an ethics assessment framework for emerging AI and robotics technologies. It is designed to help technology developers, policymakers, decision makers, and other stakeholders identify and broadly consider potential ethical issues that might arise with the military use and integration of emerging AI and robotics technologies of interest. We also provide a contextual environment for our framework, as well as an example of how our framework can be applied to a specific technology. Finally, we briefly identify and address several pervasive issues that arose during our research.</jats:italic></jats:p>",0.0
289,10.3390/ai1020012,Cities of the Future? The Potential Impact of AI,"<jats:p>AI (AI), like many revolutionary technologies in human history, will have a profound impact on societies. From this viewpoint, we analyze the combined effects of AI to raise important questions about the future form and function of cities. Combining knowledge from computer science, urban planning, and economics while reflecting on academic and business perspectives, we propose that the future of cities is far from being a determined one and cities may evolve into ghost towns if the deployment of AI is not carefully controlled. This viewpoint presents a fundamentally different argument, because it expresses a real concern over the future of cities in contrast to the many publications who exclusively assume city populations will increase predicated on the neoliberal urban growth paradigm that has for centuries attracted humans to cities in search of work.</jats:p>",0.0
290,10.3389/frai.2024.1497705,The impact of pedagogical beliefs on the adoption of GenAI in higher education: predictive model from UTAUT2,"<jats:p>AI in Education (AIEd) offers advanced tools that can personalize learning experiences and enhance teachers’ research capabilities. This paper explores the beliefs of 425 university teachers regarding the integration of GenAI in educational settings, utilizing the UTAUT2 model to predict their acceptance and usage patterns through the Partial Least Squares (PLS) method. The findings indicate that performance expectations, effort expectancy, social influence, facilitating conditions, and hedonic motivation all positively impact the intention and behavior related to the use of AIEd. Notably, the study reveals that teachers with constructivist pedagogical beliefs are more inclined to adopt AIEd, underscoring the significance of considering teachers’ attitudes and motivations for the effective integration of technology in education. This research provides valuable insights into the factors influencing teachers’ decisions to embrace AIEd, thereby contributing to a deeper understanding of technology integration in educational contexts. Moreover, the study’s results emphasize the critical role of teachers’ pedagogical orientations in their acceptance and utilization of AI technologies. Constructivist educators, who emphasize student-centered learning and active engagement, are shown to be more receptive to incorporating AIEd tools compared to their transmissive counterparts, who focus on direct instruction and information dissemination. This distinction highlights the need for tailored professional development programs that address the specific beliefs and needs of different teaching philosophies. Furthermore, the study’s comprehensive approach, considering various dimensions of the UTAUT2 model, offers a robust framework for analyzing technology acceptance in education.</jats:p>",1.0
291,10.3390/civileng5040049,Optimizing the Utilization of GenAI (AI) in the AEC Industry: GenAI Prompt Engineering and Design,"<jats:p>GenAI (AI) holds significant potential for revolutionizing the Architecture, Engineering, and Construction (AEC) industry by automating complex tasks such as construction scheduling, hazard recognition, resource leveling, information retrieval from BIM, etc. However, realizing this potential requires a strategic approach to ensure effective utilization and maximum benefit. This paper presents guidelines for prompt design and engineering to elicit desired responses from GenAI, a GenAI tool, in AEC applications. Key steps include understanding user intent, leveraging model capabilities, and optimizing prompt structures. By following these guidelines, stakeholders in the AEC industry can harness the power of GenAI to improve construction scheduling processes, increase project efficiency, and ultimately drive innovation and growth in the industry. Several illustrative examples on construction scheduling and hazard recognition are provided to demonstrate the methodology proposed in this research. It is concluded that GenAI, when effectively utilized, significantly enhances project scheduling and hazard recognition capability in the AEC industry with minimal error.</jats:p>",4.0
292,10.1007/s43681-023-00359-5,Exploring the status of AI for healthcare research in Africa: a bibliometric and thematic analysis,"<jats:title>Abstract</jats:title><jats:p>This paper explores the status of AI (AI) for healthcare research in Africa. The aim was to use bibliometric and thematic analysis methods to determine the publication counts, leading authors, top journals and publishers, most active institutions and countries, most cited institutions, funding bodies, top subject areas, co-occurrence of keywords and co-authorship. Bibliographic data were collected on April 9 2022, through the Lens database, based on the critical areas of authorship studies, such as authorship pattern, number of authors, etc. The findings showed that several channels were used to disseminate the publications, including articles, conference papers, reviews, and others. Publications on computer science topped the list of documented subject categories. The Annals of Tropical Medicine and Public Health is the top journal, where articles on AI have been published. One of the top nations that published AI research was the United Kingdom. With 143 publications, Harvard University was the higher education institution that produced the most in terms of affiliation. It was discovered that the Medical Research Council was one of the funding organizations that supported research, resulting in the publication of articles in AI. By summarizing the current research themes and trends, this work serves as a valuable resource for researchers, practitioners, and funding organizations interested in AI for healthcare research in Africa.</jats:p>",-1.0
293,10.1007/s43681-024-00497-4,Anticipating impacts: using large-scale scenario-writing to explore diverse implications of GenAI in the news environment,"<jats:title>Abstract</jats:title><jats:p>The tremendous rise of GenAI has reached every part of society—including the news environment. There are many concerns about the individual and societal impact of the increasing use of GenAI, including issues such as disinformation and misinformation, discrimination, and the promotion of social tensions. However, research on anticipating the impact of GenAI is still in its infancy and mostly limited to the views of technology developers and/or researchers. In this paper, we aim to broaden the perspective and capture the expectations of three stakeholder groups (news consumers; technology developers; content creators) about the potential negative impacts of GenAI, as well as mitigation strategies to address these. Methodologically, we apply scenario-writing and use participatory foresight in the context of a survey (n = 119) to delve into cognitively diverse imaginations of the future. We qualitatively analyze the scenarios using thematic analysis to systematically map potential impacts of GenAI on the news environment, potential mitigation strategies, and the role of stakeholders in causing and mitigating these impacts. In addition, we measure respondents' opinions on a specific mitigation strategy, namely transparency obligations as suggested in Article 52 of the draft EU AI Act. We compare the results across different stakeholder groups and elaborate on different expected impacts across these groups. We conclude by discussing the usefulness of scenario-writing and participatory foresight as a toolbox for GenAI impact assessment.</jats:p>",-1.0
294,10.1007/s10462-024-10768-5,Revolutionizing personalized medicine with GenAI: a systematic review,"<jats:title>Abstract</jats:title><jats:sec>
                <jats:title>Background</jats:title>
                <jats:p>Precision medicine, targeting treatments to individual genetic and clinical profiles, faces challenges in data collection, costs, and privacy. GenAI offers a promising solution by creating realistic, privacy-preserving patient data, potentially revolutionizing patient-centric healthcare.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Objective</jats:title>
                <jats:p>This review examines the role of deep generative models (DGMs) in clinical informatics, medical imaging, bioinformatics, and early diagnostics, showcasing their impact on precision medicine.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Methods</jats:title>
                <jats:p>Adhering to PRISMA guidelines, the review analyzes studies from databases such as Scopus and PubMed, focusing on AI's impact in precision medicine and DGMs' applications in synthetic data generation.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Results</jats:title>
                <jats:p>DGMs, particularly Generative Adversarial Networks (GANs), have improved synthetic data generation, enhancing accuracy and privacy. However, limitations exist, especially in the accuracy of foundation models like Large Language Models (LLMs) in digital diagnostics.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Conclusion</jats:title>
                <jats:p>Overcoming data scarcity and ensuring realistic, privacy-safe synthetic data generation are crucial for advancing personalized medicine. Further development of LLMs is essential for improving diagnostic precision. The application of GenAI in personalized medicine is emerging, highlighting the need for more interdisciplinary research to advance this field.</jats:p>
              </jats:sec>",5.0
295,10.1108/jeim-02-2024-0095,Assessing the impact of digital service innovation (DSI) on business performance: the mediating effect of Artificial Intelligence (AI),"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>The research aims to explore the dynamic relationship between digital service innovation (DSI), AI (AI) and business performance (BPer) in service-based models with a focus on how AI-enhanced insights from service use and customer feedback can strengthen business strategies. The aims are to show that DSI and AI are key to driving growth and efficiency in the digital economy and to underscore AI’s role in utilizing contextual data to improve decision-making and business outcomes.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>The study uses general structural equation modeling to analyze Spanish manufacturing firms, focusing on medium-sized enterprises and including both business-to-business and business-to-consumer orientations. Data are drawn from the Iberian Balance Analysis System [Sistema de Análisis de Balances Ibéricos (SABI)] database, complemented by a Qualtrics survey to assess the integration of AI in decision-making processes. The methodology is designed to evaluate the interplay between DSI, AI and BPer, with the aim of identifying actionable insights for service-based business orientations.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p> The study clarifies the relationships between DSI, AI and BPer, providing new theoretical and empirical insights. The findings confirm DSI's direct positive impact on performance and suggest AI’s nuanced mediating role, emphasizing the need for strategic DSI-AI integration in manufacturing firms for enhanced performance.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title><jats:p> The research explains the synergistic bond between DSI and AI in boosting BPer and discovering how by-product data can be transformed into strategic insights.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Practical implications</jats:title><jats:p> This study advises manufacturing sector leaders to integrate DSI and AI for enhanced performance and competitive advantage, emphasizing the value of high-quality, contextual data for AI learning and decision-making.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p> Researchers will observe that the study confirms the positive impact of DSI on BPer, while also highlighting the significant role of AI in enhancing this effect.</jats:p></jats:sec>",4.0
296,10.1007/s43681-024-00424-7,"E-coaching systems and social justice: ethical concerns about inequality, coercion, and stigmatization","<jats:title>Abstract</jats:title><jats:p>Poor self-regulation has been linked to various behaviors that contribute to pressing societal issues, including rising household debt, inefficient use of sustainable resources, and increasing healthcare demands. In light of this observation, the prospect of individuals receiving automated, tailored support by “e-coaching systems” to scaffold and improve their self-regulation is thought to hold promise for making society-wide progress in addressing such issues. Though there may be legitimate reasons for promoting the use of such systems, and individuals might welcome the support, our aim in the present article is to contribute to the ethics of e-coaching by showing how societal pressures towards the widespread adoption of automated e-coaching systems raise concerns in relation to three distinct aspects of social justice. We argue that societal inequalities may be introduced or exacerbated by (1) unequal access to the technologies, (2) unequally distributed restrictions to liberty and subjection to coercion, and (3) the potentially disparate impact of the use of e-coaching technologies on (self-)stigmatizing perceptions of competence. The article offers a research agenda for studying and addressing these concerns.</jats:p>",0.0
297,10.1007/s43681-022-00186-0,Ethical concerns with replacing human relations with humanoid robots: an ubuntu perspective,"<jats:title>Abstract</jats:title><jats:p>This paper considers ethical concerns with regard to replacing human relations with humanoid robots. Many have written about the impact that certain types of relations with robots may have on us, and why we should be concerned about robots replacing human relations. There has, however, been no consideration of this issue from an African philosophical perspective. Ubuntu philosophy provides a novel perspective on how relations with robots may impact our own moral character and moral development. This paper first discusses what humanoid robots are, why and how humans tend to anthropomorphise them, and what the literature says about robots crowding out human relations. It then explains the ideal of becoming “fully human”, which pertains to being particularly moral in character. In ubuntu philosophy, we are not only biologically human, but must strive to become better, more moral versions of ourselves, to become fully human. We can become fully human by having other regarding traits or characteristics within the context of interdependent, or humane, relationships (such as by exhibiting human equality, reciprocity, or solidarity). This concept of becoming fully human is important in ubuntu philosophy. Having explained that idea, the main argument of the paper is then put forward: that treating humanoid robots as if they are human is morally concerning if they crowd out human relations, because such relations prevent us from becoming fully human. This is because we cannot experience human equality, solidarity, and reciprocity with robots, which can be seen to characterise interdependent, or humane, relations with human beings.</jats:p>",0.0
298,10.1007/s43681-021-00131-7,The ethical issues of the application of AI in healthcare: a systematic scoping review,"<jats:title>Abstract</jats:title><jats:p>AI (AI) is being increasingly applied in healthcare. The expansion of AI in healthcare necessitates AI-related ethical issues to be studied and addressed. This systematic scoping review was conducted to identify the ethical issues of AI application in healthcare, to highlight gaps, and to propose steps to move towards an evidence-informed approach for addressing them. A systematic search was conducted to retrieve all articles examining the ethical aspects of AI application in healthcare from Medline (PubMed) and Embase (OVID), published between 2010 and July 21, 2020. The search terms were “AI” or “machine learning” or “deep learning” in combination with “ethics” or “bioethics”. The studies were selected utilizing a PRISMA flowchart and predefined inclusion criteria. Ethical principles of respect for human autonomy, prevention of harm, fairness, explicability, and privacy were charted. The search yielded 2166 articles, of which 18 articles were selected for data charting on the basis of the predefined inclusion criteria. The focus of many articles was a general discussion about ethics and AI. Nevertheless, there was limited examination of ethical principles in terms of consideration for design or deployment of AI in most retrieved studies. In the few instances where ethical principles were considered, fairness, preservation of human autonomy, explicability and privacy were equally discussed. The principle of prevention of harm was the least explored topic. Practical tools for testing and upholding ethical requirements across the lifecycle of AI-based technologies are largely absent from the body of reported evidence. In addition, the perspective of different stakeholders is largely missing.</jats:p>",6.0
299,10.1609/aimag.v41i2.5294,DARPA's Impact on AI,"<jats:p><jats:italic>The Defense Advanced Research Project Agency's (DARPA) mission is to make pivotal investments leading to research breakthroughs that support national security. DARPA AI (AI) programs have emphasized the need for machines to perceive and interact with the world around them; to frame problems and to arrive at solutions and decisions based on reasoning; to implement those decisions, perhaps through consultation with a human or another machine; to learn; to explain the rationale for decisions; to adhere to rules of ethical behavior defined for humans; to adapt to dynamic environments; and, to do all of this in real‐time. In short, DARPA has always been interested in AI frameworks that integrate AI and computer science technologies, and the application of those frameworks to DARPA‐hard problems. In this article, we describe the significant role that DARPA has played in the establishment of AI, and introduce six articles that explore DARPA's Three Waves of AI.</jats:italic></jats:p>",8.0
300,10.1007/s43681-021-00111-x,Methodology for integrating AI in healthcare systems: learning from COVID-19 to prepare for Disease X,"<jats:title>Abstract</jats:title><jats:p>AI and edge devices have been used at an increased rate in managing the COVID-19 pandemic. In this article we review the lessons learned from COVID-19 to postulate possible solutions for a Disease X event. The overall purpose of the study and the research problems investigated is the integration of AI function in digital healthcare systems. The basic design of the study includes a systematic state-of-the-art review, followed by an evaluation of different approaches to managing global pandemics. The study design then engages with constructing a new methodology for integrating algorithms in healthcare systems, followed by analysis of the new methodology and a discussion. Action research is applied to review existing state of the art, and a qualitative case study method is used to analyse the knowledge acquired from the COVID-19 pandemic. Major trends found as a result of the study derive from the synthesis of COVID-19 knowledge, presenting new insights in the form of a conceptual methodology—that includes six phases for managing a future Disease X event, resulting with a summary map of various problems, solutions and expected results from integrating functional AI in healthcare systems.</jats:p>",3.0
301,10.1007/s43681-024-00443-4,AI hype as a cyber security risk: the moral responsibility of implementing GenAI in business,"<jats:title>Abstract</jats:title><jats:p>This paper examines the ethical obligations companies have when implementing GenAI (AI). We point to the potential cyber security risks companies are exposed to when rushing to adopt GenAI solutions or buying into “AI hype”. While the benefits of implementing GenAI solutions for business have been widely touted, the inherent risks associated have been less well publicised. There are growing concerns that the race to integrate GenAI is not being accompanied by adequate safety measures. The rush to buy into the hype of GenAI and not fall behind the competition is potentially exposing companies to broad and possibly catastrophic cyber-attacks or breaches. In this paper, we outline significant cyber security threats GenAI models pose, including potential ‘backdoors’ in AI models that could compromise user data or the risk of ‘poisoned’ AI models producing false results. In light of these the cyber security concerns, we discuss the moral obligations of implementing GenAI into business by considering the ethical principles of beneficence, non-maleficence, autonomy, justice, and explicability. We identify two examples of ethical concern, <jats:italic>overreliance</jats:italic> and <jats:italic>over-trust</jats:italic> in GenAI, both of which can negatively influence business decisions, leaving companies vulnerable to cyber security threats. This paper concludes by recommending a set of checklists for ethical implementation of GenAI in business environment to minimise cyber security risk based on the discussed moral responsibilities and ethical concern.</jats:p>",8.0
302,10.1007/s10551-023-05339-7,The Ethical Implications of AI (AI) For Meaningful Work,"<jats:title>Abstract</jats:title><jats:p>The increasing workplace use of artificially intelligent (AI) technologies has implications for the experience of meaningful human work. Meaningful work refers to the perception that one’s work has worth, significance, or a higher purpose. The development and organisational deployment of AI is accelerating, but the ways in which this will support or diminish opportunities for meaningful work and the ethical implications of these changes remain under-explored. This conceptual paper is positioned at the intersection of the meaningful work and ethical AI literatures and offers a detailed assessment of the ways in which the deployment of AI can enhance or diminish employees’ experiences of meaningful work. We first outline the nature of meaningful work and draw on philosophical and business ethics accounts to establish its ethical importance. We then explore the impacts of three paths of AI deployment (replacing some tasks, ‘tending the machine’, and amplifying human skills) across five dimensions constituting a holistic account of meaningful work, and finally assess the ethical implications. In doing so we help to contextualise the meaningful work literature for the era of AI, extend the ethical AI literature into the workplace, and conclude with a range of practical implications and future research directions.
</jats:p>",-1.0
303,10.1007/s43681-021-00077-w,"The ethics of facial recognition technologies, surveillance, and accountability in an age of AI: a comparative analysis of US, EU, and UK regulatory frameworks","<jats:title>Abstract</jats:title><jats:p>The rapid development of facial recognition technologies (FRT) has led to complex ethical choices in terms of balancing individual privacy rights versus delivering societal safety. Within this space, increasingly commonplace use of these technologies by law enforcement agencies has presented a particular lens for probing this complex landscape, its application, and the acceptable extent of citizen surveillance. This analysis focuses on the regulatory contexts and recent case law in the United States (USA), United Kingdom (UK), and European Union (EU) in terms of the use and misuse of FRT by law enforcement agencies. In the case of the USA, it is one of the main global regions in which the technology is being rapidly evolved, and yet, it has a patchwork of legislation with less emphasis on data protection and privacy. Within the context of the EU and the UK, there has been a critical focus on the development of accountability requirements particularly when considered in the context of the EU’s General Data Protection Regulation (GDPR) and the legal focus on Privacy by Design (PbD). However, globally, there is no standardised human rights framework and regulatory requirements that can be easily applied to FRT rollout. This article contains a discursive discussion considering the complexity of the ethical and regulatory dimensions at play in these spaces including considering data protection and human rights frameworks. It concludes that data protection impact assessments (DPIA) and human rights impact assessments together with greater transparency, regulation, audit and explanation of FRT use, and application in individual contexts would improve FRT deployments. In addition, it sets out ten critical questions which it suggests need to be answered for the successful development and deployment of FRT and AI more broadly. It is suggested that these should be answered by lawmakers, policy makers, AI developers, and adopters.</jats:p>",0.0
304,10.1177/00986283241287203,"Using GenAI to Promote Psychological, Feedback, and AI Literacies in Undergraduate Psychology","<jats:sec><jats:title>Background</jats:title><jats:p> With the arrival of GenAI (genAI) tools, psychology educators are rethinking their assessment practices. </jats:p></jats:sec><jats:sec><jats:title>Objective</jats:title><jats:p> This paper describes one approach to integrating genAI into an assessment designed to promote psychological literacy. </jats:p></jats:sec><jats:sec><jats:title>Method</jats:title><jats:p> Students used GenAI to generate a media release about a published article and then wrote a critique. We evaluated whether students were able to use the marking rubric to assess the GenAI output, and whether working with the rubric early in the assessment process had benefits for their grades on subsequent tasks. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> The results show that students accurately assessed the GenAI output against the marking rubric, judging the output to be stylistically good but lacking in accurate coverage of the aims, methods, and results of the research. Working with genAI and the marking rubric early in the assessment process had benefits for performance, relative to cohorts that had engaged in peer review. </jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p> By allowing students to use genAI and scaffolding the process of critiquing and revising, students gained competencies in psychological, feedback, and AI literacies. </jats:p></jats:sec><jats:sec><jats:title>Teaching implications</jats:title><jats:p> Integrating genAI presents opportunities for learning, if educators can think beyond the artifact and design assessment that allows our students to showcase their learning process. </jats:p></jats:sec>",1.0
305,10.3389/frai.2024.1474019,A GenAI-driven interactive listening assessment task,"<jats:sec><jats:title>Introduction</jats:title><jats:p>Assessments of interactional competence have traditionally been limited in large-scale language assessments. The listening portion suffers from construct underrepresentation, whereas the speaking portion suffers from limited task formats such as in-person interviews or role plays. Human-delivered tasks are challenging to administer at large scales, while automated assessments are typically very narrow in their assessment of the construct because they have carried over the limitations of traditional paper-based tasks to digital formats. However, computer-based assessments do allow for more interactive, automatically administered tasks, but come with increased complexity in task creation. Large language models present new opportunities for enhanced automated item generation (AIG) processes that can create complex content types and tasks at scale that support richer assessments.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>This paper describes the use of such methods to generate content at scale for an interactive listening measure of interactional competence for the Duolingo English Test (DET), a large-scale, high-stakes test of English proficiency. The Interactive Listening task assesses test takers’ ability to participate in a full conversation, resulting in a more authentic assessment of interactive listening ability than prior automated assessments by positing comprehension and interaction as purposes of listening.</jats:p></jats:sec><jats:sec><jats:title>Results and discussion</jats:title><jats:p>The results of a pilot of 713 tasks with hundreds of responses per task, along with the results of human review, demonstrate the feasibility of a human-in-the-loop, GenAI-driven approach for automatic creation of complex educational assessments at scale.</jats:p></jats:sec>",1.0
306,10.3389/frai.2024.1437315,GenAI with WGAN-GP for boosting seizure detection accuracy,"<jats:sec><jats:title>Background</jats:title><jats:p>Imbalanced datasets pose challenges for developing accurate seizure detection systems based on electroencephalogram (EEG) data. GenAI techniques may help augment minority class data to facilitate automatic epileptic seizure detection.</jats:p></jats:sec><jats:sec><jats:title>New method</jats:title><jats:p>This study investigates the impact of various data augmentation (DA) approaches, including Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), Vanilla GAN, Conditional GAN (CGAN), and Cramer GAN, on classification performance with Random Forest models. The best-performing GAN variant, WGAN-GP, was then integrated with a bidirectional Long Short-Term Memory (LSTM) architecture and compared against traditional and synthetic oversampling methods.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The evaluation of different GAN variants for data augmentation with Random Forest classifiers identified WGAN-GP as the most effective approach. The integration of WGAN-GP with bidirectional LSTM yielded substantial performance improvements, outperforming traditional oversampling methods and achieving an accuracy of 91.73% on the augmented data, compared to 86% accuracy on real data without augmentation.</jats:p></jats:sec><jats:sec><jats:title>Comparison with existing methods</jats:title><jats:p>The proposed GenAI approach combining WGAN-GP and recurrent neural network models outperforms comparative synthetic oversampling methods on metrics relevant for reliable seizure detection from imbalanced EEG datasets.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Incorporating the WGAN-GP GenAI technique for data augmentation and integrating it with bidirectional LSTM elevates seizure detection accuracy for imbalanced EEG datasets, surpassing the performance of traditional oversampling and class weight adjustment methods. This approach shows promise for improving epilepsy monitoring and management through enhanced automated detection system effectiveness.</jats:p></jats:sec>",5.0
307,10.1007/s43681-024-00553-z,Eudemonia of a machine,"<jats:title>Abstract</jats:title><jats:p>Henry Ford once said, “For most purposes, a man with a machine is better than a man without a machine.” To this, engineers today propose an addendum – “and a man that <jats:italic>is</jats:italic> a machine is best of all” – which they have made their goal. The world over, engineers are working to make the ultimate machine, “the holy grail of AI,” a <jats:italic>conscious</jats:italic> humanoid. On the one hand, such a “machine” will be capable of relieving us of all our burdens. On the other hand, in so doing, will we not have “birthed,” as it were, a new class of slaves? In this essay I seek to summarize the various arguments made in this debate, bring to bear moral positions from the philosophy of technology, philosophy of law and philosophy of religion, as well as demonstrate the moral impropriety of such an endeavor from each of the classic moral approaches (i.e., Virtue Ethics, Consequentialism, Kantian Deontology). Finally, given that the debate centers around what is the “good life” for human or humanoid, I expand upon Aristotle’s Eudemonia and Maimonides’ <jats:italic>Summum Bonum</jats:italic> to argue that life is precious in its affordance to allow conscious beings, human or humanoid, to aspire to the best life possible.</jats:p>",0.0
308,10.1108/jices-03-2020-0034,An exploration of the impact of AI (AI) and automation for communication professionals,"<jats:sec>
<jats:title content-type=""abstract-subheading"">Purpose</jats:title>
<jats:p>AI (AI) and automation are currently changing human life with a great implication in the communication field. This research focusses on understanding the current and growing impact of AI and automation in the role of communication professionals to identify what skills and training are needed to face its impacts leading to a recommendation.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>
<jats:p>The research involves methodological triangulation, analysing and comparing data gathered from consulting with experts using the Delphi method, focus group with communication students, and literature review.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Findings</jats:title>
<jats:p>Findings show that the likely impacts are on the one hand the enhancing of efficiency and productivity, as well as freeing communication professionals to focus on the creative side, strategy and analytical thinking, on the other hand, repetitive and low-level jobs could be lost, being higher position jobs or those involving creativity and decision making harder to automate. Two types of training are needed: to gather experience with the current AI and automated tools, and to focus on developing human qualities that AI cannot replicate.</jats:p>
</jats:sec>
<jats:sec>
<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>
<jats:p>The outcomes of this research are valuable to help current and future communication practitioners, as well as organisations, to be one step ahead and survive the age AI and automation, being aware of its current and near-future impacts. The paper offers a list of recommended soft and technical skills, as well as training needed, categorizing them in low, medium and high priority.</jats:p>
</jats:sec>",-1.0
309,10.21272/bel.6(2).6-11.2022,Ethical Concerns While Using AI in Recruitment of Employees,"<jats:p>AI has evolved as an alternative to human intelligence. It affects the lives of billions of people. It mimics humans by solving problems and understanding the task. These AI technologies must have some moral values and ethics incorporated within itself. The usage of AI is growing worldwide, posing more ethical issues to consider. In recent years, many companies have used various AI tools such as chatbots and face recognition software for fulfilling their hiring needs. This research work will focus on such devices that help manage one of the important functions of human resources: recruitment. It will identify various challenges and ethical issues that a firm faces while assimilating AI tools in the process of Recruitment. The hiring companies need to make the job seekers realize that AI-powered tools would be free from discrimination and safeguard privacy. The purpose of the study is to identify the ethical issues while incorporating AI into hiring needs. The study will be based on reviews and features of applications. The study mentions various applications whose features might be unethical for job seekers. Findings reveal that the significant unethical issues faced by the hiring companies are Data privacy and unconscious biasness. The biasness is due to the algorithm that works according to the inputs fed to build it, and the programmer might have subconscious biasness in his mind. AI has restored concerns regarding privacy and data protection. According to a report by UNESCO, Women make up only 22% of all AI professionals. Gender prejudices and stereotyping are perpetuated in AI technologies due to their underrepresentation in the sector. Virtual personal assistants like Siri, Alexa, and Cortana are “female” by default, which is no accident. The submissiveness they display is an illustration of how AI (AI) might continue to support and extend gender bias in our society.</jats:p>",0.0
310,10.3390/su14031730,AI (AI) and Sustainable Development Goals (SDGs): Exploring the Impact of AI on Politics and Society,"<jats:p>In popular discourse, AI (AI) has turned into one of the most inexplicable issues It has turned into an artifact that people do not dare to question [...]</jats:p>",0.0
311,10.15407/jai2024.03.032,"AI: free will, self-consciousness and ethics.","<jats:p>The article discusses key aspects of AI creation, including issues of free will, self-awareness and ethics. The focus is on the neurobiological basis of consciousness, in particular the structure and functions of the new cerebral cortex, as well as the mechanisms of recognition, memory and prediction, which are important for modelling cognitive processes in artificial systems. The paper discusses the role of neural networks in reproducing cognitive functions, such as perception and decision making, and presents modern approaches to training neural networks. A separate part of the paper is devoted to the issue of modelling self-awareness and subjective experience in AI and how realistic it is to create self-aware machines. Ethical issues of AI creation are at the centre of the discussion, including the topics of the rights of self-aware machines, their responsibilities and their role in society. The article considers the possible social consequences of the emergence of artificial personalities, the need to develop new legal frameworks and legal protections for such beings. It also discusses the problem of free will in the context of both biological and artificial systems, citing experiments and philosophical theories that question free will as a phenomenon. It concludes that the creation of AI has great potential, but requires careful ethical and legal analysis to ensure the harmonious integration of artificial persons into social and legal structures.</jats:p>",0.0
312,10.3389/frai.2024.1377011,Ethics and responsible AI deployment,"<jats:p>As AI (AI) becomes more prevalent, protecting personal privacy is a critical ethical issue that must be addressed. This article explores the need for ethical AI systems that safeguard individual privacy while complying with ethical standards. By taking a multidisciplinary approach, the research examines innovative algorithmic techniques such as differential privacy, homomorphic encryption, federated learning, international regulatory frameworks, and ethical guidelines. The study concludes that these algorithms effectively enhance privacy protection while balancing the utility of AI with the need to protect personal data. The article emphasises the importance of a comprehensive approach that combines technological innovation with ethical and regulatory strategies to harness the power of AI in a way that respects and protects individual privacy.</jats:p>",0.0
313,10.1007/s13347-022-00498-3,Images of AI: a Blind Spot in AI Ethics,"<jats:title>Abstract</jats:title><jats:p>This paper argues that the AI ethics has generally neglected the issues related to the science communication of AI. In particular, the article focuses on visual communication about AI and, more specifically, on the use of certain stock images in science communication about AI — in particular, those characterized by an excessive use of blue color and recurrent subjects, such as androgyne faces, half-flesh and half-circuit brains, and variations on Michelangelo’s <jats:italic>The Creation of Adam</jats:italic>. In the first section, the author refers to a “referentialist” ethics of science communication for an ethical assessment of these images. From this perspective, these images are unethical. While the ethics of science communication generally promotes virtues like modesty and humility, similar images are arrogant and overconfident. In the second section, the author uses French philosopher Jacques Rancière’s concepts of “distribution of the sensible,” “disagreement,” and “pensive image.” Rancière’s thought paves the way to a deeper critique of these images of AI. The problem with similar images is not their lack of reference to the “things themselves.” It rather lies in the way they stifle any possible forms of disagreement about AI. However, the author argues that stock images and other popular images of AI are not a problem per se, and they can also be a resource. This depends on the real possibility for these images to support forms of pensiveness. In the conclusion, the question is asked whether the kind of ethics or politics of AI images proposed in this article can be applied to AI ethics tout court.</jats:p>",2.0
314,10.1007/s43681-024-00488-5,AI art and public literacy: the miseducation of Ai-Da the robot,"<jats:title>Abstract</jats:title><jats:p>This article examines the implications of the artworks and public performances of the robot artist Ai-Da. While the project claims to advance AI public literacy and foster critical debate around intelligent systems, it instead ends up perpetuating popular misunderstandings about AI creativity, agency, and consciousness. Built in 2019, Ai-Da is a humanoid robot capable of creating drawings, paintings, and composing poetry. However, the project often conceals or miscommunicates the technical aspects of Ai-Da’s capabilities in a manner that encourages the public to misattribute human-like traits to the robot. This lack of transparency in the presentation of Ai-Da’s abilities and the creative processes involved risks reinforcing existing misconceptions about AI, rather than promoting a more nuanced understanding. By employing discourse analysis and drawing on scholarship on machine and computational creativity, anthropomorphism in social robots, and posthuman embodiment, this article uses the Ai-Da project as a case study to illustrate how the dangers of AI hype can be obscured when presented through the lens of public art. The analysis examines how the Ai-Da project, despite its stated goals of advancing AI literacy, fails to effectively challenge and may even exacerbate public misperceptions about the nature of AI-generated art and creativity.</jats:p>",2.0
315,10.32609/0042-8736-2024-10-110-127,The impact of AI on creative industries: Freelancers’ anxieties and concerns,"<jats:p>The article examines the impact of the rapid development of AI (AI) technologies on the creative industries and the concerns of workers in this field regarding the potential deterioration of their working conditions and displacement from the labor market. The aim of the study is to identify the degree of concern among freelancers engaged in intellectual and creative professions regarding competition with AI and to assess their perception of AI’s current capabilities in making creative content. The empirical basis was provided by online survey data of 778 Russian freelancers receiving jobs through the Freelance.ru digital platform, conducted in spring 2024. It was found that many respondents are already actively using AI in their work. The majority of freelancers note AI’s high current capabilities in creating texts, images, translation, and other areas, and more than a third believe that in the coming years AI will be able to do their typical work as well or even better than they do it themselves. Those who were least likely to experience concerns about their future were individuals who had been trained in AI, used it to perform job tasks, satisfied with their work, and had a high level of income, i.e., generally had a stable position in the labor market. Despite the concerns of some workers, the development of AI opens up new opportunities for the creative industries; however, regular monitoring of the situation is required to develop measures to adapt the labor market.</jats:p>",-1.0
316,10.15407/jai2023.02.010,The impact of AI on modern education: prospects and challenges,"<jats:p>This paper examines the intricate links between AI (AI) and education, delving into both theoretical and practical aspects while evaluating possible ramifications for labor market dynamics, professional activity, and wider educational paradigms. Our research methodology involved analyzing relevant scientific literature, classifying data, consulting with subject matter experts, and synthesizing the results. Our research suggests that AI has the ability to greatly improve pedagogical processes, personalize learning experiences to meet individual student needs, and successfully address the time and financial limitations that are inherent in traditional educational models. However, our study also reveals challenges related to data confidentiality, potential plagiarism and fraud associated with AI use, and socioeconomic disparities resulting from unequal technology access. Additionally, we identified a significant gap in current AI usage standards legislation. It is essential for researchers, educators, and policymakers to recognize the potential risks of AI implementation in educational settings and proactively develop strategies that prioritize ethics, safety, and effectiveness. With labor market trends favoring specialists knowledgeable in utilizing AI tools, a consequent change in curricula is expected. In response to our findings, we recommend the creation of new academic disciplines that concentrate on the cultivation of AI expertise; the establishment of comprehensive national AI strategies; the crafting of retraining roadmaps for those who may be affected by AI automation; the inclusion of online AI courses in existing educational programs; and the promotion of grant funding for future AI research. Our future research will concentrate on reducing the potential negative impacts of integrating AI into educational systems.</jats:p>",1.0
317,10.1386/sfs_00082_1,AI in Indian films: Anukul and AI ethics,"<jats:p>In recent years, AI (AI) has emerged as an important theme in Indian cinema. Taking note of Indian filmmakers’ interest in AI, this article examines the Hindi-language short film <jats:italic>Anukul</jats:italic> directed by Sujoy Ghosh. Based on a short story by the legendary auteur Satyajit Ray, the film draws us to a posthuman world order in which machines have assumed rights and threaten to push humans out of the loop. The AI–human interface in <jats:italic>Anukul</jats:italic> provokes a number of ethical questions and this article aims to think through some of the ethical and legal concerns raised in the film.</jats:p>",0.0
318,10.3390/educsci14101062,University Students’ Insights of GenAI (AI) Writing Tools,"<jats:p>The current study examined university students’ insights into GenAI writing tools regarding their familiarity with, perceived concerns about, and perceived benefits of these tools in their academic work. The study used a cross-sectional descriptive research design, and data were collected using a questionnaire instrument. The participants were ninety-five undergraduate and graduate students from a College of Education at a university in Jordan. The results show that university students show moderate familiarity with GenAI writing tools (M = 3.14, SD = 0.81), especially in engagement but lacking technical knowledge. They also have moderate concerns (M = 3.35, SD = 0.85), particularly about misinformation and data security. Despite these concerns, students recognize the benefits (M = 3.62, SD = 0.81), especially regarding the capabilities of these tools in simulating creativity and fostering innovation. In addition, the results showed that gender and educational level appear to have little effect on familiarity, concerns, and perceived benefits regarding these tools. Based on the findings, the study recommends enhancing students’ familiarity with GenAI tools through providing technical training, hands-on opportunities, and ethical discussions. In addition, the study recommends addressing students’ concerns regarding GenAI writing tools by improving data security related to GenAI, providing ethical guidelines regarding the use of these tools, and boosting AI literacy. Finally, it is recommended to enhance students’ perceptions of the benefits of GenAI writing tools by highlighting the creative potential of these tools within the educational setting, using these tools to offer personalized learning experiences that adapt to individual learning styles, and promoting collaboration through GenAI writing tools.</jats:p>",1.0
319,10.3389/fsoc.2024.1417766,AI and real decisions: predictive systems and GenAI vs. emotive-cognitive legal deliberations,"<jats:p>The use of AI in law represents one of the biggest challenges across different legal systems. Supporters of predictive systems believe that decisionmaking could be more efficient, consistent and predictable by using AI. European legislation and legal scholars, however, identify areas where AI developments are at high risk or too dangerous to be used in judicial proceedings. In this article, we contribute to this debate by problematizing predictive systems based on previous judgments and the growing use of GenAI in judicial proceedings. Through illustrations from real criminal cases in Italian courts and prosecution offices, we show misalignments between the functions of AI systems and the essential features of legal decision-making and identify possible legitimate usages. We argue that current predictive systems and GenAI crunch the complexity of judicial proceedings, the dynamics of fact-finding and legal encoding. They reduce the delivery of justice to statistical connections between data or metadata, cutting off the emotive-cognitive process that lies at the core of legal decision-making.</jats:p>",0.0
320,10.1007/s43681-023-00325-1,Responsible AI in human resources management: a review of the empirical literature,"<jats:title>Abstract</jats:title><jats:p>As it is the case for many business processes and activities disciplines, AI (AI) is increasingly integrated in human resources management (HRM). While AI has great potential to augment the HRM activities in organizations, automating the management of humans is not without risks and limitations. The identification of these risks is fundamental to promote responsible use of AI in HRM. We thus conducted a review of the empirical academic literature across disciplines on the affordances and responsible principles of AI in HRM. This is the first review of responsible AI in HRM that focuses solely on studies containing observations, measurements, and tests about this phenomenon. The multi-domain and multidisciplinary approach and empirical focus provides a better understanding of the reality of the development, study, and deployment of AI in HRM and sheds light on how these are conducted responsibly. We conclude with a call for research based on what we identified as the most needed and promising avenues.</jats:p>",-1.0
321,10.1007/s10506-024-09412-y,"An interdisciplinary account of the terminological choices by EU policymakers ahead of the final agreement on the AI Act: AI system, general purpose AI system, foundation model, and GenAI","<jats:title>Abstract</jats:title><jats:p>The European Union’s AI Act (AI Act) is a groundbreaking regulatory framework that integrates technical concepts and terminology from the rapidly evolving ecosystems of AI research and innovation into the legal domain. Precise definitions accessible to both AI experts and lawyers are crucial for the legislation to be effective. This paper provides an interdisciplinary analysis of the concepts of <jats:italic>AI system</jats:italic>, <jats:italic>general purpose AI system</jats:italic>, <jats:italic>foundation model</jats:italic> and <jats:italic>GenAI</jats:italic> across the different versions of the legal text (Commission proposal, Parliament position and Council General Approach) before the final political agreement. The goal is to help bridge the understanding of these key terms between the technical and legal communities and contribute to a proper implementation of the AI Act. We provide an analysis of the concept of <jats:italic>AI system</jats:italic> considering its scientific foundation and the crucial role that it plays in the regulation, which requires a sound definition both from legal and technical standpoints. We connect the outcomes of this discussion with the analysis of the concept of <jats:italic>general purpose AI system</jats:italic> and its evolution during the negotiations. We also address the distinct conceptual meanings of <jats:italic>AI system</jats:italic> vs <jats:italic>AI model</jats:italic> and explore the technical nuances of the term <jats:italic>foundation model</jats:italic>. We conclude that rooting the definition of <jats:italic>foundation model</jats:italic> to its general purpose capabilities following standardised evaluation methodologies appears to be most appropriate approach. Lastly, we tackle the concept of <jats:italic>GenAI</jats:italic>, arguing that definitions of <jats:italic>AI system</jats:italic> that include “content” as one of the system’s outputs already captures it, and concluding that not all <jats:italic>GenAI</jats:italic> is based on <jats:italic>foundation models</jats:italic>.</jats:p>",0.0
322,10.15407/jai2023.02.094,Resonance diagnostics of production space of generative systems of AI,"<jats:p>The development of AI generative systems (AIGS) in the modern world requires addressing issues related to the quality, stability, and efficiency of the generated content. In this context, resonance diagnostics become of paramount importance. The purpose of this study is to explore the possibilities of applying resonance diagnostics for detecting, analyzing, and resolving problems in AI generative systems. To achieve the set goal, the following tasks were identified: analysis of the theoretical foundations of resonance diagnostics; investigation of the potential of using resonance signals to adjust AIGS learning parameters; studying the impact of resonance diagnostics on the stability and adaptation of AIGS to changing operating conditions. The study conducted an analysis of resonance diagnostics in the context of AIGS and revealed its powerful influence on addressing issues related to system quality and productivity. The research demonstrated that resonance diagnostics can be used to achieve realism, diversity, and quality of generated content. Additionally, it was determined that it can contribute to enhancing the stability and adaptation of systems to varying operational conditions</jats:p>",2.0
323,10.1007/s43681-024-00536-0,Minimum levels of interpretability for artificial moral agents,"<jats:title>Abstract</jats:title><jats:p>As AI (AI) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. For models involved in moral decision-making (MDM), also known as artificial moral agents (AMA), interpretability provides a way to trust and understand the agent’s internal reasoning mechanisms for effective use and error correction. In this paper, we bridge the technical approaches to interpretability with construction of AMAs to establish minimal safety requirements for deployed AMAs. We begin by providing an overview of AI interpretability in the context of MDM, thereby framing different levels of interpretability (or transparency) in relation to the different ways of constructing AMAs. Introducing the concept of the Minimum Level of Interpretability (MLI) and drawing on examples from the field, we explore two overarching questions: whether a lack of model transparency prevents trust and whether model transparency helps us sufficiently understand AMAs. Finally, we conclude by recommending specific MLIs for various types of agent constructions, aiming to facilitate their safe deployment in real-world scenarios.</jats:p>",0.0
324,10.1007/s43681-023-00296-3,AI’s right to life,"<jats:title>Abstract</jats:title><jats:p>The right to life is fundamental and primary and is a precondition for exercising other rights (Ramcharan in Ramcharan (ed), The right to life in International Law, Martinus Nijhoff Publishers, Dordrecht, 1985). Its universal recognition in the arena of international law is associated with the concept of a human being endowed with inherent and inalienable dignity. Categorization of the circle of entities covered with the right to life today seems obvious and indisputable. Intense development of AI, also the fact that it has passed the Turing test which checks AI’s thinking ability in a way similar to human reasoning, inspires a reflection on AI’s future legal status. This study will investigate a thesis of whether AI may be entitled to the right to life. The analysis will be carried out around an exploratory question: what are the requirements for being afforded protection of the right to life?</jats:p>",0.0
325,10.1007/s10676-024-09745-x,Ethics of GenAI and manipulation: a design-oriented research agenda,"<jats:title>Abstract</jats:title><jats:p>GenAI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around GenAI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of GenAI technologies.</jats:p>",2.0
326,10.1177/02666669231200628,AI in developing countries: The impact of GenAI (AI) technologies for development,"<jats:p> This paper explores the potential impact of GenAI (GenAI) on developing countries, considering both positive and negative effects across various domains of information, culture, and industry. GenAI refers to AI (AI) systems that generate content, such as text, audio, or video, aiming to produce novel and creative outputs based on training data. Compared to conversational AI, GenAI systems have the unique capability of not only providing replies but also generating the content of those responses. Recent advancements in AI during the Fourth Industrial Revolution, exemplified by tools like GenAI, have gained popularity and reshaped content production and creation. However, the benefits of GenAI are not equally accessible to all, especially in developing countries, where limited access to cutting-edge technologies and inadequate infrastructure pose challenges. This paper seeks to understand the potential impact of GenAI technologies on developing countries, considering economic growth, access to technology, and the potential paradigm shift in education, healthcare, and the environment. The findings emphasize the importance of providing the necessary support and infrastructure to ensure that GenAI contributes to inclusive development rather than deepening existing inequalities. The study highlights the significance of integrating GenAI into the context of the Fourth Industrial Revolution in developing countries, where technological change is a crucial determinant of progress and equitable growth. </jats:p>",-1.0
327,10.1017/s0963180123000464,AI and Human Enhancement: Can AI Technologies Make Us More (Artificially) Intelligent?,"<jats:title>Abstract</jats:title><jats:p>This paper discusses two opposing views about the relation between AI (AI) and human intelligence: on the one hand, a worry that heavy reliance on AI technologies might make people less intelligent and, on the other, a hope that AI technologies might serve as a form of cognitive enhancement. The worry relates to the notion that if we hand over too many intelligence-requiring tasks to AI technologies, we might end up with fewer opportunities to train our own intelligence. Concerning AI as a potential form of cognitive enhancement, the paper explores two possibilities: (1) AI as extending—and thereby enhancing—people’s minds, and (2) AI as enabling people to behave in artificially intelligent ways. That is, using AI technologies might enable people to behave as if they have been cognitively enhanced. The paper considers such enhancements both on the level of individuals and on the level of groups.</jats:p>",0.0
328,10.3390/ai5030080,Perspectives for GenAI-Assisted Art Therapy for Melanoma Patients,"<jats:p>Digital technologies are making their mark in medicine, and especially also in art therapy, offering innovative therapeutic interventions for patients, including those with melanoma skin cancer. However, the integration of novel technologies, such as AI-generated art, brings along ethical, psychological, and technical challenges that are viewed differently among therapists. We aim to gauge art therapists’ views on the ethical, application, and challenge facets of utilizing AI-generated art from medical images in therapy. The focus is on assessing its applicability and limitations for melanoma patients. Art therapists were surveyed via a questionnaire focusing on their experience, digital tool familiarity, and views on AI in therapy, encompassing ethics, benefits, challenges, and applicability for melanoma. Art therapists have already implemented digital technologies and acknowledged potential therapeutic benefits of creating personalized artworks with GenAI. Attention needs to be given to technological hurdles and the necessity for supplementary interventions. Views on the method’s adaptability varied, underscoring a need for tailored, patient-focused applications. Art therapists are welcoming AI-generated art as a promising creative therapeutic tool and acknowledge potential therapeutic benefits. There are ethical, technical, and psychological challenges that must be addressed for application in therapeutic sessions. Therapists should navigate AI integration with sensitivity, adhering to ethical norms around consent and privacy. Future studies should show the therapeutic benefit in practice with emphasis on equipping therapists to manage the technical complexities effectively. Furthermore, it is important to ensure that patients can influence the AI output, allowing for creative moments in the process.</jats:p>",2.0
329,10.1007/s43681-022-00246-5,Artificial moral experts: asking for ethical advice to artificial intelligent assistants,"<jats:title>Abstract</jats:title><jats:p>In most domains of human life, we are willing to accept that there are experts with greater knowledge and competencies that distinguish them from non-experts or laypeople. Despite this fact, the very recognition of expertise curiously becomes more controversial in the case of “moral experts”. Do moral experts exist? And, if they indeed do, are there ethical reasons for us to follow their advice? Likewise, can emerging technological developments broaden our very concept of moral expertise? In this article, we begin by arguing that the objections that have tried to deny the existence (and convenience) of moral expertise are unsatisfactory. After that, we show that people have ethical reasons to ask for a piece of moral advice in daily life situations. Then, we argue that some AI (AI) systems can play an increasing role in human morality by becoming moral experts. Some AI-based moral assistants can qualify as artificial moral experts and we would have good ethical reasons to use them.</jats:p>",0.0
330,10.1007/s00146-022-01578-w,AI ethics with Chinese characteristics? Concerns and preferred solutions in Chinese academia,"<jats:title>Abstract</jats:title><jats:p>Since Chinese scholars are playing an increasingly important role in shaping the national landscape of discussion on AI ethics, understanding their ethical concerns and preferred solutions is essential for global cooperation on governance of AI. This article, therefore, provides the first elaborated analysis on the discourse on AI ethics in Chinese academia, via a systematic literature review. This article has three main objectives. (1) to identify the most discussed ethical issues of AI in Chinese academia and those being left out (the question of “what”); (2) to analyze the solutions proposed and preferred by Chinese scholars (the question of “how”); and (3) to map out whose voices are dominating and whose are in the marginal (the question of “who”). Findings suggest that in terms of short-term implications, Chinese scholars’ concerns over AI resemble predominantly the content of international ethical guidelines. Yet in terms of long-term implications, there are some significant differences needed to be further addressed in a cultural context. Further, among a wide range of solution proposals, Chinese scholars seem to prefer strong-binding regulations to those weak ethical guidelines. In addition, this article also found that the Chinese academic discourse was dominated by male scholars and those who are from elite universities, which arguably is not a unique phenomenon in China.</jats:p>",0.0
331,10.3390/ai4010003,Ethics &amp; AI: A Systematic Review on Ethical Concerns and Related Strategies for Designing with AI in Healthcare,"<jats:p>In modern life, the application of AI (AI) has promoted the implementation of data-driven algorithms in high-stakes domains, such as healthcare. However, it is becoming increasingly challenging for humans to understand the working and reasoning of these complex and opaque algorithms. For AI to support essential decisions in these domains, specific ethical issues need to be addressed to prevent the misinterpretation of AI, which may have severe consequences for humans. However, little research has been published on guidelines that systematically addresses ethical issues when AI techniques are applied in healthcare. In this systematic literature review, we aimed to provide an overview of ethical concerns and related strategies that are currently identified when applying AI in healthcare. The review, which followed the PRISMA guidelines, revealed 12 main ethical issues: justice and fairness, freedom and autonomy, privacy, transparency, patient safety and cyber security, trust, beneficence, responsibility, solidarity, sustainability, dignity, and conflicts. In addition to these 12 main ethical issues, we derived 19 ethical sub-issues and associated strategies from the literature.</jats:p>",6.0
332,10.1108/techs-12-2022-0047,Cognitive morality and AI (AI): a proposed classification of AI systems using Kohlberg's theory of cognitive ethics,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>The widespread usage of AI (AI) is prompting a number of ethical issues, including those involving concerns for fairness, surveillance, transparency, neutrality and human rights. The purpose of this manuscript is to explore possibility of developing cognitive morality in AI systems.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>This is explorative research. The manuscript investigates the likelihood of cognitive moral development in AI systems as well as potential pathways for such development. Concurrently, it proposes a novel idea for the characterization and development of ethically conscious and artificially intelligent robotic machines.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>This manuscript explores the possibility of categorizing AI machines according to the level of cognitive morality they embody, and while doing so, it makes use of Lawrence Kohlberg's study related to cognitive moral development in humans. The manuscript further suggests that by providing appropriate inputs to AI machines in accordance with the proposed concept, humans may assist in the development of an ideal AI creature that would be morally more responsible and act as moral agents, capable of meeting the demands of morality.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title><jats:p>This manuscript has some restrictions because it focuses exclusively on Kohlberg's perspective. This theory is not flawless. Carol Gilligan, one of Kohlberg's former doctoral students, said that Kohlberg's proposal was unfair and sexist because it didn't take into account the views and experiences of women. Even if one follows the law, they may still be engaging in immoral behaviour, as Kohlberg argues, because laws and social norms are not perfect. This study makes it easier for future research in the field to look at how the ideas of people like Joao Freire and Carl Rogers can be used in AI systems.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>It is an original research that derives inspiration from the cognitive moral development theory of American Professor named Lawrence Kohlberg. The authors present a fresh way of thinking about how to classify AI systems, which should make it easier to give robots cognitive morality.</jats:p></jats:sec>",0.0
333,10.1007/s00146-021-01309-7,"Online public discourse on AI and ethics in China: context, content, and implications","<jats:title>Abstract</jats:title><jats:p>The societal and ethical implications of AI (AI) have sparked discussions among academics, policymakers and the public around the world. What has gone unnoticed so far are the likewise vibrant discussions in China. We analyzed a large sample of discussions about AI ethics on two Chinese social media platforms. Findings suggest that participants were diverse, and included scholars, IT industry actors, journalists, and members of the general public. They addressed a broad range of concerns associated with the application of AI in various fields. Some even gave recommendations on how to tackle these issues. We argue that these discussions are a valuable source for understanding the future trajectory of AI development in China as well as implications for global dialogue on AI governance.</jats:p>",0.0
334,10.1007/s43681-023-00411-4,Should we develop AGI? Artificial suffering and the moral development of humans,"<jats:title>Abstract</jats:title><jats:p>Recent research papers and tests in real life point in the direction that machines in the future may develop some form of possibly rudimentary inner life. Philosophers have warned and emphasized that the possibility of artificial suffering or the possibility of machines as moral patients should not be ruled out. In this paper, I reflect on the consequences for moral development of striving for AGI. In the introduction, I present examples which point into the direction of the future possibility of artificial suffering and highlight the increasing similarity between, for example, machine–human and human–human interaction. Next, I present and discuss responses to the possibility of artificial suffering supporting a cautious attitude for the sake of the machines. From a virtue ethical perspective and the development of human virtues, I subsequently argue that humans should not pursue the path of developing and creating AGI, not merely for the sake of possible suffering in machines, but also due to machine–human interaction becoming more alike to human–human interaction and for the sake of the human’s own moral development. Thus, for several reasons, humanity, as a whole, should be extremely cautious about pursuing the path of developing AGI—Artificial General Intelligence.</jats:p>",0.0
335,10.18034/ajhal.v5i2.659,Investigating the Prospects of GenAI,"<jats:p>In this exploratory work, we investigate cutting-edge techniques in machine learning known as GenAI (GenAI). The costs of trial and error during product development can be significantly reduced if faster, more affordable, and more accurate multi-scale materials simulations powered by fully GenAI are available. Engineers have spent decades attempting to develop humanoid robots that are both practical and resemble people in appearance and behavior. Because it enables us to circumvent the inherent dimensionality of this obstacle, GenAI has the potential to be a beneficial instrument for the current creation process. Moreover, the research underlines that GenAI, capable of producing media such as text, images, and audio in response to prompts, appears to improve daily. In addition, numerous technological companies are currently building and releasing their competing systems.</jats:p>",2.0
336,10.1007/s43681-022-00155-7,"Adopting smart glasses responsibly: potential benefits, ethical, and privacy concerns with Ray-Ban stories","<jats:title>Abstract</jats:title><jats:p>The adoption of innovative wearable technologies is potentially increasing as a new trend. Jumping into the augmented reality (AR) and Metaverse, Facebook (now known as Meta) launched smart glasses partnering with Ray-Ban sunglasses brand’s parent company EssilorLuxottica. Ray-Ban stories has several technical features for entertainment and socializing; more importantly, these features can be adopted in the future for more advanced wearable. However, these smart glasses also came with many ethical and privacy concerns along with their potential benefits. Furthermore, the unbridled deployment of these smart glasses brought several challenging questions for public social interaction when we will have more such devices in our lives. This short article has discussed the Ray-Ban stories’ ethical and privacy issues for social interaction and public places.</jats:p>",0.0
337,10.1177/26349795231221699,A multimodal grammar of AI: Measuring the gains and losses in GenAI,"<jats:p> This paper analyzes the scope of AI (AI) from the perspective of a multimodal grammar. Its focal point is GenAI, a technology that puts so-called Large Language Models to work. The first part of the paper analyzes GenAI, based as it is on the statistical probability of one token (a word or part of a word) following another. If the relation of tokens is meaningful, this is circumstantial and no more, because its mechanisms of statistical analysis eschew any theory of meaning. This is the case not only for the written text that GenAI leverages, but by extension image and multimodal forms of meaning that it can generate. The AI can only work with non-textual forms of meaning after applying language labels, and to that extent is captive not only to the limits of probabilistic statistics but the limits of written language as well. While acknowledging gains arising from the brute statistical power of GenAI, in its second part the paper goes on to map what is lost in its statistical and text-bound approaches to multimodal meaning-making. Our measure of these gains and losses is guided by the concept of grammar, defined here as a theory of the elemental patterns of meaning in the world—not just written text and speech, but also image, space, object, body, and sound. Ironically, a good deal of what is lost by GenAI is computable. The third and final part of the paper briefly discusses educational applications of GenAI. Given both its power and intrinsic limitations, we have been experimenting with the application of GenAI in educational settings and the ways it might be put to pedagogical use. How does a grammatical analysis help us to identify the scope of worthwhile application? Finally, if more of human experience is computable than can be captured in text-bound AI, how might it be possible at the level of code to create a synthesis in which grammatical and multimodal approaches complement GenAI? </jats:p>",7.0
338,10.34135/mmidentity-2023-36,AI as a creator of journalistic content,"<jats:p>The advent of AI (AI) is gradually reshaping society and impacting more and more areas, not excluding media. However, if we want to exploit the potential of this new technological phenomenon as effectively as possible, and in order to maintain established norms of media production, it is necessary to examine the capabilities of AI tools and compare them with human creations. One popular AI tool is GenAI. In media production, it is used, for example, in the creation of journalistic texts, as it is specialized to generate texts in a ‘human way’ based on input data. Therefore, the research paper aims to assess this AI tool in the context of the criteria of press release production as a basic journalistic genre. The object of investigation is to assess and evaluate the fulfilment of the basic criteria set for a quality press release, for two groups of media content – human-generated and GenAI-generated. In the evaluation, the authors will emphasize the fulfillment of the specific criteria formulated in the assignment, noting also the AI’s ability to learn and improve its creations. In its theoretical framework and analytical argumentation, the paper draws on existing knowledge from the literature, including scholarly studies by experts such as H. Pravdová (2016), T. Rončáková (2011), B. Jones et al. (2022), A. Tušer (2010), J. Mistrík (1989), M. Richter (2023), A. Kellerman (2023), B. Dhiman (2023), D. Zagorulko (2023) and others.</jats:p>",-1.0
339,10.1007/s43681-024-00452-3,The digital divide in action: how experiences of digital technology shape future relationships with AI,"<jats:title>Abstract</jats:title><jats:p>The digital divide remains an ongoing societal concern, with digital exclusion shown to have a significantly detrimental impact on people’s quality of life. AI (AI), the latest wave of digitalisation, is being integrated into the fabric of society at an accelerated rate, the speed of which has prompted ethical concerns. Without addressing the digital divide, the AI revolution risks exacerbating the existing consequences of digital exclusion and limiting the potential for all people to reap the benefits provided by AI. To understand the factors that might contribute to experiences of AI, and how these might be related to digital exclusion, we surveyed a diverse online community sample (<jats:italic>N</jats:italic> = 303). We created a novel measure of digital confidence capturing individual levels of awareness, familiarity, and sense of competence with digital technology. Results indicated that measures of digital confidence were predicted by structural, behavioural, and psychological differences, such that women, older people, those on lower salaries, people with less digital access, and those with lower digital well-being, reported significantly less digital confidence. Furthermore, digital confidence significantly moderated the relationship between people’s experiences with everyday AI technologies and their general attitudes towards AI. This understanding of the spill-over effects of digital exclusion onto experiences of AI is fundamental to the articulation and delivery of inclusive AI.</jats:p>",-1.0
340,10.1007/s43681-020-00029-w,The interrelation between data and AI ethics in the context of impact assessments,"<jats:title>Abstract</jats:title><jats:p>In the growing literature on AI (AI) impact assessments, the literature on data protection impact assessments is heavily referenced. Given the relative maturity of the data protection debate and that it has translated into legal codification, it is indeed a natural place to start for AI. In this article, we anticipate directions in what we believe will become a dominant and impactful forthcoming debate, namely, how to conceptualise the relationship between data protection and AI impact. We begin by discussing the value canvas i.e. the ethical principles that underpin data and AI ethics, and discuss how these are instantiated in the context of value trade-offs when the ethics are applied. Following this, we map three kinds of relationships that can be envisioned between data and AI ethics, and then close with a discussion of asymmetry in value trade-offs when privacy and fairness are concerned.</jats:p>",0.0
341,10.3389/frai.2024.1167137,Analyzing global utilization and missed opportunities in debt-for-nature swaps with GenAI,"<jats:p>We deploy a prompt-augmented GPT-4 model to distill comprehensive datasets on the global application of debt-for-nature swaps (DNS), a pivotal financial tool for environmental conservation. Our analysis includes 195 nations and identifies 21 countries that have not yet used DNS before as prime candidates for DNS. A significant proportion demonstrates consistent commitments to conservation finance (0.86 accuracy as compared to historical swaps records). Conversely, 35 countries previously active in DNS before 2010 have since been identified as unsuitable. Notably, Argentina, grappling with soaring inflation and a substantial sovereign debt crisis, and Poland, which has achieved economic stability and gained access to alternative EU conservation funds, exemplify the shifting suitability landscape. The study's outcomes illuminate the fragility of DNS as a conservation strategy amid economic and political volatility.</jats:p>",0.0
342,10.1007/s10676-024-09752-y,Design culture for Sustainable urban AI: Bruno Latour and the search for a different AI urbanism,"<jats:title>Abstract</jats:title><jats:p>The aim of this paper is to investigate the relationship between AI urbanism and sustainability by drawing upon some key concepts of Bruno Latour’s philosophy. The idea of a sustainable AI urbanism - often understood as the juxtaposition of smart and eco urbanism - is here critiqued through a reconstruction of the conceptual sources of these two urban paradigms. Some key ideas of smart and eco urbanism are indicated as incompatible and therefore the fusion of these two paradigms is assessed as an unstable basis for shaping sustainable AI urbanism. The concepts in question - modernity, science and nature – are subsequently redefined following Latour’s philosophical perspective, in an attempt to define a different theoretical basis for a sustainable AI urbanism in the Anthropocene. Finally, the principles of a design philosophy shaped by Latour are used to change the design culture that informs AI urbanism towards a more sustainable practice. This paper constructs and promotes a dialogue between the disciplines of philosophy and urban theory with urban design in the conviction that the principles produced by the former and the practices carried out by the latter must start a biunivocal relationship. The paper reveals that in order to change design culture in the field of AI urbanism, it is necessary to rethink some of the key ideas that inform the Western and modern worldview through novel philosophical reflections.</jats:p>",0.0
343,10.3390/philosophies9030080,"Africa, GenAI, and GenAI Systems: Ethical Benefits, Concerns, and the Need for Governance","<jats:p>This paper examines the impact and implications of GenAI and other GenAI technologies within the African context while looking at the ethical benefits and concerns that are particularly pertinent to the continent. Through a robust analysis of GenAI and other GenAI systems using established approaches for analysing the ethics of emerging technologies, this paper provides unique ethical benefits and concerns for these systems in the African context. This analysis combined approaches such as anticipatory technology ethics (ATE), ethical impact assessment (EIA), and ethical issues of emerging ICT applications with AI (ETICA) with specific issues from the literature. The findings show that GenAI and other GenAI systems raise unique ethical concerns such as bias, intergenerational justice, exploitation of labour and cultural diversity in Africa but also have significant ethical benefits. These ethical concerns and benefits are considered crucial in shaping the design and deployment of GenAI and similar technologies responsibly. It further explores the potential applications of GenAI in critical domain areas such as education, agriculture, and healthcare, thereby demonstrating the transformative possibilities that these technologies can have on Africa. This paper underscores the critical role of AI governance as Africa increasingly adopts GenAI and similar AI systems. It argues that a comprehensive understanding of AI governance is essential not only for maximising the benefits of GenAI systems but also for facilitating a global dialogue. This dialogue aims to foster shared knowledge and insights between the Global North and the Global South, which is important for the development and creation of inclusive and equitable AI policies and practices that can be beneficial for all regions.</jats:p>",-1.0
344,10.1142/s2705078524400022,From Value Realism to Inclusive Ethics: A New Path for Human Rights and AI Development,"<jats:p> This paper critically examines Carlos Montemayor’s human rights-based approach to AI ethics, as presented in his 2023 book The Prospect of a Humanitarian AI. Montemayor proposes that the concept of human rights, grounded in the cognitive needs of human beings, should guide AI development. This paper challenges Montemayor’s moral value-realism (the belief in objective moral values) by arguing for a more inclusive ethical framework that reconciles moral value-realism with skepticism. By proposing an agnostic stance towards moral truths, it suggests a new framework for AI ethics based on subjective intrinsic valuations and shared moral commitments, rather than on objective moral truths. In this proposed framework, without either denying or asserting the existence of absolute values and moral truths, human rights in AI ethics are grounded in collective moral commitments to value human beings intrinsically, rather than in inherent moral truths. This facilitates a more pragmatic and inclusive solution that allows for a diversity of ethical and metaethical perspectives while aligning with the complex nature of AI’s societal integration. The paper concludes that while Montemayor’s work is foundational, it invites further adaptation to develop AI systems that are ethically sound and in harmony with a broad spectrum of human values. </jats:p>",0.0
345,10.3390/healthcare12111082,The Impact of AI (AI) on Midwifery Education: A Scoping Review,"<jats:p>As in other healthcare professions, AI will influence midwifery education. To prepare midwifes for a future where AI plays a significant role in healthcare, educational requirements need to be adapted. This scoping review aims to outline the current state of research regarding the impact of AI on midwifery education. The review follows the framework of Arksey and O’Malley and the PRISMA-ScR. Two databases (Academic Search Premier and PubMed) were searched for different search strings, following defined inclusion criteria, and six articles were included. The results indicate that midwifery practice and education is faced with several challenges as well as opportunities when integrating AI. All articles see the urgent need to implement AI technologies into midwifery education for midwives to actively participate in AI initiatives and research. Midwifery educators need to be trained and supported to use and teach AI technologies in midwifery. In conclusion, the integration of AI in midwifery education is still at an early stage. There is a need for multidisciplinary research. The analysed literature indicates that midwifery curricula should integrate AI at different levels for graduates to be prepared for their future in healthcare.</jats:p>",-1.0
346,10.1108/md-10-2023-1968,GenAI (AI) tools in innovation management: a study on the appropriation of GenAI by innovation managers,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>Using AI to strengthen creativity and problem-solving capabilities of professionals involved in innovation management holds huge potential for improving organizational decision-making. However, there is a lack of research on the use of AI technologies by innovation managers. The study uses the theory of appropriation to explore how specific factors – agile leadership (AL), innovation orientation (IO) and individual creativity (IC) – impact innovation managers' use of GenAI tools, such as GenAI (CGA).</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>The research model is tested through a large-scale survey of 222 Italian innovation managers. Data have been analyzed using structural equation modeling following a two-step approach. First, the measurement model was assessed to ensure the constructs reliability. Subsequently, the structural model was analyzed to draw the conclusions on theorized model relationships and their statistical significance.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>The research findings reveal positive associations between IO and IC with CGA, demonstrating that innovation managers who exhibit strong innovation orientations and higher Individual Creativity are more likely to adopt and personalize GenAI. However, the study did not confirm a significant association between AL and CGA.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>Our findings have important implications for organizations seeking to maximize the potential of GenAI in innovation management. Understanding the factors that drive the adoption and customization of GenAI tools can inform strategies for better integration into the innovation process, thereby leading to enhanced innovation outcomes and improved decision-making processes.</jats:p></jats:sec>",4.0
347,10.3390/educsci14111209,"Modeling Teachers’ Acceptance of GenAI Use in Higher Education: The Role of AI Literacy, Intelligent TPACK, and Perceived Trust","<jats:p>This study delves into the factors that drive teachers’ adoption of GenAI (GenAI) technologies in higher education. Anchored by the technology acceptance model (TAM), the research expands its inquiry by integrating the constructs of intelligent technological pedagogical content knowledge (TPACK), AI literacy, and perceived trust. Data were gathered from a sample of 237 university teachers through a structured questionnaire. The study employed structural equation modeling (SEM) to determine the relationships among the constructs. The results revealed that both AI literacy and perceived ease were the most influential factors affecting teachers’ acceptance of GenAI. Notably, intelligent TPACK and perceived trust were found to be pivotal mediators in this relationship. The findings underscore the importance of fostering AI literacy and adapting intelligent TPACK frameworks to better equip educators in the age of AI. Furthermore, there is a clear need for targeted professional development initiatives focusing on practical training that enhances AI literacy. These programs should provide hands-on experience with GenAI tools, boosting educators’ confidence and ability to integrate them into their teaching practices.</jats:p>",1.0
348,10.1002/aaai.12150,Institute for AI and Fundamental Interactions (IAIFI): Infusing physics intelligence into AI,"<jats:title>Abstract</jats:title><jats:p>The NSF AI Institute for AI and Fundamental Interactions (IAIFI, pronounced /aI‐faI/) is one of the inaugural NSF AI research institutes (<jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""https://iaifi.org"">https://iaifi.org</jats:ext-link>). The IAIFI is enabling physics discoveries and advancing foundational AI through the development of novel AI approaches that incorporate first principles from fundamental physics. By combining state‐of‐the‐art research with early career talent and a growing AI + physics community in the Boston area and beyond, the IAIFI is enabling researchers to develop AI technologies to tackle some of the most challenging problems in physics, and transfer these technologies to the broader AI community. Since trustworthy AI is as important for physics discovery as it is for other applications of AI in society, IAIFI researchers are applying physics principles to develop more robust AI tools and to illuminate existing AI technologies. To cultivate human intelligence, the IAIFI promotes training, education, and public engagement at the intersection of physics and AI. In these ways, the IAIFI is fusing deep learning with deep thinking to gain a deeper understanding of our universe and AI.</jats:p>",0.0
349,10.1007/s10462-024-10973-2,Digital deception: GenAI in social engineering and phishing,"<jats:title>Abstract</jats:title><jats:p>The advancement of AI (AI) and Machine Learning (ML) has profound implications for both the utility and security of our digital interactions. This paper investigates the transformative role of GenAI in Social Engineering (SE) attacks. We conduct a systematic review of social engineering and AI capabilities and use a theory of social engineering to identify three pillars where GenAI amplifies the impact of SE attacks: Realistic Content Creation, Advanced Targeting and Personalization, and Automated Attack Infrastructure. We integrate these elements into a conceptual model designed to investigate the complex nature of AI-driven SE attacks—the GenAI Social Engineering Framework. We further explore human implications and potential countermeasures to mitigate these risks. Our study aims to foster a deeper understanding of the risks, human implications, and countermeasures associated with this emerging paradigm, thereby contributing to a more secure and trustworthy human-computer interaction.</jats:p>",8.0
350,10.3390/socsci13050250,Perpetuation of Gender Bias in Visual Representation of Professions in the GenAI Tools DALL·E and Bing Image Creator,"<jats:p>AI (AI)-based generative imaging systems such as DALL·E, Midjourney, Stable Diffusion, and Adobe Firefly, which work by transforming natural language descriptions into images, are revolutionizing computer vision. In this exploratory and qualitative research, we have replicated requests for images of women in different professions by comparing these representations in previous studies with DALL·E, observing that this model continues to provide in its last version, DALL·E 3, inequitable results in terms of gender. In addition, Bing Image Creator, Microsoft’s free tool that is widely used among the population and runs under DALL·E, has been tested for the first time. It also presents a sexualization of women and stereotypical children’s representations. The results reveal the following: 1. A slight improvement in terms of the presence of women in professions previously shown only with men. 2. They continue to offer biased results in terms of the objectification of women by showing sexualized women. 3. The representation of children highlights another level of gender bias, reinforcing traditional stereotypes associated with gender roles from childhood, which can impact future decisions regarding studies and occupations.</jats:p>",-1.0
351,10.1007/s43681-024-00444-3,The moral decision machine: a challenge for artificial moral agency based on moral deference,"<jats:title>Abstract</jats:title><jats:p>Humans are responsible moral agents in part because they can competently respond to moral reasons. Several philosophers have argued that artificial agents cannot do this and therefore cannot be responsible moral agents. I present a counterexample to these arguments: the ‘Moral Decision Machine’. I argue that the ‘Moral Decision Machine’ responds to moral reasons just as competently as humans do. However, I suggest that, while a hopeful development, this does not warrant strong optimism about ‘artificial moral agency’. The ‘Moral Decision Machine’ (and similar agents) can only respond to moral reasons by deferring to others, and there are good reasons to think this is incompatible with responsible moral agency. While the challenge to artificial moral agency based on moral reasons-responsiveness can be satisfactorily addressed; the challenge based on moral deference remains an open question. The right way to understand the challenge, I argue, is as a route to the claim that artificial agents are unlikely to be responsible moral agents because they cannot be authentic.</jats:p>",0.0
352,10.1007/s44163-022-00043-3,The tech industry hijacking of the AI ethics research agenda and why we should reclaim it,"<jats:title>Abstract</jats:title><jats:p>This paper reflects on the tech industry’s colonization of the AI ethics research field and addresses conflicts of interest in public policymaking concerning AI. The AI ethics research community faces two intertwined challenges: In the first place, we have a tech industry heavily influencing the AI ethics research agenda. Secondly, cleaning up after the tech industry has implied that we have turned to value-driven design methods to bring ethics to AI design. But by framing research questions relevant to a technical practice, we have facilitated the technological solutionism behind the tech industry’s business model. Therefore, this paper takes the first steps to reshape the AI ethics research agenda by suggesting moving toward an emancipatory framework that brings politics to design while, at the same time, bearing in mind that AI is not to be treated as an inevitability. As a research community, we must focus on the repressive power dynamics exacerbated by AI and address challenges facing the vulnerable groups seldom heard, despite the fact that they are the ones most negatively affected by AI initiatives.</jats:p>",0.0
353,10.1017/s1049096524000167,Surveying the Impact of GenAI on Political Science Education,"<jats:title>ABSTRACT</jats:title>
	  <jats:p>Recent applications of new innovations in AI have brought up questions about how this new technology will change the landscape and practices in a wide range of industries and sectors. This article focuses on the impact of generative large language models on teaching, learning, and academic assessment in political science education by analyzing two novel surveys administered by the discipline’s major professional body, the American Political Science Association. We present the results of these surveys and conclude with recommendations.</jats:p>",1.0
354,10.1007/s43681-022-00230-z,Breaking bad news in the era of AI and algorithmic medicine: an exploration of disclosure and its ethical justification using the hedonic calculus,"<jats:title>Abstract</jats:title><jats:p>An appropriate ethical framework around the use of AI (AI) in healthcare has become a key desirable with the increasingly widespread deployment of this technology. Advances in AI hold the promise of improving the precision of outcome prediction at the level of the individual. However, the addition of these technologies to patient–clinician interactions, as with any complex human interaction, has potential pitfalls. While physicians have always had to carefully consider the ethical background and implications of their actions, detailed deliberations around fast-moving technological progress may not have kept up. We use a common but key challenge in healthcare interactions, the disclosure of bad news (likely imminent death), to illustrate how the philosophical framework of the 'Felicific Calculus' developed in the eighteenth century by Jeremy Bentham, may have a timely quasi-quantitative application in the age of AI. We show how this ethical algorithm can be used to assess, across seven mutually exclusive and exhaustive domains, whether an AI-supported action can be morally justified.</jats:p>",6.0
355,10.3390/children10081372,Generative Adversarial Network (GenAI) in Pediatric Radiology: A Systematic Review,"<jats:p>GenAI, especially with regard to the generative adversarial network (GAN), is an important research area in radiology as evidenced by a number of literature reviews on the role of GAN in radiology published in the last few years. However, no review article about GAN in pediatric radiology has been published yet. The purpose of this paper is to systematically review applications of GAN in pediatric radiology, their performances, and methods for their performance evaluation. Electronic databases were used for a literature search on 6 April 2023. Thirty-seven papers met the selection criteria and were included. This review reveals that the GAN can be applied to magnetic resonance imaging, X-ray, computed tomography, ultrasound and positron emission tomography for image translation, segmentation, reconstruction, quality assessment, synthesis and data augmentation, and disease diagnosis. About 80% of the included studies compared their GAN model performances with those of other approaches and indicated that their GAN models outperformed the others by 0.1–158.6%. However, these study findings should be used with caution because of a number of methodological weaknesses. For future GAN studies, more robust methods will be essential for addressing these issues. Otherwise, this would affect the clinical adoption of the GAN-based applications in pediatric radiology and the potential advantages of GAN could not be realized widely.</jats:p>",5.0
356,10.1007/s44244-023-00012-4,The role of GenAI (GAI) in customer personalisation (CP) development in SMEs: a theoretical framework and research propositions,"<jats:title>Abstract</jats:title><jats:p>Based on the dynamic capabilities (DC) theory, the aim of this study is to investigate the contribution of GenAI (GAI) to the development of customer personalisation (CP) within business organisations, particularly SMEs. This paper also explores how the function of GAI in the development of CP is supported by technological advancements like deep learning (DL), smart data (SD), and the Internet of Things (IoT). Using a theoretical framework based on DC theory and an analysis of the literature on GAI, DL, SD, IoT, and CP, the relationship between GAI and CP is theoretically studied. The dependent variable in this theoretical framework is CP, and the independent variable is GAI. Furthermore, while DL and SD just mediate the connection between GAI and CP, IoT moderates the relationship between GAI and SD. Figure 1 presents the theoretical framework and research propositions. On the basis of the constructs in this study, research propositions were developed and discussed. Eight significant research propositions on the relationship between GAI and CP development were developed using the theoretical framework used in this study. According to the suggested theoretical framework and research propositions, context-oriented CP can be created by GAI using DL and SD in conjunction with IoT when high-level customer attributes are retrieved in a structured, accurate, and real-time manner. Additionally, it results in important marketing outcomes including interactive marketing, value co-creation, and consumer loyalty. This study develops a theoretical framework and research propositions that theorise the relationship between GAI and CP which is rooted in literature and also based on DC perspective. The mediating roles of DL and SD on the relationship between GAI and CP, and the moderating role of IoT on the relationship between GAI and SD, provide support to GAI in the development of CP. This study also provides insight into SMEs’ adoption of GAI to generate context-oriented CP that may impact on their marketing development in areas such as interactive marketing, value co-creation, better targeting and customer loyalty.</jats:p>",4.0
357,10.1007/s00146-023-01854-3,"Artists or art thieves? media use, media messages, and public opinion about AI image generators","<jats:title>Abstract</jats:title><jats:p>This study investigates how patterns of media use and exposure to media messages are related to attitudes about AI (AI) image generators. In doing so, it builds on theoretical accounts of media framing and public opinion about science and technology topics, including AI. The analyses draw on data from a survey of the US public (<jats:italic>N</jats:italic> = 1,035) that included an experimental manipulation of exposure to tweets framing AI image generators in terms of real art, artists’ concerns, artists’ outrage, or competing interpretations. The results show that technology news use and science fiction viewing predicted support for AI art but also predicted belief that AI image generators will take jobs and steal art styles from human artists. In addition, the experimental results demonstrate that exposure to specific media messages can influence these responses. The findings carry implications for understanding the future adoption, use, and regulation of AI image generators.</jats:p>",2.0
358,10.1007/s10551-022-05056-7,AI and Declined Guilt: Retailing Morality Comparison Between Human and AI,"<jats:title>Abstract</jats:title><jats:p>Several technological developments, such as self-service technologies and AI (AI), are disrupting the retailing industry by changing consumption and purchase habits and the overall retail experience. Although AI represents extraordinary opportunities for businesses, companies must avoid the dangers and risks associated with the adoption of such systems. Integrating perspectives from emerging research on AI, morality of machines, and norm activation, we examine how individuals morally behave toward AI agents and self-service machines. Across three studies, we demonstrate that consumers’ moral concerns and behaviors differ when interacting with technologies versus humans. We show that moral intention (intention to report an error) is less likely to emerge for AI checkout and self-checkout machines compared with human checkout. In addition, moral intention decreases as people consider the machine less humanlike. We further document that the decline in morality is caused by less guilt displayed toward new technologies. The non-human nature of the interaction evokes a decreased feeling of guilt and ultimately reduces moral behavior. These findings offer insights into how technological developments influence consumer behaviors and provide guidance for businesses and retailers in understanding moral intentions related to the different types of interactions in a shopping environment.</jats:p>",0.0
359,10.2196/42936,Strategies to Improve the Impact of AI on Health Equity: Scoping Review,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>Emerging AI (AI) applications have the potential to improve health, but they may also perpetuate or exacerbate inequities.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>This review aims to provide a comprehensive overview of the health equity issues related to the use of AI applications and identify strategies proposed to address them.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>We searched PubMed, Web of Science, the IEEE (Institute of Electrical and Electronics Engineers) Xplore Digital Library, ProQuest U.S. Newsstream, Academic Search Complete, the Food and Drug Administration (FDA) website, and ClinicalTrials.gov to identify academic and gray literature related to AI and health equity that were published between 2014 and 2021 and additional literature related to AI and health equity during the COVID-19 pandemic from 2020 and 2021. Literature was eligible for inclusion in our review if it identified at least one equity issue and a corresponding strategy to address it. To organize and synthesize equity issues, we adopted a 4-step AI application framework: Background Context, Data Characteristics, Model Design, and Deployment. We then created a many-to-many mapping of the links between issues and strategies.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>In 660 documents, we identified 18 equity issues and 15 strategies to address them. Equity issues related to Data Characteristics and Model Design were the most common. The most common strategies recommended to improve equity were improving the quantity and quality of data, evaluating the disparities introduced by an application, increasing model reporting and transparency, involving the broader community in AI application development, and improving governance.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>Stakeholders should review our many-to-many mapping of equity issues and strategies when planning, developing, and implementing AI applications in health care so that they can make appropriate plans to ensure equity for populations affected by their products. AI application developers should consider adopting equity-focused checklists, and regulators such as the FDA should consider requiring them. Given that our review was limited to documents published online, developers may have unpublished knowledge of additional issues and strategies that we were unable to identify.</jats:p>
          </jats:sec>",3.0
360,10.3390/computation12050101,Unveiling AI-Generated Financial Text: A Computational Approach Using Natural Language Processing and GenAI,"<jats:p>This study is an in-depth exploration of the nascent field of Natural Language Processing (NLP) and GenAI (AI), and it concentrates on the vital task of distinguishing between human-generated text and content that has been produced by AI models. Particularly, this research pioneers the identification of financial text derived from AI models such as GenAI and paraphrasing tools like QuillBot. While our primary focus is on financial content, we have also pinpointed texts generated by paragraph rewriting tools and utilized GenAI for various contexts this multiclass identification was missing in previous studies. In this paper, we use a comprehensive feature extraction methodology that combines TF–IDF with Word2Vec, along with individual feature extraction methods. Importantly, combining a Random Forest model with Word2Vec results in impressive outcomes. Moreover, this study investigates the significance of the window size parameters in the Word2Vec approach, revealing that a window size of one produces outstanding scores across various metrics, including accuracy, precision, recall and the F1 measure, all reaching a notable value of 0.74. In addition to this, our developed model performs well in classification, attaining AUC values of 0.94 for the ‘GPT’ class; 0.77 for the ‘Quil’ class; and 0.89 for the ‘Real’ class. We also achieved an accuracy of 0.72, precision of 0.71, recall of 0.72, and F1 of 0.71 for our extended prepared dataset. This study contributes significantly to the evolving landscape of AI text identification, providing valuable insights and promising directions for future research.</jats:p>",7.0
361,10.54337/jbm.v12i1.8402,Using AI (AI) Generative Technologies For Business Model Design with IDEATe Process: A Speculative Viewpoint,"<jats:p>PurposeAI (AI) and the more recent generative technologies are disrupting many activities related to strategy and operations within organizations. Business model design is no exception. We define business model design as an iterative process involving a combination of creativity, decisions, and tests, consisting of envisioning and creating a business model (for a brand-new activity) or a new business model (for an existing activity), to change an existing situation into a preferred one. In this paper, we discuss the potential impact of generative technologies on the business model design process, highlighting the opportunities and challenges that these technologies present and suggesting some methods for using generative technologies for business model design.
Design/Methodology/ApproachWe build on knowledge about business model design and on documentation from forums, social networks, and media about generative technologies. We also used GenAI platforms to test dozens of prompts related to business model design.
FindingsWe propose the IDEATe process for business model design and identify six major changes in the process or the outcome of business model design that generative technologies can trigger. We also discuss blind spots and risks associated with the use of generative technologies for business model design. Finally, we advance some functions of generative technologies that may support this process.
Originality/ValueInstead of focusing on how generative technologies could change business models, we investigate how these technologies could impact the design of business models. We make propositions to use these technologies properly for business model design.</jats:p>",4.0
362,10.1017/neu.2024.50,Use of GenAI (AI) in psychiatry and mental health care: a systematic review,"<jats:title>Abstract</jats:title>
	  <jats:sec id=""S0924270824000504_as1"">
	    <jats:title>Objectives:</jats:title>
	    <jats:p>Tools based on GenAI (AI) such as GenAI have the potential to transform modern society, including the field of medicine. Due to the prominent role of language in psychiatry, e.g., for diagnostic assessment and psychotherapy, these tools may be particularly useful within this medical field. Therefore, the aim of this study was to systematically review the literature on GenAI applications in psychiatry and mental health.</jats:p>
	  </jats:sec>
	  <jats:sec id=""S0924270824000504_as2"">
	    <jats:title>Methods:</jats:title>
	    <jats:p>We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. The search was conducted across three databases, and the resulting articles were screened independently by two researchers. The content, themes, and findings of the articles were qualitatively assessed.</jats:p>
	  </jats:sec>
	  <jats:sec id=""S0924270824000504_as3"">
	    <jats:title>Results:</jats:title>
	    <jats:p>The search and screening process resulted in the inclusion of 40 studies. The median year of publication was 2023. The themes covered in the articles were mainly mental health and well-being in general – with less emphasis on specific mental disorders (substance use disorder being the most prevalent). The majority of studies were conducted as prompt experiments, with the remaining studies comprising surveys, pilot studies, and case reports. Most studies focused on models that generate language, GenAI in particular.</jats:p>
	  </jats:sec>
	  <jats:sec id=""S0924270824000504_as4"">
	    <jats:title>Conclusions:</jats:title>
	    <jats:p>GenAI in psychiatry and mental health is a nascent but quickly expanding field. The literature mainly focuses on applications of GenAI, and finds that GenAI performs well, but notes that it is limited by significant safety and ethical concerns. Future research should strive to enhance transparency of methods, use experimental designs, ensure clinical relevance, and involve users/patients in the design phase.</jats:p>
	  </jats:sec>",-1.0
363,10.3389/fpubh.2023.1142062,The ethics of advancing AI in healthcare: analyzing ethical considerations for Japan’s innovative AI hospital system,"<jats:p>Public and private investments into developing digital health technologies—including AI (AI)—are intensifying globally. Japan is a key case study given major governmental investments, in part through a Cross-Ministerial Strategic Innovation Promotion Program (SIP) for an “Innovative AI Hospital System.” Yet, there has been little critical examination of the SIP Research Plan, particularly from an ethics approach. This paper reports on an analysis of the Plan to identify the extent to which it addressed ethical considerations set out in the World Health Organization’s 2021 Guidance on the Ethics and Governance of AI for Health. A coding framework was created based on the six ethical principles proposed in the Guidance and was used as the basis for a content analysis. 101 references to aspects of the framework were identified in the Plan, but attention to the ethical principles was found to be uneven, ranging from the strongest focus on the potential benefits of AI to healthcare professionals and patients (<jats:italic>n</jats:italic> = 44; Principle 2), to no consideration of the need for responsive or sustainable AI (<jats:italic>n</jats:italic> = 0; Principle 6). Ultimately, the findings show that the Plan reflects insufficient consideration of the ethical issues that arise from developing and implementing AI for healthcare purposes. This case study is used to argue that, given the ethical complexity of the use of digital health technologies, consideration of the full range of ethical concerns put forward by the WHO must urgently be made visible in future plans for AI in healthcare.</jats:p>",6.0
364,10.62962/001c.74354,Examining the Impact and Challenges of AI (AI) in Healthcare,"<jats:p>AI (AI) is revolutionizing the healthcare sector in various ways, from boosting diagnostic performance and patient management to speeding up clinical operations and lowering medical costs. Medical imaging analysis, medication discovery, patient monitoring, and patient communication and assistance are just a few of the uses for AI-powered solutions. The influence of AI in healthcare is broad, with the potential to change healthcare delivery. Implementing AI in healthcare would enhance patient outcomes, safety, and experience and empower healthcare organizations to function efficiently and effectively. Although there are some reservations regarding the ethical and privacy implications of AI in healthcare, the benefits are evident as technology evolves and progresses. As AI develops extensively and is incorporated into the healthcare business, we may expect to see even more significant and positive outcomes in the coming years. The purpose of this paper is to examine the effects of AI on healthcare and to investigate its potential benefits and challenges.</jats:p>",3.0
365,10.1007/s43681-021-00110-y,"Disruption, technology and the question of (artificial) identity","<jats:title>Abstract</jats:title><jats:p>The current state of human–machine interaction has set forth a process of hybridization of human identity. Technology—and most notably AI—is used as an effective cognitive extender, which enables the extension of human personhood to include artificial elements, leading to the emergence of artificial identity. Discussing—and accommodating—anthropomorphization in human–machine interaction should no longer be the primary focus. Rather, the scope and quality of frameworks in which the hybridization of human identity occurs and evolves has significant ethical implications that pose very pragmatic challenges to users, the industry, and regulators. This paper puts forth a few main principles upon which such a discussion should evolve. We illustrate why disruptiveness can easily turn into human harm when the frameworks facilitating it overlook the human vulnerabilities that arise from hybrid identity, notably the asymmetric and asynchronous relationship between the human and artificial counterparts. Finally, we claim that these new types of vulnerabilities, to which a person is exposed due to the intimate degree of pairing with technology, justifies introducing and protecting artificial identity as well.</jats:p>",8.0
366,10.59982/18294359-24.15-is-09,IMPACT OF AI (AI) ON ECONOMICS AND MANAGEMENT,"<jats:p>The scientific article presents the results of the research on the peculiarities of AI technology’s impact on the transformation of business process management systems, management decision-making, and organizations’ management improvement in all sectors of the economy. The relevance of the work on the selected issue is determined by the formation of a digital economy model in which technology, including AI, contributes to improving the efficiency of management and the economic activities of business entities. The object of the study is AI (AI) technologies. The subject of the study is the impact of AI technologies on management practices and economic activities of commercial organizations. Within the framework of the scientific article, the theoretical and practical aspects of the concept of “AI” are considered. The factors that caused the development of AI technologies in modern economics and management are identified. The main directions of influence of AI technology in the management activities of organizations are analyzed. The prospects and possibilities of AI technology in economics and management are addressed. The actual problems that create threats and barriers to the introduction and effective use of AI technology with the improvement of the management system at the enterprise are identified. The conclusions of the article establish that the impact of AI on the economy and management forms a sustainable foundation for successful digital transformation by increasing the economic efficiency of economic activity and management.</jats:p>",-1.0
367,10.32782/2707-8019/2024-2-9,THE INNOVATIVE IMPACT OF GenAI ON DIGITAL BUSINESS TRANSFORMATION,"<jats:p>In the context of intensifying competition and evolving market dynamics, the deployment of cutting-edge technologies has become not merely a discretionary choice, but an indispensable imperative for any enterprise aspiring to achieve successful growth. GenAI, with its substantial potential for automation, personalisation and optimisation of business processes, is emerging as a highly promising avenue of digital transformation. This study is dedicated to investigating approaches and delineating strategies for aligning GenAI with the requirements of digital business transformation. The research examines the development of AI, with a focus on symbolic AI, machine learning, deep learning and GenAI. In addition, it considers the impact of these developments on business processes. The article identifies the potential benefits and challenges associated with the adaptation of GenAI to the needs of modern business, in the areas of marketing, sales and data analysis. The utilisation of diverse methodologies and techniques, including prompts, fine-tuning, and the incorporation of interactive guidance systems, can enhance the efficacy and precision of GenAI in a business setting, thereby facilitating optimal outcomes in a multitude of tasks. The authors put forth the proposition of employing GenAI technology in conjunction with Retrieval-Augmented Generation, with the objective of enhancing the quality and relevance of responses to user queries. Additionally, they advocate for the utilisation of agents or orchestration tools to provide guidance to models. The successful implementation of GenAI hinges on three key factors: the clear definition of objectives, the selection of suitable tools and technologies, and the assurance of managerial and staff support. The implementation of GenAI will contribute to increased efficiency through the automation of routine tasks, enhanced competitiveness through personalisation and innovation, optimised cost structures that increase profitability, and expanded opportunities for research and development.</jats:p>",4.0
368,10.36548/jtcsst.2023.4.003,GenAI (AI) Educational Pedagogy Development: Conversational AI with User-Centric GenAI4,"<jats:p>In terms of language models, GenAI (GenAI), and more specifically GenAI, offer a significant technological achievement as a revolutionary tool for natural language processing (NLP) and a transformative educational business tool. GenAI users' suggestions have the ability to optimize teaching and learning, thereby having a substantial impact on the educational environment of the twenty-first century. Educational robots are getting easier to access for a number of reasons. The human-robot cooperation that has advanced scientifically in industry 5.0 extreme digital automation, will also probably become a regular aspect of life in the days to come. This study examines the prospective uses of GenAI for NLP synthesis as well as its potential role as a conversational agent in the classroom business. GenAI's capacity to understand and produce language that is human-like by employing NLP to generate semantics was essential to its ability to replicate the most advanced human technology through comprehensive assumptions of  patterns and structures it learns from its training data. With the rise of AI (AI) driven conversational agents, prompt engineering has become an important aspect of digital learning. It is essential to get ready for an AI-dominated future when general and educational technologies combine. The study demonstrated how society may impact and contribute to the development of AI pedagogic learning using an instructional robotics application driven by AI, emphasizing the responsibility of humans as producers to reduce any potential misfortunes. The study   highlights that since GenAI technologies have the potential to drastically change teaching and learning approaches and necessitate new ways of thinking, more research on organizational robotics, with a focus on human collaboration and education, will emerge from the technological concerns raised in this study.</jats:p>",1.0
369,10.53469/jrve.2024.6(06).10,The Impact of the Development of AI with Generative Ability on Education,"<jats:p>One kind of AI technology called GenAI is used to create new text, picture, audio, and video material. It may be used for many different things in education, such creating material, enhancing data, personalizing learning, simulating situations, and providing training. It also raises moral questions about prejudices, veracity, false information, intellectual property, loss of employment, and potential future developments like more realism and responsiveness. Content creation, personalized learning, administrative work automation, interactive learning environments, feedback and evaluation, natural language processing (NLP), forecasting and prediction, and collaborative learning are some of the educational applications of GenAI approaches. These technological advancements are intended to improve educational opportunities, streamline administrative duties, and provide individualized course materials. But there are still issues to be resolved, like protecting data privacy, managing human - AI interaction well, and preventing biases in information produced by AI. The process of creating a GenAI system for teaching includes gathering data, choosing a model, training it, and deploying it. Although scalable, personalized, and engaging learning solutions offered by GenAI hold great promise for revolutionizing education, there are a number of drawbacks that may restrict the technology's applicability and prevent it from being widely used. The difficulty of maintaining and upgrading these systems, ethical and privacy problems, and the caliber and bias of the produced information are examples of technical constraints. The applications, legal frameworks, and social consequences of GenAI will be shaped by its technological limits. To fully realize the benefits of AI in education, issues including data privacy breaches, possible bias in AI systems, and the digital divide must be resolved.</jats:p>",1.0
370,10.1007/s43681-021-00055-2,Perspectives about artificial moral agents,"<jats:title>Abstract</jats:title><jats:p>The pursuit of AMAs is complicated. Disputes about the development, design, moral agency, and future projections for these systems have been reported in the literature. This empirical study explores these controversial matters by surveying (AI) Ethics scholars with the aim of establishing a more coherent and informed debate. Using Q-methodology, we show the wide breadth of viewpoints and approaches to artificial morality. Five main perspectives about AMAs emerged from our data and were subsequently interpreted and discussed: (i) Machine Ethics: The Way Forward; (ii) Ethical Verification: Safe and Sufficient; (iii) Morally Uncertain Machines: Human Values to Avoid Moral Dystopia; (iv) Human Exceptionalism: Machines Cannot Moralize; and (v) Machine Objectivism: Machines as Superior Moral Agents. A potential source of these differing perspectives is the failure of Machine Ethics to be widely observed or explored as an applied ethic and more than a futuristic end. Our study helps improve the foundations for an informed debate about AMAs, where contrasting views and agreements are disclosed and appreciated. Such debate is crucial to realize an interdisciplinary approach to artificial morality, which allows us to gain insights into morality while also engaging practitioners.</jats:p>",0.0
371,10.1001/jama.2023.9630,GenAI in Health Care and Liability Risks for Physicians and Safety Concerns for Patients,"<jats:p>This Viewpoint discusses the potential use of GenAI (AI) in medical care and the liability risks for physicians using the technology, as well as offers suggestions for safeguards to protect patients.</jats:p>",3.0
372,10.1007/s00146-020-00992-2,"The Chinese approach to AI: an analysis of policy, ethics, and regulation","<jats:title>Abstract</jats:title><jats:p>In July 2017, China’s State Council released the country’s strategy for developing AI (AI), entitled ‘New Generation AI Development Plan’ (新一代人工智能发展规划). This strategy outlined China’s aims to become the world leader in AI by 2030, to monetise AI into a trillion-yuan (ca. 150 billion dollars) industry, and to emerge as the driving force in defining ethical norms and standards for AI. Several reports have analysed specific aspects of China’s AI policies or have assessed the country’s technical capabilities. Instead, in this article, we focus on the socio-political background and policy debates that are shaping China’s AI strategy. In particular, we analyse the main strategic areas in which China is investing in AI and the concurrent ethical debates that are delimiting its use. By focusing on the policy backdrop, we seek to provide a more comprehensive and critical understanding of China’s AI policy by bringing together debates and analyses of a wide array of policy documents.</jats:p>",0.0
373,10.1093/bjrai/ubae012,"Clinical applications of GenAI in radiology: image translation, synthesis, and text generation","<jats:title>Abstract</jats:title>
               <jats:p>GenAI (AI) has enabled tasks in radiology, including tools for improving image quality. Recently, new hotspots have emerged, such as intra- or inter-modal image translation, task-specific image synthesis, and text generation. Advances in GenAI have facilitated the move towards low-dose, cost-effective, and high-quality radiological image acquisition. Large language models can aid radiologists by generating professional answers and facilitating patient-physician communications. However, radiologists must be aware of potential inaccuracies in the generated content and should only use such tools after rigorous validation of their performance.</jats:p>",-1.0
374,10.70315/uloap.ulahu.2024.0102007,Explainability Imperative of GenAI Navigating the Moral Dilemma of AI in Nigeria and Charting a Path for the Future,"<jats:p>This paper explores the explanability imperative in the context of GenAI (GAI) and its crucial role in addressing the concerns posed by AI technology in Nigeria. This underscores the ethical necessity for AI systems, especially generative ones to provide clear and understandable explanations for their decisions and actions. Although the advent of GenAI undoubtedly heralds the future and however, has also exposed Nigerian society to new vulnerabilities that seemingly are detrimental to our epistemic agency and peaceful political settings. Employing the phenomenological method of philosophical inquiry here, we discovered that this new technology has posed big threats to the future world, and that Nigeria falls amongst this new technology users. To navigate the moral dilemma caused by GenAI, this paper suggests many proactive approaches like the development of localized AI explainability standards, the regulatory frameworks, and educational initiatives to promote awareness and understanding of AI systems in Nigeria. By prioritizing the Explanability Imperative, Nigeria can chart a path towards a future whereby AI technologies aligned with societal values, upholds standard education, and as well contributes positively to the nation’s development. This paper encapsulates the importance of AI explainability in Nigeria’s AI landscape and its potential to shape a more ethically responsible and transparent AI future.</jats:p>",-1.0
375,10.1017/s089267941900011x,The Future Impact of AI on Humans and Human Rights,"<jats:title>Abstract</jats:title><jats:p>What are the implications of AI (AI) on human rights in the next three decades? Precise answers to this question are made difficult by the rapid rate of innovation in AI research and by the effects of human practices on the adaption of new technologies. Precise answers are also challenged by imprecise usages of the term “AI.” There are several types of research that all fall under this general term. We begin by clarifying what we mean by AI. Most of our attention is then focused on the implications of artificial general intelligence (AGI), which entail that an algorithm or group of algorithms will achieve something like superintelligence. While acknowledging that the feasibility of superintelligence is contested, we consider the moral and ethical implications of such a potential development. What do machines owe humans and what do humans owe superintelligent machines?</jats:p>",0.0
376,10.1177/14614448241234040,Impact of misinformation from GenAI on user information processing: How people understand misinformation from GenAI,"<jats:p> This study examines the impact of AI (AI) on the ways in which users process and respond to misinformation in GenAI (GenAI) contexts. Drawing on the heuristic–systematic model and the concept of diagnosticity, our approach examines a cognitive model for processing misinformation in GenAI. The study’s findings revealed that users with a high-heuristic processing mechanism, which affects positive diagnostic perception, were more likely to proactively discern misinformation than users with low-heuristic processing and low-perceived diagnosticity. When exposed to misinformation from GenAI, users’ perceived diagnosticity of misinformation can be accurately predicted by the ways in which they perform heuristic systematic evaluations. With this focus on misinformation processing, this study provides theoretical insights and relevant recommendations for firms to be more resilient in protecting users from the detrimental impacts of misinformation. </jats:p>",-1.0
377,10.1142/s2705078524400034,Decolonizing AI: Implementing <i>Humanitarian AI</i>,"<jats:p> In this article, we critically examine the implementation of what Carlos Montemayor calls Humanitarian AI from a decolonizing perspective. We highlight the dichotomy of optimism and fear surrounding AI, elucidating its potential to address fundamental human problems and the risks of monopolistic control. We critique Montemayor’s proposal to align AI with a human rights framework, arguing that it insufficiently addresses global inequalities. Our tripartite analysis focuses on the distribution of AI resources, language inclusion, and content diversity to ensure AI benefits all humanity. We emphasize the need for equitable access to AI, linguistic diversity in AI training data, and the preservation of marginalized epistemologies. We advocate for strategies to mitigate environmental impacts and avoid cultural imperialism disguised as altruism, calling for a balanced approach between private sector innovation and state regulation to foster a truly humanitarian AI. </jats:p>",0.0
378,10.1186/s12910-023-00917-w,Trustworthy AI and ethical design: public perceptions of trustworthiness of an AI-based decision-support tool in the context of intrapartum care,"<jats:title>Abstract</jats:title><jats:sec>
                <jats:title>Background</jats:title>
                <jats:p>Despite the recognition that developing AI (AI) that is trustworthy is necessary for public acceptability and the successful implementation of AI in healthcare contexts, perspectives from key stakeholders are often absent from discourse on the ethical design, development, and deployment of AI. This study explores the perspectives of birth parents and mothers on the introduction of AI-based cardiotocography (CTG) in the context of intrapartum care, focusing on issues pertaining to trust and trustworthiness.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Methods</jats:title>
                <jats:p>Seventeen semi-structured interviews were conducted with birth parents and mothers based on a speculative case study. Interviewees were based in England and were pregnant and/or had given birth in the last two years. Thematic analysis was used to analyze transcribed interviews with the use of NVivo. Major recurring themes acted as the basis for identifying the values most important to this population group for evaluating the trustworthiness of AI.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Results</jats:title>
                <jats:p>Three themes pertaining to the perceived trustworthiness of AI emerged from interviews: (1) trustworthy AI-developing institutions, (2) trustworthy data from which AI is built, and (3) trustworthy decisions made with the assistance of AI. We found that birth parents and mothers trusted public institutions over private companies to develop AI, that they evaluated the trustworthiness of data by how representative it is of all population groups, and that they perceived trustworthy decisions as being mediated by humans even when supported by AI.</jats:p>
              </jats:sec><jats:sec>
                <jats:title>Conclusions</jats:title>
                <jats:p>The ethical values that underscore birth parents and mothers’ perceptions of trustworthy AI include fairness and reliability, as well as practices like patient-centered care, the promotion of publicly funded healthcare, holistic care, and personalized medicine. Ultimately, these are also the ethical values that people want to protect in the healthcare system. Therefore, trustworthy AI is best understood not as a list of design features but in relation to how it undermines or promotes the ethical values that matter most to its end users. An ethical commitment to these values when creating AI in healthcare contexts opens up new challenges and possibilities for the design and deployment of AI.</jats:p>
              </jats:sec>",6.0
379,10.3389/frai.2024.1415782,Adolescents’ use and perceived usefulness of GenAI for schoolwork: exploring their relationships with executive functioning and academic achievement,"<jats:p>In this study, we aimed to explore the frequency of use and perceived usefulness of LLM GenAI chatbots (e.g., GenAI) for schoolwork, particularly in relation to adolescents’ executive functioning (EF), which includes critical cognitive processes like planning, inhibition, and cognitive flexibility essential for academic success. Two studies were conducted, encompassing both younger (Study 1: <jats:italic>N</jats:italic> = 385, 46% girls, mean age 14 years) and older (Study 2: <jats:italic>N</jats:italic> = 359, 67% girls, mean age 17 years) adolescents, to comprehensively examine these associations across different age groups. In Study 1, approximately 14.8% of participants reported using GenAI, while in Study 2, the adoption rate among older students was 52.6%, with GenAI emerging as the preferred tool among adolescents in both studies. Consistently across both studies, we found that adolescents facing more EF challenges perceived GenAI as more useful for schoolwork, particularly in completing assignments. Notably, academic achievement showed no significant associations with AI usage or usefulness, as revealed in Study 1. This study represents the first exploration into how individual characteristics, such as EF, relate to the frequency and perceived usefulness of LLM GenAI chatbots for schoolwork among adolescents. Given the early stage of GenAI chatbots during the survey, future research should validate these findings and delve deeper into the utilization and integration of GenAI into educational settings. It is crucial to adopt a proactive approach to address the potential challenges and opportunities associated with these emerging technologies in education.</jats:p>",1.0
380,10.36548/jaicn.2024.3.003,Empowering Education by Developing and Evaluating GenAI-Powered Tutoring System for Enhanced Student Learning,"<jats:p>Personalized learning has always been a dream for schools, educators, and students but until recently, educators didn’t have the time or resources to implement it on a large scale. With the advancements in AI, GenAI can automate many of a teacher’s core tasks, such as creating lesson resources. providing lesson structures and key talking points, designing infographics, creating slideshows, and converting text into videos and images. This study details the development and evaluation of an AI-powered tutoring system designed to enhance student learning experiences. Motivated by the transformative potential of AI in education, the research aims to utilize large language models, including OpenAI, to create a personalized and adaptive learning environment. The research is a two-phase approach, involving a comprehensive literature review, problem definition, and AI integration in the Research Phase, followed by design, prototyping, and testing in the Design and Development Phase. The course creation workflow emphasizes the collaborative efforts of human tutors and AI algorithms using the GPT-3.5-Turbo model. The study identified the potential improvement in education where the course has been created by AI including the image generated by DALLE-3 and contributing to the evolving landscape of AI-assisted education using the text-to-voice, an automatic speech recognition system by Whisper, offering an innovative and transformative learning experience for students and tutors. The course content has question-answering chatbots where the students can ask any questions related to the topic while learning.</jats:p>",1.0
381,10.3390/heritage7030070,Will AI Affect How Cultural Heritage Will Be Managed in the Future? Responses Generated by Four genAI Models,"<jats:p>GenAI (genAI) language models have become firmly embedded in public consciousness. Their abilities to extract and summarise information from a wide range of sources in their training data have attracted the attention of many scholars. This paper examines how four genAI large language models (GenAI, GPT4, DeepAI, and Google Bard) responded to prompts, asking (i) whether AI would affect how cultural heritage will be managed in the future (with examples requested) and (ii) what dangers might emerge when relying heavily on genAI to guide cultural heritage professionals in their actions. The genAI systems provided a range of examples, commonly drawing on and extending the status quo. Without a doubt, AI tools will revolutionise the execution of repetitive and mundane tasks, such as the classification of some classes of artifacts, or allow for the predictive modelling of the decay of objects. Important examples were used to assess the purported power of genAI tools to extract, aggregate, and synthesize large volumes of data from multiple sources, as well as their ability to recognise patterns and connections that people may miss. An inherent risk in the ‘results’ presented by genAI systems is that the presented connections are ‘artifacts’ of the system rather than being genuine. Since present genAI tools are unable to purposively generate creative or innovative thoughts, it is left to the reader to determine whether any text that is provided by genAI that is out of the ordinary is meaningful or nonsensical. Additional risks identified by the genAI systems were that some cultural heritage professionals might use AI systems without the required level of AI literacy and that overreliance on genAI systems might lead to a deskilling of general heritage practitioners.</jats:p>",7.0
382,10.37867/te140127,IMPACT OF AI (AI) ON EDUCATION: CHANGING PARADIGMS AND APPROACHES,"<jats:p>AI (AI) technology is to make human life easy and trouble-free and contribute to the advancement of human development. AI is a driving technological force of the twenty-first century and it has been a centre of discussion in technological innovations for its unlimited potential to alter the scenario of social interaction through resolving social challenges and virtually transform every industry. Education is the top priority of present society because it is a fundamental human right that builds peace and drives sustainable development across the world. The integration and application of AI in the classrooms will make teaching and learning effective by supporting teachers and learners in the process through the usage of robotic technology and sensors. AI-based technology facilitates inclusive and equitable quality education along with ensuring universal access to life-long learning for all across the world. The technology of AI has been advanced and sophisticated that can recognize the gesture of the students and understand their mood and ease during the lecture even it can read facial expressions and posture of the students to understand difficulties and problems they are facing in the lecture and recommends altering the lesson. AI technology-based assessment system can be used to assess students' knowledge, understanding, skills such as collaboration and persistence and characteristics such as confidence and motivation etc. AI technology has developed speech-to-text transcription, predictive text and facial recognition promising an inclusive future for all learners.</jats:p>",1.0
383,10.17816/dd430356,AI ethics code in healthcare. Sustainability of AI systems: Why do we talk about their impact on the environment?,"<jats:p>Environmental problems have a tremendous impact on the entire world population, particularly on human health, which plays a leading role in individual well-being. Environmental pollution, according to some estimates, kills approximately 9 million people every year. The introduction of AI (AI) systems in many areas has enormous potential in reducing human impact on the environment; however, such systems have negative effects. The potential of AI systems to improve healthcare is inextricably linked to the ethical challenges posed by the complexity of these systems and their impact on the lives and health of communities, patients, and staff. In addition to aspects that relate directly to the algorithms, data, and clinical application of AI systems, long-term risks exist that are not obvious at first glance. One of these risks is the negative impact of AI systems on the environment, which may harm human health indirectly. AI systems are more than software, having physical components that are necessary for their functioning, such as processors, memory, and sensors. The manufacture and the energy consumption of the components has a profound effect on the environment. One study showed that when a single AI algorithm is trained, carbon emissions may reach values corresponding to the total carbon emissions from five cars lifetime.&#x0D;
This study analyzes existing literature linking the development of AI systems, especially in healthcare, to their effects on the environment. The study is intended to complement the emerging AI Ethics Code for healthcare, specifically the principles of sustainability that will be included in this code.&#x0D;
The study concludes that the environmental impact of AI systems should be considered when formulating ethical standards for AI in healthcare. These standards must be considered during the development, testing, and application phases of AI systems. All the people involved in the creation and use of AI systems (developers, physicians, and regulators) must monitor the environmental impact and minimize the environmental consequences of such systems at all stages of their existence. This principle calls for minimizing negative impacts, improving the energy efficiency, and disposing physical components in strict compliance with current legislation. Moreover, the rapid development of AI systems and the ethical dilemmas require that solutions be proposed jointly and ethical standards be developed in a manner that is consistent and sensitive to emerging technologies.</jats:p>",-1.0
384,10.7717/peerj-cs.1099,AI-SPedia: a novel ontology to evaluate the impact of research in the field of AI,"<jats:sec>
               <jats:title>Background</jats:title>
               <jats:p>Sharing knowledge such as resources, research results, and scholarly documents, is of key importance to improving collaboration between researchers worldwide. Research results from the field of AI (AI) are vital to share because of the extensive applicability of AI to several other fields of research. This has led to a significant increase in the number of AI publications over the past decade. The metadata of AI publications, including bibliometrics and altmetrics indicators, can be accessed by searching familiar bibliographical databases such as Web of Science (WoS), which enables the impact of research to be evaluated and identify rising researchers and trending topics in the field of AI.</jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title>Problem description</jats:title>
               <jats:p>In general, bibliographical databases have two limitations in terms of the type and form of metadata we aim to improve. First, most bibliographical databases, such as WoS, are more concerned with bibliometric indicators and do not offer a wide range of altmetric indicators to complement traditional bibliometric indicators. Second, the traditional format in which data is downloaded from bibliographical databases limits users to keyword-based searches without considering the semantics of the data.</jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title>Proposed solution</jats:title>
               <jats:p>To overcome these limitations, we developed a repository, named AI-SPedia. The repository contains semantic knowledge of scientific publications concerned with AI and considers both the bibliometric and altmetric indicators. Moreover, it uses semantic web technology to produce and store data to enable semantic-based searches. Furthermore, we devised related competency questions to be answered by posing smart queries against the AI-SPedia datasets.</jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title>Results</jats:title>
               <jats:p>The results revealed that AI-SPedia can evaluate the impact of AI research by exploiting knowledge that is not explicitly mentioned but extracted using the power of semantics. Moreover, a simple analysis was performed based on the answered questions to help make research policy decisions in the AI domain. The end product, AI-SPedia, is considered the first attempt to evaluate the impacts of AI scientific publications using both bibliometric and altmetric indicators and the power of semantic web technology.</jats:p>
            </jats:sec>",-1.0
385,10.3390/fintech2030024,Developing an Ethical Framework for Responsible AI (AI) and Machine Learning (ML) Applications in Cryptocurrency Trading: A Consequentialism Ethics Analysis,"<jats:p>The rise in AI (AI) and machine learning (ML) in cryptocurrency trading has precipitated complex ethical considerations, demanding a thorough exploration of responsible regulatory approaches. This research expands upon this need by employing a consequentialist theoretical framework, emphasizing the outcomes of AI and ML’s deployment within the sector and its effects on stakeholders. Drawing on critical case studies, such as SBF and FTX, and conducting an extensive review of relevant literature, this study explores the ethical implications of AI and ML in the context of cryptocurrency trading. It investigates the necessity for novel regulatory methods that address the unique characteristics of digital assets alongside existing legalities, such as those about fraud and insider trading. The author proposes a typology framework for AI and ML trading by comparing consequentialism to other ethical theories applicable to AI and ML use in cryptocurrency trading. By applying a consequentialist lens, this study underscores the significance of balancing AI and ML’s transformative potential with ethical considerations to ensure market integrity, investor protection, and overall well-being in cryptocurrency trading.</jats:p>",-1.0
386,10.1007/s11023-024-09694-w,Mapping the Ethics of GenAI: A Comprehensive Scoping Review,"<jats:title>Abstract</jats:title><jats:p>The advent of GenAI and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks. These risks often differ from those associated with traditional discriminative machine learning. To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of GenAI, including especially large language models and text-to-image models. Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature. The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding fairness, safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others. We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios.</jats:p>",-1.0
387,10.1007/s00146-024-02020-z,Drawing the full picture on diverging findings: adjusting the view on the perception of art created by AI,"<jats:title>Abstract</jats:title><jats:p>AI is becoming increasingly prevalent in creative fields that were thought to be exclusively human. Thus, it is non-surprising that a negative bias toward AI-generated artwork has been proclaimed. However, results are mixed. Studies that have presented AI-generated and human-created images simultaneously have detected a bias, but most studies in which participants saw either AI-generated or human-created images have not. Therefore, we propose that the bias arises foremost in a competitive situation between AI and humans. In a sample of <jats:italic>N</jats:italic> = 952 participants, we show that different evaluations emerge only when AI-generated and human-created pieces of art are presented simultaneously. Importantly, we demonstrate that AI art is not devalued, but rather, human art is upvalued, indicating the existence of a positive bias toward humans, rather than a negative bias. Further, we show that attitudes toward AI and empathy partially explain the different valuations of AI and human art in competitive situations.</jats:p>",2.0
388,10.1609/aimag.v41i4.5296,AI for Social Impact: Learning and Planning in the Data‐to‐Deployment Pipeline,"<jats:p><jats:italic>With the maturing of AI (AI) and multiagent systems research, we have a tremendous opportunity to direct these advances toward addressing complex societal problems. In pursuit of this goal of AI for social impact, we as AI researchers must go beyond improvements in computational methodology; it is important to step out in the field to demonstrate social impact. To this end, we focus on the problems of public safety and security, wildlife conservation, and public health in low‐resource communities, and present research advances in multiagent systems to address one key cross‐cutting challenge: how to effectively deploy our limited intervention resources in these problem domains. We present case studies from our deployments around the world as well as lessons learned that we hope are of use to researchers who are interested in AI for social impact. In pushing this research agenda, we believe AI can indeed play an important role in fighting social injustice and improving society.</jats:italic></jats:p>",-1.0
389,10.3390/ai5040095,"Digital Technologies Impact on Healthcare Delivery: A Systematic Review of AI (AI) and Machine-Learning (ML) Adoption, Challenges, and Opportunities","<jats:p>Recent significant advances in the healthcare industry due to AI (AI) and machine learning (ML) have been shown to revolutionize healthcare delivery by improving efficiency, accuracy, and patient outcomes. However, these technologies can face significant challenges and ethical considerations. This systematic review aimed to gather and synthesize the current knowledge on the impact of AI and ML adoption in healthcare delivery, with its associated challenges and opportunities. This study adhered to the PRISMA guidelines. Articles from 2014 to 2024 were selected from various databases using specific keywords. Eligible studies were included after rigorous screening and quality assessment using checklist tools. Themes were identified through data analysis and thematic analysis. From 4981 articles screened, a data synthesis of nine eligible studies revealed themes, including productivity enhancement, improved patient care through decision support and precision medicine, legal and policy challenges, technological considerations, organizational and managerial aspects, ethical concerns, data challenges, and socioeconomic implications. There exist significant opportunities, as well as substantial challenges and ethical concerns, associated with integrating AI and ML into healthcare delivery. Implementation strategies must be carefully designed, considering technical, ethical, and social factors.</jats:p>",3.0
390,10.3390/ai5020028,AI in Healthcare: GenAI and Beyond,"<jats:p>AI (AI), the simulation of human intelligence processes by machines, is having a growing impact on healthcare [...]</jats:p>",3.0
391,10.1007/s11569-024-00457-6,"An Integrated Embodiment Concept Combines Neuroethics and AI Ethics – Relational Perspectives on AI, Emerging Neurotechnologies and the Future of Work","<jats:title>Abstract</jats:title><jats:p>Applications of AI (AI) bear great transformative potential in the economic, technological and social sectors, impacting especially future work environments. Ethical regulation of AI requires a relational understanding of the technology by relevant stakeholder groups such as researchers, developers, politicians, civil servants, affected workers or other users applying AI in their work processes. The purpose of this paper is to support relational AI discourse for an improved ethical framing and regulation of the technology. The argumentation emphasizes a widespread reembodied understanding of AI technology as critical requirement for capable ethical and regulatory frameworks. A sociotechnical perspective encourages the material interpretation of AI as reembodied adaptation of biological intelligence. Reviewing Cartesian dualism as motivating the disembodiment of human intelligence for its transfer to machines, the argumentation develops an integrated embodiment concept of AI in its mechanistic, naturalistic, combined AI and neuroethical, and relational contexts. This concept is discussed in relation to basic phenomenological and postphenomenological assumptions, and is applied to the example of AI-based neurotechnology potentially disrupting future work processes. Strengthening a human-centered approach, the presented concept for a reembodied understanding of AI technology enables better integrated ethical and regulatory debates, and improves social discourse and human agency in developing and regulating AI technology.</jats:p>",0.0
392,10.54105/ijainn.b1084.04020224,Bias in Text Generative Open AI,"<jats:p>The rise of text generation models, especially those powered by advanced deep learning architectures like Open AI’s GPT-3, has unquestionably transformed various natural language processing applications. However, these models have recently faced examination due to their inherent biases, often evident in the generated text. This paper critically examines the issue of bias in text generation models, exploring the challenges posed, the ethical implications it entails, and the potential strategies to mitigate bias. Firstly, we go through the causes of the origin of the bias, ways to minimize it, and mathematical representation of Bias.</jats:p>",7.0
393,10.3389/frai.2024.1460651,Opportunities and challenges of using GenAI to personalize educational assessment,"<jats:p>In line with the positive effects of personalized learning, personalized assessments are expected to maximize learner motivation and engagement, allowing learners to show what they truly know and can do. Considering the advances in GenAI (GenAI), in this perspective article, we elaborate on the opportunities of integrating GenAI into personalized educational assessments to maximize learner engagement, performance, and access. We also draw attention to the challenges of integrating GenAI into personalized educational assessments regarding its potential risks to the assessment’s core values of validity, reliability, and fairness. Finally, we discuss possible solutions and future directions.</jats:p>",1.0
394,10.34104/ijma.023.0074090,The Impact of AI (AI) on Customer Relationship Management: A Qualitative Study,"<jats:p>Ever since the commercialization of the Internet in the '90s, technology has been evolving faster than ever with the advent of cloud computing, social media, ubiquitous mobile devices, the Internet of Things (IoT), blockchain, and more. A staggering number of three billion internet users, five billion mobile users, and six billion devices are now connected through this massive global network of networks, facilitating customer information exchange and interaction never before seen in history. Driven by recent technological advances in computing power, big data, high-speed internet connection, and easier access to models built with advanced algorithms, AI (AI) is the next wave of innovation, which has already come into widespread awareness in the consumer world with the emergence of virtual assistants and chatbots (e.g., Amazon's Alexa, Apple's Siri, Google's Assistant), image recognition (e.g., Facebook Photos, Google ImageNet), personalized recommendations (e.g., Netflix, Amazon) and autonomous driving (e.g., Tesla, Google Waymo). This qualitative research study intends to learn about the impact of AI on customer relationship management (CRM), specifically in the area of customer service of problem resolution. Most prior research focuses on the AI technologies leveraged in CRM systems, such as machine learning, natural language processing, voice recognition, chatbots, data analytics, and cloud infrastructure. Few extant studies have used a qualitative research methodology to gather data from industry experts to truly understand the impact of AI technologies on customer relationship management, especially in the area of customer service and problem resolution. This study aims to fill this research gap. This research contributes to the literature on AI in the context of CRM and is of value to both academics and practitioners as it provides a detailed analysis and documentation of the impact of AI on the customer service domain.</jats:p>",4.0
395,10.3390/ijfs8030045,Industry 4.0 in Finance: The Impact of AI (AI) on Digital Financial Inclusion,"<jats:p>This study sought to investigate the impact of AI on digital financial inclusion. Digital financial inclusion is becoming central in the debate on how to ensure that people who are at the lower levels of the pyramid become financially active. Fintech companies are using AI and its various applications to ensure that the goal of digital financial inclusion is realized that is to ensure that low-income earners, the poor, women, youths, small businesses participate in the mainstream financial market. This study used conceptual and documentary analysis of peer-reviewed journals, reports and other authoritative documents on AI and digital financial inclusion to assess the impact of AI on digital financial inclusion. The present study discovered that AI has a strong influence on digital financial inclusion in areas related to risk detection, measurement and management, addressing the problem of information asymmetry, availing customer support and helpdesk through chatbots and fraud detection and cybersecurity. Therefore, it is recommended that financial institutions and non-financial institutions and governments across the world adopt and scale up the use of AI tools and applications as they present benefits in the quest to ensure that the vulnerable groups of people who are not financially active do participate in the formal financial market with minimum challenges and maximum benefits.</jats:p>",-1.0
396,10.35295/osls.iisl/0000-0000-0000-1273,The ethics of AI: An analysis of ethical frameworks disciplining AI in justice and other contexts of application,"<jats:p>The recent introduction of AI tools in the justice sector poses several ethical implications as risks for judges’ independence and for procedural transparency, and discrimination biases. By developing ethical frameworks governing AI application, private and public agents have been increasingly dealing with risks pertaining to the use of AI. By inventorying and analyzing a set of ethical documents through content analysis, this study highlights the ethical implications involved in the application of AI. Moreover, by investigating the CEPEJ Charter (European Commission for the Effectiveness of Justice of the Council of Europe), the unique ethical document focusing on AI in justice, we were able to clarify potential differences between justice and other contexts of AI application with respect to risks prospected and the protection of ethical principles. The analysis confirms that the discipline of AI is a complex subject that involves very different aspects and therefore needs a broad focus on all contexts of application.</jats:p>",0.0
397,10.1142/s2705078522500060,Artificial Agential Intelligence,"<jats:p> Since AI (AI) emerged in the mid-20th century, it has incurred many theoretical criticisms (Dreyfus, H. [1972] What Computers Can’t Do (MIT Press, New York); Dreyfus, H. [1992] What Computers Still Can’t Do (MIT Press, New York); Searle, J. [1980] Minds, brains and programs, Behav. Brain Sci. 3, 417–457; Searle, J. [1984] Minds, Brains and Sciences (Harvard University Press, Cambridge, MA); Searle, J. [1992] The Rediscovery of the Mind (MIT Press, Cambridge, MA); Fodor, J. [2002] The Mind Doesn’t Work that Way: The Scope and Limits of Computational Psychology (MIT Press, Cambridge, MA).). The technical improvements of machine learning and deep learning, though, have been continuing and many breakthroughs have occurred recently. This makes theoretical considerations urgent again: can this new wave of AI fare better than its precursors in emulating or even having human-like minds? I propose a cautious yet positive hypothesis: current AI might create human-like mind, but only if it incorporates certain conceptual rewiring: it needs to shift from a task-based to an agent-based framework, which can be dubbed “Artificial Agential Intelligence” (AAI). It comprises practical reason (McDowell, J. [1979] Virtue and reason, Monist 62(3), 331–350; McDowell, J. [1996] Mind and World (Harvard University Press, Cambridge, MA)), imaginative understanding (Campbell, J. [2020] Causation in Psychology (Harvard University Press, Cambridge, MA)), and animal knowledge (Sosa, E. [2007] A Virtue Epistemology: Apt Belief and Reflective Knowledge, volume 1 (Oxford University Press, Oxford, UK); Sosa, E. [2015] Judgment and Agency (Oxford University Press, Cambridge, MA)). Moreover, I will explore whether and in what way neuroscience-inspired AI and predictive coding (Hassabis, D., Kumaran, D., Summerfield, C., &amp; Botvinick, M. [2017] Neuroscience-inspired AI, Neuron 95(2), 245–258) can help carry out this project. </jats:p>",0.0
398,10.51191/issn.2637-1898.2019.2.2.24,"AI and Aesthetics of Music: Intelligent Anarchy.
diagramic notebook on: AI-IA relations",<jats:p>An experimental diagramic article on the relationship between AI and the aesthetic of music.</jats:p>,2.0
399,10.61643/c16282,Emotion and Inspiration through GenAI Art,"<jats:p>Art inspires emotion. The process of creation invigorates the senses and keeps the researcher on fire. To create artistic masterpieces using words through natural language processing further stimulates the mind and helps to overcome writer’s block. An AI tool interprets the words and transforms them into artistic expressions that feel like magic. Yet, AI art is not without consequences. Critics challenge the digital rights of AI-generated images, noting that they are derivative works. This paper examines the joy of creation as well as the copyright challenges creators face when writing detailed prompts to generate amazing artwork. The role of affective computing illustrates the relationship between the designer and the AI tool while ethical concerns remind designers to use caution in their prompts.  The article examines the architecture of a prompt, the characteristics to enhance it, and examples of GenAI art. Additional content includes references to handouts, art catalogs, and legal opinions regarding the copyright debate. The article concludes with a recommendation with respect to the intellectual property rights of artists and their creative work.</jats:p>",2.0
400,10.2196/48392,The GenAI (GenAI) Revolution Has Made AI Approachable for Medical Professionals,"<jats:p>In November 2022, OpenAI publicly launched its large language model (LLM), GenAI, and reached the milestone of having over 100 million users in only 2 months. LLMs have been shown to be useful in a myriad of health care–related tasks and processes. In this paper, I argue that attention to, public access to, and debate about LLMs have initiated a wave of products and services using GenAI (AI), which had previously found it hard to attract physicians. This paper describes what AI tools have become available since the beginning of the GenAI revolution and contemplates how it they might change physicians’ perceptions about this breakthrough technology.</jats:p>",3.0
401,10.33847/2712-8148.4.1_4,Unlocking the Black Box: Explainable AI (XAI) for Trust and Transparency in AI Systems,"<jats:p>Explainable AI (XAI) has emerged as a critical field in AI research, addressing the lack of transparency and interpretability in complex AI models. This conceptual review explores the significance of XAI in promoting trust and transparency in AI systems. The paper analyzes existing literature on XAI, identifies patterns and gaps, and presents a coherent conceptual framework. Various XAI techniques, such as saliency maps, attention mechanisms, rule-based explanations, and model-agnostic approaches, are discussed to enhance interpretability. The paper highlights the challenges posed by black-box AI models, explores the role of XAI in enhancing trust and transparency, and examines the ethical considerations and responsible deployment of XAI. By promoting transparency and interpretability, this review aims to build trust, encourage accountable AI systems, and contribute to the ongoing discourse on XAI.</jats:p>",-1.0
402,10.5209/aris.83808,"Arguments for the Rise of AI Art: Does AI Art Have Creativity, Motivation, Self-awareness and Emotion?","<jats:p>With the advent of AI (AI), 'AI art' was born, and the concept of 'AI aesthetics' was derived. Despite the emergence of this new concept in art theory, the question of whether artworks created by AI have artistic and aesthetic value still needs to be debated in academia. While new concepts related to AI art are emerging, the discussion of whether a sustainable and critical theory system can be constructed in the field of computers and art, which is most closely related to it, ought to be focused on AI itself to explore whether it possesses similar characteristics of creativity and emotion as traditional art creation processes. This paper will first analyze the origins and possibilities of AI art and then explore the enormous impact of the rise of AI art on current and future human society in 4 dimensions: creativity, motivation, self-awareness, and emotion.</jats:p>",2.0
403,10.3389/frai.2024.1426761,Ethics dumping in AI,"<jats:p>AI (AI) systems encode not just statistical models and complex algorithms designed to process and analyze data, but also significant normative baggage. This ethical dimension, derived from the underlying code and training data, shapes the recommendations given, behaviors exhibited, and perceptions had by AI. These factors influence how AI is regulated, used, misused, and impacts end-users. The multifaceted nature of AI’s influence has sparked extensive discussions across disciplines like Science and Technology Studies (STS), Ethical, Legal and Social Implications (ELSI) studies, public policy analysis, and responsible innovation—underscoring the need to examine AI’s ethical ramifications. While the initial wave of AI ethics focused on articulating principles and guidelines, recent scholarship increasingly emphasizes the practical implementation of ethical principles, regulatory oversight, and mitigating unforeseen negative consequences. Drawing from the concept of “ethics dumping” in research ethics, this paper argues that practices surrounding AI development and deployment can, unduly and in a very concerning way, offload ethical responsibilities from developers and regulators to ill-equipped users and host environments. Four key trends illustrating such ethics dumping are identified: (1) AI developers embedding ethics through coded value assumptions, (2) AI ethics guidelines promoting broad or unactionable principles disconnected from local contexts, (3) institutions implementing AI systems without evaluating ethical implications, and (4) decision-makers enacting ethical governance frameworks disconnected from practice. Mitigating AI ethics dumping requires empowering users, fostering stakeholder engagement in norm-setting, harmonizing ethical guidelines while allowing flexibility for local variation, and establishing clear accountability mechanisms across the AI ecosystem.</jats:p>",0.0
404,10.1007/s44163-023-00072-6,Shall androids dream of genocides? How GenAI can change the future of memorialization of mass atrocities,"<jats:title>Abstract</jats:title><jats:p>The memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. Digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. At the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. The emergence of generative forms of AI (AI), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. AI can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. The use of GenAI in this context raises numerous questions: For example, can the paucity of training data on mass atrocities distort how AI interprets some atrocity-related inquiries? How important is the ability to differentiate between human- and AI-made content concerning mass atrocities? Can AI-made content be used to promote false information concerning atrocities? This article addresses these and other questions by examining the opportunities and risks associated with using GenAIs for memorializing mass atrocities. It also discusses recommendations for AIs integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.</jats:p>",7.0
405,10.3390/ai1020008,AI (AI) or Intelligence Augmentation (IA): What Is the Future?,"<jats:p>AI (AI) is a rapidly growing technological phenomenon that all industries wish to exploit to benefit from efficiency gains and cost reductions. At the macrolevel, AI appears to be capable of replacing humans by undertaking intelligent tasks that were once limited to the human mind. However, another school of thought suggests that instead of being a replacement for the human mind, AI can be used for intelligence augmentation (IA). Accordingly, our research seeks to address these different views, their implications, and potential risks in an age of increased artificial awareness. We show that the ultimate goal of humankind is to achieve IA through the exploitation of AI. Moreover, we articulate the urgent need for ethical frameworks that define how AI should be used to trigger the next level of IA.</jats:p>",0.0
406,10.3390/ai4010012,AI-Enhanced UUV Actuator Control,"<jats:p>This manuscript compares deterministic AI to a model-following control applied to DC motor control, including an evaluation of the threshold computation rate to let unmanned underwater vehicles correctly follow the challenging discontinuous square wave command signal. The approaches presented in the main text are validated by simulations in MATLAB®, where the motor process is discretized at multiple step sizes, which is inversely proportional to the computation rate. Performance is compared to canonical benchmarks that are evaluated by the error mean and standard deviation. With a large step size, discrete deterministic AI shows a larger error mean than the model-following self-turning regulator approach (the selected benchmark). However, the performance improves with a decreasing step size. The error mean is close to the continuous deterministic AI when the step size is reduced to 0.2 s, which means that the computation rate and the sampling period restrict discrete deterministic AI. In that case, continuous deterministic AI is the most feasible and reliable selection for future applications on unmanned underwater vehicles, since it is superior to all the approaches investigated at multiple computation rates.</jats:p>",-1.0
407,10.1007/s43681-021-00039-2,AI auditing and impact assessment: according to the UK information commissioner’s office,"<jats:title>Abstract</jats:title><jats:p>As the use of data and AI systems becomes crucial to core services and business, it increasingly demands a multi-stakeholder and complex governance approach. The Information Commissioner's Office’s ‘Guidance on the AI auditing framework: Draft guidance for consultation’ is a move forward in AI governance. The aim of this initiative is toward producing guidance that encompasses both technical (e.g. system impact assessments) and non-engineering (e.g. human oversight) components to governance and represents a significant milestone in the movement towards standardising AI governance. This paper will summarise and critically evaluate the ICO effort and try to anticipate future debates and present some general recommendations.</jats:p>",0.0
408,10.2196/48123,Effect of Benign Biopsy Findings on an AI–Based Cancer Detector in Screening Mammography: Retrospective Case-Control Study,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>AI (AI)–based cancer detectors (CAD) for mammography are starting to be used for breast cancer screening in radiology departments. It is important to understand how AI CAD systems react to benign lesions, especially those that have been subjected to biopsy.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>Our goal was to corroborate the hypothesis that women with previous benign biopsy and cytology assessments would subsequently present increased AI CAD abnormality scores even though they remained healthy.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>This is a retrospective study applying a commercial AI CAD system (Insight MMG, version 1.1.4.3; Lunit Inc) to a cancer-enriched mammography screening data set of 10,889 women (median age 56, range 40-74 years). The AI CAD generated a continuous prediction score for tumor suspicion between 0.00 and 1.00, where 1.00 represented the highest level of suspicion. A binary read (flagged or not flagged) was defined on the basis of a predetermined cutoff threshold (0.40). The flagged median and proportion of AI scores were calculated for women who were healthy, those who had a benign biopsy finding, and those who were diagnosed with breast cancer. For women with a benign biopsy finding, the interval between mammography and the biopsy was used for stratification of AI scores. The effect of increasing age was examined using subgroup analysis and regression modeling.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>Of a total of 10,889 women, 234 had a benign biopsy finding before or after screening. The proportions of flagged healthy women were 3.5%, 11%, and 84% for healthy women without a benign biopsy finding, those with a benign biopsy finding, and women with breast cancer, respectively (P&lt;.001). For the 8307 women with complete information, radiologist 1, radiologist 2, and the AI CAD system flagged 8.5%, 6.8%, and 8.5% of examinations of women who had a prior benign biopsy finding. The AI score correlated only with increasing age of the women in the cancer group (P=.01).</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>Compared to healthy women without a biopsy, the examined AI CAD system flagged a much larger proportion of women who had or would have a benign biopsy finding based on a radiologist’s decision. However, the flagging rate was not higher than that for radiologists. Further research should be focused on training the AI CAD system taking prior biopsy information into account.</jats:p>
          </jats:sec>",3.0
409,10.1145/2685328.2685334,"Cryptocurrencies, smart contracts, and AI","<jats:p>Recent developments in ""cryptocurrencies"" and ""smart contracts"" are creating new opportunities for applying AI techniques. These economic technologies would benefit from greater real world knowledge and reasoning as they become integrated with everyday commerce. Cryptocurrencies and smart contracts may also provide an infrastructure for ensuring that AI systems follow specified legal and safety regulations as they become more integrated into human society.</jats:p>",0.0
410,10.1145/3465074.3465080,Applied AI matters: AI4Code,"<jats:p>The marriage of AI (AI) techniques to problems surrounding the generation, maintenance, and use of source code has come to the fore in recent years as an important AI application area1. A large chunk of this recent attention can be attributed to contemporaneous advancements in Natural Language Processing (NLP) techniques and sub-fields. The naturalness hypothesis, which states that ""software is a form of human communication"" and that code exhibits patterns that are similar to (human) natural languages (Devanbu, 2015; Hindle, Barr, Gabel, Su, &amp; Devanbu, 2016), has allowed for the application of many of these NLP advances to code-centric usecases. This development has contributed to a spate of work in the community --- much of it captured in a survey by Allamanis, Barr, Devanbu, and Sutton (2018) that focuses on classifying these approaches by the type of probabilistic model applied to source code.</jats:p>
          <jats:p>This increase in the variety of AI techniques applied to source code has found various manifestations in the industry at large. Code and software form the backbone that underpins almost all modern technical advancements: it is thus natural that breakthroughs in this area should reflect in the emergence of real world deployments.</jats:p>",1.0
411,10.1007/s43681-024-00560-0,Ensuring fundamental rights compliance and trustworthiness of law enforcement AI systems: the ALIGNER Fundamental Rights Impact Assessment,"<jats:title>Abstract</jats:title><jats:p>AI systems can expand the capabilities and enhance the efficiency of law enforcement agencies preventing, investigating, detecting, and prosecuting criminal offences in the European Union. At the same time, the deployment of AI in the security domain often raises numerous legal and ethical concerns. The ALIGNER Fundamental Rights Impact Assessment is an operational tool, rooted in fundamental rights and in the principles of AI ethics, ready to be integrated in the AI governance measures of European law enforcement agencies to inform their decision-making processes and ensure compliance with the recently adopted AI Act. This paper first introduces the main tensions between law enforcement AI and fundamental rights, as enshrined in the Charter of Fundamental Rights of the European Union; then, it gives an overview of the main developments and best practices in AI governance and their relationship with fundamental rights as well as AI ethics; and finally, it describes the structure of the ALIGNER Fundamental Rights Impact Assessment.</jats:p>",0.0
412,10.1007/s00146-024-02004-z,Risk and artificial general intelligence,"<jats:title>Abstract</jats:title><jats:p>Artificial General Intelligence (AGI) is said to pose many risks, be they catastrophic, existential and otherwise. This paper discusses whether the notion of risk can apply to AGI, both descriptively and in the current regulatory framework. The paper argues that current definitions of risk are ill-suited to capture supposed AGI existential risks, and that the risk-based framework of the EU AI Act is inadequate to deal with truly general, agential systems.</jats:p>",0.0
413,10.1007/s00146-022-01474-3,Exposing implicit biases and stereotypes in human and AI: state of the art and challenges with a focus on gender,"<jats:title>Abstract</jats:title><jats:p>Biases in cognition are ubiquitous. Social psychologists suggested biases and stereotypes serve a multifarious set of cognitive goals, while at the same time stressing their potential harmfulness. Recently, biases and stereotypes became the purview of heated debates in the machine learning community too. Researchers and developers are becoming increasingly aware of the fact that some biases, like gender and race biases, are entrenched in the algorithms some AI applications rely upon. Here, taking into account several existing approaches that address the problem of implicit biases and stereotypes, we propose that a strategy to cope with this phenomenon is to unmask those found in AI systems by understanding their cognitive dimension, rather than simply trying to correct algorithms. To this extent, we present a discussion bridging together findings from cognitive science and insights from machine learning that can be integrated in a state-of-the-art semantic network. Remarkably, this resource can be of assistance to scholars (e.g., cognitive and computer scientists) while at the same time contributing to refine AI regulations affecting social life. We show how only through a thorough understanding of the cognitive processes leading to biases, and through an interdisciplinary effort, we can make the best of AI technology.</jats:p>",-1.0
414,10.46392/kjge.2023.17.6.333,Analysis of Professors’ Experiences with GenAI and the Concerns of Classroom Use : Application of the Concerns-Based Adoption Model (CBAM),"<jats:p>The rise of GenAI, such as GenAI, poses challenges for the education sector. To explore strategies for addressing GenAI issues and identifying educational applications, it is essential for us to understand the concerns and attitudes of educators, who are the key implementers in education. In this context, this study investigated the experiences and concerns of university professors regarding the educational use of GenAI by utilizing the Concerns-Based Adoption Model (CBAM). Data was collected from 100 professors representing various disciplines at University A in Gyeonggi-do. The majority of respondents had some experience with GenAI, but its educational utilization was limited. Concerns included the provision of incorrect or biased information, ethical issues, and users' lack of ability to use GenAI. However, 61% of respondents expressed their intention to apply GenAI in their courses in the upcoming semester. Concern levels aligned with the non-user profile, with lower concerns in the consequence stage. There were no significant differences in concern levels based on the professors' disciplines, but differences were observed based on their experience and intention to use GenAI in their classes. Professors who had already used GenAI in their classes showed higher concerns regarding the consequence, collaboration, and refocusing stages. Based on these findings, implications for the educational use of GenAI in higher education were discussed.</jats:p>",1.0
415,10.1002/ail2.100,Developing and Deploying End‐to‐End Machine Learning Systems for Social Impact: A Rubric and Practical AI Case Studies From African Contexts,"<jats:title>ABSTRACT</jats:title><jats:p>AI (AI) and machine learning have demonstrated the potential to provide solutions to societal challenges, for example, automated crop diagnostics for smallholder farmers, environmental pollution modelling and prediction for cities and machine translation systems for languages that enable information access and communication for segments of the population who are unable to speak or write official languages, among others. Despite the potential of AI, the practical and technical issues related to its development and deployment in the African context are the least documented and understood. The development and deployment of AI for social impact systems in the developing world present new intricacies and requirements emanating from the unique technology and social ecosystems in these settings. This paper provides a rubric for developing and deploying AI systems for social impact with a focus on the African context. The rubric is derived from the analysis of a series of selected real‐world case studies of AI applications in Africa. We assessed the selected AI case studies against the proposed rubric. The rubric and examples of AI applications presented in this paper are expected to contribute to the development and application of AI systems in other African contexts.</jats:p>",-1.0
416,10.3390/educsci14020172,A Primer on GenAI,"<jats:p>Many educators and professionals in different industries may need to become more familiar with the basic concepts of AI (AI) and GenAI (Gen-AI). Therefore, this paper aims to introduce some of the basic concepts of AI and Gen-AI. The approach of this explanatory paper is first to introduce some of the underlying concepts, such as AI, machine learning, deep learning, artificial neural networks, and large language models (LLMs), that would allow the reader to better understand GenAI. The paper also discusses some of the applications and implications of GenAI on businesses and education, followed by the current challenges associated with GenAI.</jats:p>",4.0
417,10.1002/cae.22806,"Impact of basic AI (AI) course on understanding concepts, literacy, and empowerment in the field of AI among students","<jats:title>Abstract</jats:title><jats:p>With the development of information technologies and information processing methods, it is important to provide high‐quality education in the field of AI (AI). The study aims to investigate the impact of an educational course on AI on the comprehension of concepts, literacy, and empowerment in the field of AI among students of higher educational institutions. The experiment involved 125 students from Hohai University in China. As a result of taking the training course, students were able to improve their understanding of concepts (increasing their average score from 6.33 to 9.69), literacy (from 2.94 to 3.99), and empowerment (from 3.90 to 4.04) in AI. The resulting data statistically confirmed the effectiveness of the developed course for improving confidence in the field of AI. The training module can be applied to improve confidence in the field of AI for students in various careers, as information competence is important these days and increases the success of graduates in employment. When it comes to further research, the encouraging results of this study suggest opportunities for promoting this training program among a diverse group of participants. To confirm the effectiveness of the developed course, it can be conducted among students in schools and other educational institutions, reducing it to even more basic if necessary.</jats:p>",1.0
418,10.3390/ai4030034,Explainable AI (XAI): Concepts and Challenges in Healthcare,"<jats:p>AI (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (“clinical decision support”), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a “black box”, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable AI (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.</jats:p>",3.0
419,10.3389/frai.2023.1239466,Does AI kill employment growth: the missing link of corporate AI posture,"<jats:sec><jats:title>Introduction</jats:title><jats:p>An intense debate has been on-going about how AI (AI) technology investments have an impact on employment. The debate has often focused on the potential of AI for human task automation, omitting the strategic incentive for firms to cooperate with their workers as to exploit AI technologies for the most relevant benefit of new product and service innovation.</jats:p></jats:sec><jats:sec><jats:title>Method</jats:title><jats:p>We calibrate an empirical probit regression model of how changes in employment relate to AI diffusion, based on formalizing a game-theoretical model of a firm exploiting the twin role of AI innovation and AI automation for both absolute and competitive advantage.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The theoretical game-theory prediction is that employment following AI technology adoption is not negative, and ultimately depends on how AI leads to new success in innovation, competition which defines the competitive reward of innovation and profit sharing between workers and firms. Our estimation, is based on a global survey of 3,000 large companies across 10 countries, demonstrates that a firm employment growth depends on two strategic postures, that is, the firm relative maturity of AI adoption as well as its relative bias toward AI-based product innovation.</jats:p></jats:sec><jats:sec><jats:title>Discussion</jats:title><jats:p>The contribution of this research is to highlight the twin role of firm and workers in shaping how technology will affect employment. AI in particular marries the potential of task automation with even more potential for expansion.</jats:p></jats:sec>",-1.0
420,10.1007/s44223-024-00066-z,Adaptive interior design method for different MBTI personality types based on GenAI,"<jats:title>Abstract</jats:title><jats:p>Accurately predicting homeowners’ aesthetic preferences is crucial in interior design. This study develops a fine-tuning model (LORA) for interior design styles corresponding to different MBTI personality types, leveraging the Stable Diffusion Web UI platform and integrating it into a GenAI framework. Subsequently, personalized aesthetic preference architectural interior renderings are recommended based on homeowners’ personality traits, aiming to achieve an adaptive interior design approach. To achieve more precise adaptive solutions, this research surveys the style and color tendencies of respondents with different MBTI personality types and adds style description prompts to assist in image generation. The study finds that this method can better predict the interior design styles favored by certain MBTI personality types. This research contributes to addressing aesthetic biases between designers and homeowners, bringing innovative ideas and methods to interior design, and is expected to enhance homeowners’ satisfaction.</jats:p>",2.0
421,10.1111/nup.12306,AI and Robotics in Nursing: Ethics of Caring as a Guide to Dividing Tasks Between AI and Humans,"<jats:title>Abstract</jats:title><jats:p>Nurses have traditionally been regarded as clinicians that deliver compassionate, safe, and empathetic health care (Nurses again outpace other professions for honesty &amp; ethics, 2018). Caring is a fundamental characteristic, expectation, and moral obligation of the nursing and caregiving professions (Nursing: Scope and standards of practice, American Nurses Association, Silver Spring, MD, 2015). Along with caring, nurses are expected to undertake ever‐expanding duties and complex tasks. In part because of the growing physical, intellectual and emotional demandingness, of nursing as well as technological advances, AI (AI) and AI care robots are rapidly changing the healthcare landscape. As technology becomes more advanced, efficient, and economical, opportunities and pressure to introduce AI into nursing care will only increase. In the first part of the article, we review recent and existing applications of AI in nursing and speculate on future use. Second, situate our project within the recent literature on the ethics of nursing and AI. Third, we explore three dominant theories of caring and the two paradigmatic expressions of caring (touch and presence) and conclude that AI—at least for the foreseeable future—is incapable of caring in the sense central to nursing and caregiving ethics. We conclude that for AI to be implemented ethically, it cannot transgress the core values of nursing, usurp aspects of caring that can only meaningfully be carried out by human beings, and it must support, open, or improve opportunities for nurses to provide the uniquely human aspects of care.</jats:p>",6.0
422,10.1145/3375637.3375643,Cosmology of AI project,"<jats:p>AI (AI) has transcended beyond buzzwords, keywords or trends. The ubiquity of AI is so profound that it has managed to seep into popular culture. It breeds throughout social media platforms. It dominates the airwaves. It is impossible to watch television without mention of the acronym, the word, and how everyone is using it. However, if we look a little closer, take a deep dive into the AI pool where everyone seems to be swimming, we began to learn that AI has been misinterpreted, misrepresented, and incorrectly defined. ""In the Laws of Thought and Thinking Machines,"" Hughes and Hughes (2019) talk about how uncovering the truth of the definition, meaning, applications, and implications of AI would be a noteworthy goal. Today we are pursuing that goal.</jats:p>",0.0
423,10.3390/ijerph20043772,When AI Voices Human Concerns: The Paradoxical Effects of AI Voice on Climate Risk Perception and Pro-Environmental Behavioral Intention,"<jats:p>AI (AI)-enabled text-to-speech transformation has been widely employed to deliver online information in various fields. However, few studies have investigated the effect of the AI voice in environmental risk communication, especially in the field of climate change, an issue that poses a severe threat to global public health. To address this gap, the current study examines how the AI voice impacts the persuasive outcome of climate-related information and the potential mechanism that underlies this process. Based on the social and affect heuristics of voice, we propose a serial mediation model to test the effect of climate-related information delivered by different voice types (AI voice vs. human voice) in eliciting risk perception and motivating pro-environmental behavioral intention. Through an online auditory experiment (N = 397), we found the following. First, the AI voice was as effective as the human voice in eliciting risk perception and motivating pro-environmental behavioral intention. Second, compared with human voice, the AI voice yielded a listener’s lower level of perceived identity oneness with the speaker, which decreased risk perception and subsequently inhibited pro-environmental behavioral intention. Third, compared with human voice, the AI voice produced a higher level of auditory fear, which increased risk perception and thereby led to stronger pro-environmental behavioral intention. The paradoxical role of the AI voice and its wise use in environmental risk communication for promoting global public health are discussed.</jats:p>",-1.0
424,10.54097/hset.v68i.12112,Research on the application of AI generated AI technology in new media art,"<jats:p>With the rapid development of information society and computer science, new media art has not made a qualitative leap in the past decade. The reason is that in big data and machine learning, the rapid development of AI technology has brought computer science up to a higher level, while art is still stuck in aesthetics, acoustics, vision, psychology and other aspects, lack of logic, intelligence and integration. Therefore, the new media art needs the support of the computer technology, especially the new technology.In the design stage of new media art, technologies such as AI, machine learning and big data mining are integrated to improve the molding speed of new media art through mathematical modeling.Through machine learning, quickly understand user experience and user needs, further optimize new media art works, use AI AI technology to simulate new media art, and simulate the whole process of user experience and interaction is the main content of this paper.This makes new media art become a typical application in the field of AICG (AI production content). GenAI will greatly reduce the marginal cost of creation and knowledge work, greatly improve labor productivity and economic value, and realize the generation of new media art original content at one tenth of the cost and thousands times production speed.</jats:p>",2.0
425,10.1097/acm.0000000000005439,GenAI and GenAI for Medical Education: Potential Impact and Opportunity,"<jats:title>Abstract</jats:title>
          <jats:p>GenAI has ushered in a new era of AI (AI) that already has significant consequences for many industries, including health care and education. GenAI tools, such as GenAI, refer to AI that is designed to create or generate new content, such as text, images, or music, from their trained parameters. With free access online and an easy-to-use conversational interface, GenAI quickly accumulated more than 100 million users within the first few months of its launch. Recent headlines in the popular press have ignited concerns relevant to medical education over the possible implications of cheating and plagiarism in assessments as well as excitement over new opportunities for learning, assessment, and research. In this Scholarly Perspective, the authors offer insights and recommendations about GenAI for medical educators based on literature review, including the AI literacy framework. The authors provide a definition of GenAI, introduce an AI literacy framework and competencies, and offer considerations for potential impacts and opportunities to optimize integration of GenAI for admissions, learning, assessment, and medical education research to help medical educators navigate and start planning for this new environment. As GenAI tools continue to expand, educators need to increase their AI literacy through education and vigilance around new advances in the technology and serve as stewards of AI literacy to foster social responsibility and ethical awareness around the use of AI.</jats:p>",3.0
426,10.21141/pjp.2023.08,"Chatbots, GenAI, and Scholarly Manuscripts","<jats:p>This statement revises our earlier “WAME Recommendations on GenAI and Chatbots in Relation to Scholarly Publications” (January 20, 2023). The revision reflects the proliferation of chatbots and their expanding use in scholarly publishing over the last few months, as well as emerging concerns regarding lack of authenticity of content when using chatbots. These Recommendations are intended to inform editors and help them develop policies for the use of chatbots in papers published in their journals. They aim to help authors and reviewers understand how best to attribute the use of chatbots in their work, and to address the need for all journal editors to have access to manuscript screening tools. In this rapidly evolving field, we will continue to modify these recommendations as the software and its applications develop.</jats:p>",-1.0
427,10.1186/s41239-023-00392-8,,,
428,10.1186/s41239-023-00436-z,,,
429,10.1186/s41239-019-0171-0,,,
430,10.1108/itse-11-2023-0218,,,
431,10.3389/feduc.2024.1391485,,,
432,10.1007/978-3-031-13351-0_13,,,
433,10.1016/j.metabol.2017.01.011,,,
434,10.1109/access.2020.2988510,,,
435,10.1016/j.cie.2017.11.026,,,
436,10.1109/access.2019.2900300,,,
437,10.1016/j.chbah.2023.100004,,,
438,10.1016/j.jbusres.2024.114720,,,
439,10.1108/etpc-08-2023-0092,,,
440,10.1016/j.yjoc.2024.100075,,,
441,10.1016/j.bushor.2024.05.002,,,
442,10.1016/j.compcom.2023.102795,,,
443,10.18500/1819-7671-2023-23-4-414-418,,,
444,10.1016/j.compcom.2024.102825,,,
445,10.17803/2311-5998.2023.109.9.108-117,,,
446,10.1016/j.clsr.2023.105892,,,
447,10.32084/bsawp.5155,,,
448,10.31675/1607-1859-2022-24-1-44-53,,,
449,10.18101/1994-0866-2022-2-64-71,,,
450,10.4324/9780429356797,,,
451,10.7551/mitpress/12549.001.0001,,,
452,10.5334/tismir.100,,,
453,10.1201/9781003240198,,,
454,10.7551/mitpress/11585.001.0001,,,
455,10.1007/978-3-030-77626-8_34,,,
456,10.25080/majora-14bd3278-006,,,
457,10.1109/icassp.2013.6639346,,,
458,10.1007/978-3-030-05318-5_8,,,
459,10.1007/s00371-017-1461-y,,,
460,10.1016/j.inffus.2020.01.002,,,
461,10.1016/b978-0-12-815822-7.00008-x,,,
462,10.1016/j.future.2018.10.034,,,
463,10.3390/su11174557,,,
464,10.1016/j.compag.2018.04.027,,,
465,10.1038/s41592-020-0750-y,,,
466,10.1186/s13321-020-0416-x.,,,
467,10.1109/iccw.2019.8756663,,,
468,10.1109/tnse.2019.2941754,,,
469,10.1109/isorc.2019.00037,,,
470,10.1109/tpami.2020.2970410,,,
471,10.1016/j.compeleceng.2017.12.009,,,
472,10.1145/2347736.2347755,,,
473,10.1109/tpami.2013.50,,,
474,10.1038/srep26094.,,,
475,10.1109/dsaa.2015.7344858,,,
476,10.1109/icdm.2016.0123,,,
477,10.1109/icdm.2017.31,,,
478,10.1109/dsaa.2017.66,,,
479,10.1007/s12293-015-0173-y,,,
480,10.1109/icdmw.2016.0190,,,
481,10.24963/ijcai.2017/352,,,
482,10.1609/aaai.v32i1.11678,,,
483,10.1109/ijcnn.2010.5596915,,,
484,10.1007/s10994-017-5684-y,,,
485,10.1007/978-3-642-25566-3_40,,,
486,10.1145/3097983.3098043,,,
487,10.1007/s13755-017-0023-z,,,
488,10.1145/2487575.2487629,,,
489,10.1007/978-3-030-05318-5_6,,,
490,10.1609/aaai.v29i1.9354,,,
491,10.1007/978-3-319-31204-0_9,,,
492,10.1145/2806777.2806945,,,
493,10.1109/bigdata.2017.8257923,,,
494,10.1137/1.9781611974973.83.,,,
495,10.1007/s10994-018-5735-z,,,
496,10.1145/3205455.3205586,,,
497,10.1016/s0262-4079(14)62145-x,,,
498,10.5281/zenodo.4588648,,,
499,10.1080/23299460.2017.1331101,,,
500,10.1016/j.neuron.2016.10.046,,,
501,10.1145/1922649.1922651,,,
502,10.1016/s0377-2217(00)00074-6,,,
503,10.4337/9781843768616,,,
504,10.1145/3448250,,,
505,10.1007/s43681-020-00012-5,,,
506,10.1080/07421222.2002.11045715,,,
507,10.1093/actrade/9780199602919.001.0001,,,
508,10.1109/mc.2020.3034950,,,
509,10.5840/beq19988219,,,
510,10.1145/3072528,,,
511,10.1145/3375627.3375813,,,
512,10.1080/21507740.2015.1135831,,,
513,10.1109/tts.2021.3077595,,,
514,10.1016/j.clsr.2009.02.002,,,
515,10.4324/9780429032066-2,,,
516,10.1007/s13347-021-00474-3,,,
517,10.1126/science.aan8871,,,
518,10.1007/978-3-030-30371-6,,,
519,10.1007/s10892-017-9252-2,,,
520,10.1023/a:1016323815802,,,
521,10.4324/9780429442643,,,
522,10.2143/ep.6.2.505355,,,
523,10.1080/10508406.2013.802652,,,
524,10.1016/j.eiar.2004.12.002,,,
525,10.1145/299157.299175,,,
526,10.1007/s42452-021-04715-0,,,
527,10.1016/j.ejpn.2022.01.021,,,
528,10.2139/ssrn.3584219,,,
529,10.1038/s42256-019-0088-2,,,
530,10.1016/j.bushor.2018.08.004,,,
531,10.1016/j.patter.2021.100314,,,
532,10.1023/a:1022950813386,,,
533,10.5840/bpej1997161/2/314,,,
534,10.1177/017084069601700302,,,
535,10.2307/258792,,,
536,10.1016/j.techfore.2018.11.010,,,
537,10.1007/978-3-030-81907-1_10,,,
538,10.1098/rsta.2018.0089,,,
539,10.1016/j.ijinfomgt.2020.102104,,,
540,10.1016/j.neuron.2014.06.001,,,
541,10.1108/jices-12-2019-0138,,,
542,10.1016/j.neuron.2019.01.005,,,
543,10.1007/s12152-018-9372-9,,,
544,10.1109/jproc.2021.3058954,,,
545,10.1109/tts.2020.2992669,,,
546,10.1007/s10676-021-09591-1,,,
547,10.1162/99608f92.cb91a35a,,,
548,10.1016/j.ijinfomgt.2021.102441,,,
549,10.1007/978-3-030-69978-9,,,
550,10.3389/fnhum.2019.00105,,,
551,10.1016/j.jbusres.2020.11.030,,,
552,10.48550/arxiv.2107.14099,,,
553,10.1007/s43681-021-00038-3,,,
554,10.1093/acprof:oso/9780190498511.001.0001,,,
555,10.1017/err.2019.65,,,
556,10.1098/rsta.2018.0083,,,
557,10.1016/j.techfore.2021.120576,,,
558,10.1145/376134.376170,,,
559,10.1177/0268396220925830,,,
560,10.1007/s10676-010-9242-6,,,
561,10.1111/rego.12158,,,
562,10.1109/tts.2021.3066209,,,
563,10.3389/fpsyg.2021.696346,,,
564,10.1504/ijte.2022.10052292,,,
565,10.1109/tii.2022.3146552,,,
566,10.1016/j.technovation.2023.102768,,,
567,10.1007/978-981-99-3177-4_41,,,
568,10.1609/icwsm.v3i1.13937,,,
569,10.1109/tkde.2007.190689,,,
570,10.1007/s11192-022-04319-4,,,
571,10.1111/1748-8583.12524,,,
572,10.1007/s10479-023-05495-z,,,
573,10.1007/s10916-023-01925-4,,,
574,10.35542/osf.io/mrz8h,,,
575,10.1109/isse46696.2019.8984472,,,
576,10.1145/3587102.3588815,,,
577,10.3389/frai.2023.1169595,,,
578,10.1109/tiv.2023.3253281,,,
579,10.1145/3561048,,,
580,10.1016/j.ijinfomgt.2023.102642,,,
581,10.1016/j.im.2022.103701,,,
582,10.1007/s10796-021-10186-w,,,
583,10.2196/46885,,,
584,10.1145/3555962.3555969,,,
585,10.1016/j.ajog.2023.03.009,,,
586,10.1016/j.jik.2022.100177,,,
587,10.1145/3593013.3594067,,,
588,10.1016/j.bspc.2023.105292,,,
589,10.3390/bdcc7020062,,,
590,10.1016/j.jik.2022.100171,,,
591,10.1016/j.physa.2018.08.002,,,
592,10.1109/icstw58534.2023.00078,,,
593,10.1016/j.giq.2020.101493,,,
594,10.1016/j.bushor.2022.03.002,,,
595,10.1016/j.lindif.2023.102274,,,
596,10.1016/j.cosrev.2022.100511,,,
597,10.1016/j.metrad.2023.100017,,,
598,10.1017/err.2023.59,,,
599,10.1038/s41587-022-01618-2,,,
600,10.2196/46924,,,
601,10.1016/j.jbusres.2024.114542,,,
602,10.1007/s11192-020-03690-4,,,
603,10.1016/j.stae.2022.100006,,,
604,10.1038/s41746-023-00873-0,,,
605,10.1109/citcon.2019.8729105,,,
606,10.1007/978-3-031-16947-2_8,,,
607,10.2139/ssrn.4375283,,,
608,10.1097/js9.0000000000000552,,,
609,10.1016/j.im.2022.103685,,,
610,10.7200/esicm.53.286,,,
611,10.1016/j.cose.2023.103476,,,
612,10.1111/ijcs.12695,,,
613,10.1016/j.ijresmar.2023.03.001,,,
614,10.3390/info14020085,,,
615,10.53761/1.20.02.07,,,
616,10.1109/tkde.2020.2992485,,,
617,10.1016/j.iotcps.2023.04.003,,,
618,10.1108/bpmj-05-2020-0216,,,
619,10.1016/j.chbah.2023.100006,,,
620,10.3390/healthcare11060887,,,
621,10.1016/j.im.2021.103463,,,
622,10.1002/jls.21605,,,
623,10.1016/j.stae.2023.100039,,,
624,10.1007/978-3-319-57870-5,,,
625,10.1016/j.dsx.2023.102744,,,
626,10.1109/jas.2023.123552,,,
627,10.1016/j.jbusres.2020.11.004,,,
628,10.3389/fphar.2023.1194216,,,
629,10.18653/v1/2023.findings-emnlp.314,,,
630,10.1016/j.techsoc.2023.102406,,,
631,10.1007/s10676-006-9113-3,,,
632,10.16997/wpcc.917,,,
633,10.1002/9781118474396.wbept0147,,,
634,10.1145/3375627.3375842,,,
635,10.1177/0191453712447770,,,
636,10.1057/9780230306752_4,,,
637,10.1017/cbo9780511490217.009,,,
638,10.3390/info15010027,,,
639,10.1186/s40163-020-00123-8,,,
640,10.1057/s41300-021-00134-w,,,
641,10.1080/10345329.2023.2248670,,,
642,10.9781/ijimai.2023.07.006,,,
643,10.1109/access.2023.3300381,,,
644,10.1177/14614448211022702,,,
645,10.1186/1687-417x-2014-4,,,
646,10.1007/s11948-018-00081-0,,,
647,10.1155/2023/8691095,,,
648,10.1177/09567976231207095,,,
649,10.1073/pnas.2120481119,,,
650,10.37016/mr-2020-024,,,
651,10.1080/08874417.2023.2261010,,,
652,10.1007/s10676-022-09624-3,,,
653,10.1016/j.tmp.2019.05.009,,,
654,10.15678/eber.2023.110201,,,
655,10.2478/ijm-2020-0004,,,
656,10.1108/imds-04-2019-0207,,,
657,10.1609/aimag.v28i4.2065,,,
658,10.13140/rg.2.2.24272.66566,,,
659,10.1017/cbo9781139046855.020,,,
660,10.1007/s11023-018-9482-5,,,
661,10.1016/j.newideapsych.2018.12.001,,,
662,10.1109/mis.2006.80,,,
663,10.1007/s11023-017-9418-5,,,
664,10.2139/ssrn.3547922,,,
665,10.1145/3278721.3278745,,,
666,10.1145/3278721.3278765,,,
667,10.5281/zenodo.5536176,,,
668,10.1007/978-3-030-69128-8_11,,,
669,10.1007/s11948-020-00276-4,,,
670,10.1007/s11569-009-0066-y,,,
671,10.1016/j.jclepro.2019.06.108,,,
672,10.1007/s11023-020-09517-8,,,
673,10.54489/ijtim.v2i2.80,,,
674,10.1145/3351095.3372873,,,
675,10.1145/3287560.3287596,,,
676,10.1145/3458723,,,
677,10.1145/3351095.3372870,,,
678,10.1109/chase.2015.27,,,
679,10.1016/b978-0-08-051574-8.50024-8,,,
680,10.31297/hkju.21.4.2,,,
681,10.1007/978-94-6265-531-7,,,
682,10.1109/rew56159.2022.00037,,,
683,10.1038/s42256-020-0214-1,,,
684,10.7551/mitpress/7585.001.0001,,,
685,10.1177/002234336900600301,,,
686,10.2307/j.ctt2jbsgw,,,
687,10.1007/978-1-4471-1835-0_6,,,
688,10.1080/0731129x.2021.1903709,,,
689,10.1093/acprof:oso/9780199688623.003.0014,,,
690,10.1007/s11023-020-09539-2,,,
691,10.4337/9781849808774,,,
692,10.2307/j.ctt1gk0941,,,
693,10.1111/j.1748-720x.2001.tb00037.x,,,
694,10.1177/13882627211031257,,,
695,10.1126/science.aax2342,,,
696,10.1145/230538.230561,,,
697,10.1080/18918131.2022.2073078,,,
698,10.1093/poq/nfw006,,,
699,10.4159/harvard.9780674736061,,,
700,10.1177/20539517211035955,,,
701,10.1080/1369118x.2019.1573912,,,
702,10.1126/science.aaz3873,,,
703,10.1093/ejil/chac007,,,
704,10.1007/s13347-020-00419-2,,,
705,10.4324/9781351025386-6,,,
706,10.14763/2021.3.1574,,,
707,10.1093/acprof:oso/9780198237907.001.0001,,,
708,10.1145/3600211.3604673,,,
709,10.1007/s10699-010-9190-y,,,
710,10.4159/9780674245792,,,
711,10.1093/ejil/chab087,,,
712,10.1145/3306618.3314274,,,
713,10.1007/s12394-008-0003-1,,,
714,10.7551/mitpress/6682.001.0001,,,
715,10.4337/9781789900972.00035,,,
716,10.1515/til-2019-0002,,,
717,10.1515/til-2019-0004,,,
718,10.7551/mitpress/9780262220859.003.0001,,,
719,10.1177/20539517231158631,,,
720,10.1515/sats-2021-0002,,,
721,10.1007/s12027-020-00633-7,,,
722,10.1177/0894439320914853,,,
723,10.2966/scrip.200123.203,,,
724,10.1017/cbo9780511605734,,,
725,10.1007/978-1-4020-6914-7_15,,,
726,10.1093/ejil/chn043,,,
727,10.1080/17579961.2023.2184132,,,
728,10.1007/978-3-031-40516-7_17,,,
729,10.2307/j.ctvxcrz2b,,,
730,10.1093/hrlr/ngac010,,,
731,10.1016/j.clsr.2023.105899,,,
732,10.1016/j.clsr.2024.105956,,,
733,10.1145/3287560.3287598,,,
734,10.7208/chicago/9780226276663.001.0001,,,
735,10.1016/j.techsoc.2020.101422,,,
736,10.1007/s43681-022-00249-2,,,
737,10.1177/1529100619832930,,,
738,10.1080/1369118x.2020.1792530,,,
739,10.1007/s12369-015-0287-x,,,
740,10.1177/1461444820924631,,,
741,10.1177/2053951719895805,,,
742,10.1016/j.clsr.2018.05.017,,,
743,10.1007/s10676-004-3422-1,,,
744,10.1108/jfc-03-2020-0037,,,
745,10.1145/3372823,,,
746,10.1080/08839514.2022.2037254,,,
747,10.1016/j.cose.2022.103006,,,
748,10.1007/978-3-030-98225-6_9,,,
749,10.1080/01969722.2022.2112539,,,
750,10.1109/icaccs51430.2021.9441925,,,
751,10.1109/mc.2021.3092480,,,
752,10.1631/fitee.1800573,,,
753,10.1088/1742-6596/1964/4/042072,,,
754,10.1145/3453158,,,
755,10.1016/j.jisa.2020.102722,,,
756,10.1109/comst.2021.3127267,,,
757,10.17705/1cais.05109,,,
758,10.1002/jsc.2559,,,
759,10.22363/2313-0660-2022-22-2-288-302,,,
760,10.1016/j.ifacol.2022.07.084,,,
761,10.3390/su151813369,,,
762,10.1016/j.cose.2011.08.004,,,
763,10.1016/s2212-5671(15)01077-1,,,
764,10.3390/electronics12061333,,,
765,10.1109/access.2022.3191790,,,
766,10.48550/arxiv.2209.13454,,,
767,10.1145/66093.66095,,,
768,10.1093/itnow/bwz013,,,
769,10.1145/1403922.1399619,,,
770,10.1016/s0167-4048(99)80027-3,,,
771,10.4018/978-1-59904-937-3.ch135,,,
772,10.1109/iecon.2011.6120048,,,
773,10.1109/mts.2011.940293,,,
774,10.1109/pst.2010.5593240,,,
775,10.1109/mnet.2010.5634434,,,
776,10.1080/00396338.2011.555586,,,
777,10.1016/j.future.2018.07.052,,,
778,10.1145/3588999,,,
779,10.1016/s1361-3723(21)00065-8,,,
780,10.1016/j.frl.2022.102715,,,
781,10.5281/zenodo.7723187,,,
782,10.1016/j.rico.2023.100268,,,
783,10.3390/electronics11020198,,,
784,10.1109/comst.2023.3273282,,,
785,10.3390/app11104580,,,
786,10.3390/s22134666,,,
787,10.17148/ijarcce.2022.11912,,,
788,10.48550/arxiv.2202.09465,,,
789,10.1007/978-3-031-19945-5_25,,,
790,10.1007/978-3-030-28374-2_35,,,
791,10.1016/j.egyr.2021.08.126,,,
792,10.1109/access.2022.3183642,,,
793,10.1080/00913367.2021.1909515,,,
794,10.1109/mitp.2022.3205529,,,
795,10.1002/widm.1306,,,
796,10.1007/s11235-020-00733-2,,,
797,10.1109/access.2022.3204171,,,
798,10.54690/ndujournal.37.152,,,
799,10.4324/9781003218326-2,,,
800,10.1016/j.ijinfomgt.2019.08.002,,,
801,10.1109/icetsis55481.2022.9888871,,,
802,10.1007/978-981-10-8681-6_67,,,
803,10.1186/s13635-018-0080-0,,,
804,10.1007/978-981-16-6186-0_4,,,
805,10.1080/10242694.2022.2161739,,,
806,10.1353/jod.2017.0025,,,
807,10.1016/j.procs.2022.10.025,,,
808,10.25300/misq/2016/40.1.09,,,
809,10.1080/19361610.2022.2079939,,,
810,10.1007/978-3-030-93921-2_22,,,
811,10.1007/s12027-022-00702-z,,,
812,10.1016/j.jbusres.2019.07.039,,,
813,10.3390/s22051860,,,
814,10.3390/s21093267,,,
815,10.4108/eai.7-7-2021.170285,,,
816,10.1016/j.inffus.2023.101804,,,
817,10.1007/978-3-031-17030-0_4,,,
818,10.1109/access.2021.3134076,,,
819,10.1063/5.0109664,,,
820,10.14429/dsj.68.12371,,,
821,10.1108/intr-10-2019-0400,,,
822,10.1007/978-981-19-4193-1_54,,,
823,10.1186/s40309-022-00202-w,,,
824,10.1108/jfc-04-2022-0090,,,
825,10.5121/ijsea.2022.13502,,,
826,10.1007/978-3-030-98225-6_2,,,
827,10.1007/s11277-021-08573-2,,,
828,10.1109/sp.2017.41,,,
829,10.1007/978-3-030-90708-2_4,,,
830,10.1007/978-3-319-92624-7_1,,,
831,10.3390/s22176662,,,
832,10.1016/j.patcog.2022.109048,,,
833,10.1016/j.clsr.2019.04.008,,,
834,10.4018/978-1-6684-8691-7.ch021,,,
835,10.1007/s10111-021-00683-y,,,
836,10.1016/j.cose.2020.101846,,,
837,10.1016/j.procs.2022.01.230,,,
838,10.1109/tts.2020.3019595,,,
839,10.1007/s00146-023-01723-z,,,
840,10.33182/bc.v12i1.2015,,,
841,10.1177/2053951715622512,,,
842,10.1007/s10699-020-09730-9,,,
843,10.7551/mitpress/14436.001.0001,,,
844,10.24963/ijcai.2023/42,,,
845,10.1162/99608f92.8cd550d1,,,
846,10.1007/s10676-017-9428-2,,,
847,10.1126/scirobotics.aay7120,,,
848,10.1007/s10676-009-9184-z,,,
849,10.1007/s10676-006-9111-5,,,
850,10.1007/978-94-007-7914-3_9,,,
851,10.1093/oso/9780199256044.001.0001,,,
852,10.1177/2053951716679679,,,
853,10.1007/s10699-022-09833-5,,,
854,10.1007/s11948-010-9241-3,,,
855,10.1007/s43681-022-00141-z,,,
856,10.7413/24208914133,,,
857,10.1007/s44163-023-00076-2,,,
858,10.1515/9783111007564-006,,,
859,10.1007/s13347-021-00450-x,,,
860,10.1007/s10676-006-9112-4,,,
861,10.29173/irie136,,,
862,10.1007/s00146-021-01378-8,,,
863,10.1007/s11023-020-09537-4,,,
864,10.1007/s11097-008-9099-x,,,
865,10.7208/chicago/9780226852904.001.0001,,,
866,10.1177/1558689812437101,,,
867,10.1386/adch.9.1.57_1,,,
868,10.1080/14606925.2021.2004717,,,
869,10.1007/978-3-031-23152-0_17,,,
870,10.1111/jade.12460,,,
871,10.1145/3467479,,,
872,10.1177/0193841x8500900505,,,
873,10.4135/9781483348858.n9,,,
874,10.1111/jpim.12523,,,
875,10.1007/978-981-15-5784-2_12,,,
876,10.1002/ev.20099,,,
877,10.1007/978-3-319-69548-8_14,,,
878,10.1007/s43681-022-00181-5,,,
879,10.1017/cbo9780511807947.002,,,
880,10.1080/09672559.2011.561616,,,
881,10.31234/osf.io/3g5ks,,,
882,10.1002/pfi.21891,,,
883,10.1057/9780230236837_5,,,
884,10.1023/a:1005233831499,,,
885,10.17705/1thci.00131,,,
886,10.4324/9780203413845,,,
887,10.1017/9789048503841,,,
888,10.1075/bct.16,,,
889,10.1108/00012530810887953,,,
890,10.36643/mjrl.26.sp.when,,,
891,10.1073/pnas.2105061118,,,
892,10.1145/585597.585616,,,
893,10.1109/98.943998,,,
894,10.1007/978-3-540-88546-7_47,,,
895,10.1016/j.pmcj.2009.04.001,,,
896,10.3233/ais-180508,,,
897,10.1109/smc42975.2020.9283454,,,
898,10.1007/978-3-319-96448-5_5,,,
899,10.1016/j.chb.2012.05.027,,,
900,10.4135/9781483326108,,,
901,10.1596/978-0-8213-8868-6,,,
902,10.1080/09505431.2013.786990,,,
903,10.1007/s11024-009-9124-4,,,
904,10.7208/chicago/9780226276663.003.0005,,,
905,10.24251/hicss.2020.502,,,
906,10.1177/0306312710380301,,,
907,10.24251/hicss.2022.572,,,
908,10.1093/oso/9780198795209.001.0001,,,
909,10.1017/s1358246100009176,,,
910,10.1093/oso/9780198237198.001.0001,,,
911,10.1017/cbo9781139059138.019,,,
912,10.4135/9781452256795,,,
913,10.1017/cbo9780511487415,,,
914,10.1207/s15327965pli1104_03,,,
915,10.1080/07360932.2012.709318,,,
916,10.1037/h0054346,,,
917,10.1016/j.cedpsych.2020.101860,,,
918,10.1007/s11017-016-9393-5,,,
919,10.1007/978-3-319-96448-5_18,,,
920,10.4159/9780674039681,,,
921,10.1007/s11948-015-9636-2,,,
922,10.1007/s12599-016-0453-1,,,
923,10.1002/pra2.487,,,
924,10.1007/978-3-319-32098-4,,,
925,10.1038/s42256-019-0136-y,,,
926,10.30965/9783957437488_009,,,
927,10.1126/science.aal4230,,,
928,10.1016/j.jpdc.2019.07.007,,,
929,10.1080/09540091.2017.1313817,,,
930,10.1007/s10506-017-9214-9,,,
931,10.1049/pbce130e,,,
932,10.1145/3274371,,,
933,10.7551/mitpress/10058.001.0001,,,
934,10.1145/3209581,,,
935,10.1126/science.adh4451,,,
936,10.1145/3299768,,,
937,10.5210/fm.v28i11.13346,,,
938,10.5210/fm.v28i6.13185,,,
939,10.1016/j.mlwa.2024.100525,,,
940,10.1038/s42256-019-0055-y,,,
941,10.1080/15228053.2023.2233814,,,
942,10.1145/1290958.1290968,,,
943,10.1145/3571730,,,
944,10.1038/s41562-021-01128-2,,,
945,10.1109/mc.2022.3144763,,,
946,10.1145/3624721,,,
947,10.1038/s41467-022-32186-3,,,
948,10.1038/s42256-022-00458-8,,,
949,10.1145/3584973,,,
950,10.1126/sciadv.adk2031,,,
951,10.2139/ssrn.4423874,,,
952,10.2139/ssrn.4507244,,,
953,10.1038/d41586-023-00288-7,,,
954,10.1145/966389.966390,,,
955,10.1126/science.aap9559,,,
956,10.51685/jqd.2024.icwsm.7,,,
957,10.1162/coli_a_00502,,,
958,10.1108/ijssp-05-2020-0174,,,
959,10.3389/fpsyg.2020.580820,,,
960,10.1016/s0140-6736(20)30226-9,,,
961,10.1093/oso/9780190905033.003.0008,,,
962,10.1016/j.jbusres.2020.11.006,,,
963,10.5771/2747-5182-2021-1-12,,,
964,10.1007/978-3-030-32361-5_1,,,
965,10.26650/b/ss07.2020.013,,,
966,10.1007/s00146-023-01750-w,,,
967,10.1007/s44202-022-00020-y,,,
968,10.1016/j.chbah.2024.100070,,,
969,10.1007/978-94-6265-531-7_4,,,
970,10.1177/1440783319873046,,,
971,10.1007/s43681-022-00184-2,,,
972,10.1007/s13347-020-00414-7,,,
973,10.1007/978-3-030-72644-7_4,,,
974,10.1007/s10796-022-10336-8,,,
975,10.3389/fpsyg.2022.954444,,,
976,10.4337/9781803928562.00011,,,
977,10.1186/s40991-019-0045-8,,,
978,10.25236/fsst.2023.051411,,,
979,10.1007/s11547-020-01135-9,,,
980,10.1017/s0963180120000985,,,
981,10.1007/s00146-020-01119-3.10.4337/9781803928562.00011,,,
982,10.1007/s13347-022-00571-x,,,
983,10.1016/j.scitotenv.2020.142561,,,
984,10.1108/ijmpb-06-2017-0058,,,
985,10.1201/9781351251389-4,,,
986,10.1093/oso/9780198883098.001.0001,,,
987,10.1515/9783110770216-002,,,
988,10.4337/9781803928562.00049,,,
989,10.1002/hbe2.140,,,
990,10.1007/978-3-031-12482-2,,,
991,10.4337/9781803928562.00018,,,
992,10.17645/pag.v8i4.3158,,,
993,10.1515/9783110770216-005,,,
994,10.11588/heibooks.1106.c,,,
995,10.1080/0960085x.2022.2026621,,,
996,10.1177/20539517211046182,,,
997,10.1080/17579961.2018.1527480,,,
998,10.1007/s00146-018-0866-0,,,
999,10.1093/cjres/rsz022,,,
1000,10.2307/j.ctv1ghv45t,,,
1001,10.4337/9781803928562.00043,,,
1002,10.1177/2057047320972029,,,
1003,10.1515/9783839447192-010,,,
1004,10.1007/s00146-017-0753-0,,,
1005,10.1016/j.telpol.2020.101961,,,
1006,10.1108/dprg-09-2018-0054,,,
1007,10.1080/14494035.2021.1928377,,,
1008,10.4337/9781803928562.00042,,,
1009,10.1177/2053951720915939,,,
1010,10.4337/9781803928562.00036,,,
1011,10.4337/9781803928562.00051,,,
1012,10.4337/9781803928562.00031,,,
1013,10.1016/j.techfore.2023.122467,,,
1014,10.1007/s00146-022-01511-1,,,
1015,10.1016/j.tele.2020.101433,,,
1016,10.1558/jalpp.21051,,,
1017,10.2478/nor-2022-0004,,,
1018,10.1177/1470,,,
1019,10.1108/jices-12-2020-0118,,,
1020,10.3390/soc10010023,,,
1021,10.1007/s00146-021-01145-9,,,
1022,10.1007/s00146-024-01884-5,,,
1023,10.1002/epa2.1203,,,
1024,10.1108/jices-04-2019-0039648849221122647,,,
1025,10.1177/0095399707300703,,,
1026,10.1111/j.1540-4560.2005.00430.x,,,
1027,10.1016/s1040-2608(03)08004-3,,,
1028,10.1017/9781009106900,,,
1029,10.1007/s43681-023-00337-x,,,
1030,10.1080/13501763.2024.2318475,,,
1031,10.3233/ip-211532,,,
1032,10.11117/rdp.v18i100.6197,,,
1033,10.1016/j.inffus.2023.101896,,,
1034,10.1007/s10676-024-09770-w,,,
1035,10.1111/rego.12512,,,
1036,10.69554/qhay8067,,,
1037,10.1016/j.clsr.2023.105798,,,
1038,10.5771/2568-9185-2021-2-392,,,
1039,10.60097/acig/162856,,,
1040,10.1111/1467-8551.12554,,,
1041,10.1080/17579961.2023.2245683,,,
1042,10.13135/2785-7867/7169,,,
1043,10.1007/s43681-024-00467-w,,,
1044,10.1007/978-3-030-89388-0_7,,,
1045,10.30958/ajl.9-1-2,,,
1046,10.14763/2024.1.1746,,,
1047,10.1007/s43681-021-00095-8,,,
1048,10.1007/978-3-031-04083-2_17,,,
1049,10.1007/s11023-022-09612-y,,,
1050,10.1007/s43681-023-00305-5,,,
1051,10.1007/s10746-011-9175-z,,,
1052,10.1017/cbo9780511790867,,,
1053,10.1177/1464884913482551,,,
1054,10.4135/9780857028020,,,
1055,10.1080/21670811.2015.1093271,,,
1056,10.4324/9781315170008-2,,,
1057,10.1007/s00146-021-01161-9,,,
1058,10.4324/9780429038839,,,
1059,10.1111/1468-4446.13043,,,
1060,10.1177/07439156221103860,,,
1061,10.1136/bmj.n26,,,
1062,10.1007/978-3-031-27789-4_4,,,
1063,10.58875/zaud1691,,,
1064,10.1093/hcr/hqad040,,,
1065,10.1177/10755470231217536,,,
1066,10.17645/mac.v10i3.5401,,,
1067,10.1080/21670811.2022.2085129,,,
1068,10.1057/s41599-023-02282-w,,,
1069,10.1038/s41598-023-34622-w,,,
1070,10.1007/s44206-024-00095-1,,,
1071,10.1007/s00146-023-01653-w,,,
1072,10.1007/s11023-021-09577-4,,,
1073,10.1080/13510347.2023.2196068,,,
1074,10.1177/1464884917743176,,,
1075,10.1515/9783110238174.325,,,
1076,10.1002/9781118584194.ch10,,,
1077,10.1038/s41562-023-01538-4,,,
1078,10.1126/sciadv.adh1850,,,
1079,10.1109/mis.2006.83,,,
1080,10.1017/cbo9780511978036,,,
1081,10.18653/v1/w17-1602,,,
1082,10.18653/v1/w17-1601,,,
1083,10.1111/j.1467-9973.1985.tb00173.x,,,
1084,10.1017/cbo9780511978036.013,,,
1085,10.1109/cloudcom.2010.59,,,
1086,10.4018/jdm.2019010104,,,
1087,10.2307/1399830,,,
1088,10.1080/03080188.2020.1840225,,,
1089,10.1108/shr-04-2019-0024,,,
1090,10.1080/13569775.2020.1791306,,,
1091,10.1016/j.infoandorg.2023.100478,,,
1092,10.1016/j.ijinfomgt.2021.102433,,,
1093,10.1007/s43681-021-00084-x,,,
1094,10.1145/3460418.3479344,,,
1095,10.1007/s13347-023-00624-9,,,
1096,10.1145/3442188.3445922,,,
1097,10.1017/beq.2020.14,,,
1098,10.7208/chicago/9780226748603.001.0001,,,
1099,10.1037/xge0001250,,,
1100,10.5840/beq201323438,,,
1101,10.1007/s11948-023-00443-3,,,
1102,10.1002/9781118922590.ch23,,,
1103,10.21552/delphi/2019/2/4,,,
1104,10.1007/s10676-006-9107-1,,,
1105,10.1089/big.2016.0047,,,
1106,10.1093/jmp/15.2.219,,,
1107,10.1177/1527476418796632,,,
1108,10.1016/j.chb.2020.106626,,,
1109,10.1007/s13347-021-00488-x,,,
1110,10.1108/jices-12-2020-0125,,,
1111,10.1007/s13347-022-00584-6,,,
1112,10.2307/20439056,,,
1113,10.54648/cola2018095,,,
1114,10.2307/1959090,,,
1115,10.1145/3287560.3287584,,,
1116,10.1007/s13347-022-00557-9,,,
1117,10.1007/978-3-030-62330-2,,,
1118,10.1007/s43681-020-00008-1,,,
1119,10.1126/science.aax0162,,,
1120,10.1007/s10551-022-05049-6,,,
1121,10.1007/s00146-021-01267-0,,,
1122,10.1177/0894439320980118,,,
1123,10.1080/15027570.2018.1552512,,,
1124,10.1038/d41586-023-02990-y,,,
1125,10.3390/jintelligence9030046,,,
1126,10.1007/978-3-319-96448-5_29,,,
1127,10.1016/j.giq.2020.101489,,,
1128,10.1287/mnsc.2020.3727,,,
1129,10.1080/0960085x.2021.1927212,,,
1130,10.3389/frai.2021.705164,,,
1131,10.1007/s10676-017-9419-3,,,
1132,10.4324/9781315197128,,,
1133,10.1007/s13347-017-0279-x,,,
1134,10.3390/bdcc3020021,,,
1135,10.1177/14748851211041702,,,
1136,10.1145/3457607,,,
1137,10.1146/annurev-statistics-042720-125902,,,
1138,10.1038/s42256-019-0114-4,,,
1139,10.48550/arxiv.2302.02404,,,
1140,10.1007/s00199-021-01344-x,,,
1141,10.1007/s11023-021-09563-w,,,
1142,10.1007/s00146-021-01308-8,,,
1143,10.2307/j.ctv272454p,,,
1144,10.1080/10357718.2020.1734772,,,
1145,10.1007/978-94-007-7914-3_6,,,
1146,10.1038/s42256-022-00449-9,,,
1147,10.1108/jeim-11-2020-0436,,,
1148,10.1007/s10676-017-9430-8,,,
1149,10.2307/j.ctvjf9z6v,,,
1150,10.4159/9780674042582,,,
1151,10.2307/j.ctv1pncngc,,,
1152,10.1177/2053951720942541,,,
1153,10.2139/ssrn.4105328,,,
1154,10.1016/j.giq.2022.101679,,,
1155,10.2307/j.ctvjnrv7n,,,
1156,10.1017/beq.2015.1,,,
1157,10.1007/s43681-021-00080-1,,,
1158,10.1111/npqu.11358,,,
1159,10.1007/s11948-020-00277-3,,,
1160,10.1007/s10676-022-09633-2,,,
1161,10.48550/arxiv.1901.04730,,,
1162,10.48550/arxiv.2101.12701,,,
1163,10.1007/s43681-024-00464-z,,,
1164,10.1080/01900692.2018.1498103,,,
1165,10.1007/s13347-018-0330-6,,,
1166,10.1007/s10618-017-0506-1,,,
1167,10.1146/annurev-biodatasci-092820-114757,,,
1168,10.1136/svn-2017-000101,,,
1169,10.1038/s41598-020-61123-x,,,
1170,10.1177/1178222618792860,,,
1171,10.1016/j.socscimed.2020.113172,,,
1172,10.2196/mental.7785,,,
1173,10.1016/b978-0-12-818438-7.00012-5,,,
1174,10.1136/bmj.m363,,,
1175,10.1093/jamia/ocz192,,,
1176,10.1016/s0140-6736(19)32975-7,,,
1177,10.1007/s11948-019-00165-5,,,
1178,10.1145/3313831.3376445,,,
1179,10.1186/s12916-019-1377-7,,,
1180,10.1109/mic.2017.4180835,,,
1181,10.1093/tbm/ibz074,,,
1182,10.1145/3377811.3380393,,,
1183,10.1109/icse-seip.2019.00042,,,
1184,10.7861/futurehosp.6-2-94,,,
1185,10.1038/s41591-019-0548-6,,,
1186,10.2196/13216,,,
1187,10.1016/b978-0-12-420248-1.00011-8,,,
1188,10.2471/blt.19.237107,,,
1189,10.1017/s0033291720001683,,,
1190,10.1145/3287560.3287587,,,
1191,10.2196/mental.9423,,,
1192,10.1080/15265161.2020.1819469,,,
1193,10.1016/j.copsyc.2016.01.004,,,
1194,10.1007/s10549-018-4688-z,,,
1195,10.1007/s41649-019-00096-0,,,
1196,10.1002/bsl.2392,,,
1197,10.1093/jamiaopen/ooz054,,,
1198,10.1145/3398069,,,
1199,10.1080/09638237.2020.1714011,,,
1200,10.1038/nature21056,,,
1201,10.1016/j.ebiom.2021.103358,,,
1202,10.1109/isbi.2018.8363547,,,
1203,10.1613/jair.953,,,
1204,10.1145/3290605.3300233,,,
1205,10.1016/j.compag.2021.106646,,,
1206,10.1038/sdata.2018.11,,,
1207,10.1609/icwsm.v13i01.3360,,,
1208,10.1145/3351095.3372829,,,
1209,10.1111/j.1529-1006.2005.00022.x,,,
1210,10.1177/1046496412453622,,,
1211,10.1109/icmla.2019.00196,,,
1212,10.1162/99608f92.644ef4a4,,,
1213,10.1038/s41591-018-0307-0,,,
1214,10.1056/nejmp1714229,,,
1215,10.1109/tkde.2019.2946162,,,
1216,10.1007/s43681-021-00067-y,,,
1217,10.1145/3287560.3287589,,,
1218,10.1186/s40537-016-0059-y,,,
1219,10.1109/icde.2007.367856,,,
1220,10.14778/2733004.2733059,,,
1221,10.1109/comst.2019.2944748,,,
1222,10.1016/j.future.2018.03.034,,,
1223,10.1145/3085504.3091117,,,
1224,10.1145/2660267.2660348,,,
1225,10.1109/csf.2017.11,,,
1226,10.1016/j.future.2018.04.018,,,
1227,10.1016/j.inffus.2020.07.009,,,
1228,10.1016/j.procs.2018.05.020,,,
1229,10.1145/3128572.3140450,,,
1230,10.1145/3375627.3375832,,,
1231,10.1109/raise.2012.6227968,,,
1232,10.1109/ijcnn.2010.5596787,,,
1233,10.1007/s10115-011-0463-8,,,
1234,10.1109/tkde.2012.72,,,
1235,10.14778/3137628.3137631,,,
1236,10.14778/2994509.2994514,,,
1237,10.1145/3329486.3329493,,,
1238,10.1109/sp.2008.11,,,
1239,10.1145/3479582,,,
1240,10.1145/3442188.3445918,,,
1241,10.1109/mic.2018.112102519,,,
1242,10.1162/tacl_a_00041,,,
1243,10.1145/3375627.3375825,,,
1244,10.1609/aaai.v33i01.33011418,,,
1245,10.1007/s10618-010-0190-x,,,
1246,10.1145/3411763.3441342,,,
1247,10.1145/3313831.3376590,,,
1248,10.1136/medethics-2019-105935,,,
1249,10.1002/hast.1079,,,
1250,10.1038/s42256-019-0048-x,,,
1251,10.1016/j.inffus.2019.12.012,,,
1252,10.1002/widm.1312,,,
1253,10.1007/978-1-4471-0513-8_46,,,
1254,10.1007/s00146-021-01154-8,,,
1255,10.1145/2996758.2996771,,,
1256,10.1007/978-3-319-20248-8_15,,,
1257,10.1145/3003816,,,
1258,10.1145/2020408.2020495,,,
1259,10.1007/978-3-642-02326-2_14,,,
1260,10.1109/tse.2016.2584050,,,
1261,10.1109/icde.2019.00139,,,
1262,10.1109/sp.2015.35,,,
1263,10.1109/cic50333.2020.00030,,,
1264,10.1377/hlthaff.2014.0048,,,
1265,10.1145/3292500.3330885,,,
1266,10.1007/s11023-021-09557-8,,,
1267,10.1016/j.jbi.2020.103500,,,
1268,10.9778/cmajo.20190151,,,
1269,10.1126/science.aaw4399,,,
1270,10.18653/v1/w17-1610,,,
1271,10.1007/978-3-030-42504-3_26,,,
1272,10.7326/m18-1990,,,
1273,10.1609/icaps.v24i1.13667,,,
1274,10.1007/978-3-642-30487-3_13,,,
1275,10.1007/s10115-012-0584-8,,,
1276,10.1609/aaai.v33i01.33013,,,
1277,10.24963/ijcai.2019/205,,,
1278,10.1136/bmjqs-2018-008551,,,
1279,10.1109/msp.2016.51,,,
1280,10.1038/s41746-020-0254-2,,,
1281,10.1145/3274463,,,
1282,10.1145/3195570.3195580,,,
1283,10.1007/s00134-020-06277-y,,,
1284,10.1145/3299887.3299891,,,
1285,10.2139/ssrn.3435011,,,
1286,10.1145/3351095.3372860,,,
1287,10.1093/ijlit/eaz002,,,
1288,10.2139/ssrn.3543112,,,
1289,10.1007/s41649-019-00099-x,,,
1290,10.1109/tse.2020.3038802,,,
1291,10.1109/iwesep49350.2019.00017,,,
1292,10.1145/3382494.3410681,,,
1293,10.1109/tse.2019.2937083,,,
1294,10.1093/cybsec/tyy001,,,
1295,10.1145/3290605.3300830,,,
1296,10.1145/3359283,,,
1297,10.1016/j.techsoc.2023.102362,,,
1298,10.4324/9781003365877,,,
1299,10.1177/00420980231203386,,,
1300,10.1007/s44163-023-00060-w,,,
1301,10.1007/s00146-022-01598-6,,,
1302,10.1007/s00146-022-01422-1,,,
1303,10.3386/w30017,,,
1304,10.1007/s43681-023-00264-x,,,
1305,10.1007/s43681-022-00160-w,,,
1306,10.1093/oso/9780198862536.003.0001,,,
1307,10.1007/s43681-023-00284-7,,,
1308,10.1007/s43681-023-00310-8,,,
1309,10.1007/s13347-020-00434-3,,,
1310,10.4324/9781003365877-26,,,
1311,10.4324/9781315652627,,,
1312,10.4337/9781803928562,,,
1313,10.1522/cla.mak.cap2,,,
1314,10.5040/9781350284951,,,
1315,10.1177/016224399502000205,,,
1316,10.1007/s00146-021-01299-6,,,
1317,10.1080/14626268.2016.1145127,,,
1318,10.1177/14614448221077278,,,
1319,10.48550/arxiv.2304.00612,,,
1320,10.1016/j.caeai.2023.100143,,,
1321,10.1007/s00146-020-00960-w,,,
1322,10.1007/s11747-019-00710-5,,,
1323,10.1007/s13347-019-00382-7,,,
1324,10.4324/9781315512013,,,
1325,10.1177/03063127231185095,,,
1326,10.1177/20539517241232630,,,
1327,10.56021/9780801829758,,,
1328,10.1177/01622439211030007,,,
1329,10.1007/s43762-022-00036-z,,,
1330,10.1007/s00146-020-01104-w,,,
1331,10.1177/00420980211062888,,,
1332,10.1080/10630732.2023.2292823,,,
1333,10.1038/d41586-020-02003-2,,,
1334,10.1007/s43681-022-00156-6,,,
1335,10.1068/d16812,,,
1336,10.31235/osf.io/yvw7d,,,
1337,10.1007/s11023-023-09627-z,,,
1338,10.1007/s10609-022-09437-5,,,
1339,10.1093/oso/9780197539538.003.0012,,,
1340,10.1093/oso/9780197539538.003.0003,,,
1341,10.1093/oso/9780197539538.003.0002,,,
1342,10.1093/oso/9780197539538.003.0006,,,
1343,10.1093/oso/9780197539538.003.0005,,,
1344,10.1126/sciadv.aao5580,,,
1345,10.1037/a0020473,,,
1346,10.1007/s10677-020-10066-3,,,
1347,10.1093/oso/9780195320503.001.0001,,,
1348,10.1017/s0841820900001594,,,
1349,10.1163/17455243-20213439,,,
1350,10.1080/0731129x.2023.2275967,,,
1351,10.1007/s10790-021-09835-9,,,
1352,10.1007/s13347-024-00694-3,,,
1353,10.1080/0731129x.2018.1552359,,,
1354,10.1177/0146167205282152,,,
1355,10.1521/soco.2007.25.1.48,,,
1356,10.1348/135532508x284293,,,
1357,10.1007/978-94-009-9461-4,,,
1358,10.1111/phib.12235,,,
1359,10.1086/688456,,,
1360,10.1017/s002122370001061x,,,
1361,10.1093/socpro/spaa004,,,
1362,10.1093/oso/9780197539538.003.0004,,,
1363,10.1093/oso/9780197539538.001.0001,,,
1364,10.1080/1369118x.2018.1444783,,,
1365,10.1017/s0012217315000487,,,
1366,10.5703/educationculture.33.2.0013,,,
1367,10.1007/s10676-015-9380-y,,,
1368,10.1086/292815,,,
1369,10.1080/02691728.2021.1949643,,,
1370,10.1007/s13347-015-0211-1,,,
1371,10.1080/02691728.2013.782585,,,
1372,10.1080/1369118x.2018.1428656,,,
1373,10.1093/oxfordhb/9780190491192.001.0001,,,
1374,10.1007/978-1-4020-8849-0_2,,,
1375,10.1093/oso/9780198823452.001.0001,,,
1376,10.1093/oso/9780198833659.003.0016,,,
1377,10.1007/bf00372372,,,
1378,10.1080/02691728.2017.1317868,,,
1379,10.3389/fcomm.2021.632317,,,
1380,10.1177/1065912920938143,,,
1381,10.1007/s43681-022-00151-x,,,
1382,10.3998/pc.12322227.0009.011,,,
1383,10.1017/epi.2018.32,,,
1384,10.1111/npqu.12143,,,
1385,10.1080/05568641.2020.1780149,,,
1386,10.1353/ken.2017.0025,,,
1387,10.1515/mopp-2020-0039,,,
1388,10.1038/s42256-022-00537-w,,,
1389,10.1177/1470594x19872505,,,
1390,10.1093/0195128923.001.0001,,,
1391,10.1007/s13347-021-00494-z,,,
1392,10.7551/mitpress/11807.001.0001,,,
1393,10.18352/ulr.420,,,
1394,10.1515/pjbr-2018-0024,,,
1395,10.1017/9781108676649,,,
1396,10.1017/9781108680844,,,
1397,10.1007/s10506-017-9211-z,,,
1398,10.1163/15718182-02803011,,,
1399,10.1098/rstb.2006.2004,,,
1400,10.1111/j.2041-6962.1992.tb00658.x,,,
1401,10.1609/aimag.v40i2.2850,,,
1402,10.2760/57493,,,
1403,10.1007/s10551-018-3956-5,,,
1404,10.4018/978-1-7998-4894-3.ch006,,,
1405,10.1016/j.chb.2019.06.012,,,
1406,10.1093/0195134613.001.0001,,,
1407,10.1007/s43681-020-00001-8,,,
1408,10.5771/9781786612281,,,
1409,10.2139/ssrn.3259344,,,
1410,10.1145/2939672.2939778,,,
1411,10.1007/s10584-014-1169-1,,,
1412,10.1353/jhi.2014.0024,,,
1413,10.1016/j.procs.2020.09.198,,,
1414,10.1007/978-3-642-29041-1_3,,,
1415,10.1186/s40648-020-00156-3,,,
1416,10.3389/fncom.2021.620281,,,
1417,10.1109/tnnls.2020.3027314,,,
1418,10.1163/15718182-02803009,,,
1419,10.21552/delphi/2019/3/5,,,
1420,10.1007/978-3-319-91899-0_1,,,
1421,10.1145/3450963,,,
1422,10.2139/ssrn.4312358,,,
1423,10.2139/ssrn.4308687,,,
1424,10.2139/ssrn.4341500,,,
1425,10.1109/tkde.2021.3130191,,,
1426,10.18653/v1/2021.nuse-1.5,,,
1427,10.1016/j.nepr.2022.103537,,,
1428,10.1177/10776958221149577,,,
1429,10.36227/techrxiv.21789434.v1,,,
1430,10.37074/jalt.2023.6.1.9,,,
1431,10.35542/osf.io/4mec3,,,
1432,10.1007/s13384-013-0089-9,,,
1433,10.1016/j.stueduc.2011.03.001,,,
1434,10.2139/ssrn.4312418,,,
1435,10.1097/ncm.0000000000000533,,,
1436,10.1007/978-3-030-96993-6_3,,,
1437,10.3197/096327112x13225063227907,,,
1438,10.1007/s13347-013-0122-y,,,
1439,10.51291/2377-7478.1200,,,
1440,10.4324/9781315755021,,,
1441,10.1017/s0140525x00038188,,,
1442,10.48550/arxiv.2005.14165,,,
1443,10.1111/j.1088-4963.2009.01166.x,,,
1444,10.1007/s10676-010-9235-5,,,
1445,10.1007/s13347-013-0133-8,,,
1446,10.1007/s11023-020-09554-3,,,
1447,10.1007/s11948-019-00119-x,,,
1448,10.1017/s0963180120001024,,,
1449,10.1017/cbo9781139172967,,,
1450,10.1111/j.2041-6962.1997.tb00839.x,,,
1451,10.1353/pbm.2022.0004,,,
1452,10.1017/9781009026710.007,,,
1453,10.1023/a:1010018611096,,,
1454,10.1007/s00146-021-01179-z,,,
1455,10.1086/710541,,,
1456,10.2307/2025709,,,
1457,10.1007/s43681-021-00050-7,,,
1458,10.1093/oxfordhb/9780199941339.013.9,,,
1459,10.1007/s10676-017-9442-4,,,
1460,10.1111/papq.12022,,,
1461,10.1023/a:1024469419944,,,
1462,10.1007/s11948-021-00331-8,,,
1463,10.1111/japp.12051,,,
1464,10.1007/s10806-018-9730-y,,,
1465,10.1086/512780,,,
1466,10.1007/s10676-018-9481-5,,,
1467,10.1093/oso/9780198829676.001.0001,,,
1468,10.1111/j.1747-9991.2008.00196.x,,,
1469,10.1007/s43681-021-00076-x,,,
1470,10.1093/oso/9780198722274.001.0001,,,
1471,10.1093/0195079981.001.0001,,,
1472,10.1142/s270507852150003x,,,
1473,10.1007/s00146-020-01002-1,,,
1474,10.1007/s13347-013-0114-y,,,
1475,10.31234/osf.io/sujwf,,,
1476,10.1016/j.chb.2022.107372,,,
1477,10.1017/9781316585061,,,
1478,10.1111/j.1468-5930.2009.00475.x,,,
1479,10.1111/papq.12420,,,
1480,10.5840/enviroethics19824220,,,
1481,10.1111/misp.12032,,,
1482,10.5840/harvardreview20185913,,,
1483,10.1017/s0963180120001012,,,
1484,10.1093/oso/9780192894076.003.0018,,,
1485,10.1017/cbo9780511975950,,,
1486,10.1093/oso/9780192894076.003.0016,,,
1487,10.1007/s11948-020-00230-4,,,
1488,10.1007/s10676-004-6491-2,,,
1489,10.1080/00455091.1987.10715920,,,
1490,10.3389/fpsyg.2018.01230,,,
1491,10.1037/a0036054,,,
1492,10.3390/bdcc3010002,,,
1493,10.1007/s11948-017-9901-7,,,
1494,10.1080/03080188.2020.1840220,,,
1495,10.1080/14494035.2020.1855800,,,
1496,10.1016/j.giq.2021.101652,,,
1497,10.1007/s00146-023-01658-5,,,
1498,10.5204/lthj.2332,,,
1499,10.1007/s43508-022-00049-8,,,
1500,10.3389/frai.2023.976887,,,
1501,10.1038/d41586-019-01413-1,,,
1502,10.1080/09505431.2021.1990875,,,
1503,10.1515/mopp-2020-0056,,,
1504,10.1007/s44206-022-00013-3,,,
1505,10.1016/j.clsr.2023.105795,,,
1506,10.1007/s43681-022-00209-w,,,
1507,10.1016/j.respol.2018.10.028,,,
1508,10.1007/s11024-020-09405-6,,,
1509,10.1098/rsta.2018.0080,,,
1510,10.1016/j.giq.2020.101476,,,
1511,10.1007/s11024-006-0012-x,,,
1512,10.1371/journal.pone.0254201,,,
1513,10.1215/s12280-007-9004-7,,,
1514,10.1177/03063127221119424,,,
1515,10.1186/1748-5908-5-69,,,
1516,10.1080/1364557032000119616,,,
1517,10.7275/z6fm-2e34,,,
1518,10.1038/s41586-018-0637-6,,,
1519,10.1055/s-0039-1677901,,,
1520,10.1007/s12369-015-0322-y,,,
1521,10.1007/s00146-021-01361-3,,,
1522,10.1016/j.trf.2018.08.010,,,
1523,10.2196/44357,,,
1524,10.1136/bmjopen-2020-039798,,,
1525,10.3389/fpubh.2021.623088,,,
1526,10.1177/20539517221092956,,,
1527,10.1016/j.giq.2022.101683,,,
1528,10.1186/s40561-023-00237-x,,,
1529,10.1080/13501763.2022.2094988,,,
1530,10.1186/s12911-020-01191-1,,,
1531,10.1186/s12967-019-02204-y,,,
1532,10.3390/ijerph18189933,,,
1533,10.1186/s12910-022-00842-4,,,
1534,10.1016/j.giq.2022.101688,,,
1535,10.2196/37611,,,
1536,10.4018/ijegr.298216,,,
1537,10.3389/fpsyg.2019.02415,,,
1538,10.1007/s00146-020-00941-z,,,
1539,10.1080/21670811.2021.1912622,,,
1540,10.1016/j.jengtecman.2018.04.006,,,
1541,10.1080/14719037.2022.2063934,,,
1542,10.1007/s10676-022-09627-0,,,
1543,10.2196/10410,,,
1544,10.1371/journal.pone.0279088,,,
1545,10.1016/j.ijmedinf.2022.104903,,,
1546,10.1080/10510974.2020.1807380,,,
1547,10.1016/j.techsoc.2021.101666,,,
1548,10.1007/s00146-022-01473-4,,,
1549,10.1007/s00146-021-01323-9,,,
1550,10.2196/26611,,,
1551,10.1186/s12913-021-07044-5,,,
1552,10.1007/s10551-022-05059-4,,,
1553,10.1016/j.chb.2019.08.010,,,
1554,10.3390/app13063887,,,
1555,10.1145/3484495,,,
1556,10.30191/ets.202301_26(1).0002,,,
1557,10.3390/ijerph19137817,,,
1558,10.1080/10447318.2021.2009669,,,
1559,10.3390/healthcare7020060,,,
1560,10.1007/s40593-022-00293-3,,,
1561,10.1177/03063127231163756,,,
1562,10.1136/jme-2022-108850,,,
1563,10.1136/bmjopen-2021-052287,,,
1564,10.1038/s41562-019-0762-8,,,
1565,10.2196/12802,,,
1566,10.1177/2055207620968355,,,
1567,10.3389/frobt.2021.781985,,,
1568,10.1098/rstb.2021.0083,,,
1569,10.1111/bioe.12885,,,
1570,10.1186/s12910-021-00598-3,,,
1571,10.1109/tts.2022.3195114,,,
1572,10.2196/35223,,,
1573,10.1007/s00146-022-01436-9,,,
1574,10.2196/10240,,,
1575,10.1007/s00146-022-01461-8,,,
1576,10.1038/s41746-022-00641-6,,,
1577,10.1186/s12909-021-02546-6,,,
1578,10.1007/s00146-022-01611-y,,,
1579,10.1007/s13347-022-00553-z,,,
1580,10.1177/09636625080985,,,
1581,10.23919/jsc.2021.0033,,,
1582,10.23987/sts.60425,,,
1583,10.1080/23299460.2015.1045280,,,
1584,10.1177/0963662516655686,,,
1585,10.1007/s00146-023-01694-1,,,
1586,10.1080/13600869.2018.1457002,,,
1587,10.1093/oso/9780192859624.001.0001,,,
1588,10.5040/9781350225725,,,
1589,10.1177/1461444816676645,,,
1590,10.1111/j.1525-1497.2004.30091.x,,,
1591,10.1111/bioe.12959,,,
1592,10.1007/s00146-022-01562-4,,,
1593,10.1371/journal.pone.0181142,,,
1594,10.1016/j.pbiomolbio.2017.04.001,,,
1595,10.1016/j.aquaculture.2014.12.031,,,
1596,10.1145/3637367,,,
1597,10.1016/j.patter.2023.100857,,,
1598,10.1016/j.healthpol.2023.104889,,,
1599,10.1080/14636778.2013.846582,,,
1600,10.1007/s11229-022-03933-2,,,
1601,10.1016/j.ejor.2012.09.027,,,
1602,10.1148/ryai.2021210097,,,
1603,10.1111/jep.13675,,,
1604,10.1148/ryai.2021200126,,,
1605,10.1007/s10670-007-9087-5,,,
1606,10.5840/philtopics199523117,,,
1607,10.1093/0198238207.001.0001,,,
1608,10.1038/s41598-022-09954-8,,,
1609,10.1007/bf00154329,,,
1610,10.1007/s10202-005-0073-1,,,
1611,10.1007/s10806-006-9023-8,,,
1612,10.1016/j.radi.2023.03.009,,,
1613,10.7208/chicago/9780226217239.001.0001,,,
1614,10.1016/j.jneumeth.2015.01.022,,,
1615,10.1007/s43681-020-00013-4,,,
1616,10.3390/diagnostics13071315,,,
1617,10.1007/s13194-022-00458-w,,,
1618,10.1093/jamia/ocaa085,,,
1619,10.1086/psaprocbienmeetp.1982.2.192409,,,
1620,10.1017/epi.2013.11,,,
1621,10.1016/j.plas.2022.100068,,,
1622,10.1177/14604582211052391,,,
1623,10.1093/oso/9780190905033.003.0009,,,
1624,10.1017/s0266462322000289,,,
1625,10.1007/s13347-022-00558-8,,,
1626,10.1136/bmjhci-2021-100444,,,
1627,10.1007/s43681-022-00232-x,,,
1628,10.1007/s43681-020-00015-2,,,
1629,10.1007/s00146-022-01617-6,,,
1630,10.1353/ppp.1996.0035,,,
1631,10.1111/bioe.12887,,,
1632,10.1145/3301275.3302308,,,
1633,10.1007/978-3-031-48057-7_15,,,
1634,10.1016/j.compbiomed.2023.107441,,,
1635,10.1148/radiol.230163,,,
1636,10.1111/j.1467-9973.2012.01759.x,,,
1637,10.1007/978-0-387-33921-4_23,,,
1638,10.1007/s43681-024-00465-y,,,
1639,10.1016/j.ssci.2022.105870,,,
1640,10.1145/3411764.3445088,,,
1641,10.1007/s11229-023-04122-5,,,
1642,10.1080/21550085.2019.1652288,,,
1643,10.1159/000509119,,,
1644,10.1213/ane.0000000000003698,,,
1645,10.1016/j.conctc.2021.100838,,,
1646,10.1038/s41746-024-01039-2,,,
1647,10.1007/s43681-022-00224-x,,,
1648,10.1007/s10699-021-09812-2,,,
1649,10.1007/s11948-023-00451-3,,,
1650,10.1073/pnas.1907377117,,,
1651,10.1038/s41586-022-04448-z,,,
1652,10.1007/s42979-021-00958-1,,,
1653,10.3390/ma16175927,,,
1654,10.1038/d41586-023-03817-6,,,
1655,10.3389/fendo.2023.1106625,,,
1656,10.1016/j.newideapsych.2018.12.002,,,
1657,10.31219/osf.io/285cs,,,
1658,10.1038/d41586-023-02361-7,,,
1659,10.1007/bf02583255,,,
1660,10.1080/17460441.2023.2273839,,,
1661,10.1111/2041-210x.13901,,,
1662,10.3390/pathogens12020317,,,
1663,10.2307/2709541,,,
1664,10.1038/d41586-022-02083-2,,,
1665,10.1080/08989621.2023.2256650,,,
1666,10.1038/d41586-024-00347-7,,,
1667,10.1007/s11948-019-00151-x,,,
1668,10.1007/s11019-023-10153-z,,,
1669,10.18632/oncoscience.571,,,
1670,10.1038/s41551-021-00751-8,,,
1671,10.1001/jamaoncol.2023.2954,,,
1672,10.1038/d41586-023-02218-z,,,
1673,10.32604/oncologie.2022.020259,,,
1674,10.1001/jamadermatol.2021.3129,,,
1675,10.1016/j.giq.2021.101666,,,
1676,10.1038/d41586-023-03722-y,,,
1677,10.1289/ehp4808,,,
1678,10.1007/s11948-020-00272-8,,,
1679,10.1371/journal.pone.0005738,,,
1680,10.1016/s0092-8240(05)80170-3,,,
1681,10.1002/mp.15359,,,
1682,10.1038/s41591-023-02264-0,,,
1683,10.1038/s41746-023-00927-3,,,
1684,10.1016/j.softx.2023.101601,,,
1685,10.1007/s10462-022-10256-8,,,
1686,10.1001/jama.2016.17216,,,
1687,10.1038/s43588-023-00527-x,,,
1688,10.1177/10778004211048379,,,
1689,10.1007/s10676-008-9167-5,,,
1690,10.1186/s41073-023-00133-5,,,
1691,10.31235/osf.io/9nyh8,,,
1692,10.1080/08989621.2023.2168535,,,
1693,10.1177/17470161231180449,,,
1694,10.1038/s41568-018-0016-5,,,
1695,10.1093/0195158709.001.0001,,,
1696,10.1038/s41598-022-26771-1,,,
1697,10.1038/d41586-023-03596-0,,,
1698,10.1080/13658816.2019.1684500,,,
1699,10.2139/ssrn.4342909,,,
1700,10.1038/s41586-021-03819-2,,,
1701,10.1007/s12036-022-09871-2,,,
1702,10.3390/diagnostics12112794,,,
1703,10.1038/s42254-022-00518-3,,,
1704,10.1007/s10916-017-0778-4,,,
1705,10.5492/wjccm.v9.i2.13,,,
1706,10.1002/hast.973,,,
1707,10.1515/9780691209753,,,
1708,10.1038/435737a,,,
1709,10.1016/j.shpsa.2010.11.040,,,
1710,10.1038/s41586-024-07146-0,,,
1711,10.1093/nargab/lqab065,,,
1712,10.1126/science.adm8175,,,
1713,10.1038/s41746-023-00858-z,,,
1714,10.1007/s10489-021-02193-w,,,
1715,10.1142/s2705078520300042,,,
1716,10.1038/d41586-023-03235-8,,,
1717,10.1016/j.geoforum.2022.01.016,,,
1718,10.1002/widm,,,
1719,10.1016/j.nepr.2023.103572,,,
1720,10.1038/s41746-023-00939-z,,,
1721,10.7249/rr1744,,,
1722,10.1007/s43681-021-00041-8,,,
1723,10.1126/science.aah4043,,,
1724,10.1001/jama.2019.18058,,,
1725,10.7551/mitpress/11218.001.0001,,,
1726,10.1038/s41591-023-02367-8,,,
1727,10.1080/08989621.2023.2239145,,,
1728,10.1016/0039-3681(96)00043-x,,,
1729,10.1093/acprof:oso/9780195309782.001.0001,,,
1730,10.1093/acprof:oso/9780195375893.001.0001,,,
1731,10.1097/acm.0b013e318260fe5c,,,
1732,10.1016/j.shpsa.2018.12.011,,,
1733,10.1007/s10838-022-09633-2,,,
1734,10.1080/08989621.2017.1327813,,,
1735,10.1080/08989621.2017.1397517,,,
1736,10.1038/s42256-021-00307-0,,,
1737,10.1093/jamia/ocaa210,,,
1738,10.1038/d41586-022-00858-1,,,
1739,10.1038/d41586-023-01445-8,,,
1740,10.1145/3514094.3539549,,,
1741,10.1126/science.370.6521.1144,,,
1742,10.1093/oso/9780197547090.001.0001,,,
1743,10.1146/annurev.so.21.080195.001445,,,
1744,10.1371/journal.pone.0240376,,,
1745,10.1001/jamaophthalmol.2023.5162,,,
1746,10.1038/s41592-023-02087-4,,,
1747,10.1126/science.adg7879,,,
1748,10.1093/mind/lix.236.433,,,
1749,10.1007/s13347-014-0156-9,,,
1750,10.1038/s41746-022-00592-y,,,
1751,10.1038/s41598-023-41032-5,,,
1752,10.1038/s41586-023-06221-2,,,
1753,10.1007/bf02583258,,,
1754,10.1017/cbo9780511541391,,,
1755,10.1007/s00146-015-0624-5,,,
1756,10.1111/j.1468-0386.2007.00378.x,,,
1757,10.1111/0029-4624.32.s12.5,,,
1758,10.1017/s0265052519000220,,,
1759,10.1007/s11229-020-02834-6,,,
1760,10.1007/s10676-016-9403-3,,,
1761,10.1093/acprof:oso/9780199641321.001.0001,,,
1762,10.1515/jtl-2017-0029,,,
1763,10.2307/2219043,,,
1764,10.1002/hbm.24886,,,
1765,10.1007/s13347-021-00475-2,,,
1766,10.1007/978-3-476-04896-7_12,,,
1767,10.1145/3306618.3314238,,,
1768,10.4159/9780674038783,,,
1769,10.4324/9781315201399-4,,,
1770,10.7208/chicago/9780226471013.001.0001,,,
1771,10.3390/philosophies4040058,,,
1772,10.1007/s11948-017-9943-x,,,
1773,10.1111/phc3.12507,,,
1774,10.1111/phc3.12506,,,
1775,10.1016/0010-0285(75)90021-3,,,
1776,10.1162/99608f92.5a8a3a3d,,,
1777,10.1086/659003,,,
1778,10.1016/j.artint.2021.103535,,,
1779,10.1111/j.1468-5930.2007.00346.x,,,
1780,10.5040/9798216032052,,,
1781,10.1007/978-94-007-1878-4_3,,,
1782,10.1007/978-94-007-1878-4_2,,,
1783,10.1093/acprof:oso/9780195374049.001.0001,,,
1784,10.5840/philtopics199624222,,,
1785,10.7551/mitpress/5804.001.0001,,,
1786,10.1109/cvpr.2010.5539957,,,
1787,10.1007/978-3-030-20680-2_1,,,
1788,10.1109/mitp.2020.2986121,,,
1789,10.4018/jcit.2021010101,,,
1790,10.1055/s-0039-1677908,,,
1791,10.1038/s41467-019-14108-y,,,
1792,10.1007/s40534-016-0117-3,,,
1793,10.1145/3419249.3421249,,,
1794,10.1145/3340531.3412048,,,
1795,10.2139/ssrn.3518482,,,
1796,10.1596/978-1-4648-1330-6,,,
1797,10.1080/02681102.2016.1246406,,,
1798,10.4324/9781315652603,,,
1799,10.1177/1536504212436479,,,
1800,10.48550/arxiv.2206.11922,,,
1801,10.24251/hicss.2019.258,,,
1802,10.1109/tts.2021.3052127,,,
1803,10.12987/9780300252392,,,
1804,10.1145/3531146.3533083,,,
1805,10.7551/mitpress/11022.001.0001,,,
1806,10.1017/s0892679419000121,,,
1807,10.1177/0306396818823172,,,
1808,10.1016/j.geoforum.2015.07.002,,,
1809,10.2966/scrip.170220.389,,,
1810,10.1007/s13347-020-00405-8,,,
1811,10.1080/1369118x.2021.1986102,,,
1812,10.1007/s10676-020-09570-y,,,
1813,10.1017/s0892679419000091,,,
1814,10.1017/s0892679419000169,,,
1815,10.1016/j.patter.2022.100462,,,
1816,10.1007/s44163-022-00028-2,,,
1817,10.1007/s10784-020-09487-3,,,
1818,10.1145/3531146.3533091,,,
1819,10.1007/s43681-021-00106-8,,,
1820,10.1177/2053951717736335,,,
1821,10.1080/1369118x.2019.1606268,,,
1822,10.48550/arxiv.2204.03090,,,
1823,10.1007/s13347-020-00428-1,,,
1824,10.1007/s13347-020-00412-9,,,
1825,10.7551/mitpress/11805.001.0001,,,
1826,10.1017/s0892679421000356,,,
1827,10.11116/jdivegendstud.2.1-2.0039,,,
1828,10.4324/9780203873106,,,
1829,10.1146/annurev.so.20.080194.001513,,,
1830,10.2307/3178066,,,
1831,10.1093/acprof:oso/9780199370368.001.0001,,,
1832,10.1080/10714413.2016.1155957,,,
1833,10.1145/3531146.3533172,,,
1834,10.1145/3531146.3533105,,,
1835,10.1145/3531146.3533124,,,
1836,10.7559/citarj.v11i2.665,,,
1837,10.1093/acrefore/9780190224851.013.48,,,
1838,10.1016/j.bushor.2021.07.010,,,
1839,10.1007/978-3-030-33617-2_36,,,
1840,10.1109/mitp.2015.3,,,
1841,10.1109/mitp.2020.3005640,,,
1842,10.1109/jbhi.2014.2344095,,,
1843,10.1007/s00779-020-01510-3,,,
1844,10.1016/j.future.2020.04.013,,,
1845,10.1007/978-981-13-7403-6_58,,,
1846,10.1126/science.aat5991,,,
1847,10.1111/beer.12040,,,
1848,10.1016/j.rmal.2023.100068,,,
1849,10.1007/s13347-020-00415-6,,,
1850,10.1215/07402775-3813015,,,
1851,10.1007/s00146-023-01752-8,,,
1852,10.1145/3208974,,,
1853,10.1007/s10676-024-09775-5,,,
1854,10.4094/chnr.2023.29.4.249,,,
1855,10.2139/ssrn.4544582,,,
1856,10.3390/diagnostics14010109,,,
1857,10.4324/9781315778389-4,,,
1858,10.1017/cbo9781139137171,,,
1859,10.1017/9781009154642,,,
1860,10.1017/9781009154642.008,,,
1861,10.1007/s00146-022-01443-w,,,
1862,10.21659/rupkatha.v15n4.01,,,
1863,10.7208/chicago/9780226713519.001.0001,,,
1864,10.1007/s00146-018-0856-2,,,
1865,10.3390/socsci12030148,,,
1866,10.1155/2024/7115633,,,
1867,10.1007/s00146-023-01786-y,,,
1868,10.1111/lsi.12305,,,
1869,10.1162/002081800551271,,,
1870,10.1007/s11558-010-9092-3,,,
1871,10.1007/s10676-021-09593-z,,,
1872,10.1146/annurev-lawsocsci-101317-030830,,,
1873,10.1177/001979391206500201,,,
1874,10.1093/oso/9780190088583.001.0001,,,
1875,10.1080/03071847.2019.1694260,,,
1876,10.2139/ssrn.3761636,,,
1877,10.21684/2412-2343-2021-8-1-86-115,,,
1878,10.1145/3278721.3278731,,,
1879,10.2139/ssrn.3779866,,,
1880,10.1613/jair.1.11222,,,
1881,10.1007/s43681-020-00019-y,,,
1882,10.1016/j.futures.2018.01.007,,,
1883,10.1145/3375627.3375859,,,
1884,10.1162/002081801317193691,,,
1885,10.1162/002081801317193592,,,
1886,10.2139/ssrn.3310421,,,
1887,10.1007/978-94-007-1356-7,,,
1888,10.4337/cilj.2019.02.07,,,
1889,10.1111/1758-5899.12668,,,
1890,10.1007/978-3-319-26485-1_33,,,
1891,10.1007/s13347-020-00402-x,,,
1892,10.2307/2586011,,,
1893,10.1093/oxfordhb/9780199548460.003.0003,,,
1894,10.1145/3375627.3375804,,,
1895,10.1007/s10676-018-9494-0,,,
1896,10.1007/s00146-021-01148-6,,,
1897,10.2139/ssrn.3756416,,,
1898,10.1007/s43681-020-00037-w,,,
1899,10.1146/annurev.polisci.2.1.369,,,
1900,10.1038/s42256-020-0195-0,,,
1901,10.1093/hepl/9780198807605.001.0001,,,
1902,10.1075/nlp.8.11bry,,,
1903,10.3389/frobt.2018.00015,,,
1904,10.1007/s11948-014-9565-5,,,
1905,10.1126/science.131.3410.1355,,,
1906,10.1093/arisoc/aow019,,,
1907,10.1017/cbo9781139017428,,,
1908,10.1111/papa.12119,,,
1909,10.1007/978-1-4471-6257-5,,,
1910,10.1007/s10677-019-10007-9,,,
1911,10.1007/s11948-019-00120-4,,,
1912,10.1093/acprof:oso/9780199915439.001.0001,,,
1913,10.1007/978-3-030-72644-7_10,,,
1914,10.1007/s13347-021-00483-2,,,
1915,10.1093/analys/58.1.7,,,
1916,10.1007/s13347-021-00454-7,,,
1917,10.7551/mitpress/11444.001.0001,,,
1918,10.1007/s10676-020-09541-3,,,
1919,10.1007/s10790-015-9519-4,,,
1920,10.1145/2874239.2874281,,,
1921,10.1007/s12369-017-0413-z,,,
1922,10.23919/mipro.2018.8400040,,,
1923,10.1007/s10462-021-10066-4,,,
1924,10.3389/frai.2021.622364,,,
1925,10.5220/0009150705700577,,,
1926,10.1148/radiol.2019191586,,,
1927,10.3390/diagnostics10040231,,,
1928,10.1148/radiol.2020192536,,,
1929,10.14763/2020.2.1469,,,
1930,10.1007/s43681-021-00085-w,,,
1931,10.1007/s13244-018-0599-0,,,
1932,10.1007/s43681-020-00006-3,,,
1933,10.1201/9781351251389-8,,,
1934,10.1111/j.1467-9760.2006.00235.x,,,
1935,10.1177/13505084221145589,,,
1936,10.1111/joms.13045,,,
1937,10.1177/01708406221107455,,,
1938,10.5465/amj.2023.4006,,,
1939,10.1177/10564926231219622,,,
1940,10.1111/joms.13032,,,
1941,10.1109/access.2018.2870052,,,
1942,10.1145/3459637.3482126,,,
1943,10.2139/ssrn.3559477,,,
1944,10.1007/s10618-023-00933-9,,,
1945,10.1016/b978-0-444-70536-5.50007-5,,,
1946,10.1115/1.4062232,,,
1947,10.18653/v1/2020.emnlp-main.634,,,
1948,10.18653/v1/2022.acl-long.581,,,
1949,10.1007/s10462-022-10148-x,,,
1950,10.1162/coli_a_00445,,,
1951,10.3389/fpsyg.2023.1199058,,,
1952,10.18653/v1/2023.findings-acl.477,,,
1953,10.18653/v1/2021.acl-short.8,,,
1954,10.1002/aaai.12182,,,
1955,10.1007/s10462-023-10562-9,,,
1956,10.18653/v1/2022.emnlp-main.3,,,
1957,10.1609/aaai.v33i01.33013681,,,
1958,10.18653/v1/2022.findings-acl.163,,,
1959,10.1007/s10618-022-00831-6,,,
1960,10.1145/3236009,,,
1961,10.3389/frai.2023.1225093,,,
1962,10.18653/v1/2023.emnlp-main.67,,,
1963,10.1007/978-3-031-26293-7_14,,,
1964,10.1609/aaai.v34i05.6311,,,
1965,10.1098/rsta.2023.0254,,,
1966,10.1109/iccv51070.2023.00371,,,
1967,10.1016/j.asoc.2023.110176,,,
1968,10.1162/99608f92.8036d03b,,,
1969,10.18653/v1/n16-1082,,,
1970,10.1109/cvpr52729.2023.00037,,,
1971,10.1016/j.aiopen.2022.10.001,,,
1972,10.18653/v1/2022.emnlp-main.483,,,
1973,10.1016/j.inffus.2024.102301,,,
1974,10.1162/coli_a_00511,,,
1975,10.18653/v1/d18-1151,,,
1976,10.18653/v1/2020.acl-main.173,,,
1977,10.1109/wacv57701.2024.00266,,,
1978,10.1080/10580530.2020.1849465,,,
1979,10.1016/j.artint.2018.07.007,,,
1980,10.1007/s10462-021-10088-y,,,
1981,10.18653/v1/2023.acl-long.149,,,
1982,10.18653/v1/2021.emnlp-main.61,,,
1983,10.1007/978-3-030-28954-6_10,,,
1984,10.1057/ejis.2012.26,,,
1985,10.23915/distill.00024.001,,,
1986,10.1162/tacl_a_00660,,,
1987,10.1109/cvpr.2018.00915,,,
1988,10.1016/j.artint.2021.103506,,,
1989,10.18653/v1/2023.acl-long.140,,,
1990,10.1109/satml54575.2023.00039,,,
1991,10.24963/ijcai.2017/371,,,
1992,10.1016/j.knosys.2023.110273,,,
1993,10.18653/v1/2022.emnlp-main.137,,,
1994,10.3233/aic-210081,,,
1995,10.1007/s10994-022-06157-0,,,
1996,10.1080/10580530.2022.2085825,,,
1997,10.1007/s42979-023-02401-z,,,
1998,10.1007/s12599-024-00851-0,,,
1999,10.5220/0012552300003693,,,
2000,10.1007/s10618-023-00920-0,,,
2001,10.1038/s42256-020-0212-3,,,
2002,10.1007/s10618-022-00867-8,,,
2003,10.1109/tpami.2023.3243465,,,
2004,10.1109/iccv.2017.74,,,
2005,10.18653/v1/p19-1282,,,
2006,10.2196/47564,,,
2007,10.18653/v1/2021.acl-long.71,,,
2008,10.1080/10447318.2022.2101698,,,
2009,10.1038/s41586-023-06291-2,,,
2010,10.18653/v1/2023.blackboxnlp-1.3,,,
2011,10.1038/s42256-023-00692-8,,,
2012,10.1109/apr59189.2023.00012,,,
2013,10.18653/v1/2023.emnlp-main.543,,,
2014,10.1145/3531146.3534639,,,
2015,10.1007/978-3-031-03767-2,,,
2016,10.18653/v1/2022.bigscience-1.11,,,
2017,10.1007/978-3-031-35891-3_22,,,
2018,10.1109/access.2022.3207765,,,
2019,10.1038/s41591-023-02448-8,,,
2020,10.1145/3539597.3570489,,,
2021,10.18653/v1/p19-3007,,,
2022,10.18653/v1/2023.findings-emnlp.416,,,
2023,10.18653/v1/2022.findings-naacl.14,,,
2024,10.1145/3531146.3533088,,,
2025,10.18653/v1/2020.acl-main.383,,,
2026,10.18653/v1/2021.acl-long.523,,,
2027,10.1109/tpami.2023.3275156,,,
2028,10.1145/3626235,,,
2029,10.18653/v1/2023.acl-long.572,,,
2030,10.18653/v1/2023.emnlp-main.370,,,
2031,10.18653/v1/2022.emnlp-main.14,,,
2032,10.18653/v1/2022.findings-emnlp.255,,,
2033,10.1145/3544548.3581388,,,
2034,10.1145/3639372,,,
2035,10.18653/v1/2021.naacl-main.398,,,
2036,10.18653/v1/2023.acl-long.372,,,
2037,10.1145/3529755,,,
2038,10.1016/j.jbankfin.2017.11.003,,,
2039,10.1007/s12525-020-00414-7,,,
2040,10.1007/s10115-017-1116-3,,,
2041,10.1016/j.chb.2020.106548,,,
2042,10.1007/s10676-018-9447-7,,,
2043,10.1016/j.inffus.2019.03.006,,,
2044,10.1147/jrd.2019.2942287,,,
2045,10.1287/mnsc.1120.1549,,,
2046,10.1007/978-3-319-60648-4,,,
2047,10.1007/s11023-017-9449-y,,,
2048,10.2139/ssrn.3783124,,,
2049,10.1613/jair.1.12228,,,
2050,10.1007/s10614-020-10042-0,,,
2051,10.1016/j.eswa.2022.118888,,,
2052,10.1037/h0046016,,,
2053,10.1109/mc.2019.2930097,,,
2054,10.1613/jair.1.12814,,,
2055,10.3390/info12070275,,,
2056,10.1147/jrd.2019.2915062,,,
2057,10.1177/001316446002000104,,,
2058,10.1007/s43681-022-00152-w,,,
2059,10.1177/20539517211044808,,,
2060,10.1109/icaccs51430.2021.9442035,,,
2061,10.1007/s10489-021-02550-9,,,
2062,10.1109/focs.2010.12,,,
2063,10.1016/j.jbankfin.2022.106401,,,
2064,10.1287/mnsc.2021.4065,,,
2065,10.4236/ica.2022.131001,,,
2066,10.1016/j.jcorpfin.2021.101889,,,
2067,10.3390/joitmc6020043,,,
2068,10.3389/frai.2021.752558,,,
2069,10.1177/0008125619864925,,,
2070,10.3389/frai.2021.695301,,,
2071,10.1007/s10489-018-1361-5,,,
2072,10.1109/sp.2018.00057,,,
2073,10.1111/fima.12295,,,
2074,10.1007/s10551-022-05051-y,,,
2075,10.1016/j.technovation.2021.102375,,,
2076,10.1109/mc.2021.3067225,,,
2077,10.1145/3514094.3534154,,,
2078,10.1057/978-1-349-95968-6,,,
2079,10.2307/2529310,,,
2080,10.54648/eulr2020022,,,
2081,10.1007/s11023-020-09529-4,,,
2082,10.3390/risks7010029,,,
2083,10.3390/e23010018,,,
2084,10.4135/9781412983778,,,
2085,10.1016/j.jbusres.2020.11.001,,,
2086,10.1016/j.artint.2014.01.001,,,
2087,10.1007/s10489-020-02160-x,,,
2088,10.1007/s10551-018-3921-3,,,
2089,10.17705/2msqe.00012,,,
2090,10.1177/0308518x231174026,,,
2091,10.1007/s00146-022-01415-0,,,
2092,10.1007/s10796-022-10269-2,,,
2093,10.1007/s10551-015-2999-0,,,
2094,10.1016/j.eswa.2020.113986,,,
2095,10.1007/s10551-019-04407-1,,,
2096,10.1007/s00146-021-01316-8,,,
2097,10.1145/3270101.3270102,,,
2098,10.1111/beer.12345,,,
2099,10.1007/s10551-011-0967-x,,,
2100,10.1177/030630700903500204,,,
2101,10.1108/s0742-3322(2010)0000027014,,,
2102,10.1088/1742-6596/1694/1/012032,,,
2103,10.1016/j.eswa.2019.113100,,,
2104,10.1016/j.artint.2020.103238,,,
2105,10.1109/mts.2021.3056286,,,
2106,10.1016/j.eswa.2022.118221,,,
2107,10.1108/14635770610676317,,,
2108,10.1108/bl-03-2020-0022,,,
2109,10.14778/3415478.3415570,,,
2110,10.1007/s11023-017-9450-5,,,
2111,10.1016/j.artint.2021.103627,,,
2112,10.1080/13600869.2016.1138628,,,
2113,10.1016/j.techsoc.2020.101413,,,
2114,10.1080/17521440.2020.1760454,,,
2115,10.1109/ms.2020.2985621,,,
2116,10.5465/amd.2018.0084,,,
2117,10.1016/j.jeconbus.2018.05.003,,,
2118,10.1109/tcad.2016.2621883,,,
2119,10.1080/24751839.2019.1570454,,,
2120,10.1038/s41746-021-00438-z,,,
2121,10.1002/rcs.1570,,,
2122,10.1038/519391a,,,
2123,10.1007/s11948-014-9541-0,,,
2124,10.1609/aaai.v32i1.11463,,,
2125,10.1145/2090236.2090255,,,
2126,10.1073/pnas.1602641113,,,
2127,10.1007/s004070100041,,,
2128,10.1016/j.ejor.2013.04.003,,,
2129,10.2139/ssrn.2644093,,,
2130,10.2861/536131,,,
2131,10.4159/9780674042605,,,
2132,10.1108/15265940810875603,,,
2133,10.1596/978-0-8213-6541-0,,,
2134,10.1109/iembs.2010.5626112,,,
2135,10.1007/bf01448847,,,
2136,10.12795/argumentos/2023.i26.10,,,
2137,10.1007/978-3-031-35081-8_32,,,
2138,10.1007/978-3-030-51110-4_2,,,
2139,10.48550/arxiv.2303.12712,,,
2140,10.1016/j.janxdis.2016.03.011,,,
2141,10.5465/annals.2018.0057,,,
2142,10.1016/j.chb.2023.108096,,,
2143,10.13140/rg.2.2.32607.84646,,,
2144,10.3390/ai3030045,,,
2145,10.1007/s10902-022-00563-w,,,
2146,10.1007/s12195-022-00754-8,,,
2147,10.1177/19467567221101637,,,
2148,10.2196/50591,,,
2149,10.1108/lhtn-01-2023-0009,,,
2150,10.1080/13600869.2024.2324540,,,
2151,10.1007/s43681-023-00314-4,,,
2152,10.1007/s43681-024-00431-8,,,
2153,10.1353/hrq.2019.0000,,,
2154,10.48550/arxiv.2403.09986,,,
2155,10.1093/oseo/instance.00169611,,,
2156,10.7208/chicago/9780226108353.001.0001,,,
2157,10.1109/cvpr52729.2023.00586,,,
2158,10.48550/arxiv.2110.00169,,,
2159,10.1007/s00146-021-01189-x,,,
2160,10.1093/oxfordhb/9780198857815.013.36,,,
2161,10.1093/oso/9780198570509.003.0021,,,
2162,10.1177/0306312719860202,,,
2163,10.1146/annurev-anthro-102116-041244,,,
2164,10.1007/978-3-031-46402-7,,,
2165,10.1177/20539517231220622,,,
2166,10.1093/0195130529.003.0014,,,
2167,10.1001/jamainternmed.2023.1838,,,
2168,10.3390/fi15090286,,,
2169,10.1016/j.tbench.2023.100105,,,
2170,10.1007/s10439-023-03172-7,,,
2171,10.1016/j.glt.2023.05.001,,,
2172,10.1016/j.futures.2021.102884,,,
2173,10.1016/j.ijinfomgt.2023.102700,,,
2174,10.1162/daed_a_01911,,,
2175,10.1177/2053951716679678,,,
2176,10.1080/09505431.2022.2036118,,,
2177,10.1177/20539517211017308,,,
2178,10.1016/j.socscimed.2009.04.020,,,
2179,10.1007/s10676-004-2895-2,,,
2180,10.1177/1350508419867206,,,
2181,10.3390/socsci12080435,,,
2182,10.1186/s13244-020-00955-7,,,
2183,10.18574/nyu/9781479833641.001.0001,,,
2184,10.1080/09546634.2021.1944970,,,
2185,10.1038/s41746-020-0288-5,,,
2186,10.1093/jlb/lsad031,,,
2187,10.48550/arxiv.2311.07590,,,
2188,10.1016/j.focat.2023.10.010,,,
2189,10.1007/978-3-030-81907-1,,,
2190,10.1177/20539517231219241,,,
2191,10.2139/ssrn.4073376,,,
2192,10.1080/17530350.2019.1684337,,,
2193,10.1080/15265161.2023.2233357,,,
2194,10.1007/s43681-021-00122-8,,,
2195,10.1007/s43681-020-00030-3,,,
2196,10.2139/ssrn.4584781,,,
2197,10.1007/s43681-022-00179-z,,,
2198,10.2139/ssrn.4874852,,,
2199,10.1136/bmj-2022-070876,,,
2200,10.1080/0952813x.2014.895111,,,
2201,10.1007/s43681-022-00222-z,,,
2202,10.1093/ijtj/iju013,,,
2203,10.1017/jme.2021.34,,,
2204,10.1177/0276146720978257,,,
2205,10.1002/emt.31089,,,
2206,10.1080/01442872.2020.1748264,,,
2207,10.2307/1051512,,,
2208,10.1163/157005869x00180,,,
2209,10.1007/978-981-16-0771-4,,,
2210,10.1007/978-94-009-9786-8,,,
2211,10.17351/ests2016.70,,,
2212,10.1515/9780748647569,,,
2213,10.1163/9789004451131,,,
2214,10.2202/1941-6008.1079,,,
2215,10.1111/j.1467-9744.2010.01056.x,,,
2216,10.1111/j.1467-9744.2011.01245.x,,,
2217,10.1142/q0014,,,
2218,10.1111/j.1949-3606.1997.tb00756.x,,,
2219,10.1007/978-981-16-0771-4_12,,,
2220,10.1007/s13347-019-00377-4,,,
2221,10.1038/s41591-018-0300-7,,,
2222,10.3390/rel13050452,,,
2223,10.1007/978-3-031-22972-5,,,
2224,10.3390/electronics11010016,,,
2225,10.1007/978-981-15-2693-0_26,,,
2226,10.3390/electronics13071305,,,
2227,10.3389/fcomp.2021.563060,,,
2228,10.1016/j.future.2022.08.013,,,
2229,10.17509/ijost.v8i1.52709,,,
2230,10.1109/cais.2019.8769504,,,
2231,10.1007/978-3-030-22475-2_1,,,
2232,10.1016/j.infoandorg.2023.100455,,,
2233,10.1109/access.2023.3296444,,,
2234,10.1007/978-3-030-98464-9_1,,,
2235,10.1016/j.infsof.2023.107197,,,
2236,10.1016/j.dajour.2022.100071,,,
2237,10.36952/uail.2023.1.63-69,,,
2238,10.1007/978-3-030-22277-2_27,,,
2239,10.1063/5.0082975,,,
2240,10.1049/cit2.12028,,,
2241,10.1145/1541880.1541882,,,
2242,10.3390/app131810258,,,
2243,10.1109/access.2022.3197899,,,
2244,10.1007/978-81-322-3972-7,,,
2245,10.1007/s43681-022-00233-w,,,
2246,10.1145/3475716.3475781,,,
2247,10.4018/978-1-59904-937-3.ch185,,,
2248,10.3390/s23031151,,,
2249,10.1007/978-3-030-44289-7_50,,,
2250,10.1561/116.00000084,,,
2251,10.3390/electronics11060898,,,
2252,10.3390/electronics10040514,,,
2253,10.1007/s00607-021-00941-x,,,
2254,10.1177/14614448211032310,,,
2255,10.1016/j.ijinfomgt.2018.08.006,,,
2256,10.1016/j.iot.2019.100059,,,
2257,10.1109/icect61618.2024.10581058,,,
2258,10.1109/access.2024.3381038,,,
2259,10.1007/s40747-024-01392-5,,,
2260,10.1007/s13278-024-01245-6,,,
2261,10.1016/j.evalprogplan.2023.102294,,,
2262,10.31234/osf.io/9vdb8,,,
2263,10.1016/j.procs.2021.12.187,,,
2264,10.1016/0167-4048(95)97088-r,,,
2265,10.1049/cit2.12078,,,
2266,10.15354/sief.23.re202,,,
2267,10.1109/mc.2018.3191268,,,
2268,10.1016/j.techfore.2021.120658,,,
2269,10.1016/j.eng.2024.04.002,,,
2270,10.1016/j.jnca.2020.102807,,,
2271,10.1016/j.jfranklin.2023.09.004,,,
2272,10.2139/ssrn.4396132,,,
2273,10.1016/j.cose.2023.103096,,,
2274,10.1037/amp0000972,,,
2275,10.1109/tnsm.2020.2967721,,,
2276,10.1109/access.2019.2953095,,,
2277,10.1080/00913367.2019.1654947,,,
2278,10.1109/ms.2022.3233582,,,
2279,10.4324/9780429329067-11,,,
2280,10.1109/dasc-picom-cbdcom-cyberscitech49142.2020.00094,,,
2281,10.3390/network4010004,,,
2282,10.1109/miucc55081.2022.9781685,,,
2283,10.1016/j.caeai.2023.100152,,,
2284,10.1109/w-ficloud.2018.00015,,,
2285,10.1109/icccnt54827.2022.9984218,,,
2286,10.1007/978-3-031-26845-8_3,,,
2287,10.1016/b978-0-12-820125-1.00017-8,,,
2288,10.3390/a16030165,,,
2289,10.1109/ficloud.2018.00067,,,
2290,10.1007/s10639-022-11316-w,,,
2291,10.1097/01.sla.0000128307.98274.dc,,,
2292,10.51594/csitrj.v5i3.910,,,
2293,10.3390/s22030927,,,
2294,10.1145/3329786,,,
2295,10.1007/978-3-031-22552-9_20,,,
2296,10.1007/s13347-022-00512-8,,,
2297,10.1016/j.comnet.2023.109776,,,
2298,10.3390/app9050909,,,
2299,10.3389/frai.2023.1020592,,,
2300,10.1515/9783111323749-015,,,
2301,10.1145/3470496.3527393,,,
2302,10.1007/s43681-022-00236-7,,,
2303,10.20944/preprints202212.0499.v1,,,
2304,10.6028/nist.sp.1270,,,
2305,10.1007/s11063-020-10381-x,,,
2306,10.3390/ma15103523,,,
2307,10.1002/9781119821908.ch1,,,
2308,10.1016/j.im.2023.103774,,,
2309,10.1080/08838151.2020.1843357,,,
2310,10.30574/wjarr.2024.21.2.0607,,,
2311,10.1016/j.cogr.2023.04.001,,,
2312,10.4108/eai.24-9-2020.166359,,,
2313,10.1002/9781118551424.ch11,,,
2314,10.1016/j.techfore.2023.122502,,,
2315,10.3390/sym13040597,,,
2316,10.1007/s10508-022-02446-w,,,
2317,10.1109/cvpr.2018.00678,,,
2318,10.1016/j.procs.2022.12.050,,,
2319,10.1016/j.orgdyn.2024.101041,,,
2320,10.1108/bij-07-2020-0354,,,
2321,10.1007/s11604-023-01474-3,,,
2322,10.1016/j.future.2021.10.033,,,
2323,10.1109/jiot.2020.3040957,,,
2324,10.1109/access.2022.3149053,,,
2325,10.3390/electronics12112355,,,
2326,10.3390/app12125826,,,
2327,10.1109/access.2019.2906934,,,
2328,10.2478/jagi-2019-0002,,,
2329,10.1007/978-94-007-2543-0_19,,,
2330,10.1109/access.2022.3182333,,,
2331,10.1016/j.neucom.2021.12.093,,,
2332,10.1007/s00500-023-09037-4,,,
2333,10.21203/rs.3.rs-3970015/v1,,,
2334,10.1109/access.2024.3389497,,,
2335,10.1007/s00521-017-3077-6,,,
2336,10.1609/aaai.v36i3.20240,,,
2337,10.1016/s0733-8635(05)70206-0,,,
2338,10.1097/00006534-200003000-00058,,,
2339,10.19030/jbcs.v7i1.1577,,,
2340,10.1017/s1464793102006085,,,
2341,10.1155/2022/9940548,,,
2342,10.1111/peps.12469,,,
2343,10.1007/s10508-021-02127-0,,,
2344,10.1111/jocd.13797,,,
2345,10.22381/emfm13420184,,,
2346,10.1093/sf/soz162,,,
2347,10.1109/access.2020.3006051,,,
2348,10.1007/s11948-017-9975-2,,,
2349,10.1111/ntwe.12124,,,
2350,10.1111/bjd.21273,,,
2351,10.1016/j.jnma.2020.07.013,,,
2352,10.4103/0378-6323.45238,,,
2353,10.1016/j.jid.2023.06.189,,,
2354,10.1002/widm.1356,,,
2355,10.1109/ijsis.1998.685460,,,
2356,10.1093/sleep/zsaa159,,,
2357,10.1109/iccicc53683.2021.9811333,,,
2358,10.1109/mts.2017.2697080,,,
2359,10.1055/s-0028-1100908,,,
2360,10.1007/978-3-658-21083-0_8,,,
2361,10.1145/3466132.3466134,,,
2362,10.1145/3194770.3194773,,,
2363,10.1007/s40804-022-00262-2,,,
2364,10.1109/mc.2023.3235712,,,
2365,10.1038/nature14539,,,
2366,10.1002/9781118896877.wbiehs046,,,
2367,10.1111/j.1468-0289.2007.00388.x,,,
2368,10.5281/zenodo.4325856,,,
2369,10.1080/17513057.2019.1580380,,,
2370,10.1177/1069031x221112642,,,
2371,10.1111/srt.13009,,,
2372,10.1080/01292986.2012.756046,,,
2373,10.3389/fpubh.2021.641605,,,
2374,10.1016/j.fsc.2007.08.006,,,
2375,10.1007/s10462-019-09765-w,,,
2376,10.1007/s11042-019-7424-8,,,
2377,10.1111/ics.12602,,,
2378,10.1111/ics.12727,,,
2379,10.1109/cvpr.2016.532,,,
2380,10.1109/cvpr42600.2020.00934,,,
2381,10.1111/ics.12555,,,
2382,10.1055/s-0031-1298595,,,
2383,10.1037/a0025065,,,
2384,10.1016/j.jhin.2020.12.009,,,
2385,10.1186/s12889-021-11738-0,,,
2386,10.1111/srt.13113,,,
2387,10.1002/col.22230,,,
2388,10.1177/1054773812446510,,,
2389,10.1002/(sici)1096-8644(200005)112:1<17::aid-ajpa3>3.0.co;2-d,,,
2390,10.1016/j.ijwd.2018.03.002,,,
2391,10.1111/srt.12132,,,
2392,10.1111/j.1346-8138.2011.01406.x,,,
2393,10.1016/j.jdermsci.2010.06.003,,,
2394,10.1159/000338978,,,
2395,10.1016/s0190-9622(96)90497-1,,,
2396,10.1111/j.1600-0846.2007.00237.x,,,
2397,10.1007/s10522-008-9141-y,,,
2398,10.1007/s41809-020-00072-3,,,
2399,10.1016/s0140-6736(09)61460-4,,,
2400,10.1038/s41598-022-27009-w,,,
2401,10.1109/cvpr.2011.5995347,,,
2402,10.7573/dic.2021-8-6,,,
2403,10.1016/j.evolhumbehav.2017.11.005,,,
2404,10.1145/3240508.3240618,,,
2405,10.1109/cvpr.2009.5206833,,,
2406,10.1145/3072959.3073683,,,
2407,10.1177/1203475420923648,,,
2408,10.4324/9781351039147,,,
2409,10.1007/978-3-030-51110-4,,,
2410,10.1016/10.1145/2696454.2696497,,,
2411,10.1177/1541931213601686,,,
2412,10.3389/frobt.2017.00021,,,
2413,10.1016/j.chb.2014.05.014,,,
2414,10.1109/hri.2013.6483606,,,
2415,10.1111/j.1460-2466.2006.00318.x,,,
2416,10.2139/ssrn.4193199,,,
2417,10.1016/j.paid.2020.109969,,,
2418,10.1111/pops.12393,,,
2419,10.1145/3411764.3445542,,,
2420,10.31234/osf.io/wgqvy,,,
2421,10.1016/j.paid.2010.01.004,,,
2422,10.1037/1040-3590.18.2.192,,,
2423,10.1016/j.jrp.2008.03.006,,,
2424,10.1037/pspp0000121,,,
2425,10.1016/j.jrp.2013.09.004,,,
2426,10.1093/obo/9780199828340-0120,,,
2427,10.1007/s13218-020-00689-0,,,
2428,10.2460/javma.22.03.0093,,,
2429,10.1007/s00146-020-01008-9,,,
2430,10.1056/nejm198208193070808,,,
2431,10.2460/ajvr.22.03.0038,,,
2432,10.1017/s0963180111000259,,,
2433,10.1136/jme.28.5.332-a,,,
2434,10.1111/vru.12912,,,
2435,10.3389/fvets.2020.590615,,,
2436,10.2460/javma.259.7.712,,,
2437,10.1001/jama.2021.22969,,,
2438,10.1136/bmj.h869,,,
2439,10.1038/538020a,,,
2440,10.1542/peds.2012-0393,,,
2441,10.1007/s10806-018-9729-4,,,
2442,10.1007/s13347-023-00627-6,,,
2443,10.1017/s0963180119000847,,,
2444,10.3389/fvets.2022.780482,,,
2445,10.1038/s41591-018-0316-z,,,
2446,10.1186/s13567-021-00902-4,,,
2447,10.1007/978-3-319-26818-7_5,,,
2448,10.1136/medethics-2020-106922,,,
2449,10.1093/med/9780199354474.001.0001,,,
2450,10.1186/s13244-019-0785-8,,,
2451,10.1177/1477750915622033,,,
2452,10.1136/amiajnl-2011-000089,,,
2453,10.3390/ani8010015,,,
2454,10.1145/3442188.3445923,,,
2455,10.1109/incet49848.2020.9154121,,,
2456,10.1016/j.vaa.2018.12.008,,,
2457,10.1017/s0963180115000110,,,
2458,10.1186/s12916-019-1426-2,,,
2459,10.1007/s00146-022-01418-x,,,
2460,10.1016/j.jocn.2019.03.001,,,
2461,10.3348/kjr.2019.0025,,,
2462,10.1007/978-3-319-05544-2_435-1,,,
2463,10.1016/j.artint.2021.103458,,,
2464,10.2460/javma.249.1.42,,,
2465,10.1016/j.tcam.2021.100550,,,
2466,10.1016/s2542-5196(20)30121-2,,,
2467,10.1111/vru.12901,,,
2468,10.1093/jamiaopen/ooaa005,,,
2469,10.1136/vr.103005,,,
2470,10.2460/javma.249.8.884,,,
2471,10.1007/s11948-015-9652-2,,,
2472,10.1080/15265161.2017.1409825,,,
2473,10.1016/s0195-5616(97)50038-6,,,
2474,10.1186/s13071-021-04591-y,,,
2475,10.1136/bmj.m689,,,
2476,10.20944/preprints202107.0368.v1,,,
2477,10.1088/1741-2552/ab172d,,,
2478,10.1016/j.avb.2016.11.007,,,
2479,10.1007/s43681-021-00065-0,,,
2480,10.1097/sla.0000000000003262,,,
2481,10.1518/001872097778543886,,,
2482,10.1016/0167-739x(87)90038-0,,,
2483,10.1186/s13620-022-00220-x,,,
2484,10.3390/ani11113010,,,
2485,10.1016/j.artmed.2021.102158,,,
2486,10.1093/jamia/ocaa268,,,
2487,10.1016/j.domaniend.2019.106396,,,
2488,10.1080/15265161.2017.1409824,,,
2489,10.1007/s43681-022-00187-z,,,
2490,10.4103/2141-9248.113688,,,
2491,10.1016/j.vas.2020.100161,,,
2492,10.1371/journal.pone.0253420,,,
2493,10.1002/vetr.1266,,,
2494,10.3390/ani11061483,,,
2495,10.2307/1600565,,,
2496,10.2460/javma.1991.198.08.1360,,,
2497,10.1109/msec.2018.2888783,,,
2498,10.1038/ajg.2010.173,,,
2499,10.1136/bmjhci-2021-100323,,,
2500,10.1016/j.idh.2018.10.002,,,
2501,10.1001/jama.2012.100,,,
2502,10.2460/javma.237.3.263,,,
2503,10.1007/s10677-016-9778-6,,,
2504,10.1177/03009858211040484,,,
2505,10.2307/2024577,,,
2506,10.1016/j.jeconom.2020.12.001,,,
2507,10.1017/pan.2016.2,,,
2508,10.1126/science.adh2586,,,
2509,10.1037/h0040373,,,
2510,10.1037/1931-3896.2.3.139,,,
2511,10.1080/10400410902855259,,,
2512,10.1109/mts.2018.2876107,,,
2513,10.1109/mts.2019.2915154,,,
2514,10.1086/292745,,,
2515,10.1007/s10676-018-9448-6,,,
2516,10.1007/s10676-010-9249-z,,,
2517,10.1007/s00146-009-0208-3,,,
2518,10.1007/s10676-011-9279-1,,,
2519,10.1111/j.2153-960x.2012.00546.x,,,
2520,10.7551/mitpress/8975.001.0001,,,
2521,10.1111/1468-0009.00223,,,
2522,10.1080/00048409412345881,,,
2523,10.2307/2216409,,,
2524,10.29297/orbit.v2i2.112,,,
2525,10.1086/233694,,,
2526,10.29297/orbit.v2i2.106,,,
2527,10.29297/orbit.v2i1.102,,,
2528,10.1080/00048400801886413,,,
2529,10.1007/s12130-010-9124-6,,,
2530,10.1017/cbo9780511606250,,,
2531,10.29297/orbit.v2i1.101,,,
2532,10.29297/orbit.v2i2.109,,,
2533,10.1007/s11948-019-00130-2,,,
2534,10.1007/s10806-019-09812-0,,,
2535,10.29297/orbit.v2i2.110,,,
2536,10.1111/j.1468-0114.2012.01438.x,,,
2537,10.1177/0306312717741687,,,
2538,10.1007/s11023-010-9201-3,,,
2539,10.4018/jthi.2009040102,,,
2540,10.1007/s13347-014-0165-8,,,
2541,10.1023/b:etin.0000006947.66879.13,,,
2542,10.1017/cbo9780511618024,,,
2543,10.11909/j.issn.1671-5411.2019.08.010,,,
2544,10.1002/aaai.12064,,,
2545,10.1007/978-981-19-0722-7_4,,,
2546,10.1007/s00146-021-01382-y,,,
2547,10.1111/j.1467-8624.2011.01663.x,,,
2548,10.1145/3306618.3314231,,,
2549,10.3991/ijet.v13i06.8275,,,
2550,10.1007/s40593-016-0105-0,,,
2551,10.1016/j.sbspro.2015.01.685,,,
2552,10.1016/j.eswa.2008.10.080,,,
2553,10.1016/s0883-0355(99)00014-2,,,
2554,10.1109/rita.2018.2831760,,,
2555,10.1016/j.compedu.2018.10.019,,,
2556,10.1007/s10639-020-10182-8,,,
2557,10.1080/01443410.2018.1495829,,,
2558,10.1098/rsta.2018.0081,,,
2559,10.1016/s0020-7373(79)80005-x,,,
2560,10.1504/ijepee.2020.111692,,,
2561,10.1109/access.2020.3036990,,,
2562,10.1007/s40593-014-0024-x,,,
2563,10.1007/3-540-47987-2_61,,,
2564,10.24059/olj.v18i3.464,,,
2565,10.1016/j.knosys.2009.01.007,,,
2566,10.1007/s00146-017-0781-9,,,
2567,10.1007/s43681-020-00016-1,,,
2568,10.2139/ssrn.3609292,,,
2569,10.1177/0278364913495721,,,
2570,10.2139/ssrn.3778998,,,
2571,10.1038/s41562-016-0028,,,
2572,10.1007/s40593-015-0072-x,,,
2573,10.1109/icalt.2017.137,,,
2574,10.1037/a0037123,,,
2575,10.1016/j.futures.2017.03.006,,,
2576,10.1142/s0218213010000406,,,
2577,10.1207/s15327809jls0203_2,,,
2578,10.1016/j.compedu.2018.05.011,,,
2579,10.31234/osf.io/3p8f6,,,
2580,10.1609/aaai.v33i01.3301687,,,
2581,10.7551/mitpress/11017.001.0001,,,
2582,10.1145/3170358.3170364,,,
2583,10.1007/978-3-319-19773-9_42,,,
2584,10.1002/cae.21900,,,
2585,10.1007/bf00117714,,,
2586,10.1007/978-3-319-61425-0_6,,,
2587,10.1080/0013191042000308341,,,
2588,10.1063/1.4960909,,,
2589,10.1007/978-3-030-02357-7_8,,,
2590,10.1038/nature16961,,,
2591,10.1006/ijhc.1999.0349,,,
2592,10.1037/a0032447,,,
2593,10.1088/1742-6596/1175/1/012165,,,
2594,10.1007/s12369-014-0268-5,,,
2595,10.24251/hicss.2021.186,,,
2596,10.1080/02619768.2020.1821185,,,
2597,10.1038/s41539-017-0016-3,,,
2598,10.24963/ijcai.2018/779,,,
2599,10.1145/3351095.3372852,,,
2600,10.1080/10494820.2023.2180191,,,
2601,10.1080/02602938.2024.2309963,,,
2602,10.1080/23735082.2023.2261106,,,
2603,10.1007/s10639-024-12611-4,,,
2604,10.1186/s40561-023-00276-4,,,
2605,10.1016/j.compedu.2024.105070,,,
2606,10.1016/j.caeai.2022.100049,,,
2607,10.1080/10494820.2023.2253858,,,
2608,10.1007/s10639-024-12723-x,,,
2609,10.1007/s11528-023-00911-4,,,
2610,10.3390/educsci13111155,,,
2611,10.1080/10494820.2023.2217864,,,
2612,10.3390/su16031166,,,
2613,10.1080/10494820.2023.2253861,,,
2614,10.1080/0144929x.2022.2072768,,,
2615,10.3390/su16010204,,,
2616,10.3389/fpsyg.2020.01063,,,
2617,10.1007/s10462-022-10155-y,,,
2618,10.1007/s10639-022-11338-4,,,
2619,10.1111/bjet.13305,,,
2620,10.1080/08839514.2021.1901032,,,
2621,10.1109/icitr61062.2023.10382802,,,
2622,10.3389/fpsyg.2020.595374,,,
2623,10.35877/454ri.eduline2592,,,
2624,10.1186/s40536-021-00115-3,,,
2625,10.1016/j.chb.2011.04.004,,,
2626,10.1016/j.ijhcs.2013.09.006,,,
2627,10.1037/0021-9010.78.1.98,,,
2628,10.1080/01587919.2020.1724768,,,
2629,10.1177/135910457000100301,,,
2630,10.1111/j.1745-3984.2004.tb01111.x,,,
2631,10.1080/03054985.2012.731208,,,
2632,10.1162/neco.2006.18.7.1527,,,
2633,10.1007/978-3-319-99978-4_2,,,
2634,10.48550/arxiv.2307.05638,,,
2635,10.1002/mp.16405,,,
2636,10.48550/arxiv.2204.06125,,,
2637,10.48550/arxiv.2112.1075,,,
2638,10.48550/arxiv.2210.00586,,,
2639,10.48550/arxiv.2205.06175,,,
2640,10.1162/daed_a_01909,,,
2641,10.2139/ssrn.4389233,,,
2642,10.1007/s11023-019-09512-8,,,
2643,10.3389/fpsyg.2018.01185,,,
2644,10.48550/arxiv.1908.09375,,,
2645,10.1017/9781009025096.002,,,
2646,10.48550/arxiv.1611.03530,,,
2647,10.1145/3446776,,,
2648,10.1073/pnas.1907373117,,,
2649,10.1126/science.360.6388.478,,,
2650,10.1007/978-3-662-59010-2,,,
2651,10.1515/9783839442876-003,,,
2652,10.48550/arxiv.2205.00002,,,
2653,10.17351/ests2020.277,,,
2654,10.1201/9781351251389-5,,,
2655,10.1007/978-3-642-32560-1_1,,,
2656,10.1093/scipol/scz023,,,
2657,10.1007/s00146-023-01704-2,,,
2658,10.1007/s00146-022-01426-x,,,
2659,10.1002/widm.1391,,,
2660,10.1109/access.2021.3070212,,,
2661,10.1145/3546577,,,
2662,10.1007/978-3-031-24628-9_41,,,
2663,10.48550/arxiv.1808.07074,,,
2664,10.1145/3282486,,,
2665,10.1145/3394486.3406707,,,
2666,10.48550/arxiv.2102.04201,,,
2667,10.1145/3236386.3241340,,,
2668,10.1007/s13347-020-00396-6,,,
2669,10.1007/978-3-030-22493-6_2,,,
2670,10.1007/s43681-021-00108-6,,,
2671,10.1007/s00146-021-01276-z,,,
2672,10.1007/s00146-021-01384-w,,,
2673,10.1093/oso/9780198846666.001.0001,,,
2674,10.1007/bf02478259,,,
2675,10.1145/3317287.3328534,,,
2676,10.48550/arxiv.1911.01547,,,
2677,10.7551/mitpress/2010.001.0001,,,
2678,10.17791/jcs.2011.12.4.325,,,
2679,10.48550/arxiv.2307.14736,,,
2680,10.3390/arts8010026,,,
2681,10.1007/978-3-476-05604-7,,,
2682,10.5840/techne201712479,,,
2683,10.48550/arxiv.1801.00631,,,
2684,10.1037/h0042519,,,
2685,10.1038/323533a0,,,
2686,10.1016/j.neunet.2014.09.003,,,
2687,10.1109/5.726791,,,
2688,10.1016/0893-6080(89)90020-8,,,
2689,10.1007/bf02551274,,,
2690,10.1016/j.acha.2019.06.004,,,
2691,10.7551/mitpress/7496.001.0001,,,
2692,10.48550/arxiv.1512.03965,,,
2693,10.5555/3305890.3305975,,,
2694,10.1007/s10955-017-1836-5,,,
2695,10.1145/1553374.1553453,,,
2696,10.1007/978-3-319-10590-1_53,,,
2697,10.1609/aaai.v31i1.10913,,,
2698,10.48550/arxiv.1803.03635,,,
2699,10.48550/arxiv.1703.00810,,,
2700,10.48550/arxiv.2107.12547,,,
2701,10.1109/cvpr.2015.7298594,,,
2702,10.1007/978-3-030-11821-1_12,,,
2703,10.23915/distill.00007,,,
2704,10.48550/arxiv.2207.02098,,,
2705,10.48550/arxiv.1312.6199,,,
2706,10.48550/arxiv.1412.6572,,,
2707,10.48550/arxiv.1809.02104,,,
2708,10.1145/3052973.3053009,,,
2709,10.48550/arxiv.1712.09665,,,
2710,10.48550/arxiv.2101.06784,,,
2711,10.1007/978-3-319-99978-4_27,,,
2712,10.48550/arxiv.2303.08774,,,
2713,10.18653/v1/2020.acl-main.463,,,
2714,10.18653/v1/2020.emnlp-main.703,,,
2715,10.48550/arxiv.2308.00109,,,
2716,10.1628/ptsc-2023-0005,,,
2717,10.1098/rsta.2022.0041,,,
2718,10.1016/j.neunet.2022.03.037,,,
2719,10.17104/9783406751264,,,
2720,10.1631/fitee.2200297,,,
2721,10.48550/arxiv.2205.10343,,,
2722,10.1017/9781009023405,,,
2723,10.1109/tit.2018.2854560,,,
2724,10.48550/arxiv.2211.03570,,,
2725,10.1109/cvpr42600.2020.01070,,,
2726,10.48550/arxiv.1710.10345,,,
2727,10.1109/itw.2015.7133169,,,
2728,10.3389/fcomp.2022.1041703,,,
2729,10.1016/j.jco.2008.11.002,,,
2730,10.1109/tnnls.2015.2496947,,,
2731,10.3390/rel14040466,,,
2732,10.14220/9783737000581.37,,,
2733,10.14375/np.9782070293353,,,
2734,10.4324/9781315884950,,,
2735,10.1080/1462317x.2019.1605725,,,
2736,10.12987/9780300188479-041,,,
2737,10.1037/0033-295x.114.4.864,,,
2738,10.1016/j.tics.2010.05.006,,,
2739,10.1016/j.artint.2011.01.006.,,,
2740,10.1080/21507740.2020.1740350,,,
2741,10.1007/s11023-019-09506-6,,,
2742,10.1145/3306618.3314232,,,
2743,10.1145/3434074.3446911,,,
2744,10.3389/fdgth.2021.735053,,,
2745,10.1093/iwc/iwad022,,,
2746,10.1177/00222429211045687,,,
2747,10.1093/oso/9780190652951.003.0012,,,
2748,10.1007/s12369-022-00920-y,,,
2749,10.1080/14672715.2014.960707,,,
2750,10.1525/california/9780520283190.001.0001,,,
2751,10.1007/s11097-022-09848-0,,,
2752,10.1038/s41598-021-87480-9,,,
2753,10.1016/b978-0-12-801873-6.00005-4,,,
2754,10.1515/pjbr-2021-0029,,,
2755,10.4324/9780429265365,,,
2756,10.18034/ei.v5i2.490,,,
2757,10.1080/09540099408915726,,,
2758,10.1109/icassp40776.2020.9054546,,,
2759,10.1145/2739480.2754703,,,
2760,10.48550/arxiv.1611.01211,,,
2761,10.1111/moth.12682,,,
2762,10.1007/978-3-030-11821-1_1,,,
2763,10.1016/j.neuron.2017.06.011,,,
2764,10.1016/0166-2236(93)90069-x,,,
2765,10.1038/482456a,,,
2766,10.1007/978-3-319-97550-4_7,,,
2767,10.48550/arxiv.2305.11252,,,
2768,10.1038/s41583-020-0277-3,,,
2769,10.1126/science.aau6595,,,
2770,10.1007/978-3-476-05604-7_1,,,
2771,10.30965/9783846756577,,,
2772,10.5771/9783451837524,,,
2773,10.1111/j.1933-1592.2007.00114.x,,,
2774,10.1017/s0140525x12000477,,,
2775,10.1093/oso/9780192898197.001.0001,,,
2776,10.4159/9780674287136,,,
2777,10.1007/s10670-013-9594-5,,,
2778,10.1111/cons.2002.9.issue-3,,,
2779,10.7551/mitpress/6155.003.0014,,,
2780,10.5040/9781350071322,,,
2781,10.1007/s11245-008-9043-2,,,
2782,10.48550/arxiv.2305.10626,,,
2783,10.1207/s15327957pspr1003_4,,,
2784,10.1111/bjso.12565,,,
2785,10.1017/9789048503841.004,,,
2786,10.1177/01622439199009,,,
2787,10.1007/978-94-007-7914-3_1,,,
2788,10.1007/s00146-022-01417-y,,,
2789,10.1016/b978-0-444-51667-1.50044-6,,,
2790,10.7208/chicago/9780226692685.001.0001,,,
2791,10.1386/eme.5.1.5\_1,,,
2792,10.3726/978-1-4331-4005-1,,,
2793,10.3726/978-1-4539-1871-5,,,
2794,10.1515/9780271033228,,,
2795,10.1007/978-94-007-7554-1,,,
2796,10.1145/2751314,,,
2797,10.7551/mitpress/9780262042482.001.0001,,,
2798,10.14361/9783839462492,,,
2799,10.1080/13600869.2017.1298499,,,
2800,10.1515/9780691239293,,,
2801,10.1016/j.futures.2006.08.001,,,
2802,10.1515/9781503616738,,,
2803,10.1515/9783110793383,,,
2804,10.1093/acprof:oso/9780198786849.003.0006,,,
2805,10.7551/mitpress/9780262018548.001.0001,,,
2806,10.7551/mitpress/9780262014601.001.0001,,,
2807,10.1093/oso/9780198794325.001.0001,,,
2808,10.1007/s11245-017-9484-6,,,
2809,10.1038/s41562-019-0626-2,,,
2810,10.1007/978-1-4020-9368-5_12,,,
2811,10.3389/fpsyg.2013.00058,,,
2812,10.1093/oxfordhb/9780195309799.001.0001,,,
2813,10.1007/s11245-008-9046-z,,,
2814,10.1007/s11097-016-9464-0,,,
2815,10.1111/sjp.12308,,,
2816,10.1093/acprof:oso/9780199682737.001.0001,,,
2817,10.1093/acprof:oso/9780190217013.001.0001,,,
2818,10.48550/arxiv.2308.08708,,,
2819,10.1017/9781009277396,,,
2820,10.3389/fpsyg.2020.01707,,,
2821,10.1007/s10676-017-9432-6,,,
2822,10.1109/access.2020.3045078,,,
2823,10.1109/tai.2021.3088084,,,
2824,10.1145/3491209,,,
2825,10.1145/3448248,,,
2826,10.1007/978-3-030-69128-8_2,,,
2827,10.1007/s11023-018-9481-6,,,
2828,10.1007/s00146-022-01401-6,,,
2829,10.1007/s00146-022-01607-8,,,
2830,10.1007/s00146-021-01328-4,,,
2831,10.1145/3555803,,,
2832,10.1093/oxfordhb/9780190067397.001.0001,,,
2833,10.1093/oxfordhb/9780198857815.001.0001,,,
2834,10.1007/978-3-030-58309-5_10,,,
2835,10.1109/sds.2019.00-15,,,
2836,10.1109/sds54800.2022.00011,,,
2837,10.48550/arxiv.1802.07228,,,
2838,10.1145/3485128,,,
2839,10.48550/arxiv.1906.02243,,,
2840,10.1080/00963402.1966.11454993,,,
2841,10.1007/s00146-017-0734-3,,,
2842,10.48550/arxiv.2304.15004,,,
2843,10.48550/arxiv.2210.02667,,,
2844,10.1007/s00146-023-01711-3,,,
2845,10.1111/1758-5899.12002,,,
2846,10.4324/9781003322290,,,
2847,10.4159/9780674970250,,,
2848,10.48550/arxiv.2208.02957,,,
2849,10.1007/978-3-030-11821-1_8,,,
2850,10.1093/oso/9780198777946.001.0001,,,
2851,10.1038/d41586-021-03499-y,,,
2852,10.1093/acprof:oso/9780199654703.001.0001,,,
2853,10.2307/jj.5973228.6,,,
2854,10.1002/poi3.198,,,
2855,10.1080/1369118x.2021.1924827,,,
2856,10.1007/978-3-030-50585-1_2,,,
2857,10.1787/f97beae7-en,,,
2858,10.4324/9781003320609-27,,,
2859,10.1257/jep.27.4.141,,,
2860,10.1057/9780230321458_45,,,
2861,10.1080/23268743.2018.1483208,,,
2862,10.4324/9781003158851,,,
2863,10.1007/s00146-021-01301-1,,,
2864,10.1109/mts.2019.2894474,,,
2865,10.1007/978-3-030-88972-2_3,,,
2866,10.1007/978-981-99-5103-1_4,,,
2867,10.1007/s11245-023-09940-3,,,
2868,10.1145/764008.763957,,,
2869,10.1145/3137574.3139451,,,
2870,10.1145/3306618.3314286,,,
2871,10.1007/s00146-020-00950-y,,,
2872,10.14763/2019.2.1410,,,
2873,10.1016/j.jbusres.2020.09.004,,,
2874,10.31234/osf.io/62kxq,,,
2875,10.1007/s00146-021-01314-w,,,
2876,10.48550/arxiv.2303.09387,,,
2877,10.1109/mc.2018.2381135,,,
2878,10.1515/9783110746433,,,
2879,10.1146/annurev-criminol-051520-012342,,,
2880,10.1007/978-3-030-96305-7_43,,,
2881,10.1098/rsos.191649,,,
2882,10.1016/j.eng.2019.08.015,,,
2883,10.1007/s41666-022-00114-1,,,
2884,10.1186/s12911-021-01488-9,,,
2885,10.1007/s00146-021-01330-w,,,
2886,10.1609/aimag.v36i4.2629,,,
2887,10.21428/8f7503e4,,,
2888,10.1007/978-94-007-7844-3_4,,,
2889,10.1201/b19060,,,
2890,10.1109/mts.2022.3197116,,,
2891,10.1093/oso/9780192845290.001.0001,,,
2892,10.1007/s00146-022-01391-5,,,
2893,10.2139/ssrn.4199467,,,
2894,10.1177/20539517231173901,,,
2895,10.1016/j.patter.2021.100205,,,
2896,10.1017/9781316831847,,,
2897,10.1080/25729861.2021.1968634,,,
2898,10.1177/0163443720904601,,,
2899,10.1093/oso/9780192865366.001.0001,,,
2900,10.7208/chicago/9780226733050.001.0001,,,
2901,10.1386/vcr_00008_7,,,
2902,10.1093/cdj/bsaa042,,,
2903,10.1215/9780822371816,,,
2904,10.1525/001c.10797,,,
2905,10.2307/j.ctv65swcp,,,
2906,10.5210/fm.v26i12.11833,,,
2907,10.48550/arxiv.2306.12001,,,
2908,10.1177/2053951715592429,,,
2909,10.1016/j.polgeo.2021.102382,,,
2910,10.1002/9781444304992.ch7,,,
2911,10.1080/25729861.2022.2035936,,,
2912,10.11573/spectrum.library.concordia.ca.00986506,,,
2913,10.48550/arxiv.2304.03271,,,
2914,10.1215/9781478021445,,,
2915,10.3390/su14095172,,,
2916,10.53765/20512201.30.5.232,,,
2917,10.1080/1369118x.2021.1909100,,,
2918,10.1215/9781478007227,,,
2919,10.1332/policypress/9781529213492.001.0001,,,
2920,10.1215/9781478021674,,,
2921,10.5749/minnesota/9780816695515.001.0001,,,
2922,10.1177/20539517231158994,,,
2923,10.7208/chicago/9780226253978.001.0001,,,
2924,10.1080/1369118x.2022.2049849,,,
2925,10.1215/9780822373810,,,
2926,10.1177/20539517221113774,,,
2927,10.1177/01634437221099,,,
2928,10.1177/14614448221145928,,,
2929,10.1525/001c.10780,,,
2930,10.1699/wpcc.210,,,
2931,10.1007/s43681-021-00043-6,,,
2932,10.1075/jlp.22125.vau,,,
2933,10.1177/2053951716684144,,,
2934,10.1177/25148486211006345,,,
2935,10.1177/20539517231177620,,,
2936,10.1186/s13000-021-01085-4,,,
2937,10.3389/fcomp.2023.1113903,,,
2938,10.3389/fpsyg.2023.1177720,,,
2939,10.1155/2021/8823383,,,
2940,10.3389/fpsyg.2015.01998,,,
2941,10.1109/bci48061.2020.9061646,,,
2942,10.1038/scientificamerican022021-7i562qnmh6t0dduwu1denh,,,
2943,10.1177/1747016113478517,,,
2944,10.1016/j.jobb.2019.06.001,,,
2945,10.2139/ssrn.2202982,,,
2946,10.3389/fnsys.2017.00093,,,
2947,10.1136/bmjopen-2019-029134,,,
2948,10.3390/bioengineering10121435,,,
2949,10.1111/j.1469-7793.2001.0099b.x,,,
2950,10.1089/hs.2019.0002,,,
2951,10.1007/s12152-022-09490-2,,,
2952,10.1371/journal.pone.0275454,,,
2953,10.1007/s12152-013-9189-5,,,
2954,10.3390/healthcare8020133,,,
2955,10.1016/s0140-6736(23)01668-9,,,
2956,10.1177/1747016120952500,,,
2957,10.1038/s42256-022-00511-6,,,
2958,10.1257/jep.29.3.3,,,
2959,10.1111/j.1468-0114.2012.01452.x,,,
2960,10.1017/apa.2015.9,,,
2961,10.1007/s11948-016-9770-5,,,
2962,10.2307/j.ctvn5txpc,,,
2963,10.1007/s43681-020-00028-x,,,
2964,10.1093/oxfordhb/9780190063504.013.36,,,
2965,10.5040/9781350251151,,,
2966,10.1515/9780691191959,,,
2967,10.5840/soctheorpract201952458,,,
2968,10.7551/mitpress/11705.001.0001,,,
2969,10.1017/s0034412510000569,,,
2970,10.1093/acprof:oso/9780199599318.001.0001,,,
2971,10.1111/j.1467-9329.2007.00385.x,,,
2972,10.5840/techne2019122110,,,
2973,10.4324/9781003205425-14,,,
2974,10.1515/jwiet-2016-0106,,,
2975,10.22613/zfpp/5.2.8,,,
2976,10.1007/s11948-021-00349-y,,,
2977,10.1007/s43681-021-00064-1,,,
2978,10.1093/acprof:oso/9780190618179.001.0001,,,
2979,10.1017/s0265052500001734,,,
2980,10.1111/1467-9264.00018,,,
2981,10.1007/s43681-023-00259-8,,,
2982,10.3390/su14073791,,,
2983,10.1088/1748-9326/abfba1,,,
2984,10.7717/peerj.7702,,,
2985,10.1038/s41746-019-0103-3,,,
2986,10.1038/s41746-019-0148-3,,,
2987,10.1038/s41563-019-0339-y,,,
2988,10.1038/s41586-019-1799-6,,,
2989,10.1038/s41563-019-0360-1,,,
2990,10.1038/s41591-019-0447-x,,,
2991,10.1016/s0140-6736(20)30260-9,,,
2992,10.1215/00318108-4173412,,,
2993,10.1109/jproc.2019.2900622,,,
2994,10.1038/s41928-019-0213-6,,,
2995,10.1016/j.artmed.2019.101785,,,
2996,10.1136/medethics-2018-105118,,,
2997,10.1001/amajethics.2019.121,,,
2998,10.1155/2014/672714,,,
2999,10.2174/157489308783329869,,,
3000,10.1016/0378-7206(92)90025-b,,,
3001,10.1136/medethics-2019-105586,,,
3002,10.1056/nejmp1606181,,,
3003,10.1016/j.ajo.2020.02.022,,,
3004,10.12987/9780300128154,,,
3005,10.1017/cbo9780511813306.007,,,
3006,10.1136/jme.29.5.297,,,
3007,10.1136/bmj.309.6948.184,,,
3008,10.1145/3306618.3314256,,,
3009,10.1038/s41746-019-0132-y,,,
3010,10.1093/annonc/mdx781,,,
3011,10.1136/medethics-2018-105281,,,
3012,10.2196/15154,,,
3013,10.1136/medethics-2019-105572,,,
3014,10.1007/s13347-019-00391-6,,,
3015,10.1001/amajethics.2019.138,,,
3016,10.1111/j.1747-9991.2009.00257.x,,,
3017,10.1016/j.heliyon.2018.e00938,,,
3018,10.1016/s0169-7218(11)02410-5,,,
3019,10.1109/mis.2006.64,,,
3020,10.1109/tsmc.1978.4309958,,,
3021,10.1177/1059712309343819,,,
3022,10.3138/cpp.2020-065,,,
3023,10.1109/mis.2006.82,,,
3024,10.1017/cbo9780511720185.012,,,
3025,10.7551/mitpress/1664.001.0001,,,
3026,10.1017/s0140525x00058611,,,
3027,10.1109/tsmc.1976.5408784,,,
3028,10.1023/b:mind.0000035461.63578.9d,,,
3029,10.1093/mind/lxvii.268.502,,,
3030,10.2307/2024717,,,
3031,10.1017/cbo9780511818172,,,
3032,10.1016/j.techfore.2016.08.019,,,
3033,10.1162/153244302320884605,,,
3034,10.1093/oso/9780198236467.003.0007,,,
3035,10.1093/oso/9780198236467.001.0001,,,
3036,10.1038/s41467-022-31150-5,,,
3037,10.1007/s11229-006-9107-z,,,
3038,10.1007/s10676-008-9174-6,,,
3039,10.1145/3440959.3440966,,,
3040,10.1007/978-3-662-44654-6_2,,,
3041,10.1093/acprof:oso/9780199552795.001.0001,,,
3042,10.1002/0471660264,,,
3043,10.1609/aaai.v33i01.33019775,,,
3044,10.1016/j.cobeha.2019.04.004,,,
3045,10.2307/2215339,,,
3046,10.1017/cbo9780511978036.003,,,
3047,10.1007/s11023-021-09567-6,,,
3048,10.3390/proceedings2022081018,,,
3049,10.1017/cbo9780511613982.011,,,
3050,10.1093/oxfordhb/9780195145397.003.0017,,,
3051,10.1093/oso/9780199261888.003.0010,,,
3052,10.1023/a:1007614523901,,,
3053,10.1515/9783110706611-006,,,
3054,10.48550/arxiv.2204.05437,,,
3055,10.1017/s0266267114000169,,,
3056,10.1007/s10676-020-09535-1,,,
3057,10.1186/1687-6180-2012-165,,,
3058,10.1086/233669,,,
3059,10.1017/cbo9780511808296,,,
3060,10.11647/obp.0029.04,,,
3061,10.1023/a:1009946911117,,,
3062,10.1007/s10115-007-0114-2,,,
3063,10.1007/s11192-021-04200-w,,,
3064,10.1007/s10734-020-00553-y,,,
3065,10.1145/1045339.1045340,,,
3066,10.1109/jrproc.1961.287775,,,
3067,10.1049/tje2.12381,,,
3068,10.2478/ijanmc-2024-0010,,,
3069,10.48185/jaai.v5i1.974,,,
3070,10.35882/jeeemi.v6i1.351,,,
3071,10.5772/intechopen.100551,,,
3072,10.1109/incet49848.2020.9154000,,,
3073,10.26140/bgz3-2020-0903-0021,,,
3074,10.31992/0869-3617-2022-31-7-79-95,,,
3075,10.4324/9781003281399-2,,,
3076,10.31992/0869-3617-2023-32-10-9-33,,,
3077,10.31992/0869-3617-2023-32-4-9-22,,,
3078,10.20339/am.02-21.017,,,
3079,10.31992/0869-3617-202231-5-25-45,,,
3080,10.31992/0869-3617-2022-31-2-9-27,,,
3081,10.4324/9781315749136,,,
3082,10.1080/10508422.2022.2026775,,,
3083,10.1207/s15326985ep4103_1,,,
3084,10.20310/1810-0201-2023-28-2-276-301,,,
3085,10.7771/2832-9414.1815,,,
3086,10.1007/s10639-023-12146-0,,,
3087,10.32744/pse.2023.3.13,,,
3088,10.17150/2500-2759.2019.29(2).207-212,,,
3089,10.15826/umpa.2018.03.028,,,
3090,10.2139/ssrn.4361548,,,
3091,10.1111/ejed.12533,,,
3092,10.1142/s0219265921430325,,,
3093,10.1177/1028315316647164,,,
3094,10.1080/01587919.2015.1019963,,,
3095,10.1108/09513540210418403,,,
3096,10.1108/02634500310490265,,,
3097,10.1177/2167696815571665,,,
3098,10.1080/03075079.2017.1293872,,,
3099,10.1300/j066v18n01_03,,,
3100,10.3390/su13147675,,,
3101,10.1007/s40593-016-0110-3,,,
3102,10.3389/frai.2022.903051,,,
3103,10.3390/app12178438,,,
3104,10.1109/icaict.2010.5612054,,,
3105,10.1016/j.caeai.2020.100002,,,
3106,10.1016/j.procs.2018.08.233,,,
3107,10.1111/j.1467-8535.2009.00979.x,,,
3108,10.1016/j.compedu.2010.10.020,,,
3109,10.1080/09588221.2018.1527361,,,
3110,10.2478/jolace-2019-0025,,,
3111,10.1007/s11528-017-0210-4,,,
3112,10.14742/ajet.3795,,,
3113,10.30831/akukeg.939836,,,
3114,10.1177/1080569904268141,,,
3115,10.18535/ijsshi/v10i01.02,,,
3116,10.1108/lht-08-2018-0105,,,
3117,10.5860/crl.81.5.865,,,
3118,10.1108/lht-07-2021-0229,,,
3119,10.1016/j.caeai.2022.100064,,,
3120,10.1145/3478281,,,
3121,10.18848/1832-3669/cgp/v07i02/56198,,,
3122,10.1109/csci51800.2020.00116,,,
3123,10.1111/j.1745-3984.1984.tb01040.x,,,
3124,10.1017/s0267190599190147,,,
3125,10.1007/s10649-018-9863-y,,,
3126,10.1080/13562517.2013.827653,,,
3127,10.1007/s10639-019-10068-4,,,
3128,10.1016/j.chb.2018.07.027,,,
3129,10.1111/bjet.12212,,,
3130,10.1080/10494820.2021.1933542,,,
3131,10.1177/0950422218770937,,,
3132,10.1111/j.1745-3984.2001.tb01115.x,,,
3133,10.1177/0002764213479366,,,
3134,10.1002/asi.24750,,,
3135,10.1109/3ict53449.2021.9582096,,,
3136,10.1109/iccica52458.2021.9697272,,,
3137,10.1007/978-3-031-13351-0,,,
3138,10.1007/s00146-021-01168-2,,,
3139,10.1145/3371647.3371659,,,
3140,10.1016/j.techfore.2022.122307,,,
3141,10.1007/s11846-022-00521-z,,,
3142,10.1177/00018392221083650,,,
3143,10.2139/ssrn.4405398,,,
3144,10.2139/ssrn.4405391,,,
3145,10.1016/j.ijhm.2020.102723,,,
3146,10.1146/annurev.cs.03.060188.000323,,,
3147,10.1108/ejim-02-2023-0156/full/pdf,,,
3148,10.1177/23294906221074311,,,
3149,10.1108/tr-02-2023-0088,,,
3150,10.1021/acs.jmedchem.1c02042,,,
3151,10.1016/j.ypmed.2022.107170,,,
3152,10.1111/radm.12186,,,
3153,10.1142/s1363919620500152,,,
3154,10.2139/ssrn.4404276,,,
3155,10.14445/22315381/ijett-v13p237,,,
3156,10.3390/jtaer16060118,,,
3157,10.21325/jotags.2023.1217,,,
3158,10.3726/978-3-0351-0351-9,,,
3159,10.2139/ssrn.4375268,,,
3160,10.1177/0149206316675927,,,
3161,10.1007/s11019-023-10136-0/metrics,,,
3162,10.1007/978-1-0716-1787-8_4,,,
3163,10.1109/mts.2023.3241309,,,
3164,10.1080/00208825.2001.11656819,,,
3165,10.2196/15065,,,
3166,10.54055/ejtr.v34i.3169,,,
3167,10.1109/tem.2023.3275643,,,
3168,10.1007/978-3-658-05014-6_2,,,
3169,10.2139/ssrn.3554286,,,
3170,10.2139/ssrn.4390529,,,
3171,10.1007/s11846-021-00506-4,,,
3172,10.1080/12460125.2022.2062848,,,
3173,10.1007/s11846-022-00588-8,,,
3174,10.1142/s0219877020500431,,,
3175,10.1108/ijebr-12-2021-0984,,,
3176,10.1145/3065386,,,
3177,10.3390/joitmc5030044,,,
3178,10.13052/jmbmit2245-456x.421,,,
3179,10.2139/ssrn.4333415,,,
3180,10.9734/bpi/rpst/v5/18240d,,,
3181,10.1186/s12910-021-00577-8/figures/4,,,
3182,10.1002/pra2.513,,,
3183,10.1007/978-3-658-30168-2_8,,,
3184,10.1007/978-1-0716-1787-8_6/cover,,,
3185,10.1017/9781009243520,,,
3186,10.3390/ai1020011,,,
3187,10.1007/s11846-022-00613-w,,,
3188,10.1007/s11846-023-00668-3,,,
3189,10.1145/3503914,,,
3190,10.1007/s11573-015-0794-0,,,
3191,10.1038/d41586-022-04397-7,,,
3192,10.1038/s41746-021-00459-8,,,
3193,10.1007/s11365-023-00882-1,,,
3194,10.7326/m18-0850,,,
3195,10.31219/osf.io/mcrfz,,,
3196,10.1007/s11277-018-5612-x/tables/6,,,
3197,10.1145/365153.365168,,,
3198,10.1177/14705931221075832,,,
3199,10.1007/s00158-021-02953-9/figures/30,,,
3200,10.2139/ssrn.4380516.accessed18april2023,,,
3201,10.2139/ssrn.4346152,,,
3202,10.3844/ajassp.2013.1298.1306,,,
3203,10.1037/0033-295x.111.4.1036,,,
3204,10.1007/s10472-019-09664-4,,,
3205,10.1073/pnas.2025764118,,,
3206,10.1007/s10506-017-9194-9,,,
3207,10.1515/pjbr-2019-0004,,,
3208,10.1007/978-3-662-48899-7_37,,,
3209,10.1080/09537325.2021.1921137,,,
3210,10.1007/s10551-006-9290-3,,,
3211,10.1109/time.2001.930691,,,
3212,10.1002/hast.261,,,
3213,10.1093/oxfordhb/9780190067397.013.1,,,
3214,10.1007/s00146-019-00888-w,,,
3215,10.1016/j.cogsys.2020.08.010,,,
3216,10.1016/j.artmed.2005.06.001,,,
3217,10.1109/thms.2013.2293535,,,
3218,10.1007/s10458-012-9202-0,,,
3219,10.1007/s10462-009-9094-9,,,
3220,10.1016/j.ins.2005.10.007,,,
3221,10.1111/puar.12685,,,
3222,10.1109/sp.2016.42,,,
3223,10.1145/2701413,,,
3224,10.1037/a0032947,,,
3225,10.1145/3278721.3278769,,,
3226,10.1007/s10676-016-9400-6,,,
3227,10.1093/acprof:osobl/9780199604432.001.0001,,,
3228,10.1007/s11948-012-9413-4,,,
3229,10.1080/09614520701469955,,,
3230,10.17485/ijst/2017/v10i1/109392,,,
3231,10.1080/13546780903395748,,,
3232,10.2991/978-94-6239-027-0_6,,,
3233,10.1086/675875,,,
3234,10.1057/978-1-137-54187-1,,,
3235,10.1017/s0140525x17000164,,,
3236,10.1007/978-3-319-09870-8_22,,,
3237,10.1007/s11097-014-9355-1,,,
3238,10.1103/physreve.51.4282,,,
3239,10.1257/aer.91.2.73,,,
3240,10.1007/s10021-001-0101-5,,,
3241,10.1007/978-94-007-6970-0_40,,,
3242,10.1111/coin.12121,,,
3243,10.1037/0003-066x.39.4.341,,,
3244,10.1016/j.future.2018.11.033,,,
3245,10.1016/j.eswa.2019.04.004,,,
3246,10.1007/s10462-018-9646-y,,,
3247,10.1016/s0743-1066(96)00137-9,,,
3248,10.1016/j.cogsys.2006.07.004,,,
3249,10.1609/aaai.v31i1.11150,,,
3250,10.1111/gec3.12444,,,
3251,10.1016/j.cogsys.2017.05.001,,,
3252,10.1023/b:bttj.0000047600.45421.6d,,,
3253,10.1006/jesp.1996.1314,,,
3254,10.1016/j.artint.2007.10.009,,,
3255,10.1147/sj.2002.5386871,,,
3256,10.1016/s1364-6613(00)01601-6,,,
3257,10.1007/3-540-45632-5_17,,,
3258,10.13140/rg.2.2.14956.46722/1,,,
3259,10.1145/345124.345145,,,
3260,10.1111/csp2.370,,,
3261,10.1093/logcom/14.5.703,,,
3262,10.1016/j.cogsys.2004.06.001,,,
3263,10.1093/llc/fql014,,,
3264,10.1016/s1574-6526(07)03017-9,,,
3265,10.3115/1119239.1119246,,,
3266,10.1007/s10676-021-09596-w,,,
3267,10.1207/s15516709cog0402_2,,,
3268,10.1017/s0140525x00069478,,,
3269,10.1093/scipol/scs093,,,
3270,10.3139/9783446431164,,,
3271,10.1136/bmj.320.7237.768,,,
3272,10.1111/j.1747-9991.2011.00472.x,,,
3273,10.7551/mitpress/4298.003.0017,,,
3274,10.1016/j.cogsys.2019.12.002,,,
3275,10.1093/acprof:oso/9780199230167.003.0006,,,
3276,10.1609/aiide.v7i2.12462,,,
3277,10.1016/j.cogsys.2010.12.007,,,
3278,10.1017/cbo9781139173452,,,
3279,10.1007/3-540-48317-9_17,,,
3280,10.1257/aer.107.4.967,,,
3281,10.1515/9780691189970,,,
3282,10.1145/3419764,,,
3283,10.4018/ijats.2017010102,,,
3284,10.1023/a:1011341803977,,,
3285,10.1023/b:bttj.0000047599.89995.3c,,,
3286,10.1145/2699916,,,
3287,10.1075/aicr.19.10slo,,,
3288,10.1093/acprof:oso/9780199998074.003.0009,,,
3289,10.7208/chicago/9780226771199.001.0001,,,
3290,10.1093/acprof:oso/9780199230167.003.0003,,,
3291,10.1080/0951508042000286721,,,
3292,10.1080/09515089.2020.1719054,,,
3293,10.2478/v10229-011-0015-3,,,
3294,10.1080/0144929x.2020.1818828,,,
3295,10.1109/tevc.2006.890274,,,
3296,10.1080/00140139.2018.1457725,,,
3297,10.1007/s00146-020-01066-z,,,
3298,10.1007/978-3-030-30391-4_12,,,
3299,10.4018/978-1-59140-987-8.ch053,,,
3300,10.1108/14779961011071088,,,
3301,10.2307/j.ctvkjb1w9,,,
3302,10.1080/02642069.2020.1727892,,,
3303,10.1016/j.artint.2020.103239,,,
3304,10.2307/j.ctv153k50r.8,,,
3305,10.1007/s13347-020-00418-3,,,
3306,10.1145/3173574.3174225,,,
3307,10.1017/asjcl.2019.34,,,
3308,10.1093/ojlr/rwaa023,,,
3309,10.1108/jices-02-2023-0015,,,
3310,10.1111/j.1468-0378.2006.00241.x,,,
3311,10.1017/cbo9780511801044,,,
3312,10.1111/j.1478-1913.1960.tb01091.x,,,
3313,10.1038/d41586-018-06610-y,,,
3314,10.1073/pnas.1320040111,,,
3315,10.1163/156851908x413757,,,
3316,10.1007/978-1-349-00248-1,,,
3317,10.1007/978-981-287-778-9_11,,,
3318,10.1145/3442188.3445896,,,
3319,10.1163/9789047409007,,,
3320,10.1093/0195138376.001.0001,,,
3321,10.1145/2460276.2460278,,,
3322,10.2307/j.ctvhrd092.18,,,
3323,10.1080/23299460.2016.1216709,,,
3324,10.1017/cbo9780511606823,,,
3325,10.1016/j.eswa.2015.03.023,,,
3326,10.1016/j.ijinfomgt.2020.102225,,,
3327,10.1126/science.aap8062,,,
3328,10.1016/j.jbusres.2020.11.003,,,
3329,10.1007/s11948-019-00146-8,,,
3330,10.1038/s42256-021-00296-0,,,
3331,10.1038/s41570-019-0124-0,,,
3332,10.1016/j.ijinfomgt.2019.01.021,,,
3333,10.1126/science.aay6636,,,
3334,10.1007/s11948-020-00213-5,,,
3335,10.1002/cphc.202000518,,,
3336,10.1016/j.jbusres.2019.09.062,,,
3337,10.1080/10590501.2018.1537118,,,
3338,10.1038/s42256-020-00236-4,,,
3339,10.1126/science.aay6637,,,
3340,10.1126/science.aaa8415,,,
3341,10.1016/j.bushor.2019.09.003,,,
3342,10.1016/j.ijinfomgt.2021.102371,,,
3343,10.1016/j.anireprosci.2016.02.027,,,
3344,10.1073/pnas.1900654116,,,
3345,10.1038/s42256-019-0038-z,,,
3346,10.1890/120375,,,
3347,10.1038/nrd.2017.232,,,
3348,10.1038/s42256-019-0030-7,,,
3349,10.1021/acs.est.7b02862,,,
3350,10.1007/s10676-009-9187-9,,,
3351,10.1021/acs.chemrestox.9b00227,,,
3352,10.1038/sdata.2016.18,,,
3353,10.1057/s41599-020-0467-7,,,
3354,10.1093/jaac/kpab054,,,
3355,10.1007/s13194-020-00310-z,,,
3356,10.1111/j.1540-6245.2009.01389.x,,,
3357,10.5406/jaesteduc.50.1.0016,,,
3358,10.7916/jla.v39i3.2077,,,
3359,10.1087/20150211,,,
3360,10.1007/s11229-022-03902-9,,,
3361,10.1016/j.isci.2020.101515,,,
3362,10.2139/ssrn.4350802,,,
3363,10.1093/acprof:oso/9780198159216.003.0007,,,
3364,10.48550/arxiv.2209.07667,,,
3365,10.1579/z38sf2mc24,,,
3366,10.48550/arxiv.2302.02337,,,
3367,10.3390/arts7020018,,,
3368,10.1111/jaac.12075,,,
3369,10.1177/01461672221149815,,,
3370,10.1145/3600211.3604681,,,
3371,10.1145/3600211.3604716,,,
3372,10.4337/9781839109973.00007,,,
3373,10.4850/arxiv.2211.09110,,,
3374,10.3390/arts7030025,,,
3375,10.1002/9781444354843.ch3,,,
3376,10.48550/arxiv.1903.02166,,,
3377,10.4324/9781003075011-1,,,
3378,10.1007/s00146-023-01692-3,,,
3379,10.4850/arxiv.2112.10752,,,
3380,10.2307/j.ctv2sp3dpd,,,
3381,10.48550/arxiv.2302.04222,,,
3382,10.2139/ssrn.4315686,,,
3383,10.1007/s00299-023-03007-8,,,
3384,10.1080/08989621.2020.1779591,,,
3385,10.1007/s13194-022-00499-1,,,
3386,10.2139/ssrn.4350925,,,
3387,10.4850/arxiv.2302.05543,,,
3388,10.3346/jkms.2023.38.e198,,,
3389,10.1186/s12961-016-0078-3,,,
3390,10.4103/2229-3485.128020,,,
3391,10.1111/j.1468-0009.2011.00644.x,,,
3392,10.1186/s12910-015-0020-1,,,
3393,10.1111/j.1748-720x.2012.00724.x,,,
3394,10.1177/03000605211000157,,,
3395,10.1056/nejmp1702071,,,
3396,10.1016/j.jpi.2023.100338,,,
3397,10.1186/s12910-020-00480-8,,,
3398,10.1080/15265161.2022.2063434,,,
3399,10.1002/eahr.500003,,,
3400,10.1525/jer.2013.8.3.58,,,
3401,10.1002/ajim.23037,,,
3402,10.1097/01.coc.0000135925.83221.b3,,,
3403,10.1001/jamanetworkopen.2021.10848,,,
3404,10.51594/csitrj.v4i3.629,,,
3405,10.47992/ijaeml.2581.7000.0191,,,
3406,10.1016/j.giq.2018.10.001,,,
3407,10.1016/j.giq.2020.101490,,,
3408,10.52539/mad.1050640,,,
3409,10.61969/jai.1311271,,,
3410,10.21541/apjess.1293702,,,
3411,10.26745/ahbvuibfd.1424290,,,
3412,10.3390/informatics10020049,,,
3413,10.1111/puar.13293,,,
3414,10.1007/978-3-662-61794-6_1,,,
3415,10.30783/nevsosbilen.1121818,,,
3416,10.24988/ije.1007551,,,
3417,10.1016/j.giq.2019.07.004,,,
3418,10.1007/978-981-99-4932-8_42,,,
3419,10.58307/kaytek.1185712,,,
3420,10.2139/ssrn.3366846,,,
3421,10.3390/make3010004,,,
3422,10.53001/uluabd.2023.59,,,
3423,10.1007/978-1-4842-3685-7,,,
3424,10.1002/poi3.276,,,
3425,10.30692/sisad.1311336,,,
3426,10.1017/s0140525x16001837,,,
3427,10.1080/07380569.2023.2256710,,,
3428,10.1145/3313831.3376727,,,
3429,10.1145/3239556,,,
3430,10.1007/s00146-024-01863-w,,,
3431,10.1111/1467-8500.12521,,,
3432,10.1177/0020852316640058,,,
3433,10.1080/14719037.2022.2063935,,,
3434,10.1145/3428502.3428513,,,
3435,10.1109/iisr.2018.8535903,,,
3436,10.1016/j.eng.2016.04.018,,,
3437,10.1109/iscon57294.2023.10112133,,,
3438,10.1016/s0262-4079(17)32044-4,,,
3439,10.14520/adyusbd.1198232,,,
3440,10.1108/k-01-2020-0060,,,
3441,10.7827/turkishstudies.14373,,,
3442,10.1080/1369118x.2023.2246526,,,
3443,10.21248/jlcl.22.2007.88,,,
3444,10.1016/j.worlddev.2018.05.007,,,
3445,10.33630/ausbf.691119,,,
3446,10.25272/icps.1354693,,,
3447,10.26650/siyasal.2022.31.1121900,,,
3448,10.1007/978-981-13-2348-5_16,,,
3449,10.17932/iau.iausbd.2021.021/iausbd_v16i2002,,,
3450,10.1093/ppmgov/gvz014,,,
3451,10.1017/9781108616188.006,,,
3452,10.1086/227762,,,
3453,10.1257/0895330053147930,,,
3454,10.1145/2422512.2422513,,,
3455,10.1037/0022-3514.71.3.479,,,
3456,10.1007/bf00383613,,,
3457,10.1007/s00146-017-0760-1,,,
3458,10.2139/ssrn.3914119,,,
3459,10.1126/science.aaf2654,,,
3460,10.1007/s43681-020-00002-7,,,
3461,10.1017/9781108914857.007,,,
3462,10.1073/pnas.2023301118,,,
3463,10.1037/h0025589,,,
3464,10.1007/s43681-021-00052-5,,,
3465,10.1007/bf00382575,,,
3466,10.1136/bmj.290.6476.1194,,,
3467,10.1111/1758-5899.12713,,,
3468,10.1126/science.162.3859.1243,,,
3469,10.1007/bf00872324,,,
3470,10.1016/0304-405x(76)90026-x,,,
3471,10.1145/1400181.1400190,,,
3472,10.1146/annurev.soc.24.1.183,,,
3473,10.1017/als.2020.19,,,
3474,10.2307/1972412,,,
3475,10.1017/s0020589319000046,,,
3476,10.2307/2576430,,,
3477,10.1007/s13347-020-00403-w,,,
3478,10.1007/978-3-030-69978-9_5,,,
3479,10.1145/3194770.3194776,,,
3480,10.3389/frobt.2021.665729,,,
3481,10.1093/oxfordhb/9780190067397.013.5.,,,
3482,10.1007/978-3-030-00719-5_2,,,
3483,10.1370/afm.1239,,,
3484,10.1016/j.colegn.2017.02.005,,,
3485,10.1186/s12910-021-00577-8,,,
3486,10.1186/s12911-019-1002-x,,,
3487,10.1177/0141076818815510,,,
3488,10.1370/afm.1226,,,
3489,10.1001/jama.1996.03530260066035,,,
3490,10.1016/s0277-9536(00)00098-8,,,
3491,10.1200/jco.2010.32.1554,,,
3492,10.1007/s11764-022-01170-7,,,
3493,10.1503/cmaj.112120,,,
3494,10.1377/hlthaff.2009.0888,,,
3495,10.1177/1090198112455175,,,
3496,10.1007/s11886-016-0729-6,,,
3497,10.1093/bmb/ldab016,,,
3498,10.1016/b978-0-12-818438-7.00002-2,,,
3499,10.1001/jama.2019.21579,,,
3500,10.2196/38397,,,
3501,10.31478/201810a,,,
3502,10.21037/atm.2019.01.13,,,
3503,10.1016/j.jval.2021.09.004,,,
3504,10.1038/s41598-022-20958-2,,,
3505,10.1186/s12910-023-00929-6,,,
3506,10.1186/s12910-021-00687-3,,,
3507,10.1191/1478088706qp063oa,,,
3508,10.1353/rhe.2019.0007,,,
3509,10.1186/s12910-022-00777-w,,,
3510,10.1080/10810730.2015.1018603,,,
3511,10.1002/anr.1780320107,,,
3512,10.3390/su141811389,,,
3513,10.1145/3593013.3594072,,,
3514,10.18653/v1/d19-1339,,,
3515,10.1609/aaai.v37i11.26596,,,
3516,10.18653/v1/2021.findings-emnlp.320,,,
3517,10.18653/v1/2023.acl-long.224,,,
3518,10.1145/3605764.3623985,,,
3519,10.4236/jsea.2024.171003,,,
3520,10.1007/978-3-031-34960-7_22,,,
3521,10.3390/computers12120255,,,
3522,10.18653/v1/2023.emnlp-main.148,,,
3523,10.2307/1410155,,,
3524,10.2139/ssrn.4620277,,,
3525,10.2139/ssrn.4404340,,,
3526,10.1017/dap.2022.10,,,
3527,10.2139/ssrn.4438593,,,
3528,10.1145/3544548.3581196,,,
3529,10.18653/v1/2021.emnlp-main.574,,,
3530,10.1145/3600211.3604672,,,
3531,10.1145/3600211.3604666,,,
3532,10.1145/3461702.3462536,,,
3533,10.2307/4135723,,,
3534,10.18653/v1/2021.findings-acl.127,,,
3535,10.1111/bjet.13370,,,
3536,10.48550/arxiv.2010.11125,,,
3537,10.3390/sym14030471,,,
3538,10.18653/v1/p19-1357,,,
3539,10.1609/icwsm.v12i1.15028,,,
3540,10.1038/s41586-023-06647-8,,,
3541,10.18653/v1/e17-4006,,,
3542,10.18653/v1/p19-1472,,,
3543,10.18653/v1/2021.acl-long.522,,,
3544,10.18653/v1/2021.findings-emnlp.210,,,
3545,10.18653/v1/2020.emnlp-main.48,,,
3546,10.1609/aaai.v35i17.17744,,,
3547,10.1038/s41562-017-0185-3,,,
3548,10.18653/v1/p19-1163,,,
3549,10.18653/v1/2023.trustnlp-1.27,,,
3550,10.1145/3507782,,,
3551,10.1017/s0003055401992019,,,
3552,10.4324/9781315251240-6,,,
3553,10.1080/14748460.2012.691284,,,
3554,10.1145/3549015.3554213,,,
3555,10.1111/j.1931-0846.2006.tb00517.x,,,
3556,10.1201/b17017-78,,,
3557,10.2139/ssrn.4069458,,,
3558,10.1007/978-3-319-25047-2_5,,,
3559,10.1093/acprof:oso/9780199548781.003.0017,,,
3560,10.2139/ssrn.3351404,,,
3561,10.2139/ssrn.4195066,,,
3562,10.1093/oxfordhb/9780198857815.013.4,,,
3563,10.1146/annurev-lawsocsci-121620-081730,,,
3564,10.1177/000271627642600123,,,
3565,10.2478/v10136-012-0031-x,,,
3566,10.1038/533452a,,,
3567,10.3115/1073012.1073017,,,
3568,10.1145/2909824.3020211,,,
3569,10.1007/978-3-319-04135-3,,,
3570,10.1126/science.359.6377.725,,,
3571,10.1126/science.aao2998,,,
3572,10.21236/ada534697,,,
3573,10.1073/pnas.1016658108,,,
3574,10.17226/25021,,,
3575,10.1126/science.1254295,,,
3576,10.1017/cbo9781316471760,,,
3577,10.1140/epjst/e2012-01703-3,,,
3578,10.1186/s40309-019-0157-0,,,
3579,10.1080/02723638.2019.1646049,,,
3580,10.3390/en13061473,,,
3581,10.3390/smartcities2010007,,,
3582,10.3390/su8111197,,,
3583,10.1016/j.caeai.2022.100099,,,
3584,10.1007/s10639-023-12080-1,,,
3585,10.24310/innoeduca.2023.v9i2.16774,,,
3586,10.1007/s10643-021-01303-0,,,
3587,10.1590/s1678-4634202450260125es,,,
3588,10.1007/bf02723327,,,
3589,10.1080/1475939x.2018.1479296,,,
3590,10.1007/s10639-022-11044-1,,,
3591,10.1080/1475939x.2017.1387602,,,
3592,10.3390/ijerph20085571,,,
3593,10.3390/educsci14070740,,,
3594,10.4135/9781412985642,,,
3595,10.1007/s10639-021-10835-2,,,
3596,10.1080/10447318.2022.2049145,,,
3597,10.2307/249008,,,
3598,10.6018/reifop.577211,,,
3599,10.1016/j.ssaho.2023.100599,,,
3600,10.1016/j.techsoc.2021.101535,,,
3601,10.1080/1554480x.2020.1781638,,,
3602,10.12795/pixelbit.102032,,,
3603,10.1016/j.compedu.2021.104264,,,
3604,10.1007/s10055-019-00424-7,,,
3605,10.1016/j.compedu.2019.103760,,,
3606,10.3390/jtaer17020032,,,
3607,10.1007/s10758-018-9355-2,,,
3608,10.1080/20004508.2022.2123121,,,
3609,10.7821/naer.2019.1.317,,,
3610,10.24320/redie.2018.20.1.1383,,,
3611,10.6018/red.582741,,,
3612,10.55612/s-5002-057-007,,,
3613,10.5944/ried.28.1.41379,,,
3614,10.1080/1475939x.2016.1258369,,,
3615,10.12795/pixelbit.100352,,,
3616,10.20511/pyr2017.v5n1.155,,,
3617,10.1080/10494820.2023.2209881,,,
3618,10.1016/j.ijinfomgt.2020.102269,,,
3619,10.1016/j.caeai.2024.100202,,,
3620,10.1007/s11423-016-9481-2,,,
3621,10.23977/aduhe.2023.051318,,,
3622,10.1007/s10055-022-00717-4,,,
3623,10.2307/30036540,,,
3624,10.2307/41410412,,,
3625,10.1016/j.actpsy.2022.103580,,,
3626,10.24815/siele.v8i2.19214,,,
3627,10.1186/s40561-023-00269-3,,,
3628,10.1007/s10639-022-11116-2,,,
3629,10.1371/journal.pcbi.1011319,,,
3630,10.1016/j.autcon.2023.105020,,,
3631,10.1017/s1351324920000601,,,
3632,10.18653/v1/d19-5602,,,
3633,10.1016/j.dibe.2023.100300,,,
3634,10.1016/j.autcon.2023.105067,,,
3635,10.3390/buildings13071772,,,
3636,10.3390/su15097121,,,
3637,10.3390/buildings13040857,,,
3638,10.1016/j.autcon.2021.103929,,,
3639,10.3390/info14030187,,,
3640,10.1016/b978-1-85617-548-7.00005-7,,,
3641,10.1016/j.autcon.2022.104256,,,
3642,10.1108/ci-09-2019-0087,,,
3643,10.3390/s21041044,,,
3644,10.1080/19373260.2010.491641,,,
3645,10.36227/techrxiv.22683919.v2,,,
3646,10.2196/50638,,,
3647,10.3390/ime2030019,,,
3648,10.21533/pen.v5i3.152,,,
3649,10.1016/j.cell.2018.02.010,,,
3650,10.5772/intechopen.74714,,,
3651,10.1007/s10916-020-1536-6,,,
3652,10.1186/s12920-018-0333-2,,,
3653,10.1093/jamia/ocy055,,,
3654,10.1038/s41598-018-30116-2,,,
3655,10.1016/j.bspc.2020.102106,,,
3656,10.4132/jptm.2018.12.16,,,
3657,10.1001/jama.2016.17438,,,
3658,10.1016/j.compeleceng.2017.09.001,,,
3659,10.1109/jbhi.2016.2636665,,,
3660,10.1016/j.ajp.2022.103021,,,
3661,10.3390/diagnostics13010002,,,
3662,10.3390/biomedicines11030887,,,
3663,10.1007/s10506-023-09353-y,,,
3664,10.1016/j.artint.2023.103861,,,
3665,10.7759/cureus.44374,,,
3666,10.1080/10447318.2022.2153320,,,
3667,10.3389/fdgth.2022.854339,,,
3668,10.1093/inthealth/ihaa007,,,
3669,10.3389/fdgth.2020.00006,,,
3670,10.1055/s-0038-1634610,,,
3671,10.1016/j.psep.2019.11.014,,,
3672,10.1177/1094428114562629,,,
3673,10.1108/bfj-03-2020-0234,,,
3674,10.1016/j.ibusrev.2019.101587,,,
3675,10.1007/s00500-018-3511-4,,,
3676,10.1016/j.cie.2019.106120,,,
3677,10.1093/jamiaopen/ooaa034,,,
3678,10.3390/jcm8030360,,,
3679,10.2196/18228,,,
3680,10.2196/18599,,,
3681,10.3390/ijgi5050066,,,
3682,10.1007/978-3-030-29374-1_21,,,
3683,10.34190/ejkm.18.3.2121,,,
3684,10.3390/ijerph19031893,,,
3685,10.1016/j.joi.2017.08.007,,,
3686,10.1016/j.xinn.2021.100179,,,
3687,10.7861/fhj.2021-0095,,,
3688,10.1007/s12032-022-01711-1,,,
3689,10.3389/fpsyt.2022.811665,,,
3690,10.3390/ijerph20054541,,,
3691,10.2196/16866,,,
3692,10.1055/s-0040-1708049,,,
3693,10.1186/s12909-023-04698-z,,,
3694,10.1136/bmjgh-2018-000832,,,
3695,10.3389/frhs.2022.961475,,,
3696,10.1007/s11606-013-2455-8,,,
3697,10.1109/access.2021.3061576,,,
3698,10.3389/frai.2023.1133677,,,
3699,10.1186/s12913-022-08215-8,,,
3700,10.3389/fsurg.2022.862322,,,
3701,10.1186/s12919-021-00228-1,,,
3702,10.13140/rg.2.2.33348.09600,,,
3703,10.1016/j.futures.2012.10.003,,,
3704,10.1002/aaai.12098,,,
3705,10.1016/j.chb.2022.107182,,,
3706,10.1145/3514094.3534145,,,
3707,10.1145/3600211.3604722,,,
3708,10.48550/arxiv.2108.07258,,,
3709,10.1016/j.techfore.2019.119855,,,
3710,10.1016/j.futures.2005.12.002,,,
3711,10.1007/s11569-012-0141-7,,,
3712,10.1016/j.futures.2014.12.005,,,
3713,10.17863/cam.34502,,,
3714,10.48550/arxiv.2302.10329,,,
3715,10.1017/pan.2018.26,,,
3716,10.1080/21670811.2020.1736946,,,
3717,10.1177/1461444820925811,,,
3718,10.1080/19331681.2023.2224316,,,
3719,10.2760/490501,,,
3720,10.1177/2053951720943234,,,
3721,10.4324/9780203793206,,,
3722,10.1108/14636680010802438,,,
3723,10.1177/2053951719897945,,,
3724,10.1177/0306312713508669,,,
3725,10.2139/ssrn.4467684,,,
3726,10.51593/20230022,,,
3727,10.1145/3461702.3462605,,,
3728,10.1080/10447318.2023.2178612,,,
3729,10.1177/20539517211069632,,,
3730,10.1080/19312458.2021.1965973,,,
3731,10.1145/3442188.3445935,,,
3732,10.1007/s10677-015-9582-8,,,
3733,10.2139/ssrn.3877437,,,
3734,10.1007/s40309-013-0033-2,,,
3735,10.1080/01944363.2010.508428,,,
3736,10.1108/fs-08-2012-0061,,,
3737,10.1002/for.3980100602,,,
3738,10.1145/3600211.3604693,,,
3739,10.1016/j.futures.2005.04.001,,,
3740,10.1007/s10462-023-10420-8,,,
3741,10.48550/arxiv.2306.07899,,,
3742,10.1080/23299460.2017.1338105,,,
3743,10.1093/bioinformatics/btab608,,,
3744,10.1016/j.jbi.2022.104190,,,
3745,10.1007/s12551-018-0446-z,,,
3746,10.1109/iccv.2017.299,,,
3747,10.3389/fgene.2021.652907,,,
3748,10.17392/1661-23,,,
3749,10.1001/jamanetworkopen.2023.43689,,,
3750,10.1007/978-3-319-68127-6_6,,,
3751,10.1016/j.compbiomed.2023.107188,,,
3752,10.1016/0002-9394(82)90410-x,,,
3753,10.1056/nejmp1500523,,,
3754,10.3390/diagnostics12040837,,,
3755,10.1016/j.cmpb.2022.106874,,,
3756,10.1200/cci.23.00071,,,
3757,10.1016/j.neunet.2020.09.004,,,
3758,10.7497/j.issn.2095-3941.2012.03.001,,,
3759,10.1109/jbhi.2023.3304388,,,
3760,10.3389/fgene.2020.585804,,,
3761,10.3390/s22051799,,,
3762,10.1007/978-3-658-40442-0_9,,,
3763,10.1093/bioadv/vbac100,,,
3764,10.3389/fonc.2023.1265024,,,
3765,10.1016/j.compbiomed.2023.107024,,,
3766,10.3390/ijms,,,
3767,10.2196/47862,,,
3768,10.1038/s41598-023-32398-7,,,
3769,10.1007/978-3-319-66179-7_48,,,
3770,10.1371/journal.pmed.1003583,,,
3771,10.1145/3159652.3176182,,,
3772,10.3390/electronics10040389,,,
3773,10.1016/j.jbi.2018.04.007,,,
3774,10.3390/diagnostics12112639,,,
3775,10.1093/bioinformatics/btz158,,,
3776,10.1155/2022/7842566,,,
3777,10.1007/978-3-319-59050-9_12,,,
3778,10.1093/bib/bbad329,,,
3779,10.1002/hbm.26146,,,
3780,10.1038/s41698-017-0032-z,,,
3781,10.1002/14651858.cd005139.pub4,,,
3782,10.1002/jmv.29080,,,
3783,10.1186/s12880-023-01128-w,,,
3784,10.1109/access.2021.3071466,,,
3785,10.1016/j.jconrel.2021.06.039,,,
3786,10.1186/s12967-023-04576-8,,,
3787,10.1016/j.compmedimag.2020.101801,,,
3788,10.1016/j.inffus.2020.10.015,,,
3789,10.1016/j.compbiomed.2023.107220,,,
3790,10.1371/journal.pone.0175508,,,
3791,10.1038/s41540-020-00158-2,,,
3792,10.1002/minf.202300064,,,
3793,10.1007/s00138-020-01152-8,,,
3794,10.1109/jbhi.2020.2980262,,,
3795,10.1200/cci.19.00047,,,
3796,10.1109/tase.2023.3333788,,,
3797,10.1109/jbhi.2023.3271615,,,
3798,10.1080/09537287.2020.1780508,,,
3799,10.1016/j.jbusres.2022.113609,,,
3800,10.1177/002224377701400320,,,
3801,10.1016/j.indmarman.2020.12.001,,,
3802,10.1016/j.techfore.2020.120557,,,
3803,10.1016/j.indmarman.2018.04.008,,,
3804,10.1108/jbim-11-2020-0528,,,
3805,10.1016/j.jbusres.2022.113402,,,
3806,10.1016/j.ijinfomgt.2015.10.002,,,
3807,10.1108/josm-12-2022-0398,,,
3808,10.1504/ijbe.2018.095819,,,
3809,10.1016/j.ibusrev.2019.01.004,,,
3810,10.1016/j.technovation.2021.102258,,,
3811,10.4337/9781800379091.00024,,,
3812,10.1016/j.ijpe.2016.12.033,,,
3813,10.1016/j.technovation.2023.102756,,,
3814,10.1177/1094670508328986,,,
3815,10.1016/j.ijresmar.2015.02.004,,,
3816,10.1016/s0167-2681(98)00096-1,,,
3817,10.1016/j.ijpe.2019.107599,,,
3818,10.1057/palgrave.jibs.8490306,,,
3819,10.1007/s10796-021-10186-w/tables/8,,,
3820,10.1016/j.indmarman.2023.04.005,,,
3821,10.1016/j.indmarman.2022.01.003,,,
3822,10.1016/j.indmarman.2020.05.011,,,
3823,10.1016/j.ijpe.2019.03.027,,,
3824,10.24840/2183-0606_004.002_0006,,,
3825,10.1108/09564239610129931,,,
3826,10.1007/978-1-4419-9707-4_5,,,
3827,10.1177/1094670517752459/asset/images/large/10.1177_1094670517752459-fig4.jpeg,,,
3828,10.1108/jbim-03-2016-0055,,,
3829,10.1108/ijopm-09-2014-0424,,,
3830,10.1016/j.technovation.2020.102218,,,
3831,10.1108/jbim-12-2021-0588,,,
3832,10.1080/07421222.2017.1334467,,,
3833,10.1016/j.jbusres.2019.06.027,,,
3834,10.1016/j.indmarman.2022.06.010,,,
3835,10.1007/s12599-014-0341-5,,,
3836,10.1007/s12599-014-0341-5/figures/6,,,
3837,10.1108/josm-12-2022-0403,,,
3838,10.1108/s1548-643520230000020010,,,
3839,10.1108/aaaj-09-2020-4934,,,
3840,10.1016/j.techfore.2023.123189,,,
3841,10.1108/jrim-12-2020-0252,,,
3842,10.3390/joitmc7040221,,,
3843,10.1016/j.indmarman.2022.03.003,,,
3844,10.1108/josm-12-2022-0404,,,
3845,10.1108/jbim-12-2018-0400,,,
3846,10.1016/j.im.2021.103434,,,
3847,10.1016/j.lrp.2018.10.005,,,
3848,10.1108/josm-12-2022-0401,,,
3849,10.1108/josm-03-2024-498,,,
3850,10.1108/josm-11-2020-0427,,,
3851,10.1108/ijpdlm-12-2021-0538,,,
3852,10.1016/j.technovation.2023.102885,,,
3853,10.1016/j.jclepro.2024.142265,,,
3854,10.1016/j.ijpe.2014.12.036,,,
3855,10.1016/j.jbusres.2021.04.047,,,
3856,10.1177/1536867x1501500303,,,
3857,10.1016/j.jretconser.2019.06.009,,,
3858,10.1108/jbim-10-2018-0295,,,
3859,10.1108/jrim-10-2020-0214,,,
3860,10.1108/jrim-01-2023-0030,,,
3861,10.1016/j.techfore.2023.122491,,,
3862,10.1108/itpd-06-2023-0014,,,
3863,10.1145/2493536,,,
3864,10.1108/josm-12-2022-0375,,,
3865,10.1016/j.indmarman.2010.03.002,,,
3866,10.1016/j.indmarman.2022.04.002,,,
3867,10.1016/j.ijinfomgt.2020.102231,,,
3868,10.1016/j.indmarman.2013.11.005,,,
3869,10.1002/(sici)1097-0266(199906)20:6<567::aid-smj36>3.0.co;2-k,,,
3870,10.1016/s0022-4359(18)30076-9,,,
3871,10.1016/j.ijpe.2017.06.006,,,
3872,10.1016/j.jbusres.2020.01.009,,,
3873,10.1016/j.jbusres.2021.05.009,,,
3874,10.1016/j.techfore.2023.122903,,,
3875,10.1016/j.im.2018.01.004,,,
3876,10.1109/sceecs57921.2023.10063008,,,
3877,10.1111/jpim.12398,,,
3878,10.1007/s10796-021-10112-0,,,
3879,10.1002/dir.20069,,,
3880,10.1080/08874417.2016.1222891,,,
3881,10.1016/j.indmarman.2019.09.003,,,
3882,10.1108/josm-02-2023-0052,,,
3883,10.1504/ijbe.2018.093301,,,
3884,10.1016/j.indmarman.2021.04.001,,,
3885,10.1016/j.jbusres.2020.01.047,,,
3886,10.1108/jkm-05-2021-0387,,,
3887,10.1016/j.ijpe.2022.108682,,,
3888,10.1016/j.jbusres.2019.09.022,,,
3889,10.1108/jbim-03-2021-0182,,,
3890,10.1016/j.jbusres.2016.08.009,,,
3891,10.1108/bpmj-10-2019-0411,,,
3892,10.1080/00472778.2022.2100897,,,
3893,10.1016/j.jbusres.2021.08.033,,,
3894,10.1007/s00500-023-08147-3/tables/6,,,
3895,10.1504/ijtm.2020.112615,,,
3896,10.1108/jeim-09-2022-0333/full/pdf,,,
3897,10.1108/jbim-07-2022-0308,,,
3898,10.4324/9780429290411-17,,,
3899,10.1017/cbo9780511610325.008,,,
3900,10.1109/percomw.2014.6815231,,,
3901,10.3390/technologies10020050,,,
3902,10.1080/14768320500051359,,,
3903,10.1093/eurpub/ckz167,,,
3904,10.4324/9781003277729-23,,,
3905,10.24908/ss.v17i1/2.12942,,,
3906,10.1080/02681102.2019.1650244,,,
3907,10.1037/0003-066x.59.7.614,,,
3908,10.3389/fdgth.2022.807886,,,
3909,10.1017/cbo9780511498725.017,,,
3910,10.1007/s11948-022-00376-3,,,
3911,10.1111/jopp.12202,,,
3912,10.1080/0268093980130402,,,
3913,10.1080/02680930210158285,,,
3914,10.1177/2050157914530297,,,
3915,10.1080/1369118x.2015.1050438,,,
3916,10.1007/s11948-022-00419-9,,,
3917,10.5210/fm.v7i4.942,,,
3918,10.1093/phe/phy020,,,
3919,10.1093/acprof:oso/9780195376685.003.0014,,,
3920,10.1007/s13347-023-00618-7,,,
3921,10.1016/j.techsoc.2021.101750,,,
3922,10.1007/s12599-018-0521-9,,,
3923,10.1007/s00779-017-1020-6,,,
3924,10.1007/s10676-023-09732-8,,,
3925,10.1007/s00146-013-0532-5,,,
3926,10.1080/17579961.2023.2184137,,,
3927,10.1177/1461444818755650,,,
3928,10.1007/s13347-018-0316-4,,,
3929,10.1016/j.hlpt.2021.100582,,,
3930,10.4324/9781315162867,,,
3931,10.1177/1461444818765154,,,
3932,10.4414/smw.2019.20089,,,
3933,10.1037/0278-6133.21.2.167,,,
3934,10.7249/rr254,,,
3935,10.1001/jama.2020.2724,,,
3936,10.1037/a0034438,,,
3937,10.1111/1467-9752.12554,,,
3938,10.1177/1461444815604328,,,
3939,10.15171/ijhpm.2013.36,,,
3940,10.1177/1368430212454925,,,
3941,10.1002/ejsp.428,,,
3942,10.1007/s00146-018-0835-7,,,
3943,10.1007/s11019-021-10049-w,,,
3944,10.1606/1044-3894.17,,,
3945,10.1007/s11948-022-00397-y,,,
3946,10.1037/0022-3514.76.3.482,,,
3947,10.1016/j.techfore.2017.09.033,,,
3948,10.1002/9781119227205,,,
3949,10.1017/s1357321719000072,,,
3950,10.1037/0022-3514.69.5.797,,,
3951,10.1017/9781009089678,,,
3952,10.1016/j.ijhm.2020.102769,,,
3953,10.1002/9780470281819.ch3,,,
3954,10.1108/s2050-206020150000010002,,,
3955,10.1177/1461444818797082,,,
3956,10.1017/err.2020.100,,,
3957,10.1007/s10916-021-01771-2,,,
3958,10.1186/s12889-019-7407-8,,,
3959,10.1080/1369118x.2016.1186713,,,
3960,10.1145/3321408.3323080,,,
3961,10.1007/s12553-021-00596-w,,,
3962,10.3390/ijerph18062823,,,
3963,10.1111/j.1540-6237.2009.00617.x,,,
3964,10.29173/irie134,,,
3965,10.1145/3375627.3375855,,,
3966,10.1080/01691864.2021.1886167,,,
3967,10.4324/9781315562698,,,
3968,10.1007/3-540-44617-6_2,,,
3969,10.2202/1941-6008.1126,,,
3970,10.1007/s11948-022-00370-9,,,
3971,10.3389/fpsyg.2018.00468,,,
3972,10.1007/s00146-017-0773-9,,,
3973,10.5325/jpoststud.3.1.0005,,,
3974,10.1007/s12369-016-0368-5,,,
3975,10.7196/sajbl.2019.v12i2.679,,,
3976,10.1007/978-3-030-66151-9_1,,,
3977,10.4324/9780429438189-17,,,
3978,10.1007/s13347-022-00531-5,,,
3979,10.1109/jproc.2004.835359,,,
3980,10.1007/978-3-642-39802-5_45,,,
3981,10.1007/s12369-021-00852-z,,,
3982,10.1111/j.1467-9760.2007.00280.x,,,
3983,10.1007/s10677-011-9302-y,,,
3984,10.1558/blth.3.2.215.65725,,,
3985,10.1177/0392192113493737,,,
3986,10.1111/j.1527-2001.2009.01086.x,,,
3987,10.1016/j.isci.2020.101993,,,
3988,10.1007/s00146-021-01173-5,,,
3989,10.1007/s10676-010-9234-6,,,
3990,10.1007/s11023-006-9030-6,,,
3991,10.4159/9780674978867,,,
3992,10.1007/978-1-4020-6591-0_7,,,
3993,10.1002/asi.24626,,,
3994,10.1007/978-3-030-49788-0_19,,,
3995,10.3390/electronics10070871,,,
3996,10.2196/13930,,,
3997,10.1177/2472630320983813,,,
3998,10.1080/09537325.2021.1883583,,,
3999,10.1016/j.tgie.2019.150636,,,
4000,10.1016/j.breast.2019.10.001,,,
4001,10.2471/blt.19.234732,,,
4002,10.1016/j.artmed.2014.06.004,,,
4003,10.1002/rcs.1968,,,
4004,10.1016/j.ejrad.2019.108768,,,
4005,10.1007/s10926-020-09895-x,,,
4006,10.1080/01947648.2019.1690604,,,
4007,10.1053/j.gastro.2019.08.058,,,
4008,10.4103/jfmpc.jfmpc_440_19,,,
4009,10.5220/0006806900010001,,,
4010,10.1007/s12599-017-0487-z,,,
4011,10.1007/s12599-020-00650-3,,,
4012,10.1001/amajethics.2019.125,,,
4013,10.1080/10400435.2019.1593259,,,
4014,10.1371/journal.pone.0217349,,,
4015,10.1055/s-0039-1677913,,,
4016,10.1109/jbhi.2019.2958437,,,
4017,10.1136/medethics-2019-105860,,,
4018,10.1007/s11019-020-09948-1,,,
4019,10.1001/amajethics.2019.167,,,
4020,10.1055/s-0040-1701980,,,
4021,10.1609/aimag.v27i4.1907,,,
4022,10.1609/aimag.v38i3.2756,,,
4023,10.1109/thfe2.1960.4503259,,,
4024,10.1609/aimag.v1i1.84,,,
4025,10.1609/aimag.v9i4.950,,,
4026,10.1016/j.cose.2021.102248,,,
4027,10.1080/21645515.2020.1822136,,,
4028,10.1016/j.healthpol.2021.03.013,,,
4029,10.1016/j.tre.2021.102271,,,
4030,10.1016/j.cmpb.2020.105617,,,
4031,10.1016/j.artmed.2020.101983,,,
4032,10.1142/s0219622020500285,,,
4033,10.1007/s10916-020-01582-x,,,
4034,10.1016/j.eswa.2020.114155,,,
4035,10.1016/j.jiph.2020.06.028,,,
4036,10.1016/j.dsx.2020.04.012,,,
4037,10.1016/j.ijinfomgt.2020.102170,,,
4038,10.1007/s00146-018-0843-7,,,
4039,10.1109/jiot.2021.3051080,,,
4040,10.1007/s12553-021-00581-3,,,
4041,10.1002/hbe2.237,,,
4042,10.1007/s13205-020-02581-y,,,
4043,10.3386/w31161,,,
4044,10.1016/j.cose.2021.102382,,,
4045,10.1007/978-3-030-29053-5_6,,,
4046,10.1093/cybsec/tyy006,,,
4047,10.1007/978-3-030-29053-5,,,
4048,10.1007/s13347-017-0299-6,,,
4049,10.4324/9781003075011-35,,,
4050,10.1016/j.techsoc.2020.101382,,,
4051,10.1007/978-3-030-29053-5_3,,,
4052,10.1007/978-3-030-29053-5_9,,,
4053,10.1007/978-3-540-69861-6_3,,,
4054,10.57159/gadl.jcmm.2.3.23064,,,
4055,10.3390/educsci13040410,,,
4056,10.1038/d41586-023-00107-z,,,
4057,10.34257/gjcstdvol23is1pg55,,,
4058,10.2139/ssrn.4573321,,,
4059,10.1080/14765284.2023.2245279,,,
4060,10.1016/j.ijpe.2023.109015,,,
4061,10.3390/app13095783,,,
4062,10.3389/fpubh.2023.1166120,,,
4063,10.4018/ijsppc.320225,,,
4064,10.1177/02683962231200411,,,
4065,10.1111/beer.12479,,,
4066,10.1007/s43681-022-00162-8,,,
4067,10.1007/s11948-015-9724-3,,,
4068,10.1007/s43681-023-00265-w,,,
4069,10.29173/irie256,,,
4070,10.2307/jie.30.1.12,,,
4071,10.1287/mnsc.2019.3564,,,
4072,10.1109/icict50521.2020.00076,,,
4073,10.1007/s10479-023-05242-4,,,
4074,10.1016/j.bushor.2021.07.012,,,
4075,10.2139/ssrn.4604736,,,
4076,10.1145/3449287,,,
4077,10.1145/3579605,,,
4078,10.1006/ijhc.1999.0252,,,
4079,10.2514/6.2004-6313,,,
4080,10.1109/sp46214.2022.9833571,,,
4081,10.1162/99608f92.5317da47,,,
4082,10.1177/0008125619867910,,,
4083,10.1016/j.hrmr.2022.100923,,,
4084,10.1080/07370024.2020.1735391,,,
4085,10.1023/b:huma.0000042130.79208.c6,,,
4086,10.1111/joms.12406,,,
4087,10.1177/1534484318804653,,,
4088,10.1007/s10676-021-09619-6,,,
4089,10.1080/1359432x.2019.1620328,,,
4090,10.1007/978-3-030-63864-1_4,,,
4091,10.1287/orsc.2020.1361,,,
4092,10.1023/a:1006023500585,,,
4093,10.1515/bis-2018-0018,,,
4094,10.1177/0001839217713748,,,
4095,10.1080/23808985.2008.11679077,,,
4096,10.1007/s13347-017-0275-1,,,
4097,10.32855/fcapital.201902.012,,,
4098,10.1017/9781316987308,,,
4099,10.1007/s11023-021-09579-2,,,
4100,10.1007/s00146-020-01089-6,,,
4101,10.15185/izawol.344,,,
4102,10.5465/amr.2007.24351328,,,
4103,10.1037/0021-9010.93.1.108,,,
4104,10.1111/jels.12276,,,
4105,10.1037/h0076546,,,
4106,10.1016/0030-5073(76)90016-7,,,
4107,10.1109/mc.2018.3620965,,,
4108,10.1016/s0262-4079(20)32269-7,,,
4109,10.55613/jeet.v24i1.12,,,
4110,10.1016/j.bushor.2018.03.007,,,
4111,10.1177/0266382119883999,,,
4112,10.5465/annals.2018.0174,,,
4113,10.1016/s0167-4870(02)00201-5,,,
4114,10.1007/s10551-019-04204-w,,,
4115,10.1007/s10551-009-0118-9,,,
4116,10.1177/1059601112461578,,,
4117,10.1016/j.jvb.2018.07.004,,,
4118,10.3389/fpsyg.2018.01157,,,
4119,10.1287/orsc.1120.0806,,,
4120,10.1007/s10551-013-1675-5,,,
4121,10.1515/9780228002888,,,
4122,10.1007/s00146-019-00887-x,,,
4123,10.4159/harvard.9780674061200,,,
4124,10.1111/apps.12241,,,
4125,10.1177/09637214221091823,,,
4126,10.1353/sor.2013.0027,,,
4127,10.1111/joms.12421,,,
4128,10.1080/00963402.2019.1604873,,,
4129,10.1037/0033-295x.110.3.403,,,
4130,10.1177/2053951720919776,,,
4131,10.1007/978-3-030-08277-2_8,,,
4132,10.1146/annurev-orgpsych-032516-113115,,,
4133,10.2307/j.ctt7t3cm,,,
4134,10.1016/j.bushor.2018.07.001,,,
4135,10.17705/1thci.00149,,,
4136,10.1787/8f394636-en,,,
4137,10.1787/36c2f31e-en,,,
4138,10.53637/kavv4291,,,
4139,10.18574/nyu/9780814732090.003.0004,,,
4140,10.1016/j.techsoc.2020.101484,,,
4141,10.1177/2056305117733344,,,
4142,10.1108/dprg-07-2018-0038,,,
4143,10.1080/02602938.2018.1463354,,,
4144,10.3389/feduc.2022.790600,,,
4145,10.1177/14757257211037149,,,
4146,10.4018/978-1-5225-0643-0.ch019,,,
4147,10.1016/j.caeai.2022.100101,,,
4148,10.1037/12063-001,,,
4149,10.1177/14757257231195344,,,
4150,10.1177/02655322221126604,,,
4151,10.1016/j.caeai.2024.100204,,,
4152,10.1007/978-3-319-93843-1_2,,,
4153,10.1111/j.1745-3984.2000.tb01077.x,,,
4154,10.1177/0013164409332231,,,
4155,10.3389/frai.2022.903077,,,
4156,10.1002/9780470712993,,,
4157,10.1016/j.caeai.2023.100161,,,
4158,10.1080/15434303.2023.2288266,,,
4159,10.1017/cbo9780511732959,,,
4160,10.18653/v1/2021.emnlp-main.530,,,
4161,10.1177/0146621616677520,,,
4162,10.1002/ets2.12319,,,
4163,10.3389/feduc.2023.858273,,,
4164,10.3726/b21295,,,
4165,10.1177/0265532209104667,,,
4166,10.1016/j.pragma.2022.03.001,,,
4167,10.1016/j.specom.2009.04.005,,,
4168,10.1080/15434300801934702,,,
4169,10.1080/15434303.2018.1453816,,,
4170,10.4324/9780203803912,,,
4171,10.1080/15434303.2023.2289173,,,
4172,10.1017/s0261444822000477,,,
4173,10.1080/00393274.2017.1358662,,,
4174,10.31274/td-20240329-228,,,
4175,10.1177/026553229901600203,,,
4176,10.1111/emip.12237,,,
4177,10.18653/v1/2020.acl-main.428,,,
4178,10.1080/15434303.2018.1472265,,,
4179,10.3390/fi15120375,,,
4180,10.1177/0265532211398110,,,
4181,10.1093/applin/amaa067,,,
4182,10.1080/15434303.2023.2237486,,,
4183,10.18653/v1/w19-5912,,,
4184,10.1177/02655322221076024,,,
4185,10.1177/0265532218772325,,,
4186,10.1002/ets2.12105,,,
4187,10.18653/v1/d19-1410,,,
4188,10.21832/9781788923828-003,,,
4189,10.1177/0265532218758128,,,
4190,10.4324/9781003220756-46,,,
4191,10.1111/emip.12590,,,
4192,10.1057/9781137003522_8,,,
4193,10.1111/modl.12385,,,
4194,10.1080/09588221.2020.1774904,,,
4195,10.1007/978-0-387-85461-8,,,
4196,10.2307/3586922,,,
4197,10.1007/s11336-018-9608-y,,,
4198,10.1037/0033-2909.93.1.179,,,
4199,10.18653/v1/2022.sigdial-1.21,,,
4200,10.1038/s42256-023-00765-8,,,
4201,10.1177/014662168400800201,,,
4202,10.1111/j.1745-3984.1993.tb00423.x,,,
4203,10.1177/0265532219860077,,,
4204,10.1075/sibil.14,,,
4205,10.18653/v1/2023.findings-acl.99,,,
4206,10.1016/j.eswa.2023.120585,,,
4207,10.1007/978-3-030-88163-4_6,,,
4208,10.3390/app12094181,,,
4209,10.3390/informatics10010028,,,
4210,10.48550/arxiv.1705.10743,,,
4211,10.1145/3453892.3461338,,,
4212,10.1080/13467581.2019.1660663,,,
4213,10.1109/msp.2017.2765202,,,
4214,10.1016/j.neuroimage.2013.10.005,,,
4215,10.1038/srep01467,,,
4216,10.1111/j.0013-9580.2005.66104.x,,,
4217,10.3389/fneur.2022.755094,,,
4218,10.1145/3422622,,,
4219,10.1109/embc.2018.8512396,,,
4220,10.48550/arxiv.1806.01875,,,
4221,10.1162/neco.1997.9.8.1735,,,
4222,10.48550/arxiv.1803.09848,,,
4223,10.1109/access.2022.3176367,,,
4224,10.1186/s40537-019-0192-5,,,
4225,10.3390/app11188546,,,
4226,10.1038/s41582-018-0055-2,,,
4227,10.1016/j.jneumeth.2020.108885,,,
4228,10.1109/embc.2018.8512865,,,
4229,10.48550/arxiv.1411.1784,,,
4230,10.1016/j.array.2022.100258,,,
4231,10.14569/ijacsa.2018.090843,,,
4232,10.3389/fninf.2018.00083,,,
4233,10.1101/cshperspect.a022426,,,
4234,10.1016/j.eswa.2018.04.021,,,
4235,10.1007/978-3-319-73600-6_8,,,
4236,10.1016/j.neucom.2021.06.048,,,
4237,10.1016/j.bspc.2019.04.028,,,
4238,10.1016/j.seizure.2017.05.018,,,
4239,10.1155/2022/4114178,,,
4240,10.1088/1741-2552/aca04f,,,
4241,10.1093/oxfordhb/9780195182903.003.0011,,,
4242,10.4478/91898,,,
4243,10.1515/9783110677485-003,,,
4244,10.1007/978-3-642-31674-6,,,
4245,10.1017/s0265052500002314,,,
4246,10.1007/s10677-019-10029-3,,,
4247,10.1007/s1194801799320,,,
4248,10.1017/s0265052500000467,,,
4249,10.7551/mitpress/9780262036689.001.0001,,,
4250,10.1017/9781316091388,,,
4251,10.1057/9781137349088,,,
4252,10.1007/s00146-012-0397-z,,,
4253,10.4324/9780203096970,,,
4254,10.1057/9781137349088_8,,,
4255,10.1515/9780691185446,,,
4256,10.1093/nc/niaa016,,,
4257,10.1007/s10676-017-9427-3,,,
4258,10.1080/0952813x.2017.1309691,,,
4259,10.1007/s00146-022-01575-z,,,
4260,10.1080/14746700.2023.2294530,,,
4261,10.1007/s43681-023-00328-y,,,
4262,10.3389/frobt.2021.715849,,,
4263,10.1080/09528130601116139,,,
4264,10.7551/mitpress/9780262036689.003.0009,,,
4265,10.1007/s10677-015-9563-y,,,
4266,10.1017/cco9781139022484.002,,,
4267,10.1111/1467-8519.00251,,,
4268,10.1017/cbo9781139046855.016,,,
4269,10.1093/oso/9780190905033.001.0001,,,
4270,10.3389/frobt.2018.00121,,,
4271,10.3389/fpsyg.2021.530560,,,
4272,10.1017/s0892679415000647,,,
4273,10.1007/s00146-020-01132-6,,,
4274,10.1007/s10676-012-9290-1,,,
4275,10.1007/978-94-007-7914-3_5,,,
4276,10.1177/2046147x19835250,,,
4277,10.1016/j.pubrev.2018.10.008,,,
4278,10.1111/j.1460-2466.1996.tb01475.x,,,
4279,10.1108/14777260310469319,,,
4280,10.1080/1369118x.2019.1568515,,,
4281,10.1108/shr-07-2018-0051,,,
4282,10.4135/9781452243351,,,
4283,10.1109/icosec51865.2021.9591935,,,
4284,10.1108/14754391111172788,,,
4285,10.17705/1cais.03712,,,
4286,10.1016/j.chb.2018.09.009,,,
4287,10.1007/978-3-030-88972-2,,,
4288,10.1007/s43681-021-00123-7,,,
4289,10.1007/978-3-030-88972-2_4,,,
4290,10.3390/su13041738,,,
4291,10.3390/su12176708,,,
4292,10.3390/su13031412,,,
4293,10.3390/su12229513,,,
4294,10.1007/978-3-030-62066-0,,,
4295,10.3390/su132212656,,,
4296,10.3390/su12208404,,,
4297,10.3390/su12093631,,,
4298,10.34121/1028-9763-2023-1-3-29,,,
4299,10.15407/jai2023.01.105,,,
4300,10.1016/s0364-0213(85)80012-4,,,
4301,10.1016/j.iot.2021.100365,,,
4302,10.1007/978-3-030-21642-9_2,,,
4303,10.1136/bmjgh-2018-000798,,,
4304,10.1007/s00146-019-00917-8,,,
4305,10.1177/1548512920951275,,,
4306,10.7717/peerj-cs.1405,,,
4307,10.1007/978-3-319-62410-5_19/cover,,,
4308,10.3390/fi15030088,,,
4309,10.1016/j.array.2022.100242,,,
4310,10.3390/app13105875,,,
4311,10.1016/j.asoc.2023.110955,,,
4312,10.1016/j.tbs.2022.11.006,,,
4313,10.1186/s42400-018-0005-8,,,
4314,10.1145/3363347.3363357,,,
4315,10.1075/jlp.15.3.08aie,,,
4316,10.1177/1075547012454597,,,
4317,10.1007/978-3-030-52620-7,,,
4318,10.5040/9781350054998.ch-010,,,
4319,10.2307/j.ctv6sj8kf,,,
4320,10.1080/02691728.2017.1410864,,,
4321,10.1177/1470594x14538570,,,
4322,10.7551/mitpress/9780262525381.003.0022,,,
4323,10.1007/978-3-030-32116-1,,,
4324,10.1007/s11948-010-9247-x,,,
4325,10.1007/s11948-002-0018-1,,,
4326,10.1007/s13347-020-00430-7,,,
4327,10.3390/rel11050253,,,
4328,10.1007/s11569-011-0127-x,,,
4329,10.1177/1461444819867318,,,
4330,10.1126/science.aay1956,,,
4331,10.2307/j.ctv12101zq,,,
4332,10.1145/985921.986205,,,
4333,10.4324/9780203508527,,,
4334,10.4324/9780203888841.ch15,,,
4335,10.1177/09636625231153985,,,
4336,10.1016/b978-0-12-801851-4.00009-4,,,
4337,10.1177/1350508416687765,,,
4338,10.1016/s0921-8890(02)00374-3,,,
4339,10.1007/978-3-642-34103-8,,,
4340,10.2312/exp.20191089,,,
4341,10.1109/roman.2008.4600728,,,
4342,10.1145/3593013.3594052,,,
4343,10.1609/aiide.v19i1.27538,,,
4344,10.1080/00043249.1997.10791834,,,
4345,10.1177/135485650100700108open_in_new,,,
4346,10.1145/3491102.3517506,,,
4347,10.1007/978-3-642-31727-9,,,
4348,10.1007/s42452-020-2192-7,,,
4349,10.3390/laws9020010,,,
4350,10.1109/conecct55679.2022.9865761,,,
4351,10.1109/accai58221.2023.10200682,,,
4352,10.1075/aicr.19.18pen,,,
4353,10.1007/s00146-012-0404-4,,,
4354,10.3390/s23156820,,,
4355,10.1177/1357034x10364767,,,
4356,10.1007/s12369-021-00813-6,,,
4357,10.1109/mmul.2008.7,,,
4358,10.1007/978-981-10-0321-9_9,,,
4359,10.3390/robotics8010010,,,
4360,10.1162/002409402760181259,,,
4361,10.48550/arxiv.2211.11483,,,
4362,10.2307/3102690,,,
4363,10.1109/mra.2019.2928823,,,
4364,10.24908/ss.v17i3/4.10821,,,
4365,10.1007/s12369-018-0502-7,,,
4366,10.1017/cbo9780511808418,,,
4367,10.1145/3478432.3499157,,,
4368,10.1080/10400419.2022.2107850,,,
4369,10.32609/0042-8736-2022-6-91-109,,,
4370,10.32609/0042-8736-2022-8-68-94,,,
4371,10.14515/monitoring.2021.1.1807,,,
4372,10.19181/socjour.2021.27.2.8088,,,
4373,10.14515/monitoring.2019.6.03,,,
4374,10.17323/727-0634-2022-20-3-433-444,,,
4375,10.17150/2500-4255.2018.12(6).753-766,,,
4376,10.22363/2313-2272-2022-22-1-58-69,,,
4377,10.22394/2074-0492-2022-3-4-128-155,,,
4378,10.1086/718327,,,
4379,10.1038/s42256-019-0020-9,,,
4380,10.1002/asi.24635,,,
4381,10.1007/s11747-019-00696-0,,,
4382,10.1093/ser/mwx005,,,
4383,10.1007/s00146-022-01496-x,,,
4384,10.1108/ijchm-06-2021-0767,,,
4385,10.1111/joms.12936,,,
4386,10.1007/s00146-024-01960-w,,,
4387,10.1108/978-1-78973-077-720191005,,,
4388,10.2139/ssrn.4527336,,,
4389,10.1145/3544549.3585657,,,
4390,10.1016/j.techsoc.2023.102232,,,
4391,10.1016/j.ifacol.2020.12.2160,,,
4392,10.1016/j.chb.2021.106878,,,
4393,10.1016/j.ijme.2023.100790,,,
4394,10.1007/s12525-021-00496-x,,,
4395,10.1016/j.hrmr.2021.100838,,,
4396,10.1057/s41599-019-0278-x,,,
4397,10.1016/j.hrmr.2021.100857,,,
4398,10.1016/j.techsoc.2023.102372,,,
4399,10.1007/s00146-024-01959-3,,,
4400,10.1080/10494820.2019.1674887,,,
4401,10.1016/j.chb.2022.107335,,,
4402,10.1093/ser/mwab016,,,
4403,10.1111/soc4.12962,,,
4404,10.54276/jupd.2023.2101,,,
4405,10.31866/2617-796x.4.2.2021.247474,,,
4406,10.1111/j.1467-8519.2005.00437.x,,,
4407,10.1111/j.1467-9795.2006.00274.x,,,
4408,10.1007/s12599-023-00834-7,,,
4409,10.1007/s10462-021-10039-7,,,
4410,10.1007/978-3-031-34411-4_32,,,
4411,10.1080/02763869.2018.1404391,,,
4412,10.1007/s12599-023-00795-x,,,
4413,10.4324/9781003431787,,,
4414,10.15294/elt.v12i1.64069,,,
4415,10.47852/bonviewijce42022489,,,
4416,10.3390/su16073034,,,
4417,10.1016/j.caeai.2023.100147,,,
4418,10.1016/j.procir.2023.05.002,,,
4419,10.1186/s41239-023-00427-0,,,
4420,10.54963/jic.v4i1.220,,,
4421,10.2196/53008,,,
4422,10.12973/ejmste/80014,,,
4423,10.29333/ejmste/85118,,,
4424,10.1504/ijtel.2019.098756,,,
4425,10.1613/jair.4992,,,
4426,10.1002/tesj.716,,,
4427,10.1021/acs.jchemed.3c00323,,,
4428,10.1007/s00146-023-01773-3,,,
4429,10.37074/jalt.2024.7.1.41,,,
4430,10.1007/s10956-023-10039-y,,,
4431,10.1145/3636555.3636856,,,
4432,10.1186/s41239-023-00411-8,,,
4433,10.20944/preprints202405.1158.v1,,,
4434,10.1155/2024/3085910,,,
4435,10.58693/ier.114,,,
4436,10.1101/2023.07.13.23292624,,,
4437,10.3390/electronics13101985,,,
4438,10.1007/978-3-031-52280-2,,,
4439,10.1080/0309877x.2024.2378298,,,
4440,10.1109/access.2024.3425172,,,
4441,10.1016/j.caeai.2024.100203,,,
4442,10.1086/227318,,,
4443,10.7717/peerj-cs.9,,,
4444,10.1017/9781316761380,,,
4445,10.1093/oso/9780197539538.003.0007,,,
4446,10.1332/263168919x15653390808962,,,
4447,10.1111/jols.12355,,,
4448,10.35295/osls.iisl/0000-0000-0000-1031,,,
4449,10.1177/0038038511422587,,,
4450,10.1007/s10506-018-9237-x,,,
4451,10.1017/s1744552319000077,,,
4452,10.36745/ijca.604,,,
4453,10.1108/17465640810870364,,,
4454,10.4337/9781803922171.00033,,,
4455,10.1332/26316897y2023d000000010,,,
4456,10.1017/lst.2019.5,,,
4457,10.30687/978-88-6969-765-4/013,,,
4458,10.2139/ssrn.4477704,,,
4459,10.36745/ijca.343,,,
4460,10.1111/jols.12421,,,
4461,10.1109/picict.2017.25,,,
4462,10.1007/s12597-019-00432-w,,,
4463,10.1111/ijsa.12306,,,
4464,10.1177/1094428120943281,,,
4465,10.1177/2053951720949566,,,
4466,10.1016/j.techfore.2021.120822,,,
4467,10.1002/pa.2050,,,
4468,10.1108/shr-12-2018-0101,,,
4469,10.1111/1748-8583.12090,,,
4470,10.3233/faia200468,,,
4471,10.1109/brics-cci-cbic.2013.27,,,
4472,10.1108/ijm-12-2020-0548,,,
4473,10.1007/s10796-021-10223-8,,,
4474,10.1080/15228835.2017.1416512,,,
4475,10.1016/j.hrmr.2022.100893,,,
4476,10.1109/msp.2021.3106615,,,
4477,10.1080/09585192.2022.2035161,,,
4478,10.1016/j.chbr.2022.100245,,,
4479,10.1037/apl0000108,,,
4480,10.1108/joepp-08-2021-0238,,,
4481,10.1257/aer.p20161029,,,
4482,10.1080/01969722.2021.1983701,,,
4483,10.3390/math8101703,,,
4484,10.1145/3287560.3287594,,,
4485,10.1016/j.eswa.2006.09.003,,,
4486,10.1016/j.hrmr.2022.100899,,,
4487,10.1016/j.ipm.2022.103224,,,
4488,10.2139/ssrn.3615404,,,
4489,10.7202/1025747ar,,,
4490,10.1016/j.ipm.2017.05.004,,,
4491,10.3390/math9151768,,,
4492,10.1007/s11205-021-02665-z,,,
4493,10.1007/s00779-018-1186-6,,,
4494,10.1037/cpb0000201,,,
4495,10.1162/99608f92.92fe150c,,,
4496,10.1007/978-3-319-60372-8_26,,,
4497,10.30534/ijatcse/2019/37832019,,,
4498,10.30534/ijeter/2019/10782019,,,
4499,10.1007/s10462-013-9414-y,,,
4500,10.1108/10662241211271545,,,
4501,10.3390/computers9040086,,,
4502,10.1016/j.infoandorg.2018.02.005,,,
4503,10.1155/2021/7149631,,,
4504,10.1016/j.ijhcs.2020.102520,,,
4505,10.1108/ijppm-08-2020-0427,,,
4506,10.1016/j.chb.2022.107179,,,
4507,10.1027/1866-5888/a000287,,,
4508,10.7202/1085269ar,,,
4509,10.1038/s41586-020-03136-0,,,
4510,10.1016/j.techsoc.2017.03.003,,,
4511,10.1037/apl0000695,,,
4512,10.1111/1748-8583.12356,,,
4513,10.1016/j.seps.2005.08.001,,,
4514,10.1109/nafips.2004.1336271,,,
4515,10.1016/j.knosys.2006.04.003,,,
4516,10.1016/j.procs.2019.12.165,,,
4517,10.1109/ifcsta.2009.90,,,
4518,10.5465/ambpp.2019.210,,,
4519,10.1177/0091026020977562,,,
4520,10.1016/j.cie.2014.09.015,,,
4521,10.1007/s43681-022-00208-x,,,
4522,10.1002/hrm.22049,,,
4523,10.1177/2053951714528481,,,
4524,10.1007/s10708-014-9601-7,,,
4525,10.1007/s12599-020-00673-w,,,
4526,10.1007/s11846-021-00514-4,,,
4527,10.1109/cogsima.2017.7929601,,,
4528,10.1038/d41586-020-00274-3,,,
4529,10.22266/ijies2018.0831.02,,,
4530,10.1007/s10869-020-09711-6,,,
4531,10.1016/j.chb.2017.11.036,,,
4532,10.1108/jmp-03-2019-0156,,,
4533,10.1111/ijsa.12246,,,
4534,10.1108/jmp-09-2018-0402,,,
4535,10.1016/j.dss.2021.113539,,,
4536,10.1016/j.cie.2020.106463,,,
4537,10.1177/2053951718756684,,,
4538,10.1145/3461702.3462531,,,
4539,10.1016/j.proeng.2011.12.699,,,
4540,10.1007/s13347-020-00406-7,,,
4541,10.1108/ijppm-08-2017-0212,,,
4542,10.1109/iacs.2019.8809154,,,
4543,10.1080/09585192.2020.1859582,,,
4544,10.1016/j.intman.2021.100871,,,
4545,10.1109/cyberneticscom.2016.7892560,,,
4546,10.1016/j.hrmr.2021.100876,,,
4547,10.1080/09585192.2021.1925326,,,
4548,10.1111/1748-8583.12393,,,
4549,10.1007/978-3-319-91189-2_34,,,
4550,10.1016/j.asoc.2009.08.035,,,
4551,10.22059/ijms.2021.299705.674004,,,
4552,10.3390/math9111226,,,
4553,10.1111/1744-7941.12245,,,
4554,10.30534/ijatcse/2019/62842019,,,
4555,10.1515/picbe-2017-0047,,,
4556,10.1016/j.obhdp.2020.03.008,,,
4557,10.1145/3014087.3014126,,,
4558,10.1287/isre.12.2.121.9700,,,
4559,10.3390/asi4030068,,,
4560,10.14569/ijacsa.2019.0100381,,,
4561,10.1016/j.chb.2018.07.022,,,
4562,10.1136/bmj.n71,,,
4563,10.1007/s41060-018-0142-x,,,
4564,10.1080/09585192.2021.1879206,,,
4565,10.1016/j.hrmr.2022.100924,,,
4566,10.1016/j.im.2014.08.008,,,
4567,10.1016/j.dss.2020.113290,,,
4568,10.1016/j.hrmr.2021.100860,,,
4569,10.14569/ijarai.2016.050904,,,
4570,10.1007/s10869-022-09824-0,,,
4571,10.1007/978-3-319-93701-4_37,,,
4572,10.1145/3351095.3372828,,,
4573,10.1007/978-3-030-86970-0_19,,,
4574,10.1145/3514094.3534189,,,
4575,10.1016/j.hrmr.2022.100925,,,
4576,10.1037/apl0000405,,,
4577,10.3389/fpsyg.2021.739711,,,
4578,10.22034/ijsom.2021.2.7,,,
4579,10.1016/j.cor.2004.06.022,,,
4580,10.1016/j.scient.2011.03.026,,,
4581,10.1111/1748-8583.12355,,,
4582,10.1016/j.chb.2019.04.012,,,
4583,10.1186/s13673-020-0208-3,,,
4584,10.3389/fpsyg.2021.620766,,,
4585,10.1186/s12874-016-0116-4,,,
4586,10.1108/pr-12-2019-0680,,,
4587,10.25300/misq/2021/16559,,,
4588,10.1007/s43681-020-00025-0,,,
4589,10.1111/1748-8583.12430,,,
4590,10.1016/j.cie.2014.12.008,,,
4591,10.1145/3442188.3445928,,,
4592,10.1145/2601248.2601268,,,
4593,10.1109/idap.2017.8090324,,,
4594,10.1145/3306618.3314240,,,
4595,10.1007/978-3-030-01057-7_56,,,
4596,10.2139/ssrn.4644701,,,
4597,10.2760/860665,,,
4598,10.14763/2023.1.1682,,,
4599,10.1007/978-1-4419-9011-2,,,
4600,10.2760/271009,,,
4601,10.1177/1023263x241248469,,,
4602,10.1080/17579961.2023.2184135,,,
4603,10.1080/17579961.2021.1898300,,,
4604,10.1016/0921-8890(95)00004-y,,,
4605,10.2139/ssrn.4471151,,,
4606,10.1080/0371750x.2021.1904290,,,
4607,10.1080/10447318.2022.2057898,,,
4608,10.1017/jfm.2019.782,,,
4609,10.4249/scholarpedia.1442,,,
4610,10.3390/s22114163,,,
4611,10.1126/science.adj0998,,,
4612,10.26434/chemrxiv-2023-fw8n4-v2,,,
4613,10.18653/v1/2022.emnlp-main.256,,,
4614,10.1038/s41591-022-01981-2,,,
4615,10.1007/s10462-017-9560-8,,,
4616,10.1007/s12559-015-9362-8,,,
4617,10.1109/joe.2017.2782959,,,
4618,10.1111/j.1745-6916.2008.00063.x,,,
4619,10.1007/s11229-022-03485-5,,,
4620,10.1007/s10618-020-00726-4,,,
4621,10.1016/j.dr.2018.06.001,,,
4622,10.4324/9781410603913,,,
4623,10.1093/acprof:osobl/9780199976171.001.0001,,,
4624,10.1037/0033-295x.108.4.814,,,
4625,10.1126/science.1062872,,,
4626,10.1016/s1364-6613(02)02011-9,,,
4627,10.1093/0198249926.001.0001,,,
4628,10.1007/bf00881307,,,
4629,10.1111/j.1369-7625.2011.00694.x,,,
4630,10.1177/0956797620948821,,,
4631,10.1016/j.clsr.2020.105456,,,
4632,10.1037/h0093718,,,
4633,10.1162/artl_a_00336,,,
4634,10.1145/3359206,,,
4635,10.1109/tase.2016.2530623,,,
4636,10.1007/s00146-019-00931-w,,,
4637,10.1007/s10892-009-9054-2,,,
4638,10.1146/annurev-anthro-102317-050129,,,
4639,10.1007/s10676-006-0004-4,,,
4640,10.30880/ijie.2019.11.06.015,,,
4641,10.1142/s1793843013500017,,,
4642,10.1007/978-3-031-21441-7_31,,,
4643,10.1007/s11023-020-09525-8,,,
4644,10.1007/s13347-019-00372-9,,,
4645,10.1037/xge0000033,,,
4646,10.4018/jte.2010100105,,,
4647,10.1080/09528130050111428,,,
4648,10.1017/s0140525x00005756,,,
4649,10.1017/s1358246100006299,,,
4650,10.1111/j.1750-8606.2011.00189.x,,,
4651,10.1007/978-3-031-21441-7_9,,,
4652,10.18653/v1/2020.acl-main.486,,,
4653,10.1002/9780470996973.ch2,,,
4654,10.1109/icapai49758.2021.9462068,,,
4655,10.1038/s41591-021-01672-4,,,
4656,10.1038/s42256-020-00257-z,,,
4657,10.1126/science.aax234,,,
4658,10.1145/3442188.3445867,,,
4659,10.1177/1527476419831640,,,
4660,10.1109/icse48619.2023.00133,,,
4661,10.1109/ictai.2019.00273,,,
4662,10.1145/3514094.3534159,,,
4663,10.1145/3278721.3278728,,,
4664,10.1145/3461702.3462522,,,
4665,10.1145/3338906.3338937,,,
4666,10.1609/aaai.v35i13.17386,,,
4667,10.1609/aaai.v37i12.26767,,,
4668,10.1007/s10676-022-09635-0,,,
4669,10.1609/aaai.v33i01.33019785,,,
4670,10.3233/faia200464,,,
4671,10.1109/tsp.2016.2626250,,,
4672,10.1056/nejmsr2214184,,,
4673,10.1145/3531146.3533229,,,
4674,10.22367/jem.2022.44.18,,,
4675,10.1016/j.dss.2021.113647,,,
4676,10.1145/3560815,,,
4677,10.18653/v1/2023.findings-emnlp.811,,,
4678,10.3233/faia200974,,,
4679,10.18653/v1/2021.findings-acl.375,,,
4680,10.1609/aaai.v35i6.16626,,,
4681,10.3389/fdata.2022.1056728,,,
4682,10.1017/cbo9780511803161,,,
4683,10.1145/3461702.3462587,,,
4684,10.1017/cbo9781139600514,,,
4685,10.1089/ast.2010.0524,,,
4686,10.1142/s179384300900013x,,,
4687,10.1093/oso/9780190652951.003.0014,,,
4688,10.1016/j.procs.2020.02.231,,,
4689,10.3354/esep00186,,,
4690,10.2139/ssrn.4358145,,,
4691,10.1109/mspec.2021.9423818,,,
4692,10.1525/9780520353060-008,,,
4693,10.1111/heyj.12237,,,
4694,10.1126/science.167.3914.86,,,
4695,10.1126/science.abc2274,,,
4696,10.3389/fpsyg.2019.01535,,,
4697,10.1093/biolinnean/blac092,,,
4698,10.2176/ijircst.2018.6.3.2,,,
4699,10.1098/rstb.2021.0292,,,
4700,10.1186/s13036-018-0109-4,,,
4701,10.5210/spir.v2020i0.11275,,,
4702,10.1109/mra.2012.2192811,,,
4703,10.1086/419948,,,
4704,10.1111/heyj.12003,,,
4705,10.1163/9789004482296,,,
4706,10.1109/mc.2018.3620963,,,
4707,10.1016/j.artint.2006.10.009,,,
4708,10.1080/0952813x.2015.1055826,,,
4709,10.1007/s12559-015-9372-6,,,
4710,10.2307/1340593,,,
4711,10.1177/20966083211056376,,,
4712,10.1016/j.engappai.2021.104178,,,
4713,10.1007/s40242-020-9086-5,,,
4714,10.1093/acprof:oso/9780199338207.003.0003,,,
4715,10.4324/9781003205425-4,,,
4716,10.2307/3219740,,,
4717,10.1093/acprof:oso/9780199338207.003.0005,,,
4718,10.5840/bpej198433/426,,,
4719,10.4324/9781003205425-5,,,
4720,10.26556/jesp.v25i2.1998,,,
4721,10.1093/acprof:oso/9780199338207.001.0001,,,
4722,10.3389/frai.2023.1216340,,,
4723,10.1007/s13347-023-00621-y,,,
4724,10.1515/9781400826537,,,
4725,10.1037/0033-295x.103.4.650,,,
4726,10.48550/arxiv.2301.04246,,,
4727,10.1093/acprof:oso/9780199338207.003.0004,,,
4728,10.1007/s13347-022-00542-2,,,
4729,10.1007/978-3-030-50585-1_4,,,
4730,10.1007/s13347-020-00401-y,,,
4731,10.2139/ssrn.3859178,,,
4732,10.1080/00346764.2021.1894350,,,
4733,10.4324/9781003205425-7,,,
4734,10.1007/s13347-023-00678-9,,,
4735,10.1038/s41598-023-31341-0,,,
4736,10.31234/osf.io/rn97c,,,
4737,10.5840/soctheorpract199521120,,,
4738,10.2307/48574436,,,
4739,10.1016/j.concog.2019.102860,,,
4740,10.1037/cns0000308,,,
4741,10.1037/cns0000343,,,
4742,10.1037/cns0000379,,,
4743,10.4324/9781003205425-6,,,
4744,10.1017/cbo9781316493021,,,
4745,10.1139/h10-079,,,
4746,10.1007/978-94-007-7762-0_20,,,
4747,10.1007/978-94-007-6970-0_5,,,
4748,10.1007/978-94-007-6970-0,,,
4749,10.1007/s10676-022-09675-6,,,
4750,10.1111/j.1467-9248.2012.00974.x,,,
4751,10.18510/hssr.2021.93106,,,
4752,10.1108/reps-11-2019-0145,,,
4753,10.1038/nature15816,,,
4754,10.1093/oxfordhb/9780190067397.013.38,,,
4755,10.5912/jcb954,,,
4756,10.18653/v1/2023.ijcnlp-main.45,,,
4757,10.1787/19970900,,,
4758,10.1111/glob.12051,,,
4759,10.1108/er-07-2019-0274,,,
4760,10.1038/nature17435,,,
4761,10.3390/genes10120978,,,
4762,10.1038/s41562-021-01146-0,,,
4763,10.1080/10382046.2023.2194036,,,
4764,10.1177/0840470419873123,,,
4765,10.1162/rest.88.3.572,,,
4766,10.1596/978-1-4648-1445-7,,,
4767,10.7551/mitpress/12255.001.0001,,,
4768,10.1080/13504851.2021.2024129,,,
4769,10.1136/bmj.2.5804.9,,,
4770,10.1007/s11886-013-0441-8,,,
4771,10.17265/2159-5313/2016.09.003,,,
4772,10.3389/frai.2022.910216,,,
4773,10.2139/ssrn.3900888,,,
4774,10.1016/j.ijme.2019.100330,,,
4775,10.1177/1534484320982891,,,
4776,10.1109/mcom.001.2000050,,,
4777,10.1016/j.ijinfomgt.2021.102454,,,
4778,10.1088/1748-9326/ab4e55,,,
4779,10.1038/s41431-021-00928-4,,,
4780,10.1016/j.techfore.2018.07.056,,,
4781,10.1016/j.techfore.2021.120970,,,
4782,10.1177/2045894019890549,,,
4783,10.1108/sl-12-2016-0085,,,
4784,10.1016/j.techsoc.2021.101737,,,
4785,10.1016/s0031-3203(99)00065-5,,,
4786,10.3390/math9222876,,,
4787,10.22454/fammed.2020.881454,,,
4788,10.1002/mar.21654,,,
4789,10.1093/jcr/ucz013,,,
4790,10.6017/ital.v40i1.13193,,,
4791,10.1080/08989621.2021.1913124,,,
4792,10.2139/ssrn.4324580,,,
4793,10.1001/amajethics.2019.131,,,
4794,10.7196/sajbl.2018.v11i2.664,,,
4795,10.1201/9781003167488-62,,,
4796,10.3390/su13115788,,,
4797,10.1136/jamia.1994.95236141,,,
4798,10.1016/j.tele.2021.101711,,,
4799,10.1038/s41545-021-00101-w,,,
4800,10.1007/s13132-023-01158-3,,,
4801,10.1109/mis.2013.51,,,
4802,10.1038/s41746-019-0155-4,,,
4803,10.1093/neuros/nyz471,,,
4804,10.1038/s42256-021-00417-9,,,
4805,10.1007/s42438-020-00121-8,,,
4806,10.1016/j.jbusres.2020.05.019,,,
4807,10.1007/s40593-021-00270-2,,,
4808,10.1016/j.displa.2022.102237,,,
4809,10.1057/ejdr.2012.9,,,
4810,10.1007/978-3-030-88234-1_9,,,
4811,10.1080/15332845.2014.904167,,,
4812,10.4324/9781003050186,,,
4813,10.1596/32365,,,
4814,10.31124/advance.22012724.v1,,,
4815,10.1080/0361526x.2023.2173357,,,
4816,10.1613/jair.1.12647,"Measuring the Occupational Impact of AI: Tasks, Cognitive Abilities and AI Benchmarks","<jats:p>In this paper we develop a framework for analysing the impact of AI (AI) on occupations. This framework maps 59 generic tasks from worker surveys and an occupational database to 14 cognitive abilities (that we extract from the cognitive science literature) and these to a comprehensive list of 328 AI benchmarks used to evaluate research intensity across a broad range of different AI areas. The use of cognitive abilities as an intermediate layer, instead of mapping work tasks to AI benchmarks directly, allows for an identification of potential AI exposure for tasks for which AI applications have not been explicitly created. An application of our framework to occupational databases gives insights into the abilities through which AI is most likely to affect jobs and allows for a ranking of occupations with respect to AI exposure. Moreover, we show that some jobs that were not known to be affected by previous waves of automation may now be subject to higher AI exposure. Finally, we find that some of the abilities where AI research is currently very intense are linked to tasks with comparatively limited labour input in the labour markets of advanced economies (e.g., visual and auditory processing using deep learning, and sensorimotor interaction through (deep) reinforcement learning).&#x0D;
This article appears in the special track on AI and Society.</jats:p>",-1.0
4817,10.1371/journal.pmed.1002689,,,
4818,10.1016/j.jrt.2020.100006,,,
4819,10.1080/10410236.2023.2275921,,,
4820,10.1016/j.jmir.2019.09.010,,,
4821,10.1038/s41551-018-0305-z,,,
4822,10.1007/s12152-016-9273-8,,,
4823,10.1073/pnas.1718793115,,,
4824,10.1080/00048402.2019.1618883,,,
4825,10.1007/s12152-019-09401-y,,,
4826,10.1136/bmj.1.4616.1105,,,
4827,10.1515/slgr-2016-0061,,,
4828,10.1038/scientificamerican0190-26,,,
4829,10.1007/s13347-017-0285-z,,,
4830,10.1080/17454832.2020.1846383,,,
4831,10.3389/frvir.2023.1065863,,,
4832,10.3322/caac.21708,,,
4833,10.1002/jcp.21935,,,
4834,10.1016/j.soncn.2013.06.007,,,
4835,10.1038/s41591-019-0566-4,,,
4836,10.1186/s13058-016-0733-1,,,
4837,10.1055/s-2003-40054,,,
4838,10.1177/0898010117726633,,,
4839,10.1080/07347332.2018.1506855,,,
4840,10.1017/s1478951511000587,,,
4841,10.1007/s11136-016-1473-5,,,
4842,10.1109/iccv.2017.244,,,
4843,10.1016/j.bspc.2024.106100,,,
4844,10.1002/mp.13617,,,
4845,10.1371/journal.pone.0293978,,,
4846,10.1016/j.compmedimag.2021.101953,,,
4847,10.3389/fpsyg.2021.600070,,,
4848,10.1109/cvpr.2016.90,,,
4849,10.1109/cvpr.2017.632,,,
4850,10.1109/cvpr.2016.207,,,
4851,10.1016/j.patcog.2019.107051,,,
4852,10.1109/cvpr.2018.00813,,,
4853,10.1109/iccv.2017.304,,,
4854,10.1038/s41597-021-00815-z,,,
4855,10.1109/icvgip.2008.47,,,
4856,10.1109/aciiw59127.2023.10388174,,,
4857,10.1016/j.ijhcs.2023.103139,,,
4858,10.1080/07421656.2024.2346689,,,
4859,10.3389/fpsyt.2024.1434172,,,
4860,10.1207/s15327752jpa6703_13,,,
4861,10.1109/cvpr.2018.00165,,,
4862,10.1109/cvpr.2019.00453,,,
4863,10.1073/pnas.0914616107,,,
4864,10.1093/analys/32.4.115,,,
4865,10.5840/monist197458444,,,
4866,10.1080/0305724750050101,,,
4867,10.1080/00455091.1978.10716212,,,
4868,10.1007/s11948-021-00318-5,,,
4869,10.1111/j.1467-8519.2009.01748.x,,,
4870,10.1017/s0265052513000137,,,
4871,10.1007/978-3-319-92759-6_5,,,
4872,10.1111/j.1467-8519.1995.tb00312.x,,,
4873,10.1007/978-1-4612-3708-2_4,,,
4874,10.1515/auk-2012-0204,,,
4875,10.5840/monist197962319,,,
4876,10.1007/978-1-4612-3708-2_5,,,
4877,10.1007/bf00999220,,,
4878,10.1080/00455091.1984.10716378,,,
4879,10.2307/3528262,,,
4880,10.1111/bioe.12173,,,
4881,10.1007/s11019-005-1588-x,,,
4882,10.1111/j.1467-8519.2012.02004.x,,,
4883,10.1093/acprof:oso/9780199252732.003.0010,,,
4884,10.1017/cbo9780511498909,,,
4885,10.1086/648610,,,
4886,10.5840/jphil201110837,,,
4887,10.5840/jphil2014111520,,,
4888,10.1007/s10892-020-09338-y,,,
4889,10.1007/978-3-319-09668-1_6,,,
4890,10.1177/0959354319853045,,,
4891,10.3163/1536-5050.104.2.009,,,
4892,10.16619/j.cnki.rmltxsqy.2020.05.002,,,
4893,10.16582/j.cnki.dzzw.2020.12.010,,,
4894,10.1093/oxfordhb/9780190067397.013.39,,,
4895,10.19836/j.cnki.37-1100/c.2020.06.001,,,
4896,10.1145/3236024.3264833,,,
4897,10.1007/s00146-020-00965-5,,,
4898,10.15994/j.1000-0763.2020.03.005,,,
4899,10.1007/bf00367746,,,
4900,10.16582/j.cnki.dzzw.2020.05.007,,,
4901,10.1145/2871196,,,
4902,10.1515/9789048550180-016,,,
4903,10.15994/j.1000-0763.2020.05.007,,,
4904,10.13549/j.cnki.cn11-3959/d.2020.02.002,,,
4905,10.1080/1369118x.2020.1776372,,,
4906,10.29024/sar.15,,,
4907,10.1007/978-3-030-05325-3_75-1,,,
4908,10.3390/su12072789,,,
4909,10.1145/3328485,,,
4910,10.1016/s1470-2045(19)30149-4,,,
4911,10.3390/medicina56090455,,,
4912,10.1016/j.imu.2021.100596,,,
4913,10.1007/s10278-017-9955-8,,,
4914,10.1038/d41586-020-00160-y,,,
4915,10.1016/j.ijhcs.2020.102551,,,
4916,10.1126/science.aal3856,,,
4917,10.1007/978-3-030-28954-6_2,,,
4918,10.1111/rssa.12227,,,
4919,10.1007/s11623-019-1183-6,,,
4920,10.2471/blt.19.237289,,,
4921,10.2139/ssrn.3570129,,,
4922,10.1145/3530019.3531329,,,
4923,10.1016/j.jacr.2017.11.035,,,
4924,10.1186/s40635-019-0286-6,,,
4925,10.1007/s12369-020-00653-w,,,
4926,10.1016/j.diii.2018.10.003,,,
4927,10.1007/s00259-020-04678-1,,,
4928,10.5152/dir.2020.19279,,,
4929,10.3389/fcvm.2020.00054,,,
4930,10.1038/s41746-020-0304-9,,,
4931,10.1038/s41591-020-1034-x,,,
4932,10.1038/s42256-020-0186-1,,,
4933,10.2471/blt.19.237487,,,
4934,10.1038/s41591-020-1011-4,,,
4935,10.1109/access.2019.2938265,,,
4936,10.23919/ituk48006.2019.8996149,,,
4937,10.1038/s42256-020-0197-y,,,
4938,10.3389/fpsyt.2018.00650,,,
4939,10.1145/3394486.3406461,,,
4940,10.1038/s41746-018-0075-8,,,
4941,10.1109/compsac.2019.10239,,,
4942,10.3233/jad-190952,,,
4943,10.1186/s12910-019-0437-z,,,
4944,10.1159/000492428,,,
4945,10.1136/bjsports-2019-101532,,,
4946,10.1055/s-0040-1702029,,,
4947,10.1109/icdm51629.2021.00100,,,
4948,10.1609/aaai.v36i11.21484,,,
4949,10.1007/s10015-019-00525-1,,,
4950,10.1007/s10551-013-1965-y,,,
4951,10.1037/a0038749,,,
4952,10.2307/1131427,,,
4953,10.1515/nzst.2005.47.1.101,,,
4954,10.1111/1467-8322.12209,,,
4955,10.1037/h0033956,,,
4956,10.1037/h0033389,,,
4957,10.1007/s10677-005-8836-2,,,
4958,10.1037/0096-3445.137.1.52,,,
4959,10.1016/b978-0-12-498640-4.50011-1,,,
4960,10.2307/2025030,,,
4961,10.1080/00405847709542675,,,
4962,10.1007/s43681-022-00170-8,,,
4963,10.1007/s43681-022-00213-0,,,
4964,10.1057/s41599-022-01043-5,,,
4965,"10.1109/mra.2012.2192811,",,,
4966,10.1108/josm-05-2020-0148,,,
4967,10.5840/monist197458440,,,
4968,10.1108/ir.2008.04935faa.002,,,
4969,10.1007/978-0-387-30164-8_206,,,
4970,10.1037/0033-295x.96.1.125,,,
4971,10.1016/j.neubiorev.2012.09.003,,,
4972,10.2307/258313,,,
4973,10.2307/3033560,,,
4974,"10.1093/mind/lix.236.433,issn0026-4423",,,
4975,10.2307/1129599,,,
4976,10.2307/1129023,,,
4977,10.1007/s12304-011-9127-z,,,
4978,10.1177/0097700419882733,,,
4979,10.1145/3306618.3314285,,,
4980,10.1080/10670564.2016.1206281,,,
4981,10.1177/0266666919893411,,,
4982,10.1177/209660831900200106,,,
4983,10.1111/j.1460-2466.1993.tb01304.x,,,
4984,10.1177/0163443714566897,,,
4985,10.1017/s0003055417000144,,,
4986,10.1177/1461444819826402,,,
4987,10.1080/12259276.2020.1767844,,,
4988,10.1007/978-94-017-9181-6_13,,,
4989,10.4018/978-1-5225-1081-9.ch022,,,
4990,10.1080/10758216.2017.1289818,,,
4991,10.1093/joc/jqaa032,,,
4992,10.1177/2059436418804274,,,
4993,10.1145/3306618.3314289,,,
4994,10.1177/209660831900200404,,,
4995,10.1088/1742-6596/1069/1/012017,,,
4996,10.4159/dlcl.aristotle-nicomachean_ethics.1926,,,
4997,10.2307/j.ctv1kz4gwm,,,
4998,10.1142/s1793843009000074,,,
4999,10.1007/s00146-021-01375-x,,,
5000,10.1007/s10676-019-09520-3,,,
5001,10.1007/s00146-021-01295-w,,,
5002,10.1007/978-94-007-1494-6_46,,,
5003,10.1007/s00146-021-01278-x,,,
5004,10.48550/arxiv.2302.02083,,,
5005,10.17104/9783406718700,,,
5006,10.3389/fsci.2023.1017235,,,
5007,10.14569/ijacsa.2016.070238,,,
5008,10.1007/s00146-016-0679-y,,,
5009,10.18034/abr.v7i3.650,,,
5010,10.18034/ra.v5i3.650,,,
5011,10.1186/1471-2105-11-348,,,
5012,10.1371/journal.pone.0162235,,,
5013,10.18034/ra.v4i3.651,,,
5014,10.18034/ajhal.v4i2.658,,,
5015,10.1371/journal.pone.0079138,,,
5016,10.1016/j.jbusres.2018.08.008,,,
5017,10.1145/2836041.2836064,,,
5018,10.1016/j.techsoc.2021.101796,,,
5019,10.5962/bhl.title.5851,,,
5020,10.1109/afgr.2000.840616,,,
5021,10.1515/9783112316009,,,
5022,10.1017/9781316459645,,,
5023,10.1007/s44163-022-00029-1,,,
5024,10.1080/00131857.2022.2033213,,,
5025,10.4324/9781003349662-1,,,
5026,10.4018/978-1-6684-5124-3.ch001,,,
5027,10.18848/2327-7882/cgp/v21i02/1-18,,,
5028,10.1533/9781780631745,,,
5029,10.1007/978-3-030-95006-4_8,,,
5030,10.1109/cvpr.2009.5206848,,,
5031,10.1111/j.1749-6632.1976.tb25467.x,,,
5032,10.1017/s0022226700001882,,,
5033,10.1075/cilt.163.21hal,,,
5034,10.4324/9780203783771,,,
5035,10.1080/00437956.1954.11659520,,,
5036,10.1075/cilt.169.11has,,,
5037,10.1007/s10579-005-2693-4,,,
5038,10.1017/9781108862059,,,
5039,10.4324/9781003032083-4,,,
5040,10.18848/2327-0136/cgp/v30i02/17-89,,,
5041,10.1017/cbo9781139196581,,,
5042,10.4324/9780203970034,,,
5043,10.4324/9780203936399,,,
5044,10.2307/412700,,,
5045,10.48550/arxiv.2306.05425,,,
5046,10.3389/fcomm.2022.830613,,,
5047,10.48550/arxiv.2308.05960,,,
5048,10.48550/arxiv.2306.09782,,,
5049,10.1073/pnas.1907367117,,,
5050,10.17763/haer.66.1.17370n67v22j160u,,,
5051,10.48550/arxiv.1801.04016,,,
5052,10.4135/9781473943506.n23,,,
5053,10.1109/t-aiee.1938.5057767,,,
5054,10.48550/arxiv.2308.03825,,,
5055,10.48550/arxiv.2305.07605,,,
5056,10.48550/arxiv.1706.03762,,,
5057,10.48550/arxiv.1502.03044,,,
5058,10.48550/arxiv.2307.10802,,,
5059,10.1080/1369118x.2020.1754877,,,
5060,10.36227/techrxiv.22649620,,,
5061,10.33542/aap-0042-4,,,
5062,10.1177/1077699020957963,,,
5063,10.1080/21670811.2022.2145328,,,
5064,10.1177/23998083231181595,,,
5065,10.32782/2710-4656/2023.1.2/50,,,
5066,10.1016/j.techfore.2021.121359,,,
5067,10.1016/j.tele.2020.101405,,,
5068,10.1016/j.chb.2012.12.022,,,
5069,10.1016/j.compedu.2015.02.004,,,
5070,10.1177/0733464818770772,,,
5071,10.1007/s11205-023-03151-4,,,
5072,10.1080/01972240309487,,,
5073,10.1016/j.caeai.2021.100017,,,
5074,10.1186/s41239-020-00191-5,,,
5075,10.25300/misq/2017/41.2.07,,,
5076,10.17705/1thci.00138,,,
5077,10.4324/9781315776880,,,
5078,10.1093/jamia/ocaa078,,,
5079,10.1007/s11920-021-01274-4,,,
5080,10.17705/1jais.00074,,,
5081,10.3390/ejihpe13100142,,,
5082,10.25916/528s-ny91,,,
5083,10.1177/0018726709104545,,,
5084,10.4324/9781315606002,,,
5085,10.1126/science.abg5298,,,
5086,10.1016/j.ijinfomgt.2020.102171,,,
5087,10.1007/s11524-020-00508-9,,,
5088,10.1177/1461444810386774,,,
5089,10.1016/j.poetic.2006.05.004,,,
5090,10.1177/1461444805056012,,,
5091,10.1016/j.chb.2008.04.013,,,
5092,10.5210/fm.v7i11.1003,,,
5093,10.1515/libr.2005.84,,,
5094,10.3389/fpsyg.2023.1092288,,,
5095,10.1093/ct/qtaa024,,,
5096,10.1145/3416797.3416832,,,
5097,10.2147/prbm.s325092,,,
5098,10.20448/journal.509.2021.82.158.172,,,
5099,10.1001/journalofethics.2017.19.11.stas2-1711,,,
5100,10.1057/978-1-137-59332-0,,,
5101,10.1007/s13721-021-00300-y,,,
5102,10.1038/s41746-021-00413-8,,,
5103,10.1177/1461444813487959,,,
5104,10.1002/jad.12193,,,
5105,10.1109/tai.2022.3194503,,,
5106,10.1080/13678868.2020.1767453,,,
5107,10.2139/ssrn.4452670,,,
5108,10.1108/14684520110410517,,,
5109,10.1177/1461444819846059,,,
5110,10.2139/ssrn.3312874,,,
5111,10.1016/j.techsoc.2020.101475,,,
5112,10.1007/s43681-023-00362-w,,,
5113,10.1007/s00146-023-01635-y,,,
5114,10.1016/j.respol.2013.05.008,,,
5115,10.3389/fpsyg.2018.00797,,,
5116,10.1007/s11031-014-9450-1,,,
5117,10.4324/9780429056765,,,
5118,10.14264/e34bfa3,,,
5119,10.1016/j.chbr.2020.100014,,,
5120,10.1016/j.tele.2023.102013,,,
5121,10.24251/hicss.2020.610,,,
5122,10.1177/1461444816634676,,,
5123,10.1080/09548963.2021.1972282,,,
5124,10.1016/j.biocon.2023.110346,,,
5125,10.1007/978-3-319-57959-7_1,,,
5126,10.14763/2019.4.1438,,,
5127,10.1136/bmjqs-2018-008370,,,
5128,10.1057/s41599-020-0492-6,,,
5129,10.2139/ssrn.3403301,,,
5130,10.2139/ssrn.3685087,,,
5131,10.1007/s42354-019-0200-0,,,
5132,10.4324/9780203516133,,,
5133,10.2307/1190674,,,
5134,10.1111/j.1467-9833.2008.00433.x,,,
5135,10.1007/s13347-016-0220-8,,,
5136,10.1017/cbo9780511527401,,,
5137,10.1007/978-3-030-43883-8_7,,,
5138,10.5771/0506-7286-1989-4-445,,,
5139,10.5089/9798400225208.066,,,
5140,10.3389/fmars.2023.899256,,,
5141,10.1016/0921-8009(96)00032-8,,,
5142,10.36227/techrxiv.22683919,,,
5143,10.48550/arxiv.2306.06085,,,
5144,10.1021/es00070a600,,,
5145,10.5751/es-04063-160313,,,
5146,10.1016/0921-8009(89)90025-6,,,
5147,10.1111/j.1468-5930.2012.00573.x,,,
5148,10.1080/09644019408414140,,,
5149,10.1016/j.acalib.2023.102720,,,
5150,10.3197/096734016x14497391602206,,,
5151,10.1007/s13280-023-01914-4,,,
5152,10.1080/00139157.1990.9929058,,,
5153,10.1016/0264-8377(91)90034-g,,,
5154,10.48550/arxiv.2302.00083,,,
5155,10.48550/arxiv.2309.11064,,,
5156,10.1007/bf02929541,,,
5157,10.1016/0016-3287(92)90074-p,,,
5158,10.1162/tacl_a_00530,,,
5159,10.48550/arxiv.2307.00855,,,
5160,10.1093/jel/6.1.57,,,
5161,10.48550/arxiv.2302.11382,,,
5162,10.48550/arxiv.2303.18223,,,
5163,10.1016/j.cities.2016.09.009,,,
5164,10.1177/2399808317751169,,,
5165,10.1007/s00146-022-01528-6,,,
5166,10.1186/s40537-019-0221-4,,,
5167,10.1186/s42162-020-00107-7,,,
5168,10.1186/s42162-023-00259-2,,,
5169,10.1007/s00146-020-01121-9,,,
5170,10.1201/9781003325086-3,,,
5171,10.1080/09644016.2021.1880713,,,
5172,10.1007/s10708-020-10320-2,,,
5173,10.1080/17535069.2016.1275618,,,
5174,10.1007/s43681-020-00007-2,,,
5175,10.1016/j.jclepro.2017.06.191,,,
5176,10.5325/utopianstudies.24.1.0066,,,
5177,10.1177/0308518x17738535,,,
5178,10.1057/s41599-019-0264-3,,,
5179,10.1038/s41893-019-0250-1,,,
5180,10.1038/s42949-021-00018-w,,,
5181,10.1093/oso/9780198833635.001.0001,,,
5182,10.1016/j.cities.2016.08.003,,,
5183,10.1111/area.12162,,,
5184,10.1007/s43681-021-00058-z,,,
5185,10.3390/smartcities3040056,,,
5186,10.1147/jrd.2010.2048257,,,
5187,10.1007/978-3-030-51812-7_177-1,,,
5188,10.4324/9780429324468-6,,,
5189,10.1080/13604813.2010.482277,,,
5190,10.1186/s42854-021-00028-y,,,
5191,10.4159/9780674039964,,,
5192,10.1080/24694452.2019.1617103,,,
5193,10.3389/frsc.2022.937933,,,
5194,10.1080/10630732.2020.1834831,,,
5195,10.1007/978-3-030-52313-8,,,
5196,10.1002/ehs2.1229,,,
5197,10.1002/9781119075615,,,
5198,10.4324/9780429324468-3,,,
5199,10.19229/2464-9309/812020,,,
5200,10.3390/su14084829,,,
5201,10.1177/00031224211029618,,,
5202,10.1201/9781003325086-19,,,
5203,10.1201/9781003325086-2,,,
5204,10.24908/ss.v17i3/4.10410,,,
5205,10.1016/j.scs.2015.09.002,,,
5206,10.1007/s11245-021-09739-0,,,
5207,10.1080/13604813.2014.906716,,,
5208,10.1016/j.scs.2023.104562,,,
5209,10.1016/j.techfore.2018.07.033,,,
5210,10.1080/02723638.2021.1887634,,,
5211,10.1093/scipol/scs114,,,
5212,10.1016/j.cosust.2019.11.009,,,
5213,10.3390/su142416352,,,
5214,10.1016/j.landurbplan.2014.01.018,,,
5215,10.7165/wtr18a1121.19,,,
5216,10.1016/j.landusepol.2018.01.034,,,
5217,10.1080/00343404.2022.2106360,,,
5218,10.19229/2464-9309/1222022,,,
5219,10.1016/j.jrt.2023.100060,,,
5220,10.1016/s0020-7373(78)80049-2,,,
5221,10.1016/0004-3702(93)90068-m,,,
5222,10.1145/357980.357991,,,
5223,10.1162/lmj_a_01037,,,
5224,10.2174/138920209789177575,,,
5225,10.1007/s00778-022-00776-8,,,
5226,10.1080/23299460.2021.1955613,,,
5227,10.1186/1471-2377-12-105,,,
5228,10.1177/14778785241231561,,,
5229,10.1007/s44206-024-00101-6,,,
5230,10.29173/irie482,,,
5231,10.1007/978-3-031-08215-3,,,
5232,10.1145/3614321.3614326,,,
5233,10.1080/02681102.2012.694794,,,
5234,10.1007/s11294-015-9559-3,,,
5235,10.1080/13563467.2022.2084524,,,
5236,10.1007/s10668-019-00372-x,,,
5237,10.1145/3517745.3561432,,,
5238,10.1016/j.telpol.2021.102199,,,
5239,10.1080/15228916.2018.1481307,,,
5240,10.3390/su12020485,,,
5241,10.1007/s12553-021-00626-7,,,
5242,10.4236/ojbm.2021.92043,,,
5243,10.1007/s11127-023-01090-9,,,
5244,10.1108/14779961111167630,,,
5245,10.1016/j.neucom.2021.07.045,,,
5246,10.1080/03081087.2016.1267104,,,
5247,10.1007/s11263-021-01453-z,,,
5248,10.1007/978-3-319-32564-4,,,
5249,10.2147/ijgm.s223882,,,
5250,10.1057/9780230393271,,,
5251,10.1016/j.eap.2020.07.006,,,
5252,10.1007/s12132-013-9210-4,,,
5253,10.1177/1354816619827712,,,
5254,10.18576/isl/121207,,,
5255,10.23919/ist-africa56635.2022.9845598,,,
5256,10.3998/phimp.2109,,,
5257,10.5040/9781350353275,,,
5258,10.1093/0198287976.003.0003,,,
5259,10.1023/a:1004333128077,,,
5260,10.1017/cbo9780511621253.008,,,
5261,10.1089/omi.2019.0038,,,
5262,10.2196/23933,,,
5263,10.1016/j.nedt.2023.105835,,,
5264,10.1016/j.nepr.2022.103451,,,
5265,10.1097/01.numa.0000578988.56622.21,,,
5266,10.1111/jocn.16478,,,
5267,10.12927/cjnl.2016.24563,,,
5268,10.1080/07399332.2022.2055760,,,
5269,10.1177/1744987119839453,,,
5270,10.1097/acm.0000000000004291,,,
5271,10.1007/s10734-022-00937-2,,,
5272,10.1186/s41239-020-00218-x,,,
5273,10.7861/clinmed.2023-0078,,,
5274,10.1111/hir.12509,,,
5275,10.1016/j.zefq.2019.05.004,,,
5276,10.1109/access.2020.3028333,,,
5277,10.18332/ejm/143166,,,
5278,10.1038/s41746-023-00819-6,,,
5279,10.3389/feduc.2023.1206936,,,
5280,10.3390/ijerph20064884,,,
5281,10.1111/caim.12173,,,
5282,10.3390/ijerph19084834,,,
5283,10.54489/ijtim.v3i1.195,,,
5284,10.1016/j.ijis.2019.06.001,,,
5285,10.1016/j.techfore.2020.120420,,,
5286,10.4018/ijesma.2019070103,,,
5287,10.3390/fi15080260,,,
5288,10.1037/0021-9010.88.2.207,,,
5289,10.3389/fpsyg.2020.00918,,,
5290,10.1016/j.lrp.2014.02.005,,,
5291,10.1108/mrr-09-2021-0701,,,
5292,10.1111/jpim.12656,,,
5293,10.1007/s11846-019-00373-0,,,
5294,10.1108/jrit-12-2019-0079,,,
5295,10.1016/j.technovation.2021.102312,,,
5296,10.1080/09585192.2011.599940,,,
5297,10.1007/s00146-021-01259-0,,,
5298,10.1007/s12144-022-03633-7,,,
5299,10.1108/14601060710720546,,,
5300,10.1080/07421222.2003.11045748,,,
5301,10.2307/30036532,,,
5302,10.1080/07421222.2002.11045696,,,
5303,10.1287/orsc.5.2.121,,,
5304,10.1016/j.lrp.2014.02.004,,,
5305,10.25300/misq/2015/39.2.02,,,
5306,10.1080/07421222.2023.2172775,,,
5307,10.2777/572275,,,
5308,10.1111/j.1467-8691.2012.00626.x,,,
5309,10.1177/002224378101800104,,,
5310,10.1016/j.techfore.2022.121598,,,
5311,10.1016/j.ijinfomgt.2022.102568,,,
5312,10.1016/j.promfg.2020.02.167,,,
5313,10.1016/j.techfore.2020.120392,,,
5314,10.1016/j.techfore.2023.122878,,,
5315,10.1007/978-3-030-80519-7,,,
5316,10.2753/mtp1069-6679190202,,,
5317,10.1108/ebr-11-2018-0203,,,
5318,10.1007/s11747-014-0403-8,,,
5319,10.1177/87569728221150436,,,
5320,10.17705/1thci.00094,,,
5321,10.2307/25148831,,,
5322,10.1287/isre.1100.0342,,,
5323,10.1016/j.techfore.2021.120994,,,
5324,10.1016/j.techsoc.2022.102086,,,
5325,10.1177/0008125619859317,,,
5326,10.1016/j.techfore.2020.120087,,,
5327,10.1016/j.jbusres.2012.05.024,,,
5328,10.1007/s11846-017-0238-z,,,
5329,10.1021/acs.molpharmaceut.5b00982,,,
5330,10.1007/s10551-013-1926-5,,,
5331,10.1016/j.jbusres.2022.113364,,,
5332,10.25300/misq/2015/39.3.04,,,
5333,10.3390/technologies11020044,,,
5334,10.1371/journal.pone.0249311,,,
5335,10.1016/j.im.2021.103534,,,
5336,10.1287/orsc.3.3.398,,,
5337,10.1007/s10845-018-1433-8,,,
5338,10.3390/ijerph18179366,,,
5339,10.1016/j.procs.2022.01.301,,,
5340,10.4324/9780203859827,,,
5341,10.25300/misq/2018/13275,,,
5342,10.1108/02651339410057491,,,
5343,10.2174/1389201024666230411091057,,,
5344,10.3389/fpsyg.2021.630145,,,
5345,10.1016/j.jmsy.2022.06.008,,,
5346,10.1016/0923-4748(94)90023-x,,,
5347,10.25300/misq/2016/40.3.07,,,
5348,10.1080/01463379809370107,,,
5349,10.3390/joitmc7030188,,,
5350,10.1016/j.im.2022.103597,,,
5351,10.1016/j.im.2017.01.005,,,
5352,10.1111/j.1540-5885.2006.00224.x,,,
5353,10.1504/ijil.2016.073288,,,
5354,10.1007/s10648-005-5617-2,,,
5355,10.1037/0003-066x.51.7.677,,,
5356,10.1038/s41929-018-0142-1,,,
5357,10.1177/02663821231195131,,,
5358,10.1038/s41586-019-1335-8,,,
5359,10.1016/j.promfg.2018.02.034,,,
5360,10.1007/978-3-642-31674-6_8,,,
5361,10.5465/amj.2008.0355,,,
5362,10.1016/j.technovation.2011.02.004,,,
5363,10.1016/j.techfore.2023.122581,,,
5364,10.1016/j.paid.2014.09.008,,,
5365,10.1111/j.1540-5414.2006.00132.x,,,
5366,10.1016/j.eng.2017.05.015,,,
5367,10.14742/ajet.8695,,,
5368,10.1016/j.chb.2022.107468,,,
5369,10.1111/bjet.13232,,,
5370,10.33394/jk.v9i2.7801,,,
5371,10.3390/educsci14091034,,,
5372,10.1016/j.caeai.2021.100025,,,
5373,10.1097/hc9.0000000000000097,,,
5374,10.2139/ssrn.4599104,,,
5375,10.3389/fpsyg.2023.1129070,,,
5376,10.3390/educsci13070692,,,
5377,10.54055/ejtr.v36i.3286,,,
5378,10.1016/j.iotcps.2023.06.002,,,
5379,10.3390/educsci13111151,,,
5380,10.1109/tlt.2024.3355015,,,
5381,10.3389/feduc.2023.1183162,,,
5382,10.3390/su14020815,,,
5383,10.3991/ijet.v16i18.24315,,,
5384,10.1016/j.caeai.2021.100041,,,
5385,10.1111/j.1467-9620.2006.00684.x,,,
5386,10.1007/s13218-021-00731-9,,,
5387,10.1007/s10639-023-11990-4,,,
5388,10.1080/21532974.2023.2247480,,,
5389,10.1016/j.compedu.2018.09.009,,,
5390,10.1080/09588221.2016.1278024,,,
5391,10.1080/10494820.2019.1627560,,,
5392,10.1080/21532974.2021.1934613,,,
5393,10.1109/access.2020.3048708,,,
5394,10.30935/cedtech/12918,,,
5395,10.1016/j.caeai.2021.100026,,,
5396,10.3390/su142114549,,,
5397,10.1016/j.chbr.2022.100223,,,
5398,10.1016/j.ijinfomgt.2020.102098,,,
5399,10.25082/amler.2024.01.005,,,
5400,10.3390/su142315620,,,
5401,10.1007/s10639-020-10159-7,,,
5402,10.1016/j.caeai.2023.100132,,,
5403,10.1080/0144929x.2016.1203024,,,
5404,10.1145/3626252.3630842,,,
5405,10.1016/j.tele.2023.102030,,,
5406,10.1016/j.chb.2020.106607,,,
5407,10.1145/3506860.3506866,,,
5408,10.14704/web/v19i1/web19163,,,
5409,10.3390/bs14090845,,,
5410,10.1007/jhep06(2023)195,,,
5411,10.1103/physrevd.103.074504,,,
5412,10.1088/2632‐2153/ace02f,,,
5413,10.1103/physrevd.108.043034,,,
5414,10.1007/s10462-023-10437-z,,,
5415,10.1016/j.cose.2017.04.006,,,
5416,10.1007/s10462-023-10631-z,,,
5417,10.1007/s10462-020-09942-2,,,
5418,10.1093/cybsec/tyad011,,,
5419,10.1016/s1353-4858(20)30105-7,,,
5420,10.1016/j.jisa.2024.103708,,,
5421,10.1080/2573234x.2018.1543535,,,
5422,10.1145/3469886,,,
5423,10.1145/3544548.3581170,,,
5424,10.1145/3544548.3581526,,,
5425,10.1093/cybsec/tyad005,,,
5426,10.1007/s10462-023-10679-x,,,
5427,10.1145/3544548.3581038,,,
5428,10.1145/3544548.3581046,,,
5429,10.1145/3383668.3419917,,,
5430,10.1007/s10462-024-10810-6,,,
5431,10.1016/j.neunet.2023.01.001,,,
5432,10.1145/3544548.3581001,,,
5433,10.1145/3582272,,,
5434,10.1109/access.2024.3374201,,,
5435,10.1145/3544548.3580985,,,
5436,10.1109/issa.2014.6950510,,,
5437,10.1016/j.cose.2023.103387,,,
5438,10.1145/3653297,,,
5439,10.1145/3544548.3580995,,,
5440,10.1016/j.jii.2023.100520,,,
5441,10.1016/j.iswa.2023.200188,,,
5442,10.2139/ssrn.4168458,,,
5443,10.1007/s10462-023-10641-x,,,
5444,10.1109/tnn.1998.712192,,,
5445,10.1038/s42256-019-0109-1,,,
5446,10.1038/s41928-022-00913-9,,,
5447,10.1365/s43439-023-00094-x,,,
5448,10.1016/j.clsr.2017.05.015,,,
5449,10.1016/j.cose.2022.102923,,,
5450,10.1038/s41562-023-01659-w,,,
5451,10.1145/3544548.3581169,,,
5452,10.1007/s10462-024-10744-z,,,
5453,10.1073/pnas.2313790120,,,
5454,10.1145/3649883,,,
5455,10.1177/0044118x19887075,,,
5456,10.1037/0022-0167.28.6.545,,,
5457,10.3390/socsci12120673,,,
5458,10.5209/crla.70880,,,
5459,10.7189/jogh.09.020318,,,
5460,10.3389/frai.2022.976838,,,
5461,10.1007/s11199-008-9410-x,,,
5462,10.1007/s43681-023-00269-6,,,
5463,10.1007/s10676-010-9221-y,,,
5464,10.1007/s12369-022-00887-w,,,
5465,10.2307/2027085,,,
5466,10.1017/cbo9780511814594,,,
5467,10.17265/2159-5313/2018.05.004,,,
5468,10.1109/cvpr.2006.322,,,
5469,10.1093/oso/9780195114744.001.0001,,,
5470,10.1007/s10676-012-9301-2,,,
5471,10.1111/misp.12024,,,
5472,10.1111/phpr.12595,,,
5473,10.1111/j.1468-0068.2012.00873.x,,,
5474,10.1007/s10676-016-9411-3,,,
5475,10.1007/s11263-011-0442-2,,,
5476,10.1093/acprof:oso/9780199591565.001.0001,,,
5477,10.1111/j.1520-8583.2009.00174.x,,,
5478,10.1007/s10892-016-9227-8,,,
5479,10.1111/phpr.12255,,,
5480,10.1007/s11245-012-9149-4,,,
5481,10.1007/s11098-012-9887-6,,,
5482,10.1007/s00146-021-01325-7,,,
5483,10.1007/s00146-007-0091-8,,,
5484,10.1007/s13347-013-0136-5,,,
5485,10.1093/acprof:oso/9780199272273.001.0001,,,
5486,10.1093/oso/9780195056167.001.0001,,,
5487,10.1145/3461702.3462563,,,
5488,10.1353/sor.2019.0022,,,
5489,10.1145/3351095.3372844,,,
5490,10.1134/s1061934811060190,,,
5491,10.1177/2053951717743530,,,
5492,10.1109/istas52410.2021.9629161,,,
5493,10.1016/s0160-791x(03)00019-8,,,
5494,10.1177/016224399301800306,,,
5495,10.1145/3152421,,,
5496,10.1561/1100000015,,,
5497,10.4324/9781315703619-27,,,
5498,10.1007/s00779-010-0341-5,,,
5499,10.1007/s10676-021-09586-y,,,
5500,10.1201/9781003063988,,,
5501,10.1080/15512169.2017.1406363,,,
5502,10.1073/pnas.1911695117,,,
5503,10.1080/15512169.2023.2164861,,,
5504,10.1038/s41433-023-02678-7,,,
5505,10.1080/15512169.2021.1914640,,,
5506,10.18653/v1/2023.emnlp-main.398,,,
5507,10.1371/journal.pcbi.1008453,,,
5508,10.1038/s41598-019-51258-x,,,
5509,10.1038/s41467-019-11311-9,,,
5510,10.1186/s13054-016-1222-8,,,
5511,10.1038/s41467-019-13585-5,,,
5512,10.1111/desc.12925,,,
5513,10.48550/arxiv.2208.06327,,,
5514,10.1016/s2589-7500(20)30275-2,,,
5515,10.48550/arxiv.2210.00608,,,
5516,10.1186/s13010-019-0074-7,,,
5517,10.1007/978-3-319-49250-6_3,,,
5518,10.1001/jamanetworkopen.2018.0926,,,
5519,10.1056/nejmra1814259,,,
5520,10.1186/s12911-018-0677-8,,,
5521,10.1136/bmjopen-2020-037860,,,
5522,10.3389/frobt.2021.612415,,,
5523,10.1186/s12916-019-1368-8,,,
5524,10.1001/jamainternmed.2018.3763,,,
5525,10.1186/1472-6947-13-s2-s7,,,
5526,10.3399/bjgp12x636236,,,
5527,10.1038/s41598-019-47712-5,,,
5528,10.1038/s43856-021-00028-w,,,
5529,10.1016/s2666-7568(21)00307-x,,,
5530,10.1136/bmj.288.6430.1597,,,
5531,10.1080/09515079308254494,,,
5532,10.1080/00909880701262997,,,
5533,10.1136/jme.11.4.184,,,
5534,10.1093/med/9780199798483.001.0001/med-9780199798483-chapter-3,,,
5535,10.1016/s0140-6736(03)15392-5,,,
5536,10.1191/0269216305pm996oa,,,
5537,10.1109/icdmw.2013.70,,,
5538,10.1177/0038038591025002009,,,
5539,10.12968/ijpn.2004.10.11.17130,,,
5540,10.1016/j.jpainsymman.2006.04.003,,,
5541,10.1001/jama.284.23.3051,,,
5542,10.1093/annonc/mdg098,,,
5543,10.1016/j.pec.2005.05.005,,,
5544,10.1093/annonc/mdg010,,,
5545,10.1016/0022-3999(67)90010-4,,,
5546,10.1001/archinternmed.2011.424,,,
5547,10.1017/cbo9780511664076,,,
5548,10.1080/00455091.2017.1279517,,,
5549,10.1136/bmj.320.7233.469,,,
5550,10.1378/chest.10-0289,,,
5551,10.1503/cmaj.160775,,,
5552,10.1007/s00134-015-3762-9,,,
5553,10.1176/appi.psy.45.4.311,,,
5554,10.1136/bmjspcare-2013-000604,,,
5555,10.1059/0003-4819-153-9-201011020-00005,,,
5556,10.4088/pcc.v08n0608,,,
5557,10.1136/bmj.295.6593.318,,,
5558,10.1038/s41598-020-73525-y,,,
5559,10.1177/0969141314555350,,,
5560,10.1136/bmjopen-2019-034682,,,
5561,10.1007/s11606-019-04860-8,,,
5562,10.1136/dtb.2019.000008,,,
5563,10.1177/0030222817691870,,,
5564,10.1002/da.22902,,,
5565,10.1111/j.1559-1816.2003.tb01972.x,,,
5566,10.1089/jpm.2006.0157,,,
5567,10.1016/j.jamda.2005.09.011,,,
5568,10.1016/j.cnur.2017.10.010,,,
5569,10.1098/rstb.2009.0134,,,
5570,10.1086/284410,,,
5571,10.1097/spc.0000000000000321,,,
5572,10.1200/jco.19.01493,,,
5573,10.1097/00001648-199809000-00010,,,
5574,10.1080/07481187.2014.920435,,,
5575,10.21037/apm.2018.s017,,,
5576,10.1192/bjp.145.4.424,,,
5577,10.1002/wps.20057,,,
5578,10.2196/jmir.5870,,,
5579,10.1016/j.ijpsycho.2010.04.001,,,
5580,10.1007/s005200050240,,,
5581,10.1016/j.ijpsycho.2017.06.006,,,
5582,10.1080/07481187.2018.1426656,,,
5583,10.1191/0269216304pm870oa,,,
5584,10.1016/j.jpainsymman.2016.12.348,,,
5585,10.1038/d41586-021-02758-2,,,
5586,10.1038/d41586-020-03189-1,,,
5587,10.1073/pnas.1913678117,,,
5588,10.1098/rsos.181870,,,
5589,10.1007/978-3-319-67401-8_55,,,
5590,10.1016/j.chb.2014.04.043,,,
5591,10.1016/s0167-6296(01)00130-8,,,
5592,10.1016/0168-8510(90)90421-9,,,
5593,10.1016/j.nicl.2022.102993,,,
5594,10.3892/br.2017.922,,,
5595,10.1136/mh.29.2.65,,,
5596,10.1093/qjmed/hci056,,,
5597,10.1037/h0089471,,,
5598,10.1001/virtualmentor.2014.16.8.jdsc1-1408,,,
5599,10.1186/s13010-019-0078-3,,,
5600,10.1080/11287462.2021.1879462,,,
5601,10.1016/j.tcmj.2012.05.004,,,
5602,10.1148/rg.2021200151,,,
5603,10.1186/s41747-018-0061-6,,,
5604,10.20944/preprints202307.0214.v1,,,
5605,10.1002/jmv.28787,,,
5606,10.3390/healthcare11091204,,,
5607,10.1016/j.radi.2020.09.010,,,
5608,10.1016/j.cmpbup.2021.100025,,,
5609,10.7150/ijms.76515,,,
5610,10.1371/journal.pone.0221720,,,
5611,10.3390/healthcare10102040,,,
5612,10.1016/j.compbiomed.2022.105298,,,
5613,10.3390/healthcare10010154,,,
5614,10.1148/radiol.2020202317,,,
5615,10.1109/bigcomp54360.2022.00078,,,
5616,10.1007/s00247-021-05279-2,,,
5617,10.1007/s00330-020-07349-9,,,
5618,10.2214/ajr.21.27255,,,
5619,10.21037/qims-20-1158,,,
5620,10.21037/qims-20-1159,,,
5621,10.1007/s11547-021-01384-2,,,
5622,10.1186/s12880-021-00637-w,,,
5623,10.1148/ryai.2021200232,,,
5624,10.1186/s12880-021-00677-2,,,
5625,10.21037/qims-21-936,,,
5626,10.3390/children9071044,,,
5627,10.1007/s00247-009-1259-9,,,
5628,10.1007/s00247-021-05146-0,,,
5629,10.3390/children10030525,,,
5630,10.1148/radiol.2018180237,,,
5631,10.1093/cid/ciy967,,,
5632,10.1001/jamanetworkopen.2019.1095,,,
5633,10.1016/j.crad.2019.08.005,,,
5634,10.1371/journal.pone.0204155,,,
5635,10.1007/s00330-020-07269-8,,,
5636,10.1038/s41598-019-51503-3,,,
5637,10.4103/jcis.jcis_75_16,,,
5638,10.1097/rti.0b013e31826c29ec,,,
5639,10.1148/radiol.14131315,,,
5640,10.1148/radiol.2019182465,,,
5641,10.1038/s41598-020-62148-y,,,
5642,10.1148/radiol.2020201874,,,
5643,10.1007/s00330-019-06532-x,,,
5644,10.1007/s00330-015-4030-7,,,
5645,10.1016/j.media.2017.06.015,,,
5646,10.2214/ajr.17.18718,,,
5647,10.1007/s10916-019-1180-1,,,
5648,10.1007/s00330-014-3427-z,,,
5649,10.1097/rti.0000000000000500,,,
5650,10.3390/app122211681,,,
5651,10.3389/fonc.2022.833816,,,
5652,10.1016/j.phro.2020.06.006,,,
5653,10.1186/s13014-020-01617-0,,,
5654,10.7861/fhj.2022-0013,,,
5655,10.1186/s13244-022-01237-0,,,
5656,10.3390/jimaging8040083,,,
5657,10.2196/37365,,,
5658,10.1016/j.jacr.2019.05.040,,,
5659,10.21037/atm-20-6325,,,
5660,10.1016/j.media.2019.101552,,,
5661,10.1007/s00259-022-05805-w,,,
5662,10.1007/s10278-021-00556-w,,,
5663,10.3390/diagnostics12040991,,,
5664,10.3390/jpm12091354,,,
5665,10.1016/j.jmir.2015.03.003,,,
5666,10.1259/bjr.20200975,,,
5667,10.1007/978-3-030-00937-3_18,,,
5668,10.1186/s12874-019-0782-0,,,
5669,10.1016/j.radi.2021.07.026,,,
5670,10.1159/000504390,,,
5671,10.1001/jamanetworkopen.2021.1276,,,
5672,10.1001/jamanetworkopen.2019.19325,,,
5673,10.1111/j.1365-2753.2011.01662.x,,,
5674,10.1109/access.2022.3157613,,,
5675,10.1109/embc46164.2021.9630473,,,
5676,10.1007/978-3-030-59830-3_57,,,
5677,10.1109/isbi52829.2022.9761617,,,
5678,10.1371/journal.pone.0260369,,,
5679,10.1109/isbi.2019.8759255,,,
5680,10.1016/j.compbiomed.2020.103884,,,
5681,10.1117/12.2582127,,,
5682,10.1186/s12968-020-00678-0,,,
5683,10.1007/978-3-031-09342-5_22,,,
5684,10.1007/978-981-16-0749-3_75,,,
5685,10.1186/s12880-020-00511-1,,,
5686,10.1007/978-981-16-6328-4_79,,,
5687,10.1007/978-3-031-20601-6_22,,,
5688,10.1007/978-3-031-12053-4_49,,,
5689,10.1038/s41598-020-73278-8,,,
5690,10.1016/j.cmpb.2021.106456,,,
5691,10.1016/j.bbe.2022.08.001,,,
5692,10.1016/j.measen.2023.100717,,,
5693,10.1016/j.bspc.2021.102810,,,
5694,10.1093/jamia/ocab192,,,
5695,10.1016/j.neuroimage.2022.119091,,,
5696,10.1007/978-3-030-32248-9_36,,,
5697,10.1016/j.radonc.2020.09.029,,,
5698,10.1007/978-3-030-60334-2_28,,,
5699,10.1007/978-3-030-32226-7_48,,,
5700,10.1007/978-3-030-59861-7_19,,,
5701,10.14338/ijpt-20-00099.1,,,
5702,10.1117/12.2579767,,,
5703,10.1002/mp.15479,,,
5704,10.1007/978-3-030-32251-9_52,,,
5705,10.1007/978-3-030-32248-9_15,,,
5706,10.1148/rg.2020190112,,,
5707,10.1007/s10554-022-02558-3,,,
5708,10.1038/s41746-021-00497-2,,,
5709,10.1016/j.cmpb.2019.06.023,,,
5710,10.1016/j.cmpb.2019.105162,,,
5711,10.1016/s2589-7500(19)30123-2,,,
5712,10.21037/qims.2018.07.05,,,
5713,10.1016/j.ejrad.2009.01.027,,,
5714,10.1177/1526602817732315,,,
5715,10.1371/journal.pone.0221339,,,
5716,10.1016/j.ejrad.2022.110592,,,
5717,10.1080/08839514.2021.2001177,,,
5718,10.1109/mis.2019.2895788,,,
5719,10.1007/s10796-023-10369-7,,,
5720,10.52812/msbd.63,,,
5721,10.1177/10963480231188663,,,
5722,10.1007/s11277-023-10199-5,,,
5723,10.1016/j.ins.2018.12.002,,,
5724,10.1145/3437963.3441657,,,
5725,10.1049/trit.2018.1025,,,
5726,10.1007/s11747-019-00711-4,,,
5727,10.1016/j.jretai.2021.01.005,,,
5728,10.1080/12460125.2022.2143618,,,
5729,10.1093/jcr/ucx104,,,
5730,10.1016/j.eswa.2021.115537,,,
5731,10.1007/s00500-019-04095-z,,,
5732,10.1007/s12063-021-00242-8,,,
5733,10.1016/j.cogr.2023.06.001,,,
5734,10.1016/j.ijresmar.2020.04.005,,,
5735,10.1186/s40537-014-0007-7,,,
5736,10.1016/j.bushor.2020.01.003,,,
5737,10.1108/ijbm-09-2021-0449,,,
5738,10.1108/ijrdm-03-2020-0091,,,
5739,10.1177/09520767231188401,,,
5740,10.1002/dir.20052,,,
5741,10.1016/j.lrp.2018.12.001,,,
5742,10.3390/app11188562,,,
5743,10.1016/j.ijhcs.2019.06.006,,,
5744,10.1016/j.procir.2019.03.156,,,
5745,10.1016/j.ipm.2023.103446,,,
5746,10.1007/s10956-006-9001-y,,,
5747,10.1177/0963662520907255,,,
5748,10.1177/1075547005275443,,,
5749,10.1177/1075547021998069,,,
5750,10.4324/9781003190721,,,
5751,10.1177/10755470221130307,,,
5752,10.1207/s15327825mcs0603_4,,,
5753,10.1111/j.1460-2466.2006.00331_3.x,,,
5754,10.1177/1075547005281473,,,
5755,10.1007/s10584-014-1148-6,,,
5756,10.1177/1075547011408928,,,
5757,10.1111/j.1460-2466.2011.01562.x,,,
5758,10.1111/1540-5907.00051,,,
5759,10.1177/0093650210384988,,,
5760,10.5555/3298239.3298381,,,
5761,10.1086/229213,,,
5762,10.1037/0022-3514.79.5.701,,,
5763,10.3389/fpsyg.2022.941163/full,,,
5764,10.1111/0022-3816.00077,,,
5765,10.1145/3326337,,,
5766,10.1177/0963662503123005,,,
5767,10.1111/j.1460-2466.2011.01580.x,,,
5768,10.1177/0093650210384990,,,
5769,10.3389/frobt.2021.719944/full,,,
5770,10.1177/0963662508097625,,,
5771,10.1080/10584609.2012.694985,,,
5772,10.1145/3530875,,,
5773,10.1007/s00146-022-01427-w,,,
5774,10.1504/ijbt.2004.004610,,,
5775,10.1023/a:1024834831093,,,
5776,10.1177/0963662506065558,,,
5777,10.1177/107554700202300401,,,
5778,10.1177/009365002236196,,,
5779,10.7312/perk14280,,,
5780,10.1177/1075547005281541,,,
5781,10.1007/bf02712649,,,
5782,10.1111/j.1460-2466.1999.tb02784.x,,,
5783,10.1177/1077699002079002,,,
5784,10.1007/s00146-021-01268-z,,,
5785,10.1207/s1532785xmep0101_2,,,
5786,10.4148/1051-0834.2291,,,
5787,10.1177/00027642221124664,,,
5788,10.1080/13683500.2023.2214353,,,
5789,10.1080/15205430903296077,,,
5790,10.1007/s00146-022-01450-x,,,
5791,10.1080/01973530802659885,,,
5792,10.1086/383426,,,
5793,10.1016/j.chb.2020.106572,,,
5794,10.1007/s10551-010-0386-4,,,
5795,10.1037/0033-2909.115.2.243,,,
5796,10.1016/j.cognition.2018.08.003,,,
5797,10.1016/j.tics.2019.02.008,,,
5798,10.1016/j.jretai.2016.12.004,,,
5799,10.1016/j.ijhcs.2004.11.002,,,
5800,10.1007/s10551-016-3061-6,,,
5801,10.1016/j.jretconser.2018.01.016,,,
5802,10.1037/0022-3514.34.5.907,,,
5803,10.1037/0022-3514.37.1.12,,,
5804,10.1037/0022-3514.53.1.94,,,
5805,10.1371/journal.pone.0094842,,,
5806,10.1016/s0148-2963(03)00102-4,,,
5807,10.1007/s10551-009-0354-z,,,
5808,10.1371/journal.pone.0057410,,,
5809,10.1016/j.intmar.2020.04.007,,,
5810,10.1016/j.ausmj.2018.07.003,,,
5811,10.1080/09515089.2015.1126706,,,
5812,10.1038/523024a,,,
5813,10.1007/s00146-020-00977-1,,,
5814,10.1093/jcr/ucaa018,,,
5815,10.1016/j.copsyc.2017.06.011,,,
5816,10.1016/j.cognition.2012.06.007,,,
5817,10.1177/0273475318755838,,,
5818,10.1080/17544750.2020.1769701,,,
5819,10.1007/s12369-016-0380-9,,,
5820,10.1177/1094670517752459,,,
5821,10.1016/j.ijnonlinmec.2019.103257,,,
5822,10.1108/9781787566873,,,
5823,10.5465/amd.2017.0002,,,
5824,10.1007/s10551-014-2180-1,,,
5825,10.1016/j.jbusres.2018.10.027,,,
5826,10.1108/978-1-83867-663-620201020,,,
5827,10.1002/cb.341,,,
5828,10.1007/bf01666535,,,
5829,10.1002/mar.21498,,,
5830,10.1007/s11002-012-9194-1,,,
5831,10.1016/j.ijhm.2020.102795,,,
5832,10.1037/0022-3514.62.2.318,,,
5833,10.1016/j.jbusres.2020.01.007,,,
5834,10.1007/s10551-019-04408-0,,,
5835,10.1016/j.jretconser.2012.10.005,,,
5836,10.1016/j.jretconser.2013.06.003,,,
5837,10.1007/s12369-009-0022-6,,,
5838,10.4271/2016-01-0164,,,
5839,10.1145/2696454.2696458,,,
5840,10.1007/s10676-015-9367-8,,,
5841,10.1007/978-3-030-12524-0_11,,,
5842,10.1023/b:busi.0000043492.42150.b6,,,
5843,10.1007/s10551-019-04213-9,,,
5844,10.1509/jmkg.64.3.50.18024,,,
5845,10.1007/s10551-016-3337-x,,,
5846,10.1145/259964.260137,,,
5847,10.1006/ijhc.1995.1042,,,
5848,10.1016/j.jbusres.2011.09.005,,,
5849,10.1509/jm.11.0454,,,
5850,10.1177/1094670516679275,,,
5851,10.1057/s41599-020-00614-8,,,
5852,10.1016/j.chb.2012.11.006,,,
5853,10.1016/j.ijhcs.2014.05.002,,,
5854,10.1016/j.chb.2018.05.014,,,
5855,10.1007/s10551-004-5969-5,,,
5856,10.1007/s10551-006-9090-9,,,
5857,10.1016/0148-2963(94)90055-8,,,
5858,10.1371/journal.pone.0180952,,,
5859,10.1177/1748895816643353,,,
5860,10.2307/796133,,,
5861,10.1037/0022-3514.74.1.118,,,
5862,10.1016/j.jesp.2014.01.005,,,
5863,10.1108/dprg-09-2018-0050,,,
5864,10.1108/17410390810904238,,,
5865,10.1080/15534510.2018.1546616,,,
5866,10.1016/j.jesp.2019.103870,,,
5867,10.1016/j.jretai.2012.10.004,,,
5868,10.2196/25759,,,
5869,10.1016/j.amjmed.2019.01.017,,,
5870,10.1038/s41591-021-01614-0,,,
5871,10.1038/s41746-021-00412-9,,,
5872,10.1136/bmj.m1328,,,
5873,10.3390/ijerph18084287,,,
5874,10.1007/s12539-021-00431-w,,,
5875,10.3390/ai3020028,,,
5876,10.1002/rmv.2205,,,
5877,10.1016/j.chaos.2020.110337,,,
5878,10.3390/healthcare10071313,,,
5879,10.3390/sym14071398,,,
5880,10.1097/mlr.0000000000001112,,,
5881,10.1093/jamia/ocz132,,,
5882,10.1038/s41591-019-0649-2,,,
5883,10.1057/s41271-021-00319-5,,,
5884,10.1016/s2589-7500(19)30002-0,,,
5885,10.1038/s42256-021-00373-4,,,
5886,10.1371/journal.pdig.0000132,,,
5887,10.1111/hir.12108,,,
5888,10.11613/bm.2012.031,,,
5889,10.1038/s42256-021-00396-x,,,
5890,10.1017/s0047279420000203,,,
5891,10.1145/3461702.3462519,,,
5892,10.1007/s10287-022-00425-z,,,
5893,10.1002/hast.1125,,,
5894,10.7326/m18-1376,,,
5895,10.1136/bmjhci-2020-100289,,,
5896,10.1136/bmjhci-2022-100617,,,
5897,10.3390/info14080467,,,
5898,10.36227/techrxiv.23589741.v2,,,
5899,10.1007/s11277-023-10312-8,,,
5900,10.1007/s11042-022-13428-4,,,
5901,10.1111/epi.17474,,,
5902,10.3390/fi12110187,,,
5903,10.3390/math11163567,,,
5904,10.33394/jtp.v8i1.6392,,,
5905,10.3390/info14080462,,,
5906,10.2196/preprints.48904,,,
5907,10.1109/sieds58326.2023.10137767,,,
5908,10.1371/journal.pone.0220976,,,
5909,10.1016/j.procs.2013.05.005,,,
5910,10.3233/ida-150390,,,
5911,10.1017/s1351324920000224,,,
5912,10.1111/j.2517-6161.1958.tb00292.x,,,
5913,10.2307/1403797,,,
5914,10.1080/00031305.1992.10475879,,,
5915,10.4108/eai.26-5-2023.2334244,,,
5916,10.1016/j.scp.2023.101060,,,
5917,10.3390/su15021408,,,
5918,10.1016/j.cor.2022.106131,,,
5919,10.1007/s11063-022-11111-1,,,
5920,10.3390/app131810114,,,
5921,10.1016/j.resconrec.2023.106876,,,
5922,10.1016/j.datak.2022.102110,,,
5923,10.1002/sej.1200,,,
5924,10.1093/oso/9780190090883.003.0038,,,
5925,10.1016/j.jclepro.2013.11.039,,,
5926,10.1016/j.lrp.2010.01.004,,,
5927,10.1177/0170840606067250,,,
5928,10.1016/j.lrp.2010.02.004,,,
5929,10.1108/s0742-332220150000033003,,,
5930,10.1504/ijpd.2013.055012,,,
5931,10.3139/9783446467620.035,,,
5932,10.1016/j.futures.2017.04.003,,,
5933,10.1016/0004-3702(89)90003-9,,,
5934,10.1109/mc.2022.3192720,,,
5935,10.1002/sej.1191,,,
5936,10.5465/annals.2014.0072,,,
5937,10.1093/acrefore/9780190224851.013.296,,,
5938,10.1177/0001839219852349,,,
5939,10.1016/j.lrp.2009.07.005,,,
5940,10.1108/s0742-332220150000033024,,,
5941,10.17705/1jais.00333,,,
5942,10.1007/978-1-4471-3581-4,,,
5943,10.1007/978-3-642-54516-0,,,
5944,10.1002/jsc.2241,,,
5945,10.1016/j.techfore.2015.07.023,,,
5946,10.1111/caim.12208,,,
5947,10.1016/j.lrp.2009.07.004,,,
5948,10.2196/27807,,,
5949,10.2196/20251,,,
5950,10.1038/s41746-023-00979-5,,,
5951,10.1002/wps.21145,,,
5952,10.1101/2024.07.24.24310930,,,
5953,10.1016/j.psychres.2024.115724,,,
5954,10.1093/schbul/sbad068,,,
5955,10.1002/jcpy.1393,,,
5956,10.2147/jmdh.s447368,,,
5957,10.3389/fpsyt.2023.1234397,,,
5958,10.1089/cyber.2023.0202,,,
5959,10.1111/bdi.13379,,,
5960,10.1136/fmch-2023-002583,,,
5961,10.1007/s11920-019-1094-0,,,
5962,10.1007/s10439-023-03201-5,,,
5963,10.1016/j.ajp.2023.103770,,,
5964,10.1093/jamia/ocac127,,,
5965,10.1016/j.psychres.2023.115334,,,
5966,10.1136/tc-2023-058009,,,
5967,10.2196/51243,,,
5968,10.1055/s-0038-1675822,,,
5969,10.1038/d41586-023-00056-7,,,
5970,10.1021/acs.jcim.3c01429,,,
5971,10.1136/jnnp.23.1.56,,,
5972,10.2196/48966,,,
5973,10.1017/neu.2021.22,,,
5974,10.1111/j.2044-8341.1959.tb00467.x,,,
5975,10.53738/revmed.2023.19.818.532,,,
5976,10.1177/20552076231170499,,,
5977,10.2196/49240,,,
5978,10.3389/fpsyt.2023.1277756,,,
5979,10.1016/j.ajp.2023.103736,,,
5980,10.1136/amiajnl-2011-000464,,,
5981,10.1177/00207640231178451,,,
5982,10.7189/jogh.13.04102,,,
5983,10.1111/jpm.12965,,,
5984,10.1016/j.procs.2022.11.205,,,
5985,10.1097/jcp.0000000000001734,,,
5986,10.1097/yct.0000000000000941,,,
5987,10.3389/fpsyt.2023.1213141,,,
5988,10.2196/52113,,,
5989,10.1007/s12144-023-04989-0,,,
5990,10.48550/arxiv.2311.08303,,,
5991,10.1093/schbul/13.2.261,,,
5992,10.3389/fdgth.2023.1133987,,,
5993,10.1371/journal.pmed.1000097,,,
5994,10.1371/journal.pdig.0000198,,,
5995,10.1016/j.psychres.2023.115655,,,
5996,10.1111/acps.13680,,,
5997,10.3390/biom13020387,,,
5998,10.1111/j.1600-0447.1987.tb10566.x,,,
5999,10.2196/54369,,,
6000,10.1056/nejmra2302038,,,
6001,10.1186/s40560-020-00452-5,,,
6002,10.1038/s41746-017-0012-2,,,
6003,10.1186/s12913-021-06861-y,,,
6004,10.31662/jmaj.2021-0133,,,
6005,10.1186/s12910-022-00746-3,,,
6006,10.1177/1049732305276687,,,
6007,10.1038/s41746-018-0048-y,,,
6008,10.1080/03637751.2017.1375130,,,
6009,10.1136/bmjhci-2020-100146,,,
6010,10.2188/jea.je20200211,,,
6011,10.3389/fmed.2020.00233,,,
6012,10.1177/20552076211063682,,,
6013,10.1007/978-3-030-55289-3,,,
6014,10.3389/fpubh.2022.915438,,,
6015,10.1353/sor.2019.0023,,,
6016,10.1108/ijius-09-2019-0051,,,
6017,10.3390/cancers15020470,,,
6018,10.1108/ajim-11-2019-0316,,,
6019,10.4103/0028-3886.236971,,,
6020,10.30954/2322-0465.2.2021.3,,,
6021,10.3390/ijerph18010271,,,
6022,10.1088/1742-6596/1831/1/012006,,,
6023,10.1155/2022/8028275,,,
6024,10.3390/ijerph192013691,,,
6025,10.1007/s10916-021-01743-6,,,
6026,10.3389/978-2-88963-038-7,,,
6027,10.1093/acprof:oso/9780199774111.001.0001,,,
6028,10.1016/j.chb.2015.02.029,,,
6029,10.2307/3034634,,,
6030,10.1007/978-94-011-2464-5,,,
6031,10.1017/apa.2016.28,,,
6032,10.1111/1467-8284.00096,,,
6033,10.1007/s13164-013-0130-y,,,
6034,10.1086/694039,,,
6035,10.1007/978-3-030-42307-0,,,
6036,10.1111/j.0737-6782.2004.00076.x,,,
6037,10.1080/02691728908578545,,,
6038,10.1007/s11098-011-9778-2,,,
6039,10.1521/soco.2008.26.2.143,,,
6040,10.1080/08956308.2016.1185347,,,
6041,10.1007/s11097-015-9448-5,,,
6042,10.1093/med/9780190459802.003.0001,,,
6043,10.1016/j.techfore.2017.09.034,,,
6044,10.1145/3287560.3287591,,,
6045,10.24251/hicss.2021.493,,,
6046,10.1007/s11569-020-00375-3,,,
6047,10.14324/lre.19.1.05,,,
6048,10.1007/s00146-017-0699-2,,,
6049,10.1353/ppp.2005.0032,,,
6050,10.1474/eip.2018.1.3,,,
6051,10.3233/faia200900,,,
6052,10.1126/science.1207745,,,
6053,10.1609/aimag.v36i4.2618,,,
6054,10.1075/nlp.8,,,
6055,10.1017/cbo9780511621253.006,,,
6056,10.1142/s2705078520500071,,,
6057,10.26730/2587-5574-2023-3-15-24,,,
6058,10.22394/1726-1139-2023-1-107-120,,,
6059,10.24182/2073-9885-2023-16-4-128-133,,,
6060,10.22281/2542-1697-2023-02-01-14-23,,,
6061,10.24891/ni.16.4.600,,,
6062,10.32782/2224-6282/188-13,,,
6063,10.32702/2307-2105.2022.7.4,,,
6064,10.1016/j.jjimei.2024.100232,,,
6065,10.69554/auij4734,,,
6066,10.26906/eir.2023.3(90).3035,,,
6067,10.32782/2707-8019/2023-2-3,,,
6068,10.1145/3589252,,,
6069,10.37965/jait.2023.0184,,,
6070,10.3389/frai.2023.1237704,,,
6071,10.1080/2331186x.2023.2210461,,,
6072,10.1016/j.iotcps.2023.05.004,,,
6073,10.3390/robotics10040120,,,
6074,10.1016/j.procs.2022.12.332,,,
6075,10.30935/cedtech/13605,,,
6076,10.3390/su14052773,,,
6077,10.1109/access.2023.3326474,,,
6078,10.3390/s22051698,,,
6079,10.3390/buildings12122057,,,
6080,10.3390/systems11030152,,,
6081,10.4108/eetel.v8i1.2342,,,
6082,10.4108/eai.31-3-2021.169173,,,
6083,10.36548/jtcsst.2022.4.002,,,
6084,10.2139/ssrn.4354422,,,
6085,10.1145/3605943,,,
6086,10.3390/en15176276,,,
6087,10.37074/jalt.2023.6.1.29,,,
6088,10.1126/scirobotics.abd5186,,,
6089,10.3390/electronics12132864,,,
6090,10.4236/ojapps.2017.74013,,,
6091,10.1007/978-3-030-27005-6_2,,,
6092,10.1038/scientificamerican1010-72,,,
6093,10.1017/cbo9780511978036.032,,,
6094,10.1007/s43681-020-00003-6,,,
6095,10.1016/j.artint.2020.103349,,,
6096,10.1007/s00146-018-0871-3,,,
6097,10.1007/s11023-017-9448-z,,,
6098,10.1109/jproc.2019.2898267,,,
6099,10.1080/0952813x.2014.895108,,,
6100,10.1037/e722352011-114,,,
6101,10.1007/s10676-018-9451-y,,,
6102,10.3384/de-ethica.2001-8819.20613,,,
6103,10.1007/s10676-008-9163-9,,,
6104,10.1080/09540091.2019.1640185,,,
6105,10.1145/3278721.3278753,,,
6106,10.1007/978-3-319-61043-6_7,,,
6107,10.1007/978-3-030-17974-8_5,,,
6108,10.1007/s10676-020-09527-1,,,
6109,10.1080/01441647.2020.1862355,,,
6110,10.4135/9781483384412,,,
6111,10.1007/s12115-018-0229-y,,,
6112,10.1016/j.artint.2019.103179,,,
6113,10.1007/s11948-020-00236-y,,,
6114,10.3127/ajis.v23i0.1688,,,
6115,10.1109/mis.2006.77,,,
6116,10.1109/mspec.2019.8847590,,,
6117,10.1126/science.349.6245.252,,,
6118,10.1007/978-3-319-26485-1_30,,,
6119,10.1088/0031-8949/90/1/018001,,,
6120,10.1038/136297b0,,,
6121,10.1109/tits.2016.2609339,,,
6122,10.1145/3419633,,,
6123,10.1007/s11023-009-9159-1,,,
6124,10.1016/j.cogsys.2017.04.002,,,
6125,10.1145/3278721.3278726,,,
6126,10.1007/s11023-020-09532-9,,,
6127,10.1007/s00146-007-0099-0,,,
6128,10.1016/j.procs.2015.12.213,,,
6129,10.1191/1478088705qp022oa,,,
6130,10.4135/9781446251911,,,
6131,10.1007/s11948-018-0030-8,,,
6132,10.1007/978-3-642-31674-6_29,,,
6133,10.1111/cobi.13123,,,
6134,10.1001/jama.2023.5321,,,
6135,10.1126/science.abg1834,,,
6136,10.1038/s41746-023-00823-w,,,
6137,10.1001/jama.2019.15064,,,
6138,10.1080/01402390.2011.574979,,,
6139,10.1111/j.1749-124x.2013.12012.x,,,
6140,10.1007/s00439-018-1903-2,,,
6141,10.1007/s11366-017-9474-y,,,
6142,10.1126/science.aay9400,,,
6143,10.1038/d41586-019-00673-1,,,
6144,10.4324/9781315617886,,,
6145,10.1080/1067056022000054632,,,
6146,10.1017/cbo9780511510045,,,
6147,10.1145/3287560.3287585,,,
6148,10.1177/0097700413497551,,,
6149,10.1177/0097700413499129,,,
6150,10.1080/00396338.2010.506820,,,
6151,10.2979/indjglolegstu.20.1.185,,,
6152,10.1017/als.2014.9,,,
6153,10.1080/09644016.2018.1491116,,,
6154,10.1038/d41586-019-01408-y,,,
6155,10.1016/j.jbi.2019.103149,,,
6156,10.1016/j.jpubeco.2004.06.009,,,
6157,10.1109/access.2019.2910838,,,
6158,10.1098/rsta.2016.0130,,,
6159,10.1038/s41587-019-0138-7,,,
6160,10.1093/cjip/poq016,,,
6161,10.1017/cbo9781139962858,,,
6162,10.1136/bmjopen-2017-021245,,,
6163,10.1111/jeea.12142,,,
6164,10.1057/978-1-137-53377-7_8,,,
6165,10.1016/s0306-9192(97)00011-0,,,
6166,10.1080/01495930903025276,,,
6167,10.1016/s0305-750x(97)00055-7,,,
6168,10.1007/s13347-011-0040-9,,,
6169,10.1007/s11023-016-9408-z,,,
6170,10.1038/d41586-018-04602-6,,,
6171,10.1017/als.2018.40,,,
6172,10.1186/s13584-019-0293-9,,,
6173,10.1177/0010414016688008,,,
6174,10.4324/9780203982990,,,
6175,10.1007/s11712-013-9329-y,,,
6176,10.1093/jmp/jhx026,,,
6177,10.1111/j.1469-8676.2008.00055.x,,,
6178,10.1136/bmj.j5910,,,
6179,10.1017/s1463423617000421,,,
6180,10.1080/10670564.2014.953808,,,
6181,10.1007/s10916-023-01987-4,,,
6182,10.1148/radiol.221257,,,
6183,10.1016/j.xcrm.2023.101119,,,
6184,10.1088/1361-6560/acc2ab,,,
6185,10.1007/s10278-023-00831-y,,,
6186,10.1007/s10278-023-00842-9,,,
6187,10.1109/tmi.2023.3320812,,,
6188,10.1016/j.artmed.2023.102609,,,
6189,10.1016/j.compbiomed.2024.108112,,,
6190,10.1016/j.compmedimag.2023.102280,,,
6191,10.5624/isd.20220125,,,
6192,10.1007/s00330-023-09512-4,,,
6193,10.3389/fninf.2023.1197330,,,
6194,10.1148/radiol.221425,,,
6195,10.3389/fneur.2022.1051397,,,
6196,10.1109/jbhi.2023.3340325,,,
6197,10.1148/radiol.222211,,,
6198,10.1002/acm2.14120,,,
6199,10.1088/1361-6560/acda78,,,
6200,10.1002/mp.16884,,,
6201,10.1186/s13244-023-01373-1,,,
6202,10.1002/jor.25557,,,
6203,10.1016/j.radonc.2023.110052,,,
6204,10.1186/s13014-023-02349-7,,,
6205,10.1186/s41747-023-00385-2,,,
6206,10.1002/mp.16847,,,
6207,10.1148/radiol.230052,,,
6208,10.1088/1361-6560/acefa3,,,
6209,10.1016/j.compbiomed.2023.106738,,,
6210,10.1002/jor.25707,,,
6211,10.1088/1361-6560/ad0ddc,,,
6212,10.1016/j.cmpb.2023.107571,,,
6213,10.1016/j.phro.2023.100511,,,
6214,10.1016/j.zemedi.2023.07.001,,,
6215,10.1016/j.compbiomed.2023.107054,,,
6216,10.1109/tmi.2023.3290149,,,
6217,10.1007/s00330-023-10534-1,,,
6218,10.1002/mp.16369,,,
6219,10.1007/s11548-022-02828-4,,,
6220,10.1148/radiol.230681,,,
6221,10.1038/s41598-023-36712-1,,,
6222,10.1007/s00261-023-04004-x,,,
6223,10.1016/j.crad.2023.09.021,,,
6224,10.3390/cancers15113061,,,
6225,10.1007/s00330-023-10135-y,,,
6226,10.3390/cancers15225479,,,
6227,10.1088/1361-6560/acc921,,,
6228,10.1016/j.compmedimag.2023.102300,,,
6229,10.1016/j.compbiomed.2023.106889,,,
6230,10.1038/s41598-023-33472-w,,,
6231,10.1007/s13246-023-01244-5,,,
6232,10.1007/s00066-022-02039-5,,,
6233,10.1109/tmi.2023.3325703,,,
6234,10.1016/j.cmpb.2023.107767,,,
6235,10.1007/s13246-023-01290-z,,,
6236,10.1002/acm2.14212,,,
6237,10.1002/mp.16329,,,
6238,10.1109/trpms.2022.3223275,,,
6239,10.1364/boe.467683,,,
6240,10.3389/fmed.2023.1171118,,,
6241,10.1016/j.cmpb.2023.107642,,,
6242,10.1038/s41598-023-48595-3,,,
6243,10.1016/j.compmedimag.2023.102272,,,
6244,10.1109/tmi.2023.3261822,,,
6245,10.1088/1361-6560/acf8ac,,,
6246,10.1038/s41598-023-50566-7,,,
6247,10.1007/s11517-023-02809-y,,,
6248,10.1016/j.zemedi.2023.12.001,,,
6249,10.7717/peerj-cs.1318,,,
6250,10.1016/j.compbiomed.2023.107395,,,
6251,10.1016/j.compbiomed.2023.106620,,,
6252,10.1371/journal.pone.0279349,,,
6253,10.1055/a-2026-0784,,,
6254,10.1007/s40477-023-00837-w,,,
6255,10.1016/j.media.2023.102762,,,
6256,10.1016/j.radi.2023.05.011,,,
6257,10.5114/pjr.2023.131215,,,
6258,10.1148/radiol.230582,,,
6259,10.1148/radiol.230987,,,
6260,10.1177/08465371231193716,,,
6261,10.1093/dmfr/twad015,,,
6262,10.1007/s00270-023-03563-2,,,
6263,10.1148/radiol.230877,,,
6264,10.1016/j.jacr.2023.05.003,,,
6265,10.1148/radiol.230725,,,
6266,10.1016/j.jvir.2023.11.014,,,
6267,10.1016/j.ejro.2023.100512,,,
6268,10.2196/49041,,,
6269,10.1016/j.clinimag.2023.06.008,,,
6270,10.1109/tbme.2023.3280987,,,
6271,10.1001/jamanetworkopen.2022.55113,,,
6272,10.1016/j.media.2023.102798,,,
6273,10.1038/s41598-023-31223-5,,,
6274,10.1109/jbhi.2023.3345932,,,
6275,10.1016/j.compbiomed.2023.107650,,,
6276,10.1007/s13347-017-0263-5,,,
6277,10.1080/14754830802476787,,,
6278,10.1007/s13347-017-0293-z,,,
6279,10.1007/s10677-022-10323-7,,,
6280,10.2307/3129654,,,
6281,10.1093/idpl/ipx005,,,
6282,10.1353/jod.2019.0002,,,
6283,10.1509/jmkr.38.4.458.18903,,,
6284,10.1177/10776990221110113.,,,
6285,10.1108/intr-10-2019-0442,,,
6286,10.48550/arxiv.2310.04631.,,,
6287,10.1177/0093650220911814,,,
6288,10.1007/s42113-022-00136-3,,,
6289,10.1093/poq/nfaa011,,,
6290,10.1111/j.1460-2466.2006.00293.x,,,
6291,10.1108/jpbm-12-2018-2145,,,
6292,10.1080/08838151.2020.1757365,,,
6293,10.1080/21670811.2016.1208053,,,
6294,10.1038/s44159-021-00006-y,,,
6295,10.1080/1369118x.2020.1736124,,,
6296,10.1080/10410236.2021.1964187,,,
6297,10.1016/j.techfore.2020.120201,,,
6298,10.1177/1075547020959670,,,
6299,10.1108/intr-03-2020-0127,,,
6300,10.1080/10410236.2022.2031452,,,
6301,10.1080/10463283.2021.1876983,,,
6302,10.1093/ct/qtaa002,,,
6303,10.1177/14614448211038762,,,
6304,10.1016/j.ijinfomgt.2021.102362,,,
6305,10.1177/10776990211012953,,,
6306,10.1016/bs.aesp.2022.11.003,,,
6307,10.1038/s41467-022-30073-5,,,
6308,10.1145/3588594,,,
6309,10.1080/0960085x.2021.1895682,,,
6310,10.1007/978-3-031-52569-8,,,
6311,10.1016/j.chb.2019.04.019,,,
6312,10.1080/10810730.2020.1776423,,,
6313,10.1016/j.dss.2022.113782,,,
6314,10.1177/20539517221115189,,,
6315,10.1002/asi.20511,,,
6316,10.48550/arxiv.2102.02503.,,,
6317,10.4135/9781412976046.n11,,,
6318,10.1177/2056305120978377,,,
6319,10.1126/science.185.4157.1124,,,
6320,10.1016/j.jhep.2023.07.028.,,,
6321,10.1080/10584609.2020.1716500,,,
6322,10.1080/10410236.2023.2206177.,,,
6323,10.1177/0093650219854600,,,
6324,10.1080/10410236.2020.1794553,,,
6325,10.1080/10410236.2022.2159143,,,
6326,10.1080/10410236.2021.2010891,,,
6327,10.1287/isre.2017.0695,,,
6328,10.1080/10410236.2023.2184452.,,,
6329,10.1016/j.ipm.2021.102739,,,
6330,10.7312/fren90672,,,
6331,10.1007/978-3-319-50359-2,,,
6332,10.2139/ssrn.3670986,,,
6333,10.1080/17439760.2020.1791944,,,
6334,10.1017/9781009207898.009,,,
6335,10.1016/j.jpsychires.2021.03.002,,,
6336,10.3389/fpsyt.2023.1183740,,,
6337,10.1002/9781118551424,,,
6338,10.1093/oso/9780198843900.001.0001,,,
6339,10.1007/s11019-016-9721-6,,,
6340,10.1002/hast.1207,,,
6341,10.1111/bioe.12891,,,
6342,10.1098/rsta.2018.0085,,,
6343,10.1007/978-94-007-3940-6,,,
6344,10.1002/14651858.cd006066.pub3,,,
6345,10.4236/ojog.2017.73036,,,
6346,10.1186/1471-2393-14-16,,,
6347,10.1016/j.wombi.2019.10.002,,,
6348,10.1177/0883073814543306,,,
6349,10.3389/fped.2022.1007799,,,
6350,10.1111/j.1479-828x.1994.tb01032.x,,,
6351,10.1016/j.jbi.2014.04.010,,,
6352,10.3389/frai.2021.765210,,,
6353,10.1111/aogs.13136,,,
6354,10.1002/14651858.cd007863.pub4,,,
6355,10.1097/acm.0000000000002027,,,
6356,10.18549/pharmpract.2016.01.708,,,
6357,10.1089/jwh.2020.8682,,,
6358,10.1093/acprof:oso/9780198237907.001.0001/acprof-9780198237907,,,
6359,10.1136/bmj.n152,,,
6360,10.1016/j.jacr.2018.12.043,,,
6361,10.1001/jamadermatol.2019.5014,,,
6362,10.2196/24127,,,
6363,10.1017/9781108620024.011,,,
6364,10.1186/s12910-020-00553-8,,,
6365,10.1136/medethics-2021-107464,,,
6366,10.2777/531856,,,
6367,10.1038/s42256-019-0115-3,,,
6368,10.1111/j.1467-9833.1996.tb00250.x,,,
6369,10.1111/j.1536-7150.1995.tb02684.x,,,
6370,10.3389/frai.2020.561802,,,
6371,10.1016/j.mlwa.2020.100006,,,
6372,10.1016/j.caeai.2024.100219,,,
6373,10.1002/brx2.30,,,
6374,10.1196/annals.1412.011,,,
6375,10.1016/j.lindif.2011.01.007,,,
6376,10.1001/jama.2017.7797,,,
6377,10.4324/9780203771587,,,
6378,10.1146/annurev-psych-113011-143750,,,
6379,10.1126/science.1204529,,,
6380,10.1186/s12888-014-0237-4,,,
6381,10.1177/00222194070400010401,,,
6382,10.1016/j.susoc.2022.05.004,,,
6383,10.1027/2698-1866/a000038,,,
6384,10.3390/su151814025,,,
6385,10.1037/neu0000948,,,
6386,10.1016/j.compbiomed.2022.105525,,,
6387,10.1145/3323771.3323824,,,
6388,10.1037/a0024768,,,
6389,10.1080/21622965.2019.1660984,,,
6390,10.1080/00220671.2014.979913,,,
6391,10.1038/s41467-023-42540-8,,,
6392,10.1007/978-3-031-35897-5_32,,,
6393,10.1057/s41599-023-02304-7,,,
6394,10.18178/ijiet.2024.14.3.2061,,,
6395,10.1109/icac353642.2021.9697300,,,
6396,10.1109/cvpr52729.2023.01762,,,
6397,10.36548/jaicn.2023.4.006,,,
6398,10.1109/iccci56745.2023.10128180,,,
6399,10.1016/j.focat.2023.07.009,,,
6400,10.1109/access.2023.3267968,,,
6401,10.1109/tai.2024.3394392,,,
6402,10.1109/iceict57916.2023.10244824,,,
6403,10.36548/jitdw.2024.1.006,,,
6404,10.1016/j.caeai.2022.100050,,,
6405,10.1016/s0262-4079(23)01633-0,,,
6406,10.1080/10494820.2020.1808794,,,
6407,10.1109/cac.2018.8623327,,,
6408,10.3389/fpsyg.2022.870777,,,
6409,10.1016/j.cola.2023.101200,,,
6410,10.20944/preprints202309.1528.v1,,,
6411,10.2139/ssrn.4627814,,,
6412,10.2139/ssrn.4359405,,,
6413,10.1108/dts-01-2023-0003,,,
6414,10.1109/weef-gedc59520.2023.10343638,,,
6415,10.1002/ev.20556,,,
6416,10.2139/ssrn.4590006,,,
6417,10.18653/v1/2023.emnlp-main.453,,,
6418,10.3390/knowledge3030032,,,
6419,10.1162/dint_a_00243,,,
6420,10.2139/ssrn.4614223,,,
6421,10.20944/preprints202308.1271.v1,,,
6422,10.1007/s00146-023-01689-y,,,
6423,10.1109/digitalheritage.2018.8810002,,,
6424,10.1038/d41586-022-00641-2,,,
6425,10.3390/a16020079,,,
6426,10.3390/ijgi11010045,,,
6427,10.1038/d41586-023-03212-1,,,
6428,10.1371/journal.pone.0237962,,,
6429,10.1007/s11042-022-12673-x,,,
6430,10.1016/j.jas.2022.105640,,,
6431,10.3390/drones7090578,,,
6432,10.1109/icecocs50124.2020.9314443,,,
6433,10.1016/j.patrec.2019.12.009,,,
6434,10.1016/j.culher.2019.06.005,,,
6435,10.1016/j.patrec.2020.01.012,,,
6436,10.1016/j.jas.2005.04.011,,,
6437,10.3390/su141811214,,,
6438,10.1016/j.patrec.2019.12.007,,,
6439,10.1016/j.jocs.2019.02.005,,,
6440,10.1038/s41598-020-75994-7,,,
6441,10.1109/icdiime56946.2022.00022,,,
6442,10.1117/12.2624371,,,
6443,10.3390/heritage6050214,,,
6444,10.32388/jpecon,,,
6445,10.20944/preprints202307.1393.v1,,,
6446,10.20944/preprints202306.1618.v1,,,
6447,10.1145/3609987.3610018,,,
6448,10.3390/heritage6080302,,,
6449,10.20944/preprints202307.2035.v1,,,
6450,10.3390/su142416432,,,
6451,10.3390/app12168131,,,
6452,10.11141/ia.52.7,,,
6453,10.1007/978-3-662-44630-0,,,
6454,10.1016/j.culher.2006.08.003,,,
6455,10.1080/13527258.2017.1378909,,,
6456,10.3390/heritage5030105,,,
6457,10.1179/2159032x13z.0000000009,,,
6458,10.1108/jchmsd-07-2022-0116,,,
6459,10.1177/1367877914554539,,,
6460,10.1353/ks.2011.0011,,,
6461,10.1109/ismar-adjunct51615.2020.00092,,,
6462,10.1038/s41537-023-00379-4,,,
6463,10.1038/s41746-023-00896-7,,,
6464,10.1111/1742-6723.14233,,,
6465,10.1080/00450618.2022.2079722,,,
6466,10.1080/00131857.2020.1728732,,,
6467,10.1007/978-3-319-60013-0_107-1,,,
6468,10.1016/0898-1221(85)90054-9,,,
6469,10.4324/9781410601629-16,,,
6470,10.1007/s40593-014-0028-6,,,
6471,10.33516/maj.v55i5.64-67p,,,
6472,10.1016/j.jrt.2020.100005,,,
6473,10.51542/ijscia.v2i1.2,,,
6474,10.1034/j.1600-0579.2001.050301.x,,,
6475,10.3389/frai.2022.867832,,,
6476,10.1007/s40593-016-0095-y,,,
6477,10.1016/s2542-5196(22)00090-0,,,
6478,10.18653/v1/p19-1355,,,
6479,10.1111/bioe.13018,,,
6480,10.1787/7babf571-en,,,
6481,10.3390/philosophies7010004,,,
6482,10.1038/s42256-020-0219-9,,,
6483,10.5815/ijieeb.2018.05.02,,,
6484,10.1007/978-3-319-39937-9_37,,,
6485,10.1016/j.websem.2007.03.001,,,
6486,10.1002/asi.21062,,,
6487,10.7717/peerj-cs.445,,,
6488,10.4018/ijswis.2017010108,,,
6489,10.1016/j.jss.2021.07.025,,,
6490,10.1007/s11192-015-1814-0,,,
6491,10.1148/radiol.09090626,,,
6492,10.1007/s11192-009-0021-2,,,
6493,10.1109/ictbig.2016.7892635,,,
6494,10.1007/978-3-540-92673-3_0,,,
6495,10.1007/s11192-017-2512,,,
6496,10.1002/asi.23456,,,
6497,10.1088/1742-6596/1168/2/022027,,,
6498,10.20935/al2781,,,
6499,10.1016/j.pecs.2008.01.001,,,
6500,10.1016/j.procs.2014.06.042,,,
6501,10.3145/epi.2020.ene.07,,,
6502,10.1038/493159a,,,
6503,10.5210/fm.v15i7.2874,,,
6504,10.1016/j.bja.2020.04.086,,,
6505,10.7717/peerj-cs.921,,,
6506,10.1016/j.joi.2018.01.008,,,
6507,10.1111/cid.12876,,,
6508,10.1016/j.joi.2017.02.011,,,
6509,10.1007/s41060-021-00278-w,,,
6510,10.3390/ai2040030,,,
6511,10.3390/healthcare11010081,,,
6512,10.1007/978-3-030-22808-8,,,
6513,10.3390/digital2010006,,,
6514,10.3386/w30006,,,
6515,10.1016/j.finmar.2013.06.004,,,
6516,10.3390/app13010259,,,
6517,10.5089/9781589063952.087,,,
6518,10.1145/3097983.3098095,,,
6519,10.1057/s41264-022-00176-7,,,
6520,10.2307/2325486,,,
6521,10.1007/978-3-031-16008-0,,,
6522,10.1007/978-3-030-91159-1,,,
6523,10.1016/j.erss.2023.103072,,,
6524,10.1093/oseo/instance.00077240,,,
6525,10.1017/cbo9780511840852,,,
6526,10.1093/mind/xciv.374.196,,,
6527,10.1017/cbo9780511625053,,,
6528,10.1080/00048402.2018.1501078,,,
6529,10.1111/j.1468-0114.2007.00280.x,,,
6530,10.1093/mind/os-2.7.411,,,
6531,10.1007/s11098-016-0768-2,,,
6532,10.1093/0198235119.001.0001,,,
6533,10.1111/phpr.12080,,,
6534,10.1086/660696,,,
6535,10.1086/719517,,,
6536,10.1111/j.0031-8094.2004.00370.x,,,
6537,10.1037/e597132010-001,,,
6538,10.1016/0007-6813(91)90005-g,,,
6539,10.1515/9781400838431,,,
6540,10.2139/ssrn.4350992,,,
6541,10.2139/ssrn.4350999,,,
6542,10.2139/ssrn.4353923,,,
6543,10.2139/ssrn.4360876,,,
6544,10.1017/err.2022.14,,,
6545,10.1017/s0266267100004132,,,
6546,10.3390/fintech1040029,,,
6547,10.2196/11270,,,
6548,10.1016/j.ijmedinf.2022.104738,,,
6549,10.1038/s41586-019-1138-y,,,
6550,10.1109/tai.2023.3318183,,,
6551,10.1109/nlbse59153.2023.00008,,,
6552,10.1136/jme-2023-109347,,,
6553,10.2139/ssrn.4776793,,,
6554,10.1162/dint_a_00235,,,
6555,10.1613/jair.1.14020,,,
6556,10.1145/3600211.3604686,,,
6557,10.1126/science.adn0117,,,
6558,10.1136/jme-2023-109574,,,
6559,10.21203/rs.3.rs-2895792/v1,,,
6560,10.1016/j.techsoc.2021.101678,,,
6561,10.1002/sd.2596,,,
6562,10.1080/15265161.2023.2250333,,,
6563,10.5114/biolsport.2023.125623,,,
6564,10.14361/dcs-2022-0205,,,
6565,10.1007/s11229-023-04367-0,,,
6566,10.1007/s00146-024-01930-2,,,
6567,10.32628/cseit2390533,,,
6568,10.1007/s10676-023-09716-8,,,
6569,10.2139/ssrn.4064091,,,
6570,10.3389/frai.2022.826207,,,
6571,10.1145/3462741.3466809,,,
6572,10.1016/j.cognition.2007.11.004,,,
6573,10.1007/s11023-020-09526-7,,,
6574,10.1007/s43681-022-00173-5,,,
6575,10.1073/pnas.2317967121,,,
6576,10.1007/s43681-022-00199-9,,,
6577,10.18653/v1/2022.findings-emnlp.148,,,
6578,10.1007/s13347-023-00606-x,,,
6579,10.2139/ssrn.4387984,,,
6580,10.36227/techrxiv.22619932.v2,,,
6581,10.1038/s42256-024-00820-y,,,
6582,10.1007/s10676-023-09728-4,,,
6583,10.2139/ssrn.4104003,,,
6584,10.1126/science.adi8982,,,
6585,10.7326/0003-4819-151-4-200908180-00135,,,
6586,10.2139/ssrn.4361607,,,
6587,10.1016/j.bushor.2019.11.001,,,
6588,10.1080/0144929x.2023.2286532,,,
6589,10.1016/j.ssci.2023.106244,,,
6590,10.31222/osf.io/v7gm2,,,
6591,10.31235/osf.io/c5hf3,,,
6592,10.58729/1941-6679.1564,,,
6593,10.1111/j.1756-8765.2010.01096.x,,,
6594,10.1145/3597512.3597528,,,
6595,10.1038/s42256-023-00653-1,,,
6596,10.1007/s13347-023-00643-6,,,
6597,10.1109/cvpr52688.2022.01042,,,
6598,10.1207/s15327957pspr0504_2,,,
6599,10.1007/s13347-022-00591-7,,,
6600,10.2139/ssrn.4602790,,,
6601,10.1007/s40889-023-00179-5,,,
6602,10.1038/nature24270,,,
6603,10.1007/s43681-022-00243-8,,,
6604,10.2139/ssrn.4378735,,,
6605,10.1016/b978-0-443-18851-0.00007-x,,,
6606,10.1093/jcmc/zmad045,,,
6607,10.21203/rs.3.rs-2724922/v1,,,
6608,10.22541/au.168323192.20543041/v1,,,
6609,10.18559/ebr.2023.2.743,,,
6610,10.1007/s11633-022-1410-8,,,
6611,10.1109/ojcs.2023.3300321,,,
6612,10.36227/techrxiv.23968311.v1,,,
6613,10.1145/3571884.3603754,,,
6614,10.58496/mjcsc/2023/003,,,
6615,10.1136/jme-2023-108909,,,
6616,10.1016/j.jjimei.2020.10000,,,
6617,10.1016/j.csda.2018.06.011,,,
6618,10.18637/jss.v103.i01,,,
6619,10.18637/jss.v067.i01,,,
6620,10.1016/s0065-2601(08)60412-8,,,
6621,10.1186/s41235-023-00499-6,,,
6622,10.4135/9781412985208,,,
6623,10.1109/cscs59211.2023.00068,,,
6624,10.1145/335191.335388,,,
6625,10.1007/s00362-019-01148-1,,,
6626,10.1037/aca0000136,,,
6627,10.1016/j.chb.2022.107406,,,
6628,10.1007/bf02310555,,,
6629,10.1111/j.1468-5914.1992.tb00210.x,,,
6630,10.1016/j.heliyon.2022.e12750,,,
6631,10.1037/0022-3514.44.1.113,,,
6632,10.3389/fpsyg.2021.781346,,,
6633,10.48550/arxiv.1706.07068,,,
6634,10.7243/2055-3447-1-5,,,
6635,10.1016/j.tics.2007.02.003,,,
6636,10.1177/0276237421994697,,,
6637,10.2307/2528963,,,
6638,10.1007/978-3-658-40232-7_1,,,
6639,10.1177/1088868315574978,,,
6640,10.1002/jcpy.1181,,,
6641,10.3390/digital2010001,,,
6642,10.3389/fpsyg.2022.941163,,,
6643,10.3389/feduc.2022.970212,,,
6644,10.1145/3347092,,,
6645,10.1016/j.chb.2022.107502,,,
6646,10.1007/978-3-319-91244-8_24,,,
6647,10.1177/1461444820925798,,,
6648,10.31566/arts.3.011,,,
6649,10.2190/em.32.2.c,,,
6650,10.1080/10447318.2023.2219961,,,
6651,10.3758/s13428-019-01257-7,,,
6652,10.1037/aca0000269,,,
6653,10.1016/s0022-1031(03)00065-9,,,
6654,10.1177/2515245918770963,,,
6655,10.18148/srm/2019.v13i3.7403,,,
6656,10.1016/j.jesp.2017.09.011,,,
6657,10.1109/icpads60453.2023.00183,,,
6658,10.21105/joss.00754,,,
6659,10.21105/joss.03139,,,
6660,10.14744/nci.2022.55649,,,
6661,10.2139/ssrn.3827314,,,
6662,10.1177/02762374221095701,,,
6663,10.1016/j.chb.2023.107707,,,
6664,10.1111/j.2041-210x.2012.00261.x,,,
6665,10.1007/s12369-023-01004-1,,,
6666,10.21105/joss.03167,,,
6667,10.23668/psycharchives.9249,,,
6668,10.3389/fpsyg.2017.01729,,,
6669,10.1146/annurev.psych.56.091103.070141,,,
6670,10.1080/09672559.2012.668308,,,
6671,10.1007/b98882,,,
6672,10.32614/rj-2017-024,,,
6673,10.2224/sbp.2004.32.4.355,,,
6674,10.1145/3334480.3382892,,,
6675,10.1016/0191-8869(81)90084-2,,,
6676,10.1037/aca0000570,,,
6677,10.1037/aca0000348,,,
6678,10.1177/1745691617708630,,,
6679,10.1080/00223890802484381,,,
6680,10.1007/s11165-016-9602-2,,,
6681,10.1177/1948550617720275,,,
6682,10.4172/2161-1165.1000227,,,
6683,10.1016/b0-12-369398-5/00428-x,,,
6684,10.1007/978-0-387-98141-3,,,
6685,10.21105/joss.01686,,,
6686,10.1037/aca0000418,,,
6687,10.1027/1864-9335/a000460,,,
6688,10.1016/j.chb.2019.106186,,,
6689,10.1080/08838151.2020.1835136,,,
6690,10.1145/3613904.3642404,,,
6691,10.1007/978-3-031-29956-8_20,,,
6692,10.1007/s10461-007-9302-z,,,
6693,10.1609/aaai.v32i1.11414,,,
6694,10.1609/aaai.v30i1.10023,,,
6695,10.1613/jair.4317,,,
6696,10.1609/aaai.v30i2.19070,,,
6697,10.1177/1049731509358424,,,
6698,10.1145/3290605.3300329,,,
6699,10.1287/inte.1100.0505,,,
6700,10.1016/s0140-6736(97)07439-4,,,
6701,10.1145/3292500.3330777,,,
6702,10.1609/aimag.v30i1.2173,,,
6703,10.1086/701439,,,
6704,10.24963/ijcai.2017/54,,,
6705,10.24963/ijcai.2018/775,,,
6706,10.1136/bmjgh-2018-001018,,,
6707,10.1017/9781316676714.026,,,
6708,10.1145/2940716.2940796,,,
6709,10.1007/978-3-319-68711-7_24,,,
6710,10.1609/aaai.v29i2.19063,,,
6711,10.1016/j.jacr.2017.12.026,,,
6712,10.1016/j.jbi.2017.07.012,,,
6713,10.1016/j.drudis.2020.10.010,,,
6714,10.2196/27850,,,
6715,10.1016/j.jik.2023.100333,,,
6716,10.3390/jpm13060951,,,
6717,10.51594/imsrj.v4i4.1052,,,
6718,10.1177/1460458219874641,,,
6719,10.1109/access.2022.3197671,,,
6720,10.1371/journal.pmed.1004326,,,
6721,10.2196/29301,,,
6722,10.1016/j.giq.2018.09.008,,,
6723,10.2196/49303,,,
6724,10.3389/fdgth.2023.1229308,,,
6725,10.2196/10010,,,
6726,10.1016/s2589-7500(20)30218-1,,,
6727,10.1093/bib/bbx044,,,
6728,10.1007/978-3-030-81907-1_18,,,
6729,10.1016/j.techfore.2015.12.019,,,
6730,10.1186/s12911-023-02103-9,,,
6731,10.1016/b978-0-443-19413-9.00017-5,,,
6732,10.1016/j.cct.2021.106397,,,
6733,10.1101/2020.03.09.983429,,,
6734,10.1016/b978-0-12-818438-7.00010-1,,,
6735,10.1146/annurev-publhealth-052620-093850,,,
6736,10.15265/iy-2015-012,,,
6737,10.1001/jama.2018.20563,,,
6738,10.4172/2157-7420.1000321,,,
6739,10.1208/s12248-021-00644-3,,,
6740,10.1016/j.healthpol.2020.01.001,,,
6741,10.1515/cclm-2022-1096,,,
6742,10.3122/jabfm.2022.01.210226,,,
6743,10.1007/s10439-022-03121-w,,,
6744,10.1136/medethics-2021-107529,,,
6745,10.1109/meco58584.2023.10155107,,,
6746,10.1016/j.joule.2023.09.004,,,
6747,10.1007/s10796-021-10146-4,,,
6748,10.1109/ismsit50672.2020.9255249,,,
6749,10.1515/almed-2023-0124,,,
6750,10.7551/mitpress/1416.001.0001,,,
6751,10.1016/j.techfore.2020.120482,,,
6752,10.1016/j.compbiomed.2021.104660,,,
6753,10.1016/j.jphysparis.2007.11.003,,,
6754,10.1515/9780748629305,,,
6755,10.1201/b23355,,,
6756,10.3389/fpsyg.2017.01612,,,
6757,10.1007/s11097-022-09833-7,,,
6758,10.1177/13505084231189269,,,
6759,10.1080/14767430.2022.2134618,,,
6760,10.4324/9781351233477-2,,,
6761,10.1080/1474225x.2018.1448674,,,
6762,10.1007/s11569-012-0155-1,,,
6763,10.1007/978-90-481-2229-5_8,,,
6764,10.1558/jcr.v9i2.199,,,
6765,10.1080/1600910x.2005.9672913,,,
6766,10.1017/cbo9780511818998,,,
6767,10.1017/apa.2019.47,,,
6768,10.4135/9781473914810,,,
6769,10.1002/sdr.4260060203,,,
6770,10.1007/s00422-002-0353-y,,,
6771,10.1002/2017gl072716,,,
6772,10.1017/s000708749900391x,,,
6773,10.1093/jaarel/lfm101,,,
6774,10.5840/wcp20-paideia199819358,,,
6775,10.7551/mitpress/9780262529365.001.0001,,,
6776,10.1177/014107688507800813,,,
6777,10.4103/0973-1229.77436,,,
6778,10.1191/0269215506cr952ed,,,
6779,10.1007/s11948-020-00241-1,,,
6780,10.1109/imcec.2018.8469650,,,
6781,10.1038/d41586-019-02212-4,,,
6782,10.1038/551159a,,,
6783,10.1089/big.2018.0083,,,
6784,10.1109/msp.2018.2701164,,,
6785,10.1016/j.tins.2006.07.002,,,
6786,10.1007/s12152-021-09464-w,,,
6787,10.1017/s0963180118000130,,,
6788,10.1007/s11023-007-9067-1,,,
6789,10.1016/s1389-0417(01)00036-5,,,
6790,10.1007/978-3-642-00616-6_12,,,
6791,10.1007/978-3-540-77296-5_3,,,
6792,10.1177/0894439317698637,,,
6793,10.1016/j.jmacro.2016.08.003,,,
6794,10.1016/j.robot.2016.09.017,,,
6795,10.1038/s41551-016-0014,,,
6796,10.1016/j.econmod.2018.12.015,,,
6797,10.1007/s10676-018-9453-9,,,
6798,10.3732/ajb.0900041,,,
6799,10.2139/ssrn.2439112,,,
6800,10.1023/a:1011218919464,,,
6801,10.4324/9780203994610,,,
6802,10.1080/17439884.2020.1686017,,,
6803,10.1111/1467-9566.12900,,,
6804,10.1111/j.1467-954x.1999.tb03483.x,,,
6805,10.4324/9781003367451-4,,,
6806,10.1177/1461444819885334,,,
6807,10.3390/s120201211,,,
6808,10.1007/s12152-011-9132-6,,,
6809,10.1109/mc.2012.107,,,
6810,10.1097/npt.0b013e31825064cc,,,
6811,10.1186/s12910-017-0220-y,,,
6812,10.1007/s12152-018-9364-9,,,
6813,10.1007/978-3-319-42235-0,,,
6814,10.1080/10447318.2020.1741118,,,
6815,10.1007/s11948-020-00240-2,,,
6816,10.35940/ijeat.f8587.088619,,,
6817,10.35940/ijitee.b1006.1292s19,,,
6818,10.35940/ijrte.b3838.079220,,,
6819,10.54105/ijipr.a8041.04030424,,,
6820,10.35940/ijsce.c3265.099319,,,
6821,10.1080/0969594x.2010.513678,,,
6822,10.1080/10627197.2023.2202312,,,
6823,10.1007/s10648-021-09615-8,,,
6824,10.1037/edu0000250,,,
6825,10.1007/s11092-008-9068-5,,,
6826,10.1007/s40593-015-0090-8,,,
6827,10.48550/arxiv.2406.18900,,,
6828,10.1111/emip.12551,,,
6829,10.1177/07356331241226592,,,
6830,10.31234/osf.io/aj46b,,,
6831,10.48550/arxiv.2402.11111,,,
6832,10.1016/b978-044481862-1.50103-5,,,
6833,10.17705/1jais.00867,,,
6834,10.1002/ets2.12067,,,
6835,10.1057/9781137385765,,,
6836,10.4018/979-8-3693-1351-0.ch008,,,
6837,10.1037/13275-018,,,
6838,10.1111/emip.12602,,,
6839,10.1207/s15326985ep4102_4,,,
6840,10.48550/arxiv.2401.09395,,,
6841,10.1111/jedm.12334,,,
6842,10.7275/pare.2102,,,
6843,10.1080/15348431.2020.1731693,,,
6844,10.48550/arxiv.2308.02439,,,
6845,10.3390/analytics2040046,,,
6846,10.1016/j.caeai.2023.100199,,,
6847,10.1002/j.2333-8504.2003.tb01908.x,,,
6848,10.1007/978-3-031-42682-7_19,,,
6849,10.48550/arxiv.2307.00150,,,
6850,10.1371/journal.pone.0304013,,,
6851,10.1007/978-3-030-20062-6_21,,,
6852,10.1080/10627197.2022.2042682,,,
6853,10.48550/arxiv.2406.06599,,,
6854,10.1111/jedm.12381,,,
6855,10.1111/emip.12377,,,
6856,10.48550/arxiv.2309.02427,,,
6857,10.48550/arxiv.2401.10444,,,
6858,10.1016/j.caeai.2022.100075,,,
6859,10.4324/9781003278658-8,,,
6860,10.1037/a0031882,,,
6861,10.1080/00220973.2017.1380590,,,
6862,10.1007/s40593-018-0168-1,,,
6863,10.1111/jcal.12502,,,
6864,10.1080/17439884.2020.1798995,,,
6865,10.1111/emip.12165,,,
6866,10.1207/s15324818ame1802_2,,,
6867,10.1080/19313152.2024.2339757,,,
6868,10.48550/arxiv.2309.06794,,,
6869,10.1007/978-3-642-31454-4_32,,,
6870,10.1007/3-540-47987-2_47,,,
6871,10.48550/arxiv.2403.02726,,,
6872,10.4324/9781351137713-4,,,
6873,10.25300/misq/2019/15049,,,
6874,10.34104/cjbis.022.012023,,,
6875,10.1049/pbpc035g,,,
6876,10.1109/confluence.2018.8442900,,,
6877,10.4135/9781412986229,,,
6878,10.34104/ijma.020.074095,,,
6879,10.9781/ijimai.2016.369,,,
6880,10.5120/4870-7297,,,
6881,10.1080/08853134.2018.1557525,,,
6882,10.1002/9781394260485,,,
6883,10.1145/3285029,,,
6884,10.1093/wbro/lkn008,,,
6885,10.1016/j.jdeveco.2018.01.005,,,
6886,10.1093/oxfordjournals.oep.a028625,,,
6887,10.1111/ecaf.12321,,,
6888,10.1016/b978-0-12-810441-5.00006-3,,,
6889,10.31887/dcns.2010.12.4/rcolom,,,
6890,10.1016/b978-0-12-812282-2.00004-8,,,
6891,10.1007/s11573-017-0852-x,,,
6892,10.1016/s0160-2896(97)90014-3,,,
6893,10.5772/intechopen.84624,,,
6894,10.1016/b978-0-12-812282-2.00016-4,,,
6895,10.1057/9781137574633_6,,,
6896,10.1016/b978-0-444-52944-2.00009-4,,,
6897,10.1016/j.jaccpubpol.2016.04.003,,,
6898,10.1016/b978-0-12-812282-2.00015-2,,,
6899,10.1016/b978-0-12-812282-2.00009-7,,,
6900,10.1080/1540496x.2018.1564658,,,
6901,10.1007/978-3-030-16184-2_9,,,
6902,10.1007/978-3-030-02351-5_49,,,
6903,10.18632/oncotarget.22345,,,
6904,10.20944/preprints202004.0195.v1,,,
6905,10.18646/2056.52.18-007,,,
6906,10.1108/ajems-03-2017-148,,,
6907,10.1016/j.bir.2017.12.003,,,
6908,10.2139/ssrn.2558936,,,
6909,10.1142/s0217590818410059,,,
6910,10.1016/b978-0-12-812282-2.00018-8,,,
6911,10.1016/b978-0-12-812282-2.00014-0,,,
6912,10.1037/0003-066x.42.2.137,,,
6913,10.1016/s1573-448x(89)01016-2,,,
6914,10.3390/su11030568,,,
6915,10.3390/su12041668,,,
6916,10.3390/su12093733,,,
6917,10.22329/wyaj.v31i2.4419,,,
6918,10.1177/0093854808326545,,,
6919,10.5204/lthj.v2i1.1478,,,
6920,10.1057/9780230227293,,,
6921,10.1007/s10260-010-0142-z,,,
6922,10.16997/book55.g,,,
6923,10.2139/ssrn.3414805,,,
6924,10.1111/j.1468-4446.2009.01303.x,,,
6925,10.1016/j.afjem.2017.08.001,,,
6926,10.11648/j.ajtas.20160501.11,,,
6927,10.1007/s13347-018-0303-9,,,
6928,10.2139/ssrn.2687120,,,
6929,10.2139/ssrn.3759349,,,
6930,10.1007/978-3-319-29272-4_4,,,
6931,10.1111/jlme.12040,,,
6932,10.1057/9780230227293_2,,,
6933,10.7551/mitpress/10642.001.0001,,,
6934,10.1016/j.artint.2010.11.026,,,
6935,10.1093/ijlit/eaz001,,,
6936,10.1093/oso/9780198838494.003.0010,,,
6937,10.3390/laws3020353,,,
6938,10.1080/10383441.2011.10854728,,,
6939,10.1080/03085149800000020,,,
6940,10.4337/9781786433039.00032,,,
6941,10.1017/s0269888906000701,,,
6942,10.1108/s1521-6136(2009)0000012003,,,
6943,10.1145/41735.41736,,,
6944,10.1007/s11024-009-9127-1,,,
6945,10.69554/olhs2696,,,
6946,10.18352/ulr.41,,,
6947,10.1007/978-3-319-63284-1_3,,,
6948,10.69554/uadi4135,,,
6949,10.1007/s12027-020-00602-0,,,
6950,10.1016/bs.pbr.2017.06.004,,,
6951,10.4159/9780674249547,,,
6952,10.2307/2371045,,,
6953,10.1023/a:1005192006642,,,
6954,10.1016/j.isci.2020.101656,,,
6955,10.1007/s11245-006-0006-1,,,
6956,10.1080/00201740701489245,,,
6957,10.1080/00201740701489401,,,
6958,10.1057/s41599-020-0494-4,,,
6959,10.7551/mitpress/5684.001.0001,,,
6960,10.1016/j.neunet.2003.06.005,,,
6961,10.1016/j.tics.2009.04.005,,,
6962,10.1126/science.aac6076,,,
6963,10.1007/978-94-009-9493-5_1,,,
6964,10.1093/0195138929.001.0001,,,
6965,10.1002/9780470690024.ch10,,,
6966,10.1201/9780429258985,,,
6967,10.1080/00201740701489211,,,
6968,10.1080/00201740701489351,,,
6969,10.7551/mitpress/4124.001.0001,,,
6970,10.1080/09515089.2019.1607692,,,
6971,10.1142/s2705078520500113,,,
6972,10.1093/analys/65.4.278,,,
6973,10.1016/j.concog.2016.06.011,,,
6974,10.1007/s42979-021-00592-x,,,
6975,10.1007/bf00160894,,,
6976,10.7551/mitpress/5834.001.0001,,,
6977,10.15252/embr.201949177,,,
6978,10.1093/acprof:oso/9780199297023.001.0001,,,
6979,10.1093/acprof:oso/9780198719694.001.0001,,,
6980,10.2307/2678403,,,
6981,10.1112/plms/s2-42.1.230,,,
6982,10.7551/mitpress/11041.001.0001,,,
6983,10.5040/9781501338403,,,
6984,10.7551/mitpress/7723.001.0001,,,
6985,10.5040/9781350284913,,,
6986,10.4324/9781315643670-8,,,
6987,10.4324/9781315643670-18,,,
6988,10.4324/9780203723647,,,
6989,10.5040/9781501382901,,,
6990,10.1093/oxfordhb/9780199279456.003.0024,,,
6991,10.1016/j.intcom.2003.07.001,,,
6992,10.1007/978-3-540-45012-2_2,,,
6993,10.1101/2023.02.19.23286155,,,
6994,10.1038/s41746-022-00742-2,,,
6995,10.1007/s00432-023-04824-w,,,
6996,10.1093/eurjcn/zvad022,,,
6997,10.1016/s2589-7500(23)00021-3,,,
6998,10.1148/radiol.230276,,,
6999,10.1016/s2589-7500(23)00083-3,,,
7000,10.2139/ssrn.4363843,,,
7001,10.2196/38926,,,
7002,10.3233/shti200312,,,
7003,10.3389/fpubh.2021.755644,,,
7004,10.1038/s41746-020-00324-0,,,
7005,10.1021/acs.jcim.2c00246,,,
7006,10.1038/s41746-021-00432-5,,,
7007,10.1145/3568444.3568456,,,
7008,10.1016/s2589-7500(21)00208-9,,,
7009,10.1016/j.knosys.2020.106685,,,
7010,10.1002/widm.1424,,,
7011,10.6017/ital.v41i2.14683,,,
7012,10.1038/s41746-018-0029-1,,,
7013,10.3390/risks10120230,,,
7014,10.1145/3233231,,,
7015,10.1609/aimag.v38i3.2741,,,
7016,10.1609/aaai.v33i01.33019780,,,
7017,10.1162/leon.2005.38.4.314,,,
7018,10.4236/ojpp.2019.92009,,,
7019,10.9781/ijimai.2022.11.005,,,
7020,10.1109/access.2019.2960832,,,
7021,10.17104/9783406704024,,,
7022,10.3390/info12020068,,,
7023,10.1088/1742-6596/1673/1/012052,,,
7024,10.1088/1742-6596/1648/3/032187,,,
7025,10.4108/eai.13-7-2018.163834,,,
7026,10.1080/17540763.2021.1959387,,,
7027,10.14236/ewic/eva2017.47,,,
7028,10.1386/padm.3.2-3.123_1,,,
7029,10.5922/0207-6918-2022-1-3,,,
7030,10.1017/s0140525x00080675,,,
7031,10.2196/17707,,,
7032,10.1007/s00146-021-01380-0,,,
7033,10.1007/s13347-020-00421-8,,,
7034,10.1007/s13347-021-00460-x,,,
7035,10.32992/erlacs.10526,,,
7036,10.1146/annurev-biodatasci-092820-114759,,,
7037,10.1080/02681102.2019.1590638,,,
7038,10.1038/538311a,,,
7039,10.1038/sdata.2018.286,,,
7040,10.1007/s10676-018-9450-z,,,
7041,10.1007/s43681-020-00011-6,,,
7042,10.2903/j.efsa.2019.e170709,,,
7043,10.1186/s12877-020-01764-9,,,
7044,10.1109/rbme.2021.3119263,,,
7045,10.1016/j.clsr.2020.105490,,,
7046,10.1093/idpl/ipu012,,,
7047,10.1145/1518701.1518875,,,
7048,10.48550/arxiv.2112.05392,,,
7049,10.3389/frai.2022.836557,,,
7050,10.3389/fphar.2023.1214590,,,
7051,10.1007/s40593-020-00096-3,,,
7052,10.1609/aimag.v36i4.2577,,,
7053,10.2471/blt.19.237370,,,
7054,10.1561/1100000073,,,
7055,10.1126/science.aat5989,,,
7056,10.1201/9780429446726,,,
7057,10.1007/s10676-021-09618-z,,,
7058,10.1016/j.patter.2021.100264,,,
7059,10.1177/0163443714532983,,,
7060,10.1186/s13054-023-04425-6,,,
7061,10.1093/llc/fqy082,,,
7062,10.2139/ssrn.4114905,,,
7063,10.48550/arxiv.2303.04226,,,
7064,10.48550/arxiv.2012.07805,,,
7065,10.48550/arxiv.2301.13188,,,
7066,10.7312/chen20400,,,
7067,10.4324/9781003010708,,,
7068,10.1525/9780520924857,,,
7069,10.1353/hrq.2017.0019,,,
7070,10.1080/13642529.2023.2181534,,,
7071,10.48550/arxiv.2304.09873,,,
7072,10.1353/gia.2022.0017,,,
7073,10.48550/arxiv.2303.15056,,,
7074,10.1177/01634437221088951,,,
7075,10.31234/osf.io/qnjkf,,,
7076,10.1080/13534645.2011.605573,,,
7077,10.1017/s1816383121000102,,,
7078,10.1007/s40647-018-0222-2,,,
7079,10.1007/978-3-319-45195-4,,,
7080,10.1093/jncics/pkad015,,,
7081,10.1111/hith.12282,,,
7082,10.1093/ijtj/ijr016,,,
7083,10.14764/10.aseas-2016.2-2,,,
7084,10.1017/9781839703089.008,,,
7085,10.1007/978-3-031-22789-9_5,,,
7086,10.48550/arxiv.2304.05197,,,
7087,10.1002/9781118970492.ch35,,,
7088,10.1080/17504902.2018.1468667,,,
7089,10.1353/gia.2021.0027,,,
7090,10.1177/1750635220906254,,,
7091,10.1080/21670811.2021.1970601,,,
7092,10.5210/fm.v26i10.11562,,,
7093,10.1177/17506980221133732,,,
7094,10.1080/10714413.2020.1862582,,,
7095,10.1080/13527258.2022.2131879,,,
7096,10.48550/arxiv.2009.06807,,,
7097,10.1007/978-3-030-39395-3_4,,,
7098,10.2139/ssrn.3173579,,,
7099,10.1057/9781137322067_12,,,
7100,10.2139/ssrn.4413913,,,
7101,10.1111/j.1467-9655.2006.00278.x,,,
7102,10.1080/17504902.2005.11087153,,,
7103,10.3390/analytics2020020,,,
7104,10.48550/arxiv.2212.09292,,,
7105,10.1007/978-3-319-27036-4_26,,,
7106,10.48550/arxiv.2304.06588,,,
7107,10.1177/2056305120903408,,,
7108,10.1177/1750698019888712,,,
7109,10.5038/1911-9933.13.3.1674,,,
7110,10.1177/0267323120940908,,,
7111,10.48550/arxiv.2302.13793,,,
7112,10.21313/hawaii/9780824836115.001.0001,,,
7113,10.1007/978-3-030-39395-3,,,
7114,10.1017/nps.2019.64,,,
7115,10.3233/isu-2005-253-402,,,
7116,10.1108/lht-11-2019-0229,,,
7117,10.1080/21670811.2017.1343648,,,
7118,10.1177/0163443716643157,,,
7119,10.1177/1461444822109953,,,
7120,10.1515/jetl-2020-0131,,,
7121,10.2139/ssrn.4046123,,,
7122,10.1177/2053951720983865,,,
7123,10.1177/01655515221093029,,,
7124,10.1080/21670811.2017.1366865,,,
7125,10.1177/016344371876456,,,
7126,10.1162/coli_a_00261,,,
7127,10.1162/daed_a_01905,,,
7128,10.4324/9780203083635,,,
7129,10.1016/s0160-2896(97)90011-8,,,
7130,10.23943/9781400889433,,,
7131,10.1007/s40745-015-0029-9,,,
7132,10.21236/ad0289565,,,
7133,10.1016/1061-7361(93)90026-n,,,
7134,10.1515/semi.1996.111.3-4.217,,,
7135,10.4018/978-1-60960-551-3.ch013,,,
7136,10.1002/9780470512517,,,
7137,10.1109/86.736154,,,
7138,10.1007/s00170-009-2191-8,,,
7139,10.1016/j.engappai.2006.07.002,,,
7140,10.1016/j.ymssp.2018.02.016,,,
7141,10.1016/j.engappai.2003.09.006,,,
7142,10.1016/j.jhydrol.2009.06.019,,,
7143,10.1016/s0038-092x(99)00064-x,,,
7144,10.1007/s11604-018-0804-6,,,
7145,10.36548/jitdw.2019.2,,,
7146,10.4018/joeuc.2008070103,,,
7147,10.1007/s11042-009-0284-x,,,
7148,10.1631/fitee.1700053,,,
7149,10.1016/s0166-4115(96)80023-9,,,
7150,10.1016/j.sysarc.2019.02.009,,,
7151,10.1145/2342509.2342513,,,
7152,10.1109/jiot.2016.2579198,,,
7153,10.1109/mc.2015.207,,,
7154,10.1142/s2705078520500058,,,
7155,10.1108/jbs-04-2019-0062,,,
7156,10.1613/jair.1.11345,,,
7157,10.3390/app11052144,,,
7158,10.3390/s20071816,,,
7159,10.3390/jmse10070974,,,
7160,10.3390/jmse8080578,,,
7161,10.3390/jmse10030419,,,
7162,10.3390/app11114972,,,
7163,10.3182/20110828-6-it-1002.01497,,,
7164,10.1007/s00202-010-0182-2,,,
7165,10.1109/allerton.2012.6483398,,,
7166,10.3390/s21134603,,,
7167,10.3390/en14175359,,,
7168,10.3390/app10124270,,,
7169,10.1109/9.57028,,,
7170,10.1109/9.250547,,,
7171,10.1109/aero.2009.4839565,,,
7172,10.3390/a13010023,,,
7173,10.1002/9781119575016,,,
7174,10.3390/jmse6030098,,,
7175,10.2514/6.2009-6625,,,
7176,10.1109/tvt.2017.2760980,,,
7177,10.1109/tia.2018.2810804,,,
7178,10.1109/tia.2018.2849725,,,
7179,10.1109/tia.2018.2888801,,,
7180,10.1109/tia.2020.2970150,,,
7181,10.3390/a12110232,,,
7182,10.1017/9781108614139,,,
7183,10.1007/978-3-319-44760-5,,,
7184,10.31228/osf.io/97upg,,,
7185,10.1086/scr.1979.3109565,,,
7186,10.1016/j.ausmj.2020.04.003,,,
7187,10.1561/9781601988195,,,
7188,10.1016/j.clsr.2017.03.004,,,
7189,10.1007/978-3-540-28650-9_5,,,
7190,10.1109/mc.2018.2888774,,,
7191,10.5281/zenodo.3240529,,,
7192,10.1002/ijc.29210,,,
7193,10.3322/caac.21660,,,
7194,10.1016/0140-6736(93)91067-v,,,
7195,10.1038/bjc.2013.177,,,
7196,10.1038/s41416-021-01503-w,,,
7197,10.1148/radiol.2018181371,,,
7198,10.1001/jamanetworkopen.2020.0265,,,
7199,10.1001/jamaoncol.2020.3321,,,
7200,10.1007/s00330-019-06186-9,,,
7201,10.1016/s2589-7500(20)30185-0,,,
7202,10.1093/jnci/djy222,,,
7203,10.1111/j.1582-4934.2005.tb00350.x,,,
7204,10.1056/nejmoa044383,,,
7205,10.1093/jnci/djj331,,,
7206,10.1186/bcr455,,,
7207,10.1007/s10278-019-00278-0,,,
7208,10.1148/radiol.2016161174,,,
7209,10.1148/radiol.2019182908,,,
7210,10.5210/fm.v2i9.548,,,
7211,10.1145/3212695,,,
7212,10.5555/2819009.2819097,,,
7213,10.1145/2902362,,,
7214,10.1109/ic2e.2019.00025,,,
7215,10.1145/2591062.2591072,,,
7216,10.1109/ase.2015.36,,,
7217,10.5555/2969033.2969173,,,
7218,10.1145/3379336.3381474,,,
7219,10.1145/3397481.3450656,,,
7220,10.24963/ijcai.2017/654,,,
7221,10.1007/s43681-022-00143-x,,,
7222,10.1017/aju.2020.30,,,
7223,10.1007/s10506-023-09347-w,,,
7224,10.1111/phpe.12036,,,
7225,10.1007/s00146-024-01949-5,,,
7226,10.1007/s00146-023-01748-4,,,
7227,10.1093/monist/onae002,,,
7228,10.1016/0304-4068(92)90021-x,,,
7229,10.2139/ssrn.4464783,,,
7230,10.1007/s00146-023-01850-7,,,
7231,10.1017/err.2023.1,,,
7232,10.1007/s00146-023-01811-0,,,
7233,10.1007/bf01766393,,,
7234,10.1111/phc3.12709,,,
7235,10.1007/s00146-023-01698-x,,,
7236,10.1037/0022-3514.91.4.652,,,
7237,10.1109/taffc.2019.2934444,,,
7238,10.1016/b0-08-043076-7/01754-x,,,
7239,10.1111/j.1467-9280.1996.tb00346.x,,,
7240,10.3115/v1/p14-1023,,,
7241,10.1037/h0036215,,,
7242,10.1037/rev0000047,,,
7243,10.3758/s13421-018-0869-6,,,
7244,10.1007/978-1-4614-9539-0,,,
7245,10.1098/rstb.2017.0121,,,
7246,10.3233/ao-210259,,,
7247,10.31234/osf.io/d84kg,,,
7248,10.1146/annurev-soc-073014-112142,,,
7249,10.1037/0033-295x.82.6.407,,,
7250,10.1080/01650250344000091,,,
7251,10.1109/tts.2021.3125998,,,
7252,10.1037/0022-3514.56.1.5,,,
7253,10.1146/annurev-psych-122216-011719,,,
7254,10.4324/9780203127971,,,
7255,10.1037/dev0000033,,,
7256,10.1007/978-3-030-37439-6,,,
7257,10.1016/j.tics.2006.11.005,,,
7258,10.1017/cbo9780511676536.010,,,
7259,10.1073/pnas.1720347115,,,
7260,10.1037/0033-295x.103.3.592,,,
7261,10.1111/j.1756-8765.2008.01006.x,,,
7262,10.1007/978-3-031-02165-7,,,
7263,10.1037/0033-295x.102.1.4,,,
7264,10.1037/0022-3514.74.6.1464,,,
7265,10.1037/pspa0000016,,,
7266,10.1080/01690960701702035,,,
7267,10.3758/s13421-017-0732-1,,,
7268,10.1016/j.foodqual.2009.04.001,,,
7269,10.1037/0003-066x.60.6.581,,,
7270,10.1037/amp0000307,,,
7271,10.1073/pnas.1316909110,,,
7272,10.1037/0022-3514.85.4.616,,,
7273,10.1073/pnas.1509654112,,,
7274,10.5465/amj.2013.0721,,,
7275,10.1037/h0034747,,,
7276,10.1017/cbo9780511809477,,,
7277,10.18653/v1/w17-1902,,,
7278,10.1007/bf01068252,,,
7279,10.1037/xge0000179,,,
7280,10.1016/0163-6383(93)80038-a,,,
7281,10.1038/s41562-020-0918-6,,,
7282,10.31234/osf.io/u6wd2,,,
7283,10.1017/langcog.2020.15,,,
7284,10.31234/osf.io/dpa8s,,,
7285,10.3758/s13428-013-0409-z,,,
7286,10.1080/0163853x.2018.1541382,,,
7287,10.1073/pnas.1211286109,,,
7288,10.1111/1475-3995.00375,,,
7289,10.1016/j.artint.2012.07.001,,,
7290,10.1162/coli_a_00379,,,
7291,10.1037/1089-2699.6.1.101,,,
7292,10.1609/hcomp.v7i1.5281,,,
7293,10.3115/v1/d14-1162,,,
7294,10.1038/s41467-018-03068-4,,,
7295,10.1037/0012-1649.30.3.436,,,
7296,10.1111/j.1467-8721.2007.00528.x,,,
7297,10.1177/0891243204265349,,,
7298,10.1162/coli_a_00294,,,
7299,10.1177/0146167201279009,,,
7300,10.1098/rstb.2020.0141,,,
7301,10.3758/s13423-019-01652-3,,,
7302,10.1037/0033-295x.84.2.127,,,
7303,10.2307/1884852,,,
7304,10.1609/aaai.v31i1.11164,,,
7305,10.1145/3306618.3314270,,,
7306,10.1111/j.1467-8624.2009.01272.x,,,
7307,10.1007/s10508-015-0490-8,,,
7308,10.31235/osf.io/r7ewx,,,
7309,10.1371/journal.pone.0239858,,,
7310,10.1177/0891243287001002002,,,
7311,10.1609/aaai.v30i1.9959,,,
7312,10.18653/v1/n19-1064,,,
7313,10.1038/s41467-020-15804-w,,,
7314,10.33645/cnc.2023.05.45.05.109,,,
7315,10.31565/korrow.2023..56.001,,,
7316,10.3102/00028312006002207,,,
7317,10.24159/joec.2023.29.4.243,,,
7318,10.32431/kace.2023.26.4.006,,,
7319,10.18108/jeer.2023.26.5.17,,,
7320,10.14697/jkase.2023.43.3.307,,,
7321,10.23016/kllj.2023.82.82.469,,,
7322,10.22251/jlcci.2023.23.19.567,,,
7323,10.5392/jkca.2020.20.08.075,,,
7324,10.15738/kjell.23..202309.741,,,
7325,10.33851/jmis.2023.10.1.79,,,
7326,10.46392/kjge.2023.17.5.113,,,
7327,10.22846/kafil.27.1.202304.003,,,
7328,10.29096/jee.32.4.06,,,
7329,10.7468/jksmee.2023.37.2.233,,,
7330,10.29113/skpaer.2020.26.4.001,,,
7331,10.35832/kmlc..77.202303.125,,,
7332,10.47142/gec.8.1,,,
7333,10.14352/jkaie.2018.22.5.509,,,
7334,10.1038/s41467‐020‐15871‐z,,,
7335,10.1609/aaai.v25i1.7811,,,
7336,10.1016/j.socimp.2024.100044,,,
7337,10.1002/ail2.76,,,
7338,10.1021/acs.est.1c01443,,,
7339,10.1007/978-3-030-49342-4_6,,,
7340,10.1145/3533378,,,
7341,10.1145/3514094.3534187,,,
7342,10.1038/s42256‐019‐0088‐2,,,
7343,10.1016/j.patter.2021.100381,,,
7344,10.1007/s11023‐018‐9482‐5,,,
7345,10.1109/mra.2019.2946107,,,
7346,10.1002/ail2.92,,,
7347,10.1145/3287560.3287567,,,
7348,10.1016/j.patter.2022.100449,,,
7349,10.3390/atmos14020368,,,
7350,10.1016/0004‐3702(96)00012‐4,,,
7351,10.3390/atmos14020354,,,
7352,10.1609/hcomp.v6i1.13322,,,
7353,10.1148/radiol.2020192224,,,
7354,10.1016/j.deveng.2022.100102,,,
7355,10.1080/17565529.2015.1086294,,,
7356,10.1080/0969160x.2011.556399,,,
7357,10.1136/bmjgh‐2018‐000798,,,
7358,10.1609/aimag.v35i3.2529,,,
7359,10.2196/45132,,,
7360,10.1016/j.media.2021.102062,,,
7361,10.1016/j.dib.2022.107911,,,
7362,10.1016/j.dib.2020.106170,,,
7363,10.1016/j.atech.2023.100291,,,
7364,10.1002/isd2.12249,,,
7365,10.1038/s41558‐022‐01519‐x,,,
7366,10.1162/tacl_a_00416,,,
7367,10.18653/v1/2022.emnlp-main.298,,,
7368,10.1080/0952813x.2016.1186228,,,
7369,10.1016/j.cobeha.2018.12.010,,,
7370,10.1007/s11023-020-09548-1,,,
7371,10.1038/s41598-023-40858-3,,,
7372,10.1109/emr.2023.3272799,,,
7373,10.1038/d41586-023-01516-w,,,
7374,10.1109/mis.2016.93,,,
7375,10.14569/ijacsa.2022.0130937,,,
7376,10.1108/lht-02-2015-0010,,,
7377,10.3390/su151511524,,,
7378,10.1007/s11528-023-00863-9,,,
7379,10.3390/healthcare11091298,,,
7380,10.1109/educon54358.2023.10125121,,,
7381,10.1177/20965311231168423,,,
7382,10.1186/s41239-023-00408-3,,,
7383,10.1016/j.caeo.2023.100151,,,
7384,10.2139/ssrn.4553787,,,
7385,10.1016/j.inffus.2023.101805,,,
7386,10.2139/ssrn.4568684,,,
7387,10.1111/imcb.12689,,,
7388,10.3389/fmed.2023.1279707,,,
7389,10.1186/s40594-023-00418-7,,,
7390,10.1016/j.caeai.2022.100118,,,
7391,10.1007/s12369-020-00640-1,,,
7392,10.4324/9781003121923,,,
7393,10.1016/j.compedu.2018.08.026,,,
7394,10.1186/s40359-024-01895-3,,,
7395,10.3390/educsci13100978,,,
7396,10.1145/3408877.3432513,,,
7397,10.1007/s12599-019-00600-8,,,
7398,10.31866/2709-846x.2.2020.222640,,,
7399,10.46966/ijae.v1i1.23,,,
7400,10.1016/j.tsc.2020.100637,,,
7401,10.1080/15332985.2022.2055439,,,
7402,10.1007/s11528-018-0356-8,,,
7403,10.1037/rev0000153,,,
7404,10.1145/3445815.3445863,,,
7405,10.1155/2021/8812542,,,
7406,10.21037/atm-2022-50,,,
7407,10.3389/fmed.2019.00034,,,
7408,10.1148/radiol.223312,,,
7409,10.1371/journal.pdig.0000022,,,
7410,10.20944/preprints202003.0141.v1,,,
7411,10.1007/978-3-030-20055-8_22,,,
7412,10.1523/jneurosci.0508-17.2018,,,
7413,10.1109/tai.2023.3266418,,,
7414,10.3390/s22208068,,,
7415,10.1016/j.heliyon.2023.e16110,,,
7416,10.1109/access.2019.2949286,,,
7417,10.6028/nist.ir.8312-draft,,,
7418,10.1007/s43681-022-00142-y,,,
7419,10.4103/jfmpc.jfmpc_155_19,,,
7420,10.3390/app11115088,,,
7421,10.1186/s12911-020-01332-6,,,
7422,10.1016/j.artint.2021.103498,,,
7423,10.1007/978-3-540-92916-1,,,
7424,10.1007/978-1-4419-9863-7,,,
7425,10.1002/9781119193210,,,
7426,10.1016/j.ins.2019.01.033,,,
7427,10.1016/s0165-0114(97)00077-8,,,
7428,10.1007/s13042-020-01230-3,,,
7429,10.1109/tcss.2022.3221933,,,
7430,10.1007/s15004-023-9905-1,,,
7431,10.1177/2053951719860542,,,
7432,10.1007/978-3-030-57321-8_12,,,
7433,10.1017/dap.2023.8,,,
7434,10.1007/s12243-022-00926-7,,,
7435,10.1016/j.patrec.2021.06.030,,,
7436,10.1016/j.inffus.2023.03.008,,,
7437,10.1109/eurospw51379.2020.00045,,,
7438,10.1109/ijcnn48605.2020.9206780,,,
7439,10.1007/s00146-020-01085-w,,,
7440,10.1038/s42256-020-0216-z,,,
7441,10.1016/j.inffus.2021.07.016,,,
7442,10.1093/jamia/ocab127,,,
7443,10.1145/1273496.1273556,,,
7444,10.1016/j.media.2022.102470,,,
7445,10.3390/s23020634,,,
7446,10.1016/j.future.2020.10.030,,,
7447,10.1038/s41746-023-00751-9,,,
7448,10.3233/ds-190020,,,
7449,10.3386/w23285,,,
7450,10.1257/aer.20160696,,,
7451,10.3917/rel.681.0148,,,
7452,10.1016/j.technovation.2023.102846,,,
7453,10.1287/mnsc.1110.1460,,,
7454,10.3386/w31325,,,
7455,10.2139/ssrn.3651052,,,
7456,10.1016/j.telpol.2020.101977,,,
7457,10.1016/j.respol.2019.03.010,,,
7458,10.1002/job.2735,,,
7459,10.1016/s0048-7333(03)00055-6,,,
7460,10.1016/s0014-2921(98)00036-1,,,
7461,10.2139/ssrn.4411051,,,
7462,10.1177/00197939221137822,,,
7463,10.1093/oxrep/grab012,,,
7464,10.1016/j.jebo.2023.05.008,,,
7465,10.2307/2937936,,,
7466,10.1111/j.1742-7363.2010.00143.x,,,
7467,10.3389/frai.2022.869282,,,
7468,10.1016/j.techfore.2021.121381,,,
7469,10.3389/frai.2022.832736,,,
7470,10.2307/3440529,,,
7471,10.1186/s12651-022-00319-2,,,
7472,10.2307/2555965,,,
7473,10.1007/s00191-019-00636-9,,,
7474,10.1037/a0038510,,,
7475,10.1007/978-3-030-92916-9_4,,,
7476,10.1111/obes.12312,,,
7477,10.1016/j.technovation.2022.102590,,,
7478,10.1177/00221856221129639,,,
7479,10.1016/j.tele.2021.101672,,,
7480,10.2307/1911700,,,
7481,10.1111/ecin.12412,,,
7482,10.1016/j.technovation.2023.102764,,,
7483,10.1080/10438599.2022.2051020,,,
7484,10.1146/annurev-psych-120710-100452,,,
7485,10.1086/209833,,,
7486,10.2753/jei0021-3624480106,,,
7487,10.1177/1350508420978831,,,
7488,10.2307/2950501,,,
7489,10.1016/j.econmod.2021.01.009,,,
7490,10.1016/j.respol.2022.104536,,,
7491,10.1162/003355398555847,,,
7492,10.1016/s0142-694x(99)00011-3,,,
7493,10.1016/s0142-694x(00)00022-3,,,
7494,10.1016/j.jobe.2023.108014,,,
7495,10.1016/j.engappai.2021.104460,,,
7496,10.1145/3607822.3618018,,,
7497,10.52842/conf.acadia.2018.156,,,
7498,10.3390/app10207299,,,
7499,10.17289/jkscs.35.4.202111.28,,,
7500,10.1002/col.20047,,,
7501,10.1007/978-3-642-27851-8_70-13,,,
7502,10.52842/conf.caadria.2021.1.061,,,
7503,10.1016/j.jag.2021.102942,,,
7504,10.1109/tpami.2017.2723009,,,
7505,10.1111/jocn.14081,,,
7506,10.4103/ijo.ijo_1292_18,,,
7507,10.1046/j.1365-2648.2001.01768.x,,,
7508,10.1177/1049732315586550,,,
7509,10.1016/s0166-2236(96)20049-9,,,
7510,10.1016/j.mnl.2018.07.015,,,
7511,10.5964/ejop.v14i4.1823,,,
7512,10.1111/jan.13105,,,
7513,10.1075/is.18.2.02etz,,,
7514,10.1046/j.1365-2648.1999.01192.x,,,
7515,10.1111/jnu.12081,,,
7516,10.1177/0969733014549881,,,
7517,10.1177/1460458213509806,,,
7518,10.20849/ijsn.v3i2.456,,,
7519,10.4135/9781526451293,,,
7520,10.1097/00012272-199009000-00002,,,
7521,10.1007/s10728-010-0163-7,,,
7522,10.1075/is.11.2.01sha,,,
7523,10.1177/0969733010388924,,,
7524,10.1145/3320254.3320263,,,
7525,10.1016/j.gloenvcha.2021.102243,,,
7526,10.1111/1539-6924.00373,,,
7527,10.1136/bmj.b3669,,,
7528,10.1016/j.puhe.2006.01.002,,,
7529,10.1098/rsos.190876,,,
7530,10.1111/j.1539-6924.2005.00692.x,,,
7531,10.18653/v1/2020.acl-demos.1,,,
7532,10.1080/03623319.2022.2027163,,,
7533,10.1080/10447318.2022.2089813,,,
7534,10.1080/00224545.1976.9924774,,,
7535,10.1037/0022-3514.37.5.715,,,
7536,10.1037/0022-3514.73.3.481,,,
7537,10.1177/00187208211029444,,,
7538,10.1016/j.jbusres.2020.12.012,,,
7539,10.1177/2333721420985975,,,
7540,10.1518/001872099779656680,,,
7541,10.2466/pr0.94.3c.1283-1292,,,
7542,10.1145/1124772.1124945,,,
7543,10.1109/roman.2009.5326352,,,
7544,10.1016/j.gloenvcha.2018.03.002,,,
7545,10.1016/j.jesp.2022.104422,,,
7546,10.1080/08934215.2020.1799049,,,
7547,10.1007/s10584-012-0424-6,,,
7548,10.1080/02650487.2015.1101908,,,
7549,10.1016/j.resconrec.2018.05.012,,,
7550,10.1016/j.scitotenv.2020.144782,,,
7551,10.1177/014616702237586,,,
7552,10.1016/j.tourman.2018.12.006,,,
7553,10.1037/0022-3514.88.3.532,,,
7554,10.5465/ambpp.2018.249,,,
7555,10.1002/job.4030130202,,,
7556,10.1145/191666.191703,,,
7557,10.4324/9780203135457,,,
7558,10.30658/hmc.1.5,,,
7559,10.1145/3025453.3025539,,,
7560,10.1016/j.chb.2020.106585,,,
7561,10.1080/02650487.2021.1982529,,,
7562,10.3389/fpsyg.2020.01275,,,
7563,10.1016/j.chb.2020.106278,,,
7564,10.1016/s0304-3940(03)00027-2,,,
7565,10.1121/1.413832,,,
7566,10.1016/s0167-6393(97)00059-9,,,
7567,10.1037/1076-898x.7.3.171,,,
7568,10.1111/j.1467-9221.2005.00428.x,,,
7569,10.1093/hcr/hqy011,,,
7570,10.1111/jabr.12038,,,
7571,10.3389/fnins.2015.00055,,,
7572,10.1037/1528-3542.8.4.494,,,
7573,10.1145/3342775.3342806,,,
7574,10.1016/0022-1031(71)90070-9,,,
7575,10.1289/ehp.8141291,,,
7576,10.21437/interspeech.2018-1093,,,
7577,10.1007/978-3-319-10816-2_72,,,
7578,10.1145/3487983.3488296,,,
7579,10.1080/026999300402763,,,
7580,10.1037/0022-3514.81.1.146,,,
7581,10.1016/s1057-7408(07)70023-2,,,
7582,10.1111/j.1559-1816.2001.tb01412.x,,,
7583,10.1080/10410230701808327,,,
7584,10.1080/01292986.2020.1784967,,,
7585,10.1080/00223980.1975.9915803,,,
7586,10.1111/1467-9280.01433,,,
7587,10.1037/h0100799,,,
7588,10.1177/1075547008329201,,,
7589,10.1007/s10584-021-02975-8,,,
7590,10.1016/j.jsr.2013.01.009,,,
7591,10.1080/02699931.2020.1731428,,,
7592,10.1016/j.jenvp.2004.12.004,,,
7593,10.1016/j.jenvman.2020.110806,,,
7594,10.1016/j.jenvp.2020.101410,,,
7595,10.1080/10807039.2018.1471340,,,
7596,10.1002/jtr.2336,,,
7597,10.1080/13683500.2022.2044291,,,
7598,10.1080/09669582.2021.1949016,,,
7599,10.1016/j.energy.2022.125192,,,
7600,10.1080/25742442.2021.2007718,,,
7601,10.2196/35744,,,
7602,10.1080/02650487.2020.1763090,,,
7603,10.1037/0022-3514.63.4.596,,,
7604,10.1002/mar.20041,,,
7605,10.1007/s10584-019-02609-0,,,
7606,10.1111/1467-8500.12504,,,
7607,10.2190/u43p-9qlx-hj5p-u2j5,,,
7608,10.1023/a:1022382413579,,,
7609,10.1111/j.1460-2466.2004.tb02611.x,,,
7610,10.1111/j.1468-2958.1993.tb00311.x,,,
7611,10.1016/j.ipm.2022.103074,,,
7612,10.1080/08824096.2022.2045929,,,
7613,10.1080/08824096.2018.1447454,,,
7614,10.1016/j.chb.2013.01.021,,,
7615,10.1037/0022-3514.95.3.679,,,
7616,10.1002/ejsp.356,,,
7617,10.1007/978-3-030-29516-5_76,,,
7618,10.1109/icassp.2012.6289140,,,
7619,10.1109/roman.2012.6343795,,,
7620,10.1109/iros.2009.5354116,,,
7621,10.1016/j.jbusres.2021.12.007,,,
7622,10.1016/j.chb.2010.09.002,,,
7623,10.30658/jicrcr.5.1.2,,,
7624,10.3389/frvir.2021.739038,,,
7625,10.1177/1075547018776019,,,
7626,10.1111/spc3.12265,,,
7627,10.1177/0013916517710685,,,
7628,10.1016/j.ecolecon.2017.04.017,,,
7629,10.1016/j.jclepro.2020.122305,,,
7630,10.1057/978-1-137-58449-6_1,,,
7631,10.7551/mitpress/9780262035064.001.0001,,,
7632,10.4324/9781315453255,,,
7633,10.1097/acm.0000000000004963,,,
7634,10.1038/s41746-020-0294-7,,,
7635,10.1080/10401334.2012.741536,,,
7636,10.1111/medu.12878,,,
7637,10.1007/s40670-021-01409-5,,,
7638,10.1038/s41591-018-0335-9,,,
7639,10.1056/nejmra054784,,,
7640,10.3109/0142159x.2012.652239,,,
7641,10.1089/end.2018.0035,,,
7642,10.1007/s10639-022-10927-7,,,
7643,10.1001/jama.2023.1344,,,
7644,10.1007/s10459-020-10009-8,,,
7645,10.1109/jas.2023.123618,,,
